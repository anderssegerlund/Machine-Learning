{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing done.\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import edec.afterprocessing as ap\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_labels(label=1, db=\"df.db\",n_random=10):\n",
    "        \"\"\"\n",
    "        This function import all cars from sql with stated label 0/1\n",
    "        Example: Car \"abc\" and \"bcd\" has at some point been labeled with a Cell fail 1\n",
    "\n",
    "        tin_an  Fail\n",
    "        \"abc\"   0\n",
    "        \"abc\"   1\n",
    "        \"abc\"   0\n",
    "        \"bcd\"   0\n",
    "        \"bcd\"   0\n",
    "        \"bcd\"   1\n",
    "        \"\"\"\n",
    "        # Select all cell fails\n",
    "        # limit the result to only x for non fail cars\n",
    "        #if label == 0:\n",
    "        #        lim = \"LIMIT 3\"\n",
    "        #else:\n",
    "        #        lim = \"\"\n",
    "\n",
    "        ## SELECT ALL UNIQUE CARS WITH 0 / 1\n",
    "        \"\"\"\n",
    "        First sub query select lim random number of tin numbers with label \n",
    "        Seccond query select timestamp, sorted soc etc from the sub_query\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        #if label == 0:\n",
    "                #n_random = 100\n",
    "        #if label == 1:\n",
    "               # n_random = 1000\n",
    "                \n",
    "        sub_query  = f\"SELECT DISTINCT(main_table.tin_an) FROM main_table \\\n",
    "                        INNER JOIN cell_fail \\\n",
    "                        ON main_table.rid = cell_fail.rid \\\n",
    "                        AND cell_fail.Fail = {label} \\\n",
    "                        ORDER BY RANDOM() \\\n",
    "                         LIMIT {n_random}\"\n",
    "\n",
    "        query = f\"SELECT main_table.tin_an, [timestamp], [Sorted_SOC], cell_fail.Fail FROM main_table \\\n",
    "                INNER JOIN cell_fail \\\n",
    "                ON main_table.rid = cell_fail.rid \\\n",
    "                WHERE [tin_an] in ({sub_query})\"\n",
    "\n",
    "        df = ap.load_sql(query , db=db)\n",
    "\n",
    "        # Change to Timestamp format\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "        # Sort by timestamp\n",
    "        df = df.sort_values(by=[\"timestamp\"],ascending=True)\n",
    "\n",
    "        # Print import result\n",
    "        n_unique = len(df[\"tin_an\"].unique())\n",
    "        print(f\"{n_unique} Unique cars with label {label} has been imported with {len(df)} subsamples\")\n",
    "        return df\n",
    "\n",
    "def sample_to_tensor_x(df_tensor, sub_samples=3):\n",
    "    x = torch.zeros(sub_samples,108) # 20 as we use 20 subsample cars\n",
    "    i_from = len(df_tensor) - sub_samples\n",
    "    i_to =  len(df_tensor)\n",
    "\n",
    "    for tensor_i, df_i in enumerate(reversed(range(i_from, i_to))):\n",
    "        carcell_voltage = df_tensor[\"Sorted_SOC\"].iloc[df_i]     \n",
    "        x[tensor_i] = torch.FloatTensor(carcell_voltage)\n",
    "    return x\n",
    "\n",
    "\n",
    "def create_dataset(db=\"df.db\", parameters=None, normalize_data=False):\n",
    "    # Output\n",
    "    data = []\n",
    "    run_details = {\"Fail types loaded\":0, \n",
    "                    \"Samples removed\":{\n",
    "                                        0:0, \n",
    "                                        1:0,\n",
    "                                        },\n",
    "                    \"Dropped tin\":{\n",
    "                                    0:[],\n",
    "                                    1:[]},\n",
    "                                    }\n",
    "\n",
    "        \n",
    "    # Paramters\n",
    "    roll = parameters[\"roll\"]\n",
    "    sub_sample = parameters[\"Sub sample\"]\n",
    "    drop_sample = parameters[\"drop_sample\"]+1 #Definierar 0 som ingen drop, men måste lägga till 1 pga att loopen f\n",
    "\n",
    "    for fail in range(2): ##################################################################### ÄNDRA TILL 2\n",
    "        run_details[\"Fail types loaded\"] +=1\n",
    "        print(f\"Run {fail} started\")\n",
    "        # Create df with 0 and 1\n",
    "        df = import_labels(label=fail, db=db, n_random = parameters[\"nRandom samples\"][fail])\n",
    "        cars = df[\"tin_an\"].unique()\n",
    "        print(f\"{len(cars)} Cars in df\")\n",
    "        for i_car in range(len(cars)): # Change to 2\n",
    "\n",
    "            # Get unique tin_number for fail / healthy\n",
    "            df_tin = df[df[\"tin_an\"] == cars[i_car]]\n",
    "            df_tin = df_tin.reset_index()\n",
    "            if fail == 1:\n",
    "                max_index  = df_tin[df_tin[\"Fail\"] == 1].index.values\n",
    "                last_idx = max_index[-1]\n",
    "                df_tensor = df_tin[:last_idx+1]\n",
    "            else:\n",
    "                last_idx = len(df_tin) +1 #-1\n",
    "                df_tensor = df_tin[:last_idx+1]\n",
    "\n",
    "\n",
    "            #for roll in range(1,5):\n",
    "                #display(df_tensor[-roll-sub_sample:-roll])\n",
    "\n",
    "            ### Create rolling\n",
    "            # 1 should be standard\n",
    "            last_idx_temp = last_idx\n",
    "            for r in range(1,roll+1):\n",
    "                #df_tensor_short = df_tensor[last_idx_temp-(parameters[\"Sub sample\"]):last_idx_temp+1]\n",
    "                df_tensor_short = df_tensor[-r-sub_sample:-r]\n",
    "                last_idx_temp -= 1\n",
    "\n",
    "                #if len(df_tensor_short) < parameters[\"Sub sample\"]:\n",
    "                    #   min_length = parameters[\"Sub sample\"]\n",
    "                    #   print(f\"Skipping df. df_tensor length {len(df_tensor_short)}. (min lentht {min_length})\")\n",
    "\n",
    "                # Create database\n",
    "                if (len(df_tensor_short) >= (sub_sample)) & (r >= (drop_sample)):\n",
    "                #if (len(df_tensor_short) >= (sub_sample)) & (r > (drop_sample)):\n",
    "                #if (len(df_tensor_short) >= (sub_sample)):\n",
    "                    x = sample_to_tensor_x(df_tensor_short, sub_samples=sub_sample)\n",
    "                    if normalize_data:\n",
    "                        #x = normalize(x, p=2, dim = 1) # 1 or 0 \n",
    "                        # Normalize by xi - min(x) / (max(x) - min(x))\n",
    "                        min_i = x.min().item()\n",
    "                        max_i = x.max().item()\n",
    "                        x = (x-min_i)/(max_i - min_i)\n",
    "                    y = fail\n",
    "                    data.append((x,y))\n",
    "                if (len(df_tensor_short) < (sub_sample)):\n",
    "                    run_details[\"Samples removed\"][fail] +=1\n",
    "                    run_details[\"Dropped tin\"][fail].append(df_tensor[\"tin_an\"].unique()[0])\n",
    "                    #continue\n",
    "    print(run_details)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#db = \"df_10.db\"\n",
    "db = \"df.db\"\n",
    "\n",
    "# Annotate data (ONLY RUN IF NEW ANNOTATION IS REQUIRED)\n",
    "#ap.annotate_db(fail_type=\"SOC\", db=db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tin_an</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fail</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tin_an\n",
       "Fail        \n",
       "0      18575\n",
       "1        369"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = f\"SELECT DISTINCT(main_table.tin_an), cell_fail.Fail FROM main_table \\\n",
    "        INNER JOIN cell_fail \\\n",
    "        ON main_table.rid = cell_fail.rid\"\n",
    "\n",
    "df_count = ap.load_sql(query , db=db)\n",
    "df_count.groupby(\"Fail\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select parameters for data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Paramater values:\n",
    "\n",
    "Sub_sample: How mmany timesamples backwards should be included (0 only give 1 sample, i.e [1,108])\n",
    "drop_sample: How many samples should be dropped (0 for no drop). DROP SAMPLE MUST BE LARGER THAN ROLL\n",
    "roll: How many timesteps back should we roll (1 for no rolling) \n",
    "\"\"\"\n",
    "parameters = {\"nRandom samples\":{\n",
    "                                0 : 20,\n",
    "                                1 : 20,\n",
    "                                },\n",
    "            \"Sub sample\":20,\n",
    "            \"drop_sample\":0, # Default 0\n",
    "            \"roll\":5         # Default 1 (Must be larger or equal to subsample)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0 started\n",
      "20 Unique cars with label 0 has been imported with 3067 subsamples\n",
      "20 Cars in df\n",
      "Run 1 started\n",
      "20 Unique cars with label 1 has been imported with 3634 subsamples\n",
      "20 Cars in df\n",
      "{'Fail types loaded': 2, 'Samples removed': {0: 6, 1: 34}, 'Dropped tin': {0: ['3cc368a9a91a8ff45b31de9f707b1d210e9d6f70a451dfc2da2e038e3ad287ec', '3cc368a9a91a8ff45b31de9f707b1d210e9d6f70a451dfc2da2e038e3ad287ec', '3cc368a9a91a8ff45b31de9f707b1d210e9d6f70a451dfc2da2e038e3ad287ec', '3cc368a9a91a8ff45b31de9f707b1d210e9d6f70a451dfc2da2e038e3ad287ec', '3cc368a9a91a8ff45b31de9f707b1d210e9d6f70a451dfc2da2e038e3ad287ec', '3f55d2d21e9fd9e921101fb3d82fd8fdbefc6128d55ed894e226e12878626c66'], 1: ['36a48fe41b39d708172a7e612a2aa76ef3ef389dfbea33ac03708c75a936c330', '36a48fe41b39d708172a7e612a2aa76ef3ef389dfbea33ac03708c75a936c330', '36a48fe41b39d708172a7e612a2aa76ef3ef389dfbea33ac03708c75a936c330', '36a48fe41b39d708172a7e612a2aa76ef3ef389dfbea33ac03708c75a936c330', '36a48fe41b39d708172a7e612a2aa76ef3ef389dfbea33ac03708c75a936c330', '8b4d45c1f8182c83bb1c63d0f1fe862f1be94cf56c540b55444f281fe0702ada', '8b4d45c1f8182c83bb1c63d0f1fe862f1be94cf56c540b55444f281fe0702ada', '8b4d45c1f8182c83bb1c63d0f1fe862f1be94cf56c540b55444f281fe0702ada', '8b4d45c1f8182c83bb1c63d0f1fe862f1be94cf56c540b55444f281fe0702ada', '8b4d45c1f8182c83bb1c63d0f1fe862f1be94cf56c540b55444f281fe0702ada', '3199cf88bbafd500462ca41324bd4ba2c2209c66ec78f8ef4e02b99f6a79e047', '3199cf88bbafd500462ca41324bd4ba2c2209c66ec78f8ef4e02b99f6a79e047', '3199cf88bbafd500462ca41324bd4ba2c2209c66ec78f8ef4e02b99f6a79e047', '3199cf88bbafd500462ca41324bd4ba2c2209c66ec78f8ef4e02b99f6a79e047', '3199cf88bbafd500462ca41324bd4ba2c2209c66ec78f8ef4e02b99f6a79e047', '7807d664d8fa58bf0f2ce66a4eca2b176f365eea5a4e0cfe03ca12defb097d94', '7807d664d8fa58bf0f2ce66a4eca2b176f365eea5a4e0cfe03ca12defb097d94', '7807d664d8fa58bf0f2ce66a4eca2b176f365eea5a4e0cfe03ca12defb097d94', '7807d664d8fa58bf0f2ce66a4eca2b176f365eea5a4e0cfe03ca12defb097d94', '2add7f96f14074dcde2027480de42e8a85c96aaecf97ee057aa605318a4742fc', '2add7f96f14074dcde2027480de42e8a85c96aaecf97ee057aa605318a4742fc', '2add7f96f14074dcde2027480de42e8a85c96aaecf97ee057aa605318a4742fc', '2add7f96f14074dcde2027480de42e8a85c96aaecf97ee057aa605318a4742fc', '2add7f96f14074dcde2027480de42e8a85c96aaecf97ee057aa605318a4742fc', '7048ea3ed205559992e9eaa9147020ba50ab9e7e349268f22555f90a6757d6bb', '7048ea3ed205559992e9eaa9147020ba50ab9e7e349268f22555f90a6757d6bb', '7048ea3ed205559992e9eaa9147020ba50ab9e7e349268f22555f90a6757d6bb', '7048ea3ed205559992e9eaa9147020ba50ab9e7e349268f22555f90a6757d6bb', '7048ea3ed205559992e9eaa9147020ba50ab9e7e349268f22555f90a6757d6bb', 'a2ab8f77a76a53afaecfb2c96e0cbe6cbf218726fb4f57127c2a0c51889cd4fa', 'a2ab8f77a76a53afaecfb2c96e0cbe6cbf218726fb4f57127c2a0c51889cd4fa', 'a2ab8f77a76a53afaecfb2c96e0cbe6cbf218726fb4f57127c2a0c51889cd4fa', 'a2ab8f77a76a53afaecfb2c96e0cbe6cbf218726fb4f57127c2a0c51889cd4fa', 'a2ab8f77a76a53afaecfb2c96e0cbe6cbf218726fb4f57127c2a0c51889cd4fa']}}\n"
     ]
    }
   ],
   "source": [
    "data =create_dataset(db=\"df.db\", parameters=parameters, normalize_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([[0.5816, 0.5791, 0.5805,  ..., 0.5860, 0.5857, 0.5850],\n",
       "          [0.8202, 0.8176, 0.8189,  ..., 0.8238, 0.8237, 0.8228],\n",
       "          [0.5412, 0.5381, 0.5399,  ..., 0.5456, 0.5451, 0.5443],\n",
       "          ...,\n",
       "          [0.3831, 0.3807, 0.3825,  ..., 0.3906, 0.3892, 0.3880],\n",
       "          [0.4478, 0.4438, 0.4459,  ..., 0.4509, 0.4496, 0.4492],\n",
       "          [0.5356, 0.5310, 0.5333,  ..., 0.5393, 0.5381, 0.5376]]),\n",
       "  0),\n",
       " (tensor([[0.8202, 0.8176, 0.8189,  ..., 0.8238, 0.8237, 0.8228],\n",
       "          [0.5412, 0.5381, 0.5399,  ..., 0.5456, 0.5451, 0.5443],\n",
       "          [0.7267, 0.7239, 0.7254,  ..., 0.7309, 0.7304, 0.7299],\n",
       "          ...,\n",
       "          [0.4478, 0.4438, 0.4459,  ..., 0.4509, 0.4496, 0.4492],\n",
       "          [0.5356, 0.5310, 0.5333,  ..., 0.5393, 0.5381, 0.5376],\n",
       "          [0.9231, 0.9207, 0.9219,  ..., 0.9272, 0.9271, 0.9263]]),\n",
       "  0),\n",
       " (tensor([[0.5412, 0.5381, 0.5399,  ..., 0.5456, 0.5451, 0.5443],\n",
       "          [0.7267, 0.7239, 0.7254,  ..., 0.7309, 0.7304, 0.7299],\n",
       "          [0.8644, 0.8619, 0.8630,  ..., 0.8680, 0.8679, 0.8671],\n",
       "          ...,\n",
       "          [0.5356, 0.5310, 0.5333,  ..., 0.5393, 0.5381, 0.5376],\n",
       "          [0.9231, 0.9207, 0.9219,  ..., 0.9272, 0.9271, 0.9263],\n",
       "          [0.9938, 0.9916, 0.9924,  ..., 0.9968, 0.9967, 0.9962]]),\n",
       "  0),\n",
       " (tensor([[0.7267, 0.7239, 0.7254,  ..., 0.7309, 0.7304, 0.7299],\n",
       "          [0.8644, 0.8619, 0.8630,  ..., 0.8680, 0.8679, 0.8671],\n",
       "          [0.5882, 0.5855, 0.5872,  ..., 0.5924, 0.5920, 0.5914],\n",
       "          ...,\n",
       "          [0.9231, 0.9207, 0.9219,  ..., 0.9272, 0.9271, 0.9263],\n",
       "          [0.9938, 0.9916, 0.9924,  ..., 0.9968, 0.9967, 0.9962],\n",
       "          [0.7697, 0.7670, 0.7684,  ..., 0.7731, 0.7727, 0.7721]]),\n",
       "  0),\n",
       " (tensor([[0.8644, 0.8619, 0.8630,  ..., 0.8680, 0.8679, 0.8671],\n",
       "          [0.5882, 0.5855, 0.5872,  ..., 0.5924, 0.5920, 0.5914],\n",
       "          [0.7502, 0.7474, 0.7486,  ..., 0.7540, 0.7536, 0.7531],\n",
       "          ...,\n",
       "          [0.9938, 0.9916, 0.9924,  ..., 0.9968, 0.9967, 0.9962],\n",
       "          [0.7697, 0.7670, 0.7684,  ..., 0.7731, 0.7727, 0.7721],\n",
       "          [0.4323, 0.4287, 0.4306,  ..., 0.4361, 0.4353, 0.4347]]),\n",
       "  0),\n",
       " (tensor([[0.0810, 0.0808, 0.0662,  ..., 0.0862, 0.0710, 0.0749],\n",
       "          [0.2949, 0.2953, 0.2804,  ..., 0.3020, 0.2864, 0.2901],\n",
       "          [0.6646, 0.6666, 0.6536,  ..., 0.6720, 0.6599, 0.6616],\n",
       "          ...,\n",
       "          [0.7160, 0.7179, 0.7053,  ..., 0.7229, 0.7110, 0.7123],\n",
       "          [0.7346, 0.7365, 0.7242,  ..., 0.7415, 0.7296, 0.7309],\n",
       "          [0.7439, 0.7463, 0.7335,  ..., 0.7512, 0.7393, 0.7406]]),\n",
       "  0),\n",
       " (tensor([[0.2949, 0.2953, 0.2804,  ..., 0.3020, 0.2864, 0.2901],\n",
       "          [0.6646, 0.6666, 0.6536,  ..., 0.6720, 0.6599, 0.6616],\n",
       "          [0.2042, 0.2039, 0.1858,  ..., 0.2111, 0.1929, 0.1979],\n",
       "          ...,\n",
       "          [0.7346, 0.7365, 0.7242,  ..., 0.7415, 0.7296, 0.7309],\n",
       "          [0.7439, 0.7463, 0.7335,  ..., 0.7512, 0.7393, 0.7406],\n",
       "          [0.7640, 0.7662, 0.7534,  ..., 0.7712, 0.7595, 0.7608]]),\n",
       "  0),\n",
       " (tensor([[0.6646, 0.6666, 0.6536,  ..., 0.6720, 0.6599, 0.6616],\n",
       "          [0.2042, 0.2039, 0.1858,  ..., 0.2111, 0.1929, 0.1979],\n",
       "          [0.5231, 0.5248, 0.5118,  ..., 0.5306, 0.5179, 0.5198],\n",
       "          ...,\n",
       "          [0.7439, 0.7463, 0.7335,  ..., 0.7512, 0.7393, 0.7406],\n",
       "          [0.7640, 0.7662, 0.7534,  ..., 0.7712, 0.7595, 0.7608],\n",
       "          [0.7720, 0.7742, 0.7614,  ..., 0.7792, 0.7675, 0.7688]]),\n",
       "  0),\n",
       " (tensor([[0.2042, 0.2039, 0.1858,  ..., 0.2111, 0.1929, 0.1979],\n",
       "          [0.5231, 0.5248, 0.5118,  ..., 0.5306, 0.5179, 0.5198],\n",
       "          [0.3263, 0.3271, 0.3124,  ..., 0.3330, 0.3185, 0.3219],\n",
       "          ...,\n",
       "          [0.7640, 0.7662, 0.7534,  ..., 0.7712, 0.7595, 0.7608],\n",
       "          [0.7720, 0.7742, 0.7614,  ..., 0.7792, 0.7675, 0.7688],\n",
       "          [0.7777, 0.7800, 0.7673,  ..., 0.7850, 0.7731, 0.7744]]),\n",
       "  0),\n",
       " (tensor([[0.5231, 0.5248, 0.5118,  ..., 0.5306, 0.5179, 0.5198],\n",
       "          [0.3263, 0.3271, 0.3124,  ..., 0.3330, 0.3185, 0.3219],\n",
       "          [0.4601, 0.4616, 0.4484,  ..., 0.4676, 0.4538, 0.4564],\n",
       "          ...,\n",
       "          [0.7720, 0.7742, 0.7614,  ..., 0.7792, 0.7675, 0.7688],\n",
       "          [0.7777, 0.7800, 0.7673,  ..., 0.7850, 0.7731, 0.7744],\n",
       "          [0.7893, 0.7917, 0.7790,  ..., 0.7967, 0.7848, 0.7861]]),\n",
       "  0),\n",
       " (tensor([[0.8189, 0.8204, 0.8198,  ..., 0.8254, 0.8266, 0.8279],\n",
       "          [0.8241, 0.8256, 0.8251,  ..., 0.8308, 0.8319, 0.8333],\n",
       "          [0.8296, 0.8313, 0.8308,  ..., 0.8363, 0.8378, 0.8390],\n",
       "          ...,\n",
       "          [0.0015, 0.0033, 0.0017,  ..., 0.0083, 0.0075, 0.0087],\n",
       "          [0.5292, 0.5304, 0.5294,  ..., 0.5359, 0.5357, 0.5369],\n",
       "          [0.8640, 0.8660, 0.8653,  ..., 0.8705, 0.8718, 0.8735]]),\n",
       "  0),\n",
       " (tensor([[0.8241, 0.8256, 0.8251,  ..., 0.8308, 0.8319, 0.8333],\n",
       "          [0.8296, 0.8313, 0.8308,  ..., 0.8363, 0.8378, 0.8390],\n",
       "          [0.8279, 0.8298, 0.8294,  ..., 0.8344, 0.8358, 0.8371],\n",
       "          ...,\n",
       "          [0.5292, 0.5304, 0.5294,  ..., 0.5359, 0.5357, 0.5369],\n",
       "          [0.8640, 0.8660, 0.8653,  ..., 0.8705, 0.8718, 0.8735],\n",
       "          [0.9181, 0.9204, 0.9196,  ..., 0.9249, 0.9264, 0.9277]]),\n",
       "  0),\n",
       " (tensor([[0.8296, 0.8313, 0.8308,  ..., 0.8363, 0.8378, 0.8390],\n",
       "          [0.8279, 0.8298, 0.8294,  ..., 0.8344, 0.8358, 0.8371],\n",
       "          [0.8279, 0.8296, 0.8289,  ..., 0.8343, 0.8358, 0.8369],\n",
       "          ...,\n",
       "          [0.8640, 0.8660, 0.8653,  ..., 0.8705, 0.8718, 0.8735],\n",
       "          [0.9181, 0.9204, 0.9196,  ..., 0.9249, 0.9264, 0.9277],\n",
       "          [0.8583, 0.8600, 0.8596,  ..., 0.8650, 0.8667, 0.8680]]),\n",
       "  0),\n",
       " (tensor([[0.8279, 0.8298, 0.8294,  ..., 0.8344, 0.8358, 0.8371],\n",
       "          [0.8279, 0.8296, 0.8289,  ..., 0.8343, 0.8358, 0.8369],\n",
       "          [0.5946, 0.5958, 0.5950,  ..., 0.6010, 0.6015, 0.6031],\n",
       "          ...,\n",
       "          [0.9181, 0.9204, 0.9196,  ..., 0.9249, 0.9264, 0.9277],\n",
       "          [0.8583, 0.8600, 0.8596,  ..., 0.8650, 0.8667, 0.8680],\n",
       "          [0.8920, 0.8942, 0.8937,  ..., 0.8990, 0.9005, 0.9019]]),\n",
       "  0),\n",
       " (tensor([[0.8279, 0.8296, 0.8289,  ..., 0.8343, 0.8358, 0.8369],\n",
       "          [0.5946, 0.5958, 0.5950,  ..., 0.6010, 0.6015, 0.6031],\n",
       "          [0.6941, 0.6956, 0.6949,  ..., 0.7004, 0.7013, 0.7028],\n",
       "          ...,\n",
       "          [0.8583, 0.8600, 0.8596,  ..., 0.8650, 0.8667, 0.8680],\n",
       "          [0.8920, 0.8942, 0.8937,  ..., 0.8990, 0.9005, 0.9019],\n",
       "          [0.6489, 0.6502, 0.6492,  ..., 0.6550, 0.6557, 0.6570]]),\n",
       "  0),\n",
       " (tensor([[0.2027, 0.2055, 0.2052,  ..., 0.2114, 0.2099, 0.2085],\n",
       "          [0.4546, 0.4562, 0.4576,  ..., 0.4664, 0.4633, 0.4624],\n",
       "          [0.6391, 0.6402, 0.6416,  ..., 0.6480, 0.6450, 0.6454],\n",
       "          ...,\n",
       "          [0.2822, 0.2842, 0.2850,  ..., 0.2901, 0.2887, 0.2875],\n",
       "          [0.4510, 0.4541, 0.4546,  ..., 0.4614, 0.4589, 0.4589],\n",
       "          [0.3293, 0.3321, 0.3336,  ..., 0.3407, 0.3397, 0.3371]]),\n",
       "  0),\n",
       " (tensor([[0.4546, 0.4562, 0.4576,  ..., 0.4664, 0.4633, 0.4624],\n",
       "          [0.6391, 0.6402, 0.6416,  ..., 0.6480, 0.6450, 0.6454],\n",
       "          [0.0778, 0.0797, 0.0797,  ..., 0.0842, 0.0837, 0.0821],\n",
       "          ...,\n",
       "          [0.4510, 0.4541, 0.4546,  ..., 0.4614, 0.4589, 0.4589],\n",
       "          [0.3293, 0.3321, 0.3336,  ..., 0.3407, 0.3397, 0.3371],\n",
       "          [0.8754, 0.8759, 0.8769,  ..., 0.8860, 0.8825, 0.8827]]),\n",
       "  0),\n",
       " (tensor([[0.6391, 0.6402, 0.6416,  ..., 0.6480, 0.6450, 0.6454],\n",
       "          [0.0778, 0.0797, 0.0797,  ..., 0.0842, 0.0837, 0.0821],\n",
       "          [0.2956, 0.2989, 0.2989,  ..., 0.3058, 0.3048, 0.3026],\n",
       "          ...,\n",
       "          [0.3293, 0.3321, 0.3336,  ..., 0.3407, 0.3397, 0.3371],\n",
       "          [0.8754, 0.8759, 0.8769,  ..., 0.8860, 0.8825, 0.8827],\n",
       "          [0.4996, 0.5023, 0.5029,  ..., 0.5108, 0.5081, 0.5076]]),\n",
       "  0),\n",
       " (tensor([[0.0778, 0.0797, 0.0797,  ..., 0.0842, 0.0837, 0.0821],\n",
       "          [0.2956, 0.2989, 0.2989,  ..., 0.3058, 0.3048, 0.3026],\n",
       "          [0.0000, 0.0014, 0.0023,  ..., 0.0085, 0.0059, 0.0061],\n",
       "          ...,\n",
       "          [0.8754, 0.8759, 0.8769,  ..., 0.8860, 0.8825, 0.8827],\n",
       "          [0.4996, 0.5023, 0.5029,  ..., 0.5108, 0.5081, 0.5076],\n",
       "          [0.8365, 0.8374, 0.8384,  ..., 0.8467, 0.8433, 0.8433]]),\n",
       "  0),\n",
       " (tensor([[0.2919, 0.2952, 0.2952,  ..., 0.3020, 0.3010, 0.2988],\n",
       "          [0.0000, 0.0014, 0.0022,  ..., 0.0084, 0.0058, 0.0060],\n",
       "          [0.1414, 0.1431, 0.1438,  ..., 0.1520, 0.1483, 0.1488],\n",
       "          ...,\n",
       "          [0.4933, 0.4961, 0.4966,  ..., 0.5045, 0.5017, 0.5012],\n",
       "          [0.8260, 0.8269, 0.8279,  ..., 0.8361, 0.8327, 0.8327],\n",
       "          [0.9839, 0.9847, 0.9856,  ..., 0.9943, 0.9904, 0.9904]]),\n",
       "  0),\n",
       " (tensor([[0.9886, 0.9924, 0.9902,  ..., 0.9758, 0.9760, 0.9773],\n",
       "          [0.0237, 0.0305, 0.0290,  ..., 0.0146, 0.0126, 0.0146],\n",
       "          [0.3402, 0.3463, 0.3443,  ..., 0.3286, 0.3274, 0.3291],\n",
       "          ...,\n",
       "          [0.4167, 0.4220, 0.4202,  ..., 0.4048, 0.4043, 0.4056],\n",
       "          [0.5909, 0.5957, 0.5936,  ..., 0.5793, 0.5793, 0.5805],\n",
       "          [0.7029, 0.7077, 0.7055,  ..., 0.6913, 0.6913, 0.6926]]),\n",
       "  0),\n",
       " (tensor([[0.0252, 0.0324, 0.0308,  ..., 0.0155, 0.0134, 0.0155],\n",
       "          [0.3612, 0.3676, 0.3655,  ..., 0.3489, 0.3475, 0.3494],\n",
       "          [0.5871, 0.5919, 0.5900,  ..., 0.5753, 0.5750, 0.5766],\n",
       "          ...,\n",
       "          [0.6273, 0.6324, 0.6302,  ..., 0.6150, 0.6150, 0.6163],\n",
       "          [0.7462, 0.7513, 0.7489,  ..., 0.7339, 0.7339, 0.7353],\n",
       "          [0.8872, 0.8909, 0.8891,  ..., 0.8743, 0.8741, 0.8754]]),\n",
       "  0),\n",
       " (tensor([[0.4768, 0.4821, 0.4804,  ..., 0.4668, 0.4657, 0.4672],\n",
       "          [0.6618, 0.6658, 0.6643,  ..., 0.6522, 0.6520, 0.6533],\n",
       "          [0.5850, 0.5894, 0.5881,  ..., 0.5758, 0.5756, 0.5767],\n",
       "          ...,\n",
       "          [0.7922, 0.7964, 0.7944,  ..., 0.7821, 0.7821, 0.7832],\n",
       "          [0.9076, 0.9107, 0.9092,  ..., 0.8971, 0.8969, 0.8980],\n",
       "          [0.0184, 0.0263, 0.0252,  ..., 0.0110, 0.0103, 0.0125]]),\n",
       "  0),\n",
       " (tensor([[0.6618, 0.6658, 0.6643,  ..., 0.6522, 0.6520, 0.6533],\n",
       "          [0.5850, 0.5894, 0.5881,  ..., 0.5758, 0.5756, 0.5767],\n",
       "          [0.8411, 0.8449, 0.8429,  ..., 0.8310, 0.8308, 0.8328],\n",
       "          ...,\n",
       "          [0.9076, 0.9107, 0.9092,  ..., 0.8971, 0.8969, 0.8980],\n",
       "          [0.0184, 0.0263, 0.0252,  ..., 0.0110, 0.0103, 0.0125],\n",
       "          [0.1393, 0.1457, 0.1446,  ..., 0.1293, 0.1284, 0.1303]]),\n",
       "  0),\n",
       " (tensor([[0.5850, 0.5894, 0.5881,  ..., 0.5758, 0.5756, 0.5767],\n",
       "          [0.8411, 0.8449, 0.8429,  ..., 0.8310, 0.8308, 0.8328],\n",
       "          [0.9743, 0.9778, 0.9761,  ..., 0.9642, 0.9640, 0.9651],\n",
       "          ...,\n",
       "          [0.0184, 0.0263, 0.0252,  ..., 0.0110, 0.0103, 0.0125],\n",
       "          [0.1393, 0.1457, 0.1446,  ..., 0.1293, 0.1284, 0.1303],\n",
       "          [0.2622, 0.2671, 0.2660,  ..., 0.2532, 0.2517, 0.2535]]),\n",
       "  0),\n",
       " (tensor([[0.1706, 0.1714, 0.1750,  ..., 0.1771, 0.1746, 0.1750],\n",
       "          [0.5262, 0.5250, 0.5271,  ..., 0.5302, 0.5286, 0.5277],\n",
       "          [0.7346, 0.7333, 0.7346,  ..., 0.7376, 0.7360, 0.7349],\n",
       "          ...,\n",
       "          [0.4375, 0.4355, 0.4388,  ..., 0.4420, 0.4406, 0.4388],\n",
       "          [0.6551, 0.6538, 0.6550,  ..., 0.6581, 0.6566, 0.6557],\n",
       "          [0.8991, 0.8977, 0.8983,  ..., 0.9012, 0.8996, 0.8989]]),\n",
       "  0),\n",
       " (tensor([[0.5143, 0.5132, 0.5153,  ..., 0.5183, 0.5167, 0.5158],\n",
       "          [0.7181, 0.7168, 0.7181,  ..., 0.7209, 0.7194, 0.7183],\n",
       "          [0.8378, 0.8365, 0.8374,  ..., 0.8403, 0.8389, 0.8381],\n",
       "          ...,\n",
       "          [0.6404, 0.6391, 0.6402,  ..., 0.6433, 0.6418, 0.6409],\n",
       "          [0.8788, 0.8775, 0.8780,  ..., 0.8809, 0.8793, 0.8787],\n",
       "          [0.9925, 0.9914, 0.9914,  ..., 0.9933, 0.9919, 0.9911]]),\n",
       "  0),\n",
       " (tensor([[0.7181, 0.7168, 0.7181,  ..., 0.7209, 0.7194, 0.7183],\n",
       "          [0.8378, 0.8365, 0.8374,  ..., 0.8403, 0.8389, 0.8381],\n",
       "          [0.2266, 0.2266, 0.2308,  ..., 0.2346, 0.2321, 0.2308],\n",
       "          ...,\n",
       "          [0.8788, 0.8775, 0.8780,  ..., 0.8809, 0.8793, 0.8787],\n",
       "          [0.9925, 0.9914, 0.9914,  ..., 0.9933, 0.9919, 0.9911],\n",
       "          [0.6370, 0.6357, 0.6370,  ..., 0.6400, 0.6383, 0.6375]]),\n",
       "  0),\n",
       " (tensor([[0.8378, 0.8365, 0.8374,  ..., 0.8403, 0.8389, 0.8381],\n",
       "          [0.2266, 0.2266, 0.2308,  ..., 0.2346, 0.2321, 0.2308],\n",
       "          [0.3865, 0.3854, 0.3873,  ..., 0.3903, 0.3886, 0.3875],\n",
       "          ...,\n",
       "          [0.9925, 0.9914, 0.9914,  ..., 0.9933, 0.9919, 0.9911],\n",
       "          [0.6370, 0.6357, 0.6370,  ..., 0.6400, 0.6383, 0.6375],\n",
       "          [0.7506, 0.7491, 0.7500,  ..., 0.7528, 0.7511, 0.7503]]),\n",
       "  0),\n",
       " (tensor([[0.2266, 0.2266, 0.2308,  ..., 0.2346, 0.2321, 0.2308],\n",
       "          [0.3865, 0.3854, 0.3873,  ..., 0.3903, 0.3886, 0.3875],\n",
       "          [0.5835, 0.5826, 0.5839,  ..., 0.5865, 0.5851, 0.5839],\n",
       "          ...,\n",
       "          [0.6370, 0.6357, 0.6370,  ..., 0.6400, 0.6383, 0.6375],\n",
       "          [0.7506, 0.7491, 0.7500,  ..., 0.7528, 0.7511, 0.7503],\n",
       "          [0.0622, 0.0629, 0.0655,  ..., 0.0667, 0.0652, 0.0649]]),\n",
       "  0),\n",
       " (tensor([[0.5937, 0.5929, 0.5962,  ..., 0.5938, 0.5938, 0.5968],\n",
       "          [0.8448, 0.8448, 0.8476,  ..., 0.8449, 0.8455, 0.8482],\n",
       "          [0.9864, 0.9868, 0.9897,  ..., 0.9870, 0.9876, 0.9904],\n",
       "          ...,\n",
       "          [0.1292, 0.1297, 0.1326,  ..., 0.1317, 0.1317, 0.1341],\n",
       "          [0.2937, 0.2942, 0.2971,  ..., 0.2962, 0.2962, 0.2986],\n",
       "          [0.4062, 0.4062, 0.4090,  ..., 0.4074, 0.4074, 0.4096]]),\n",
       "  0),\n",
       " (tensor([[0.8448, 0.8448, 0.8476,  ..., 0.8449, 0.8455, 0.8482],\n",
       "          [0.9864, 0.9868, 0.9897,  ..., 0.9870, 0.9876, 0.9904],\n",
       "          [0.5002, 0.4987, 0.5041,  ..., 0.5022, 0.5013, 0.5025],\n",
       "          ...,\n",
       "          [0.2937, 0.2942, 0.2971,  ..., 0.2962, 0.2962, 0.2986],\n",
       "          [0.4062, 0.4062, 0.4090,  ..., 0.4074, 0.4074, 0.4096],\n",
       "          [0.5216, 0.5206, 0.5245,  ..., 0.5222, 0.5219, 0.5240]]),\n",
       "  0),\n",
       " (tensor([[0.9864, 0.9868, 0.9897,  ..., 0.9870, 0.9876, 0.9904],\n",
       "          [0.5002, 0.4987, 0.5041,  ..., 0.5022, 0.5013, 0.5025],\n",
       "          [0.7281, 0.7281, 0.7311,  ..., 0.7286, 0.7289, 0.7316],\n",
       "          ...,\n",
       "          [0.4062, 0.4062, 0.4090,  ..., 0.4074, 0.4074, 0.4096],\n",
       "          [0.5216, 0.5206, 0.5245,  ..., 0.5222, 0.5219, 0.5240],\n",
       "          [0.7202, 0.7201, 0.7232,  ..., 0.7204, 0.7205, 0.7236]]),\n",
       "  0),\n",
       " (tensor([[0.5398, 0.5382, 0.5440,  ..., 0.5419, 0.5409, 0.5422],\n",
       "          [0.7857, 0.7857, 0.7889,  ..., 0.7862, 0.7865, 0.7894],\n",
       "          [0.6258, 0.6253, 0.6285,  ..., 0.6261, 0.6266, 0.6293],\n",
       "          ...,\n",
       "          [0.5629, 0.5617, 0.5659,  ..., 0.5635, 0.5632, 0.5654],\n",
       "          [0.7772, 0.7770, 0.7804,  ..., 0.7773, 0.7775, 0.7809],\n",
       "          [0.9860, 0.9861, 0.9892,  ..., 0.9864, 0.9869, 0.9900]]),\n",
       "  0),\n",
       " (tensor([[0.7857, 0.7857, 0.7889,  ..., 0.7862, 0.7865, 0.7894],\n",
       "          [0.6258, 0.6253, 0.6285,  ..., 0.6261, 0.6266, 0.6293],\n",
       "          [0.2291, 0.2306, 0.2335,  ..., 0.2340, 0.2335, 0.2359],\n",
       "          ...,\n",
       "          [0.7772, 0.7770, 0.7804,  ..., 0.7773, 0.7775, 0.7809],\n",
       "          [0.9860, 0.9861, 0.9892,  ..., 0.9864, 0.9869, 0.9900],\n",
       "          [0.7068, 0.7065, 0.7095,  ..., 0.7066, 0.7070, 0.7099]]),\n",
       "  0),\n",
       " (tensor([[6.1040e-01, 6.0496e-01, 6.0295e-01,  ..., 6.0582e-01, 6.1570e-01,\n",
       "           6.0825e-01],\n",
       "          [4.9749e-01, 4.9119e-01, 4.8904e-01,  ..., 4.9205e-01, 5.0236e-01,\n",
       "           4.9448e-01],\n",
       "          [7.7375e-03, 2.8658e-03, 2.8658e-04,  ..., 6.0181e-03, 1.6478e-02,\n",
       "           7.0211e-03],\n",
       "          ...,\n",
       "          [4.5221e-01, 4.4448e-01, 4.4218e-01,  ..., 4.4519e-01, 4.5651e-01,\n",
       "           4.4806e-01],\n",
       "          [3.0649e-01, 2.9990e-01, 2.9761e-01,  ..., 3.0105e-01, 3.1122e-01,\n",
       "           3.0320e-01],\n",
       "          [2.3585e-01, 2.3012e-01, 2.2826e-01,  ..., 2.3170e-01, 2.4044e-01,\n",
       "           2.3327e-01]]),\n",
       "  0),\n",
       " (tensor([[4.9749e-01, 4.9119e-01, 4.8904e-01,  ..., 4.9205e-01, 5.0236e-01,\n",
       "           4.9448e-01],\n",
       "          [7.7375e-03, 2.8658e-03, 2.8658e-04,  ..., 6.0181e-03, 1.6478e-02,\n",
       "           7.0211e-03],\n",
       "          [4.0292e-01, 3.9375e-01, 3.9132e-01,  ..., 3.9519e-01, 4.0694e-01,\n",
       "           3.9819e-01],\n",
       "          ...,\n",
       "          [3.0649e-01, 2.9990e-01, 2.9761e-01,  ..., 3.0105e-01, 3.1122e-01,\n",
       "           3.0320e-01],\n",
       "          [2.3585e-01, 2.3012e-01, 2.2826e-01,  ..., 2.3170e-01, 2.4044e-01,\n",
       "           2.3327e-01],\n",
       "          [5.8834e-01, 5.8261e-01, 5.8046e-01,  ..., 5.8289e-01, 5.9321e-01,\n",
       "           5.8533e-01]]),\n",
       "  0),\n",
       " (tensor([[7.7375e-03, 2.8658e-03, 2.8658e-04,  ..., 6.0181e-03, 1.6478e-02,\n",
       "           7.0211e-03],\n",
       "          [4.0292e-01, 3.9375e-01, 3.9132e-01,  ..., 3.9519e-01, 4.0694e-01,\n",
       "           3.9819e-01],\n",
       "          [2.5133e-01, 2.4531e-01, 2.4287e-01,  ..., 2.4774e-01, 2.5806e-01,\n",
       "           2.4961e-01],\n",
       "          ...,\n",
       "          [2.3585e-01, 2.3012e-01, 2.2826e-01,  ..., 2.3170e-01, 2.4044e-01,\n",
       "           2.3327e-01],\n",
       "          [5.8834e-01, 5.8261e-01, 5.8046e-01,  ..., 5.8289e-01, 5.9321e-01,\n",
       "           5.8533e-01],\n",
       "          [7.1529e-01, 7.0999e-01, 7.0784e-01,  ..., 7.1013e-01, 7.2030e-01,\n",
       "           7.1271e-01]]),\n",
       "  0),\n",
       " (tensor([[0.3639, 0.3541, 0.3515,  ..., 0.3557, 0.3682, 0.3589],\n",
       "          [0.2024, 0.1960, 0.1934,  ..., 0.1986, 0.2096, 0.2006],\n",
       "          [0.2163, 0.2102, 0.2074,  ..., 0.2132, 0.2244, 0.2149],\n",
       "          ...,\n",
       "          [0.5614, 0.5553, 0.5530,  ..., 0.5556, 0.5666, 0.5582],\n",
       "          [0.6967, 0.6910, 0.6887,  ..., 0.6912, 0.7020, 0.6939],\n",
       "          [0.3357, 0.3282, 0.3262,  ..., 0.3293, 0.3401, 0.3319]]),\n",
       "  0),\n",
       " (tensor([[0.2024, 0.1960, 0.1934,  ..., 0.1986, 0.2096, 0.2006],\n",
       "          [0.2163, 0.2102, 0.2074,  ..., 0.2132, 0.2244, 0.2149],\n",
       "          [0.1613, 0.1546, 0.1511,  ..., 0.1578, 0.1694, 0.1601],\n",
       "          ...,\n",
       "          [0.6967, 0.6910, 0.6887,  ..., 0.6912, 0.7020, 0.6939],\n",
       "          [0.3357, 0.3282, 0.3262,  ..., 0.3293, 0.3401, 0.3319],\n",
       "          [0.6191, 0.6135, 0.6111,  ..., 0.6136, 0.6245, 0.6162]]),\n",
       "  0),\n",
       " (tensor([[0.6123, 0.6125, 0.6114,  ..., 0.6143, 0.6115, 0.6107],\n",
       "          [0.6963, 0.6964, 0.6955,  ..., 0.6983, 0.6956, 0.6950],\n",
       "          [0.4960, 0.4963, 0.4951,  ..., 0.4980, 0.4955, 0.4948],\n",
       "          ...,\n",
       "          [0.9943, 0.9946, 0.9936,  ..., 0.9952, 0.9918, 0.9907],\n",
       "          [0.8298, 0.8305, 0.8298,  ..., 0.8324, 0.8309, 0.8298],\n",
       "          [0.0056, 0.0073, 0.0064,  ..., 0.0107, 0.0088, 0.0072]]),\n",
       "  0),\n",
       " (tensor([[0.6963, 0.6964, 0.6955,  ..., 0.6983, 0.6956, 0.6950],\n",
       "          [0.4960, 0.4963, 0.4951,  ..., 0.4980, 0.4955, 0.4948],\n",
       "          [0.8707, 0.8708, 0.8698,  ..., 0.8721, 0.8692, 0.8679],\n",
       "          ...,\n",
       "          [0.8298, 0.8305, 0.8298,  ..., 0.8324, 0.8309, 0.8298],\n",
       "          [0.0056, 0.0073, 0.0064,  ..., 0.0107, 0.0088, 0.0072],\n",
       "          [0.2660, 0.2680, 0.2671,  ..., 0.2719, 0.2703, 0.2694]]),\n",
       "  0),\n",
       " (tensor([[0.4960, 0.4963, 0.4951,  ..., 0.4980, 0.4955, 0.4948],\n",
       "          [0.8707, 0.8708, 0.8698,  ..., 0.8721, 0.8692, 0.8679],\n",
       "          [0.9247, 0.9248, 0.9240,  ..., 0.9259, 0.9227, 0.9218],\n",
       "          ...,\n",
       "          [0.0056, 0.0073, 0.0064,  ..., 0.0107, 0.0088, 0.0072],\n",
       "          [0.2660, 0.2680, 0.2671,  ..., 0.2719, 0.2703, 0.2694],\n",
       "          [0.0985, 0.1000, 0.0991,  ..., 0.1031, 0.1000, 0.0983]]),\n",
       "  0),\n",
       " (tensor([[0.8707, 0.8708, 0.8698,  ..., 0.8721, 0.8692, 0.8679],\n",
       "          [0.9247, 0.9248, 0.9240,  ..., 0.9259, 0.9227, 0.9218],\n",
       "          [0.8944, 0.8946, 0.8936,  ..., 0.8958, 0.8926, 0.8916],\n",
       "          ...,\n",
       "          [0.2660, 0.2680, 0.2671,  ..., 0.2719, 0.2703, 0.2694],\n",
       "          [0.0985, 0.1000, 0.0991,  ..., 0.1031, 0.1000, 0.0983],\n",
       "          [0.9188, 0.9192, 0.9182,  ..., 0.9198, 0.9164, 0.9155]]),\n",
       "  0),\n",
       " (tensor([[0.9247, 0.9248, 0.9240,  ..., 0.9259, 0.9227, 0.9218],\n",
       "          [0.8944, 0.8946, 0.8936,  ..., 0.8958, 0.8926, 0.8916],\n",
       "          [0.6572, 0.6576, 0.6565,  ..., 0.6590, 0.6560, 0.6552],\n",
       "          ...,\n",
       "          [0.0985, 0.1000, 0.0991,  ..., 0.1031, 0.1000, 0.0983],\n",
       "          [0.9188, 0.9192, 0.9182,  ..., 0.9198, 0.9164, 0.9155],\n",
       "          [0.4492, 0.4491, 0.4477,  ..., 0.4499, 0.4477, 0.4466]]),\n",
       "  0),\n",
       " (tensor([[0.5510, 0.5518, 0.5529,  ..., 0.5603, 0.5632, 0.5692],\n",
       "          [0.8021, 0.8025, 0.8038,  ..., 0.8108, 0.8141, 0.8201],\n",
       "          [0.7925, 0.7931, 0.7944,  ..., 0.8011, 0.8045, 0.8108],\n",
       "          ...,\n",
       "          [0.5898, 0.5905, 0.5918,  ..., 0.5985, 0.6017, 0.6075],\n",
       "          [0.4500, 0.4506, 0.4515,  ..., 0.4584, 0.4613, 0.4669],\n",
       "          [0.7587, 0.7592, 0.7605,  ..., 0.7670, 0.7705, 0.7766]]),\n",
       "  0),\n",
       " (tensor([[0.8021, 0.8025, 0.8038,  ..., 0.8108, 0.8141, 0.8201],\n",
       "          [0.7925, 0.7931, 0.7944,  ..., 0.8011, 0.8045, 0.8108],\n",
       "          [0.5547, 0.5554, 0.5565,  ..., 0.5632, 0.5664, 0.5726],\n",
       "          ...,\n",
       "          [0.4500, 0.4506, 0.4515,  ..., 0.4584, 0.4613, 0.4669],\n",
       "          [0.7587, 0.7592, 0.7605,  ..., 0.7670, 0.7705, 0.7766],\n",
       "          [0.2612, 0.2610, 0.2618,  ..., 0.2690, 0.2706, 0.2759]]),\n",
       "  0),\n",
       " (tensor([[0.8405, 0.8409, 0.8418,  ..., 0.8470, 0.8496, 0.8545],\n",
       "          [0.6575, 0.6581, 0.6589,  ..., 0.6641, 0.6666, 0.6713],\n",
       "          [0.6993, 0.6997, 0.7005,  ..., 0.7060, 0.7083, 0.7131],\n",
       "          ...,\n",
       "          [0.8144, 0.8148, 0.8158,  ..., 0.8208, 0.8235, 0.8282],\n",
       "          [0.4319, 0.4317, 0.4323,  ..., 0.4378, 0.4391, 0.4431],\n",
       "          [0.0000, 0.0021, 0.0010,  ..., 0.0093, 0.0104, 0.0148]]),\n",
       "  0),\n",
       " (tensor([[0.6575, 0.6581, 0.6589,  ..., 0.6641, 0.6666, 0.6713],\n",
       "          [0.6993, 0.6997, 0.7005,  ..., 0.7060, 0.7083, 0.7131],\n",
       "          [0.7526, 0.7530, 0.7541,  ..., 0.7592, 0.7615, 0.7664],\n",
       "          ...,\n",
       "          [0.4319, 0.4317, 0.4323,  ..., 0.4378, 0.4391, 0.4431],\n",
       "          [0.0000, 0.0021, 0.0010,  ..., 0.0093, 0.0104, 0.0148],\n",
       "          [0.3660, 0.3671, 0.3678,  ..., 0.3751, 0.3762, 0.3813]]),\n",
       "  0),\n",
       " (tensor([[0.6993, 0.6997, 0.7005,  ..., 0.7060, 0.7083, 0.7131],\n",
       "          [0.7526, 0.7530, 0.7541,  ..., 0.7592, 0.7615, 0.7664],\n",
       "          [0.2406, 0.2411, 0.2408,  ..., 0.2473, 0.2486, 0.2531],\n",
       "          ...,\n",
       "          [0.0000, 0.0021, 0.0010,  ..., 0.0093, 0.0104, 0.0148],\n",
       "          [0.3660, 0.3671, 0.3678,  ..., 0.3751, 0.3762, 0.3813],\n",
       "          [0.6677, 0.6675, 0.6684,  ..., 0.6733, 0.6751, 0.6791]]),\n",
       "  0),\n",
       " (tensor([[0.6053, 0.6009, 0.6016,  ..., 0.6096, 0.6116, 0.6094],\n",
       "          [0.6806, 0.6762, 0.6770,  ..., 0.6851, 0.6867, 0.6849],\n",
       "          [0.4967, 0.4940, 0.4946,  ..., 0.5018, 0.5037, 0.5017],\n",
       "          ...,\n",
       "          [0.7831, 0.7784, 0.7793,  ..., 0.7873, 0.7893, 0.7873],\n",
       "          [0.5606, 0.5566, 0.5574,  ..., 0.5651, 0.5669, 0.5649],\n",
       "          [0.8763, 0.8710, 0.8718,  ..., 0.8802, 0.8819, 0.8802]]),\n",
       "  0),\n",
       " (tensor([[0.6806, 0.6762, 0.6770,  ..., 0.6851, 0.6867, 0.6849],\n",
       "          [0.4967, 0.4940, 0.4946,  ..., 0.5018, 0.5037, 0.5017],\n",
       "          [0.7015, 0.6963, 0.6971,  ..., 0.7058, 0.7075, 0.7058],\n",
       "          ...,\n",
       "          [0.5606, 0.5566, 0.5574,  ..., 0.5651, 0.5669, 0.5649],\n",
       "          [0.8763, 0.8710, 0.8718,  ..., 0.8802, 0.8819, 0.8802],\n",
       "          [0.8876, 0.8822, 0.8830,  ..., 0.8914, 0.8932, 0.8916]]),\n",
       "  0),\n",
       " (tensor([[0.4967, 0.4940, 0.4946,  ..., 0.5018, 0.5037, 0.5017],\n",
       "          [0.7015, 0.6963, 0.6971,  ..., 0.7058, 0.7075, 0.7058],\n",
       "          [0.9006, 0.8952, 0.8959,  ..., 0.9048, 0.9066, 0.9050],\n",
       "          ...,\n",
       "          [0.8763, 0.8710, 0.8718,  ..., 0.8802, 0.8819, 0.8802],\n",
       "          [0.8876, 0.8822, 0.8830,  ..., 0.8914, 0.8932, 0.8916],\n",
       "          [0.9649, 0.9592, 0.9598,  ..., 0.9684, 0.9699, 0.9684]]),\n",
       "  0),\n",
       " (tensor([[0.7015, 0.6963, 0.6971,  ..., 0.7058, 0.7075, 0.7058],\n",
       "          [0.9006, 0.8952, 0.8959,  ..., 0.9048, 0.9066, 0.9050],\n",
       "          [0.7236, 0.7185, 0.7192,  ..., 0.7282, 0.7299, 0.7281],\n",
       "          ...,\n",
       "          [0.8876, 0.8822, 0.8830,  ..., 0.8914, 0.8932, 0.8916],\n",
       "          [0.9649, 0.9592, 0.9598,  ..., 0.9684, 0.9699, 0.9684],\n",
       "          [0.6787, 0.6755, 0.6762,  ..., 0.6836, 0.6854, 0.6834]]),\n",
       "  0),\n",
       " (tensor([[9.1022e-01, 9.0538e-01, 9.0599e-01,  ..., 9.1400e-01, 9.1566e-01,\n",
       "           9.1415e-01],\n",
       "          [7.5030e-01, 7.4577e-01, 7.4637e-01,  ..., 7.5453e-01, 7.5605e-01,\n",
       "           7.5438e-01],\n",
       "          [5.2328e-01, 5.2252e-01, 5.2328e-01,  ..., 5.2902e-01, 5.3114e-01,\n",
       "           5.2887e-01],\n",
       "          ...,\n",
       "          [9.6826e-01, 9.6312e-01, 9.6372e-01,  ..., 9.7143e-01, 9.7279e-01,\n",
       "           9.7143e-01],\n",
       "          [7.0979e-01, 7.0692e-01, 7.0753e-01,  ..., 7.1418e-01, 7.1584e-01,\n",
       "           7.1403e-01],\n",
       "          [9.0688e-04, 3.0230e-03, 3.4764e-03,  ..., 7.8597e-03, 9.2201e-03,\n",
       "           7.4063e-03]]),\n",
       "  0),\n",
       " (tensor([[0.4848, 0.4862, 0.4862,  ..., 0.4945, 0.4942, 0.4959],\n",
       "          [0.6257, 0.6281, 0.6281,  ..., 0.6361, 0.6347, 0.6366],\n",
       "          [0.7774, 0.7801, 0.7802,  ..., 0.7873, 0.7859, 0.7882],\n",
       "          ...,\n",
       "          [0.9885, 0.9911, 0.9914,  ..., 0.9980, 0.9959, 0.9986],\n",
       "          [0.0080, 0.0111, 0.0105,  ..., 0.0193, 0.0196, 0.0203],\n",
       "          [0.8873, 0.8845, 0.8851,  ..., 0.8925, 0.8917, 0.8956]]),\n",
       "  0),\n",
       " (tensor([[0.6257, 0.6281, 0.6281,  ..., 0.6361, 0.6347, 0.6366],\n",
       "          [0.7774, 0.7801, 0.7802,  ..., 0.7873, 0.7859, 0.7882],\n",
       "          [0.1377, 0.1405, 0.1405,  ..., 0.1510, 0.1513, 0.1532],\n",
       "          ...,\n",
       "          [0.0080, 0.0111, 0.0105,  ..., 0.0193, 0.0196, 0.0203],\n",
       "          [0.8873, 0.8845, 0.8851,  ..., 0.8925, 0.8917, 0.8956],\n",
       "          [0.4727, 0.4736, 0.4736,  ..., 0.4827, 0.4823, 0.4845]]),\n",
       "  0),\n",
       " (tensor([[0.7774, 0.7801, 0.7802,  ..., 0.7873, 0.7859, 0.7882],\n",
       "          [0.1377, 0.1405, 0.1405,  ..., 0.1510, 0.1513, 0.1532],\n",
       "          [0.5086, 0.5104, 0.5102,  ..., 0.5179, 0.5173, 0.5190],\n",
       "          ...,\n",
       "          [0.8873, 0.8845, 0.8851,  ..., 0.8925, 0.8917, 0.8956],\n",
       "          [0.4727, 0.4736, 0.4736,  ..., 0.4827, 0.4823, 0.4845],\n",
       "          [0.6170, 0.6192, 0.6192,  ..., 0.6268, 0.6257, 0.6276]]),\n",
       "  0),\n",
       " (tensor([[0.1377, 0.1405, 0.1405,  ..., 0.1510, 0.1513, 0.1532],\n",
       "          [0.5086, 0.5104, 0.5102,  ..., 0.5179, 0.5173, 0.5190],\n",
       "          [0.6766, 0.6790, 0.6791,  ..., 0.6865, 0.6852, 0.6871],\n",
       "          ...,\n",
       "          [0.4727, 0.4736, 0.4736,  ..., 0.4827, 0.4823, 0.4845],\n",
       "          [0.6170, 0.6192, 0.6192,  ..., 0.6268, 0.6257, 0.6276],\n",
       "          [0.8790, 0.8816, 0.8818,  ..., 0.8892, 0.8876, 0.8898]]),\n",
       "  0),\n",
       " (tensor([[0.5086, 0.5104, 0.5102,  ..., 0.5179, 0.5173, 0.5190],\n",
       "          [0.6766, 0.6790, 0.6791,  ..., 0.6865, 0.6852, 0.6871],\n",
       "          [0.8879, 0.8906, 0.8906,  ..., 0.8980, 0.8964, 0.8986],\n",
       "          ...,\n",
       "          [0.6170, 0.6192, 0.6192,  ..., 0.6268, 0.6257, 0.6276],\n",
       "          [0.8790, 0.8816, 0.8818,  ..., 0.8892, 0.8876, 0.8898],\n",
       "          [0.3535, 0.3542, 0.3542,  ..., 0.3628, 0.3628, 0.3650]]),\n",
       "  0),\n",
       " (tensor([[3.8432e-01, 3.8160e-01, 3.8225e-01,  ..., 3.9362e-01, 3.8729e-01,\n",
       "           3.9801e-01],\n",
       "          [6.4979e-01, 6.4669e-01, 6.4694e-01,  ..., 6.5547e-01, 6.4992e-01,\n",
       "           6.5754e-01],\n",
       "          [9.0686e-01, 9.0363e-01, 9.0389e-01,  ..., 9.1267e-01, 9.0660e-01,\n",
       "           9.1448e-01],\n",
       "          ...,\n",
       "          [1.0335e-03, 1.2919e-04, 3.8756e-04,  ..., 9.0428e-03, 4.6506e-03,\n",
       "           1.1368e-02],\n",
       "          [1.7943e-01, 1.7853e-01, 1.7918e-01,  ..., 1.8654e-01, 1.8344e-01,\n",
       "           1.8990e-01],\n",
       "          [7.7419e-01, 7.7096e-01, 7.7109e-01,  ..., 7.8026e-01, 7.7432e-01,\n",
       "           7.8220e-01]]),\n",
       "  0),\n",
       " (tensor([[0.6498, 0.6467, 0.6469,  ..., 0.6555, 0.6499, 0.6575],\n",
       "          [0.9069, 0.9036, 0.9039,  ..., 0.9127, 0.9066, 0.9145],\n",
       "          [0.1716, 0.1716, 0.1716,  ..., 0.1805, 0.1770, 0.1831],\n",
       "          ...,\n",
       "          [0.1794, 0.1785, 0.1792,  ..., 0.1865, 0.1834, 0.1899],\n",
       "          [0.7742, 0.7710, 0.7711,  ..., 0.7803, 0.7743, 0.7822],\n",
       "          [0.9031, 0.9009, 0.9009,  ..., 0.9098, 0.9131, 0.9119]]),\n",
       "  0),\n",
       " (tensor([[0.9069, 0.9036, 0.9039,  ..., 0.9127, 0.9066, 0.9145],\n",
       "          [0.1716, 0.1716, 0.1716,  ..., 0.1805, 0.1770, 0.1831],\n",
       "          [0.4630, 0.4596, 0.4601,  ..., 0.4720, 0.4654, 0.4745],\n",
       "          ...,\n",
       "          [0.7742, 0.7710, 0.7711,  ..., 0.7803, 0.7743, 0.7822],\n",
       "          [0.9031, 0.9009, 0.9009,  ..., 0.9098, 0.9131, 0.9119],\n",
       "          [0.3308, 0.3277, 0.3280,  ..., 0.3388, 0.3440, 0.3422]]),\n",
       "  0),\n",
       " (tensor([[0.1716, 0.1716, 0.1716,  ..., 0.1805, 0.1770, 0.1831],\n",
       "          [0.4630, 0.4596, 0.4601,  ..., 0.4720, 0.4654, 0.4745],\n",
       "          [0.6925, 0.6893, 0.6897,  ..., 0.6985, 0.6929, 0.7009],\n",
       "          ...,\n",
       "          [0.9031, 0.9009, 0.9009,  ..., 0.9098, 0.9131, 0.9119],\n",
       "          [0.3308, 0.3277, 0.3280,  ..., 0.3388, 0.3440, 0.3422],\n",
       "          [0.4611, 0.4580, 0.4582,  ..., 0.4691, 0.4742, 0.4724]]),\n",
       "  0),\n",
       " (tensor([[0.4630, 0.4596, 0.4601,  ..., 0.4720, 0.4654, 0.4745],\n",
       "          [0.6925, 0.6893, 0.6897,  ..., 0.6985, 0.6929, 0.7009],\n",
       "          [0.7724, 0.7692, 0.7694,  ..., 0.7783, 0.7728, 0.7805],\n",
       "          ...,\n",
       "          [0.3308, 0.3277, 0.3280,  ..., 0.3388, 0.3440, 0.3422],\n",
       "          [0.4611, 0.4580, 0.4582,  ..., 0.4691, 0.4742, 0.4724],\n",
       "          [0.6606, 0.6575, 0.6578,  ..., 0.6670, 0.6702, 0.6689]]),\n",
       "  0),\n",
       " (tensor([[0.9839, 0.9852, 0.9870,  ..., 0.9854, 0.9889, 0.9868],\n",
       "          [0.1663, 0.1687, 0.1699,  ..., 0.1683, 0.1732, 0.1690],\n",
       "          [0.7018, 0.7033, 0.7050,  ..., 0.7030, 0.7069, 0.7045],\n",
       "          ...,\n",
       "          [0.3357, 0.3391, 0.3409,  ..., 0.3391, 0.3428, 0.3399],\n",
       "          [0.9903, 0.9915, 0.9931,  ..., 0.9915, 0.9954, 0.9931],\n",
       "          [0.3831, 0.3857, 0.3872,  ..., 0.3847, 0.3895, 0.3857]]),\n",
       "  0),\n",
       " (tensor([[0.1663, 0.1687, 0.1699,  ..., 0.1683, 0.1732, 0.1690],\n",
       "          [0.7018, 0.7033, 0.7050,  ..., 0.7030, 0.7069, 0.7045],\n",
       "          [0.7246, 0.7260, 0.7275,  ..., 0.7260, 0.7298, 0.7274],\n",
       "          ...,\n",
       "          [0.9903, 0.9915, 0.9931,  ..., 0.9915, 0.9954, 0.9931],\n",
       "          [0.3831, 0.3857, 0.3872,  ..., 0.3847, 0.3895, 0.3857],\n",
       "          [0.7813, 0.7830, 0.7848,  ..., 0.7830, 0.7869, 0.7842]]),\n",
       "  0),\n",
       " (tensor([[0.7018, 0.7033, 0.7050,  ..., 0.7030, 0.7069, 0.7045],\n",
       "          [0.7246, 0.7260, 0.7275,  ..., 0.7260, 0.7298, 0.7274],\n",
       "          [0.4606, 0.4623, 0.4629,  ..., 0.4615, 0.4650, 0.4623],\n",
       "          ...,\n",
       "          [0.3831, 0.3857, 0.3872,  ..., 0.3847, 0.3895, 0.3857],\n",
       "          [0.7813, 0.7830, 0.7848,  ..., 0.7830, 0.7869, 0.7842],\n",
       "          [0.0632, 0.0662, 0.0677,  ..., 0.0656, 0.0701, 0.0662]]),\n",
       "  0),\n",
       " (tensor([[0.7246, 0.7260, 0.7275,  ..., 0.7260, 0.7298, 0.7274],\n",
       "          [0.4606, 0.4623, 0.4629,  ..., 0.4615, 0.4650, 0.4623],\n",
       "          [0.4082, 0.4109, 0.4120,  ..., 0.4101, 0.4144, 0.4116],\n",
       "          ...,\n",
       "          [0.7813, 0.7830, 0.7848,  ..., 0.7830, 0.7869, 0.7842],\n",
       "          [0.0632, 0.0662, 0.0677,  ..., 0.0656, 0.0701, 0.0662],\n",
       "          [0.7547, 0.7564, 0.7579,  ..., 0.7559, 0.7597, 0.7576]]),\n",
       "  0),\n",
       " (tensor([[0.4606, 0.4623, 0.4629,  ..., 0.4615, 0.4650, 0.4623],\n",
       "          [0.4082, 0.4109, 0.4120,  ..., 0.4101, 0.4144, 0.4116],\n",
       "          [0.2052, 0.2091, 0.2100,  ..., 0.2091, 0.2127, 0.2101],\n",
       "          ...,\n",
       "          [0.0632, 0.0662, 0.0677,  ..., 0.0656, 0.0701, 0.0662],\n",
       "          [0.7547, 0.7564, 0.7579,  ..., 0.7559, 0.7597, 0.7576],\n",
       "          [0.5805, 0.5821, 0.5835,  ..., 0.5815, 0.5862, 0.5829]]),\n",
       "  0),\n",
       " (tensor([[0.9915, 0.9908, 0.9950,  ..., 0.9932, 0.9901, 0.9952],\n",
       "          [0.4579, 0.4564, 0.4605,  ..., 0.4591, 0.4568, 0.4626],\n",
       "          [0.7496, 0.7480, 0.7523,  ..., 0.7508, 0.7482, 0.7537],\n",
       "          ...,\n",
       "          [0.7733, 0.7718, 0.7764,  ..., 0.7761, 0.7762, 0.7800],\n",
       "          [0.9904, 0.9896, 0.9940,  ..., 0.9930, 0.9926, 0.9961],\n",
       "          [0.0147, 0.0128, 0.0169,  ..., 0.0174, 0.0173, 0.0217]]),\n",
       "  0),\n",
       " (tensor([[0.4579, 0.4564, 0.4605,  ..., 0.4591, 0.4568, 0.4626],\n",
       "          [0.7496, 0.7480, 0.7523,  ..., 0.7508, 0.7482, 0.7537],\n",
       "          [0.7776, 0.7761, 0.7807,  ..., 0.7795, 0.7769, 0.7824],\n",
       "          ...,\n",
       "          [0.9904, 0.9896, 0.9940,  ..., 0.9930, 0.9926, 0.9961],\n",
       "          [0.0147, 0.0128, 0.0169,  ..., 0.0174, 0.0173, 0.0217],\n",
       "          [0.3768, 0.3747, 0.3790,  ..., 0.3793, 0.3795, 0.3838]]),\n",
       "  0),\n",
       " (tensor([[0.7496, 0.7480, 0.7523,  ..., 0.7508, 0.7482, 0.7537],\n",
       "          [0.7776, 0.7761, 0.7807,  ..., 0.7795, 0.7769, 0.7824],\n",
       "          [0.8332, 0.8321, 0.8366,  ..., 0.8354, 0.8328, 0.8381],\n",
       "          ...,\n",
       "          [0.0147, 0.0128, 0.0169,  ..., 0.0174, 0.0173, 0.0217],\n",
       "          [0.3768, 0.3747, 0.3790,  ..., 0.3793, 0.3795, 0.3838],\n",
       "          [0.8130, 0.8118, 0.8164,  ..., 0.8161, 0.8162, 0.8200]]),\n",
       "  0),\n",
       " (tensor([[0.7776, 0.7761, 0.7807,  ..., 0.7795, 0.7769, 0.7824],\n",
       "          [0.8332, 0.8321, 0.8366,  ..., 0.8354, 0.8328, 0.8381],\n",
       "          [0.8301, 0.8287, 0.8333,  ..., 0.8320, 0.8294, 0.8347],\n",
       "          ...,\n",
       "          [0.3768, 0.3747, 0.3790,  ..., 0.3793, 0.3795, 0.3838],\n",
       "          [0.8130, 0.8118, 0.8164,  ..., 0.8161, 0.8162, 0.8200],\n",
       "          [0.8053, 0.8039, 0.8087,  ..., 0.8085, 0.8085, 0.8126]]),\n",
       "  0),\n",
       " (tensor([[0.8332, 0.8321, 0.8366,  ..., 0.8354, 0.8328, 0.8381],\n",
       "          [0.8301, 0.8287, 0.8333,  ..., 0.8320, 0.8294, 0.8347],\n",
       "          [0.5320, 0.5304, 0.5344,  ..., 0.5337, 0.5320, 0.5374],\n",
       "          ...,\n",
       "          [0.8130, 0.8118, 0.8164,  ..., 0.8161, 0.8162, 0.8200],\n",
       "          [0.8053, 0.8039, 0.8087,  ..., 0.8085, 0.8085, 0.8126],\n",
       "          [0.6417, 0.6403, 0.6450,  ..., 0.6453, 0.6453, 0.6492]]),\n",
       "  0),\n",
       " (tensor([[0.2913, 0.2931, 0.2986,  ..., 0.2986, 0.2991, 0.2997],\n",
       "          [0.4331, 0.4352, 0.4407,  ..., 0.4407, 0.4412, 0.4418],\n",
       "          [0.8574, 0.8587, 0.8652,  ..., 0.8650, 0.8652, 0.8657],\n",
       "          ...,\n",
       "          [0.7100, 0.7121, 0.7181,  ..., 0.7181, 0.7187, 0.7189],\n",
       "          [0.8241, 0.8254, 0.8320,  ..., 0.8317, 0.8317, 0.8325],\n",
       "          [0.4753, 0.4771, 0.4829,  ..., 0.4829, 0.4831, 0.4834]]),\n",
       "  0),\n",
       " (tensor([[0.4331, 0.4352, 0.4407,  ..., 0.4407, 0.4412, 0.4418],\n",
       "          [0.8574, 0.8587, 0.8652,  ..., 0.8650, 0.8652, 0.8657],\n",
       "          [0.9822, 0.9838, 0.9903,  ..., 0.9898, 0.9903, 0.9908],\n",
       "          ...,\n",
       "          [0.8241, 0.8254, 0.8320,  ..., 0.8317, 0.8317, 0.8325],\n",
       "          [0.4753, 0.4771, 0.4829,  ..., 0.4829, 0.4831, 0.4834],\n",
       "          [0.6420, 0.6435, 0.6496,  ..., 0.6496, 0.6501, 0.6501]]),\n",
       "  0),\n",
       " (tensor([[0.8551, 0.8564, 0.8630,  ..., 0.8627, 0.8630, 0.8635],\n",
       "          [0.9796, 0.9812, 0.9877,  ..., 0.9872, 0.9877, 0.9883],\n",
       "          [0.0992, 0.1015, 0.1065,  ..., 0.1068, 0.1073, 0.1078],\n",
       "          ...,\n",
       "          [0.4740, 0.4759, 0.4816,  ..., 0.4816, 0.4819, 0.4821],\n",
       "          [0.6403, 0.6419, 0.6479,  ..., 0.6479, 0.6484, 0.6484],\n",
       "          [0.9823, 0.9836, 0.9898,  ..., 0.9885, 0.9888, 0.9896]]),\n",
       "  0),\n",
       " (tensor([[0.9796, 0.9812, 0.9877,  ..., 0.9872, 0.9877, 0.9883],\n",
       "          [0.0992, 0.1015, 0.1065,  ..., 0.1068, 0.1073, 0.1078],\n",
       "          [0.4612, 0.4633, 0.4688,  ..., 0.4691, 0.4693, 0.4696],\n",
       "          ...,\n",
       "          [0.6403, 0.6419, 0.6479,  ..., 0.6479, 0.6484, 0.6484],\n",
       "          [0.9823, 0.9836, 0.9898,  ..., 0.9885, 0.9888, 0.9896],\n",
       "          [0.2185, 0.2203, 0.2253,  ..., 0.2258, 0.2268, 0.2268]]),\n",
       "  0),\n",
       " (tensor([[0.0992, 0.1015, 0.1065,  ..., 0.1068, 0.1073, 0.1078],\n",
       "          [0.4612, 0.4633, 0.4688,  ..., 0.4691, 0.4693, 0.4696],\n",
       "          [0.8259, 0.8277, 0.8337,  ..., 0.8335, 0.8340, 0.8345],\n",
       "          ...,\n",
       "          [0.9823, 0.9836, 0.9898,  ..., 0.9885, 0.9888, 0.9896],\n",
       "          [0.2185, 0.2203, 0.2253,  ..., 0.2258, 0.2268, 0.2268],\n",
       "          [0.5014, 0.5033, 0.5082,  ..., 0.5085, 0.5093, 0.5093]]),\n",
       "  0),\n",
       " (tensor([[0.3306, 0.3340, 0.3347,  ..., 0.3388, 0.3367, 0.3395],\n",
       "          [0.1549, 0.1577, 0.1593,  ..., 0.1623, 0.1603, 0.1618],\n",
       "          [0.9898, 0.9933, 0.9933,  ..., 0.9983, 0.9963, 0.9994],\n",
       "          ...,\n",
       "          [0.8559, 0.8593, 0.8597,  ..., 0.8640, 0.8619, 0.8654],\n",
       "          [0.0000, 0.0050, 0.0065,  ..., 0.0108, 0.0089, 0.0102],\n",
       "          [0.2268, 0.2285, 0.2311,  ..., 0.2337, 0.2307, 0.2324]]),\n",
       "  0),\n",
       " (tensor([[0.1549, 0.1577, 0.1593,  ..., 0.1623, 0.1603, 0.1618],\n",
       "          [0.9898, 0.9933, 0.9933,  ..., 0.9983, 0.9963, 0.9994],\n",
       "          [0.0991, 0.1027, 0.1053,  ..., 0.1094, 0.1066, 0.1085],\n",
       "          ...,\n",
       "          [0.0000, 0.0050, 0.0065,  ..., 0.0108, 0.0089, 0.0102],\n",
       "          [0.2268, 0.2285, 0.2311,  ..., 0.2337, 0.2307, 0.2324],\n",
       "          [0.2926, 0.2952, 0.2969,  ..., 0.3002, 0.2976, 0.3002]]),\n",
       "  0),\n",
       " (tensor([[0.9898, 0.9933, 0.9933,  ..., 0.9983, 0.9963, 0.9994],\n",
       "          [0.0991, 0.1027, 0.1053,  ..., 0.1094, 0.1066, 0.1085],\n",
       "          [0.0401, 0.0432, 0.0457,  ..., 0.0490, 0.0460, 0.0477],\n",
       "          ...,\n",
       "          [0.2268, 0.2285, 0.2311,  ..., 0.2337, 0.2307, 0.2324],\n",
       "          [0.2926, 0.2952, 0.2969,  ..., 0.3002, 0.2976, 0.3002],\n",
       "          [0.6603, 0.6640, 0.6644,  ..., 0.6685, 0.6664, 0.6696]]),\n",
       "  0),\n",
       " (tensor([[0.2606, 0.2640, 0.2665,  ..., 0.2705, 0.2678, 0.2696],\n",
       "          [0.2038, 0.2068, 0.2091,  ..., 0.2124, 0.2095, 0.2111],\n",
       "          [0.4005, 0.4038, 0.4052,  ..., 0.4081, 0.4061, 0.4081],\n",
       "          ...,\n",
       "          [0.4468, 0.4493, 0.4509,  ..., 0.4542, 0.4517, 0.4542],\n",
       "          [0.8007, 0.8043, 0.8047,  ..., 0.8086, 0.8066, 0.8097],\n",
       "          [0.0032, 0.0070, 0.0086,  ..., 0.0124, 0.0100, 0.0111]]),\n",
       "  0),\n",
       " (tensor([[0.2038, 0.2068, 0.2091,  ..., 0.2124, 0.2095, 0.2111],\n",
       "          [0.4005, 0.4038, 0.4052,  ..., 0.4081, 0.4061, 0.4081],\n",
       "          [0.5335, 0.5358, 0.5374,  ..., 0.5406, 0.5385, 0.5412],\n",
       "          ...,\n",
       "          [0.8007, 0.8043, 0.8047,  ..., 0.8086, 0.8066, 0.8097],\n",
       "          [0.0032, 0.0070, 0.0086,  ..., 0.0124, 0.0100, 0.0111],\n",
       "          [0.7790, 0.7822, 0.7824,  ..., 0.7865, 0.7844, 0.7874]]),\n",
       "  0),\n",
       " (tensor([[0.0100, 0.0111, 0.0119,  ..., 0.0123, 0.0132, 0.0128],\n",
       "          [0.4335, 0.4338, 0.4350,  ..., 0.4340, 0.4347, 0.4344],\n",
       "          [0.0761, 0.0800, 0.0800,  ..., 0.0814, 0.0825, 0.0825],\n",
       "          ...,\n",
       "          [0.5522, 0.5519, 0.5531,  ..., 0.5519, 0.5537, 0.5526],\n",
       "          [0.7800, 0.7797, 0.7815,  ..., 0.7801, 0.7816, 0.7807],\n",
       "          [0.9652, 0.9650, 0.9671,  ..., 0.9659, 0.9673, 0.9661]]),\n",
       "  0),\n",
       " (tensor([[0.3885, 0.3888, 0.3901,  ..., 0.3890, 0.3898, 0.3895],\n",
       "          [0.0026, 0.0069, 0.0069,  ..., 0.0084, 0.0095, 0.0095],\n",
       "          [0.2073, 0.2068, 0.2078,  ..., 0.2068, 0.2083, 0.2080],\n",
       "          ...,\n",
       "          [0.7625, 0.7622, 0.7641,  ..., 0.7626, 0.7643, 0.7633],\n",
       "          [0.9624, 0.9622, 0.9645,  ..., 0.9632, 0.9647, 0.9634],\n",
       "          [0.7794, 0.7789, 0.7810,  ..., 0.7796, 0.7814, 0.7801]]),\n",
       "  0),\n",
       " (tensor([[0.0026, 0.0069, 0.0069,  ..., 0.0084, 0.0095, 0.0095],\n",
       "          [0.2073, 0.2068, 0.2078,  ..., 0.2068, 0.2083, 0.2080],\n",
       "          [0.3812, 0.3817, 0.3826,  ..., 0.3819, 0.3829, 0.3819],\n",
       "          ...,\n",
       "          [0.9624, 0.9622, 0.9645,  ..., 0.9632, 0.9647, 0.9634],\n",
       "          [0.7794, 0.7789, 0.7810,  ..., 0.7796, 0.7814, 0.7801],\n",
       "          [0.0310, 0.0332, 0.0337,  ..., 0.0347, 0.0379, 0.0375]]),\n",
       "  0),\n",
       " (tensor([[0.1930, 0.1925, 0.1935,  ..., 0.1925, 0.1940, 0.1936],\n",
       "          [0.3701, 0.3706, 0.3714,  ..., 0.3707, 0.3717, 0.3707],\n",
       "          [0.6375, 0.6386, 0.6403,  ..., 0.6395, 0.6400, 0.6385],\n",
       "          ...,\n",
       "          [0.7754, 0.7749, 0.7771,  ..., 0.7756, 0.7774, 0.7761],\n",
       "          [0.0135, 0.0157, 0.0162,  ..., 0.0172, 0.0206, 0.0201],\n",
       "          [0.0137, 0.0157, 0.0162,  ..., 0.0174, 0.0207, 0.0201]]),\n",
       "  0),\n",
       " (tensor([[0.3701, 0.3706, 0.3714,  ..., 0.3707, 0.3717, 0.3707],\n",
       "          [0.6375, 0.6386, 0.6403,  ..., 0.6395, 0.6400, 0.6385],\n",
       "          [0.0562, 0.0579, 0.0587,  ..., 0.0582, 0.0600, 0.0592],\n",
       "          ...,\n",
       "          [0.0135, 0.0157, 0.0162,  ..., 0.0172, 0.0206, 0.0201],\n",
       "          [0.0137, 0.0157, 0.0162,  ..., 0.0174, 0.0207, 0.0201],\n",
       "          [0.8380, 0.8378, 0.8401,  ..., 0.8385, 0.8401, 0.8388]]),\n",
       "  0),\n",
       " (tensor([[0.7174, 0.7192, 0.7202,  ..., 0.7211, 0.7197, 0.7211],\n",
       "          [0.8241, 0.8261, 0.8270,  ..., 0.8281, 0.8266, 0.8283],\n",
       "          [0.9144, 0.9166, 0.9170,  ..., 0.9178, 0.9167, 0.9178],\n",
       "          ...,\n",
       "          [0.9281, 0.9300, 0.9309,  ..., 0.9312, 0.9297, 0.9315],\n",
       "          [0.8162, 0.8181, 0.8187,  ..., 0.8196, 0.8179, 0.8194],\n",
       "          [0.9341, 0.9357, 0.9364,  ..., 0.9368, 0.9357, 0.9370]]),\n",
       "  0),\n",
       " (tensor([[0.8241, 0.8261, 0.8270,  ..., 0.8281, 0.8266, 0.8283],\n",
       "          [0.9144, 0.9166, 0.9170,  ..., 0.9178, 0.9167, 0.9178],\n",
       "          [0.8943, 0.8962, 0.8969,  ..., 0.8977, 0.8962, 0.8978],\n",
       "          ...,\n",
       "          [0.8162, 0.8181, 0.8187,  ..., 0.8196, 0.8179, 0.8194],\n",
       "          [0.9341, 0.9357, 0.9364,  ..., 0.9368, 0.9357, 0.9370],\n",
       "          [0.0816, 0.0871, 0.0880,  ..., 0.0869, 0.0845, 0.0869]]),\n",
       "  0),\n",
       " (tensor([[0.9144, 0.9166, 0.9170,  ..., 0.9178, 0.9167, 0.9178],\n",
       "          [0.8943, 0.8962, 0.8969,  ..., 0.8977, 0.8962, 0.8978],\n",
       "          [0.9303, 0.9324, 0.9329,  ..., 0.9332, 0.9319, 0.9332],\n",
       "          ...,\n",
       "          [0.9341, 0.9357, 0.9364,  ..., 0.9368, 0.9357, 0.9370],\n",
       "          [0.0816, 0.0871, 0.0880,  ..., 0.0869, 0.0845, 0.0869],\n",
       "          [0.5434, 0.5454, 0.5461,  ..., 0.5469, 0.5454, 0.5470]]),\n",
       "  0),\n",
       " (tensor([[0.8943, 0.8962, 0.8969,  ..., 0.8977, 0.8962, 0.8978],\n",
       "          [0.9303, 0.9324, 0.9329,  ..., 0.9332, 0.9319, 0.9332],\n",
       "          [0.9304, 0.9326, 0.9330,  ..., 0.9333, 0.9321, 0.9333],\n",
       "          ...,\n",
       "          [0.0816, 0.0871, 0.0880,  ..., 0.0869, 0.0845, 0.0869],\n",
       "          [0.5434, 0.5454, 0.5461,  ..., 0.5469, 0.5454, 0.5470],\n",
       "          [0.9849, 0.9871, 0.9874,  ..., 0.9875, 0.9863, 0.9875]]),\n",
       "  0),\n",
       " (tensor([[0.5760, 0.5760, 0.5757,  ..., 0.5816, 0.5828, 0.5807],\n",
       "          [0.6801, 0.6803, 0.6796,  ..., 0.6850, 0.6864, 0.6838],\n",
       "          [0.8722, 0.8727, 0.8716,  ..., 0.8766, 0.8781, 0.8752],\n",
       "          ...,\n",
       "          [0.9470, 0.9480, 0.9470,  ..., 0.9442, 0.9442, 0.9498],\n",
       "          [0.3378, 0.3389, 0.3389,  ..., 0.3351, 0.3342, 0.3444],\n",
       "          [0.5231, 0.5230, 0.5230,  ..., 0.5192, 0.5184, 0.5280]]),\n",
       "  1),\n",
       " (tensor([[0.6801, 0.6803, 0.6796,  ..., 0.6850, 0.6864, 0.6838],\n",
       "          [0.8722, 0.8727, 0.8716,  ..., 0.8766, 0.8781, 0.8752],\n",
       "          [0.0037, 0.0038, 0.0043,  ..., 0.0106, 0.0107, 0.0095],\n",
       "          ...,\n",
       "          [0.3378, 0.3389, 0.3389,  ..., 0.3351, 0.3342, 0.3444],\n",
       "          [0.5231, 0.5230, 0.5230,  ..., 0.5192, 0.5184, 0.5280],\n",
       "          [0.6750, 0.6756, 0.6751,  ..., 0.6703, 0.6701, 0.6793]]),\n",
       "  1),\n",
       " (tensor([[0.8722, 0.8727, 0.8716,  ..., 0.8766, 0.8781, 0.8752],\n",
       "          [0.0037, 0.0038, 0.0043,  ..., 0.0106, 0.0107, 0.0095],\n",
       "          [0.0536, 0.0534, 0.0541,  ..., 0.0636, 0.0636, 0.0614],\n",
       "          ...,\n",
       "          [0.5231, 0.5230, 0.5230,  ..., 0.5192, 0.5184, 0.5280],\n",
       "          [0.6750, 0.6756, 0.6751,  ..., 0.6703, 0.6701, 0.6793],\n",
       "          [0.8897, 0.8905, 0.8895,  ..., 0.8839, 0.8839, 0.8926]]),\n",
       "  1),\n",
       " (tensor([[0.0037, 0.0038, 0.0043,  ..., 0.0106, 0.0107, 0.0095],\n",
       "          [0.0536, 0.0534, 0.0541,  ..., 0.0636, 0.0636, 0.0614],\n",
       "          [0.1824, 0.1835, 0.1839,  ..., 0.1903, 0.1906, 0.1891],\n",
       "          ...,\n",
       "          [0.6750, 0.6756, 0.6751,  ..., 0.6703, 0.6701, 0.6793],\n",
       "          [0.8897, 0.8905, 0.8895,  ..., 0.8839, 0.8839, 0.8926],\n",
       "          [0.5670, 0.5673, 0.5671,  ..., 0.5630, 0.5627, 0.5715]]),\n",
       "  1),\n",
       " (tensor([[0.0054, 0.0052, 0.0059,  ..., 0.0159, 0.0159, 0.0135],\n",
       "          [0.1407, 0.1419, 0.1423,  ..., 0.1490, 0.1494, 0.1478],\n",
       "          [0.4303, 0.4300, 0.4298,  ..., 0.4401, 0.4399, 0.4358],\n",
       "          ...,\n",
       "          [0.8840, 0.8849, 0.8839,  ..., 0.8780, 0.8780, 0.8871],\n",
       "          [0.5449, 0.5453, 0.5450,  ..., 0.5408, 0.5404, 0.5497],\n",
       "          [0.7650, 0.7657, 0.7648,  ..., 0.7597, 0.7594, 0.7682]]),\n",
       "  1),\n",
       " (tensor([[0.6573, 0.6709, 0.6731,  ..., 0.6748, 0.6754, 0.6781],\n",
       "          [0.7430, 0.7569, 0.7585,  ..., 0.7596, 0.7607, 0.7632],\n",
       "          [0.4142, 0.4272, 0.4294,  ..., 0.4311, 0.4314, 0.4347],\n",
       "          ...,\n",
       "          [0.8933, 0.8750, 0.8766,  ..., 0.8775, 0.8786, 0.8813],\n",
       "          [0.9956, 0.9773, 0.9789,  ..., 0.9795, 0.9809, 0.9836],\n",
       "          [0.8758, 0.8575, 0.8592,  ..., 0.8600, 0.8614, 0.8630]]),\n",
       "  1),\n",
       " (tensor([[0.7430, 0.7569, 0.7585,  ..., 0.7596, 0.7607, 0.7632],\n",
       "          [0.4142, 0.4272, 0.4294,  ..., 0.4311, 0.4314, 0.4347],\n",
       "          [0.3673, 0.3806, 0.3840,  ..., 0.3851, 0.3859, 0.3884],\n",
       "          ...,\n",
       "          [0.9956, 0.9773, 0.9789,  ..., 0.9795, 0.9809, 0.9836],\n",
       "          [0.8758, 0.8575, 0.8592,  ..., 0.8600, 0.8614, 0.8630],\n",
       "          [0.8653, 0.8464, 0.8481,  ..., 0.8489, 0.8503, 0.8525]]),\n",
       "  1),\n",
       " (tensor([[0.4142, 0.4272, 0.4294,  ..., 0.4311, 0.4314, 0.4347],\n",
       "          [0.3673, 0.3806, 0.3840,  ..., 0.3851, 0.3859, 0.3884],\n",
       "          [0.4741, 0.4877, 0.4902,  ..., 0.4907, 0.4924, 0.4940],\n",
       "          ...,\n",
       "          [0.8758, 0.8575, 0.8592,  ..., 0.8600, 0.8614, 0.8630],\n",
       "          [0.8653, 0.8464, 0.8481,  ..., 0.8489, 0.8503, 0.8525],\n",
       "          [0.7968, 0.7779, 0.7801,  ..., 0.7840, 0.7865, 0.7871]]),\n",
       "  1),\n",
       " (tensor([[0.2916, 0.3022, 0.3048,  ..., 0.3057, 0.3063, 0.3083],\n",
       "          [0.3763, 0.3871, 0.3891,  ..., 0.3895, 0.3908, 0.3922],\n",
       "          [0.5876, 0.5986, 0.6001,  ..., 0.6010, 0.6019, 0.6037],\n",
       "          ...,\n",
       "          [0.6868, 0.6719, 0.6732,  ..., 0.6739, 0.6750, 0.6767],\n",
       "          [0.6325, 0.6175, 0.6193,  ..., 0.6224, 0.6243, 0.6248],\n",
       "          [0.9897, 0.9756, 0.9758,  ..., 0.9769, 0.9778, 0.9806]]),\n",
       "  1),\n",
       " (tensor([[0.3763, 0.3871, 0.3891,  ..., 0.3895, 0.3908, 0.3922],\n",
       "          [0.5876, 0.5986, 0.6001,  ..., 0.6010, 0.6019, 0.6037],\n",
       "          [0.5810, 0.5924, 0.5942,  ..., 0.5944, 0.5957, 0.5975],\n",
       "          ...,\n",
       "          [0.6325, 0.6175, 0.6193,  ..., 0.6224, 0.6243, 0.6248],\n",
       "          [0.9897, 0.9756, 0.9758,  ..., 0.9769, 0.9778, 0.9806],\n",
       "          [0.1342, 0.1175, 0.1195,  ..., 0.1206, 0.1215, 0.1224]]),\n",
       "  1),\n",
       " (tensor([[0.9623, 0.9734, 0.9639,  ..., 0.9722, 0.9675, 0.9727],\n",
       "          [0.7482, 0.7600, 0.7496,  ..., 0.7577, 0.7531, 0.7593],\n",
       "          [0.5431, 0.5554, 0.5440,  ..., 0.5526, 0.5483, 0.5545],\n",
       "          ...,\n",
       "          [0.4985, 0.5115, 0.5004,  ..., 0.5091, 0.5046, 0.5099],\n",
       "          [0.3824, 0.3952, 0.3840,  ..., 0.3928, 0.3888, 0.3940],\n",
       "          [0.4479, 0.4614, 0.4498,  ..., 0.4591, 0.4548, 0.4600]]),\n",
       "  1),\n",
       " (tensor([[0.7482, 0.7600, 0.7496,  ..., 0.7577, 0.7531, 0.7593],\n",
       "          [0.5431, 0.5554, 0.5440,  ..., 0.5526, 0.5483, 0.5545],\n",
       "          [0.9547, 0.9658, 0.9556,  ..., 0.9627, 0.9580, 0.9642],\n",
       "          ...,\n",
       "          [0.3824, 0.3952, 0.3840,  ..., 0.3928, 0.3888, 0.3940],\n",
       "          [0.4479, 0.4614, 0.4498,  ..., 0.4591, 0.4548, 0.4600],\n",
       "          [0.5606, 0.5732, 0.5623,  ..., 0.5709, 0.5666, 0.5716]]),\n",
       "  1),\n",
       " (tensor([[0.5431, 0.5554, 0.5440,  ..., 0.5526, 0.5483, 0.5545],\n",
       "          [0.9547, 0.9658, 0.9556,  ..., 0.9627, 0.9580, 0.9642],\n",
       "          [0.5379, 0.5502, 0.5393,  ..., 0.5483, 0.5443, 0.5500],\n",
       "          ...,\n",
       "          [0.4479, 0.4614, 0.4498,  ..., 0.4591, 0.4548, 0.4600],\n",
       "          [0.5606, 0.5732, 0.5623,  ..., 0.5709, 0.5666, 0.5716],\n",
       "          [0.5668, 0.5792, 0.5682,  ..., 0.5763, 0.5718, 0.5770]]),\n",
       "  1),\n",
       " (tensor([[0.9547, 0.9658, 0.9556,  ..., 0.9627, 0.9580, 0.9642],\n",
       "          [0.5379, 0.5502, 0.5393,  ..., 0.5483, 0.5443, 0.5500],\n",
       "          [0.9746, 0.9860, 0.9760,  ..., 0.9839, 0.9791, 0.9848],\n",
       "          ...,\n",
       "          [0.5606, 0.5732, 0.5623,  ..., 0.5709, 0.5666, 0.5716],\n",
       "          [0.5668, 0.5792, 0.5682,  ..., 0.5763, 0.5718, 0.5770],\n",
       "          [0.5628, 0.5751, 0.5642,  ..., 0.5725, 0.5685, 0.5735]]),\n",
       "  1),\n",
       " (tensor([[0.5379, 0.5502, 0.5393,  ..., 0.5483, 0.5443, 0.5500],\n",
       "          [0.9746, 0.9860, 0.9760,  ..., 0.9839, 0.9791, 0.9848],\n",
       "          [0.4446, 0.4569, 0.4460,  ..., 0.4545, 0.4500, 0.4560],\n",
       "          ...,\n",
       "          [0.5668, 0.5792, 0.5682,  ..., 0.5763, 0.5718, 0.5770],\n",
       "          [0.5628, 0.5751, 0.5642,  ..., 0.5725, 0.5685, 0.5735],\n",
       "          [0.5682, 0.5803, 0.5694,  ..., 0.5777, 0.5737, 0.5789]]),\n",
       "  1),\n",
       " (tensor([[0.9348, 0.9340, 0.9486,  ..., 0.9371, 0.9372, 0.9430],\n",
       "          [0.2420, 0.2428, 0.2572,  ..., 0.2471, 0.2480, 0.2536],\n",
       "          [0.8447, 0.8439, 0.8584,  ..., 0.8469, 0.8471, 0.8528],\n",
       "          ...,\n",
       "          [0.4102, 0.4102, 0.4256,  ..., 0.4144, 0.4149, 0.4210],\n",
       "          [0.5191, 0.5188, 0.5199,  ..., 0.5098, 0.5105, 0.5156],\n",
       "          [0.2396, 0.2408, 0.2426,  ..., 0.2328, 0.2333, 0.2386]]),\n",
       "  1),\n",
       " (tensor([[0.2420, 0.2428, 0.2572,  ..., 0.2471, 0.2480, 0.2536],\n",
       "          [0.8447, 0.8439, 0.8584,  ..., 0.8469, 0.8471, 0.8528],\n",
       "          [0.9830, 0.9817, 0.9970,  ..., 0.9851, 0.9849, 0.9910],\n",
       "          ...,\n",
       "          [0.5191, 0.5188, 0.5199,  ..., 0.5098, 0.5105, 0.5156],\n",
       "          [0.2396, 0.2408, 0.2426,  ..., 0.2328, 0.2333, 0.2386],\n",
       "          [0.4069, 0.4069, 0.4066,  ..., 0.3933, 0.3942, 0.4013]]),\n",
       "  1),\n",
       " (tensor([[0.8447, 0.8439, 0.8584,  ..., 0.8469, 0.8471, 0.8528],\n",
       "          [0.9830, 0.9817, 0.9970,  ..., 0.9851, 0.9849, 0.9910],\n",
       "          [0.6631, 0.6624, 0.6764,  ..., 0.6658, 0.6661, 0.6719],\n",
       "          ...,\n",
       "          [0.2396, 0.2408, 0.2426,  ..., 0.2328, 0.2333, 0.2386],\n",
       "          [0.4069, 0.4069, 0.4066,  ..., 0.3933, 0.3942, 0.4013],\n",
       "          [0.5580, 0.5579, 0.5595,  ..., 0.5483, 0.5489, 0.5545]]),\n",
       "  1),\n",
       " (tensor([[0.9830, 0.9817, 0.9970,  ..., 0.9851, 0.9849, 0.9910],\n",
       "          [0.6631, 0.6624, 0.6764,  ..., 0.6658, 0.6661, 0.6719],\n",
       "          [0.7620, 0.7611, 0.7755,  ..., 0.7646, 0.7648, 0.7702],\n",
       "          ...,\n",
       "          [0.4069, 0.4069, 0.4066,  ..., 0.3933, 0.3942, 0.4013],\n",
       "          [0.5580, 0.5579, 0.5595,  ..., 0.5483, 0.5489, 0.5545],\n",
       "          [0.9957, 0.9947, 0.9974,  ..., 0.9854, 0.9853, 0.9912]]),\n",
       "  1),\n",
       " (tensor([[0.6631, 0.6624, 0.6764,  ..., 0.6658, 0.6661, 0.6719],\n",
       "          [0.7620, 0.7611, 0.7755,  ..., 0.7646, 0.7648, 0.7702],\n",
       "          [0.3050, 0.3057, 0.3203,  ..., 0.3100, 0.3108, 0.3169],\n",
       "          ...,\n",
       "          [0.5580, 0.5579, 0.5595,  ..., 0.5483, 0.5489, 0.5545],\n",
       "          [0.9957, 0.9947, 0.9974,  ..., 0.9854, 0.9853, 0.9912],\n",
       "          [0.5327, 0.5324, 0.5340,  ..., 0.5230, 0.5236, 0.5289]]),\n",
       "  1),\n",
       " (tensor([[0.7524, 0.7553, 0.7539,  ..., 0.7507, 0.7508, 0.7535],\n",
       "          [0.8929, 0.8961, 0.8950,  ..., 0.8923, 0.8920, 0.8944],\n",
       "          [0.6673, 0.6703, 0.6687,  ..., 0.6662, 0.6659, 0.6685],\n",
       "          ...,\n",
       "          [0.9943, 0.9977, 0.9962,  ..., 0.9925, 0.9932, 0.9956],\n",
       "          [0.3489, 0.3528, 0.3516,  ..., 0.3498, 0.3487, 0.3511],\n",
       "          [0.6700, 0.6726, 0.6714,  ..., 0.6696, 0.6690, 0.6711]]),\n",
       "  1),\n",
       " (tensor([[0.8929, 0.8961, 0.8950,  ..., 0.8923, 0.8920, 0.8944],\n",
       "          [0.6673, 0.6703, 0.6687,  ..., 0.6662, 0.6659, 0.6685],\n",
       "          [0.9652, 0.9684, 0.9667,  ..., 0.9635, 0.9642, 0.9667],\n",
       "          ...,\n",
       "          [0.3489, 0.3528, 0.3516,  ..., 0.3498, 0.3487, 0.3511],\n",
       "          [0.6700, 0.6726, 0.6714,  ..., 0.6696, 0.6690, 0.6711],\n",
       "          [0.8419, 0.8455, 0.8438,  ..., 0.8407, 0.8410, 0.8436]]),\n",
       "  1),\n",
       " (tensor([[0.6673, 0.6703, 0.6687,  ..., 0.6662, 0.6659, 0.6685],\n",
       "          [0.9652, 0.9684, 0.9667,  ..., 0.9635, 0.9642, 0.9667],\n",
       "          [0.9901, 0.9931, 0.9917,  ..., 0.9888, 0.9891, 0.9917],\n",
       "          ...,\n",
       "          [0.6700, 0.6726, 0.6714,  ..., 0.6696, 0.6690, 0.6711],\n",
       "          [0.8419, 0.8455, 0.8438,  ..., 0.8407, 0.8410, 0.8436],\n",
       "          [0.8809, 0.8835, 0.8823,  ..., 0.8797, 0.8792, 0.8818]]),\n",
       "  1),\n",
       " (tensor([[0.9652, 0.9684, 0.9667,  ..., 0.9635, 0.9642, 0.9667],\n",
       "          [0.9901, 0.9931, 0.9917,  ..., 0.9888, 0.9891, 0.9917],\n",
       "          [0.9796, 0.9830, 0.9816,  ..., 0.9782, 0.9789, 0.9815],\n",
       "          ...,\n",
       "          [0.8419, 0.8455, 0.8438,  ..., 0.8407, 0.8410, 0.8436],\n",
       "          [0.8809, 0.8835, 0.8823,  ..., 0.8797, 0.8792, 0.8818],\n",
       "          [0.5673, 0.5704, 0.5691,  ..., 0.5660, 0.5662, 0.5690]]),\n",
       "  1),\n",
       " (tensor([[0.9611, 0.9640, 0.9626,  ..., 0.9599, 0.9602, 0.9626],\n",
       "          [0.9510, 0.9543, 0.9529,  ..., 0.9496, 0.9503, 0.9527],\n",
       "          [0.5411, 0.5431, 0.5424,  ..., 0.5398, 0.5391, 0.5415],\n",
       "          ...,\n",
       "          [0.8551, 0.8577, 0.8565,  ..., 0.8540, 0.8534, 0.8560],\n",
       "          [0.5507, 0.5537, 0.5525,  ..., 0.5495, 0.5496, 0.5523],\n",
       "          [0.9931, 0.9966, 0.9949,  ..., 0.9913, 0.9922, 0.9948]]),\n",
       "  1),\n",
       " (tensor([[0.2029, 0.2017, 0.2014,  ..., 0.2017, 0.2047, 0.2059],\n",
       "          [0.9700, 0.9682, 0.9685,  ..., 0.9687, 0.9715, 0.9732],\n",
       "          [0.7370, 0.7353, 0.7353,  ..., 0.7346, 0.7375, 0.7390],\n",
       "          ...,\n",
       "          [0.7671, 0.7688, 0.7688,  ..., 0.7683, 0.7715, 0.7698],\n",
       "          [0.0164, 0.0189, 0.0184,  ..., 0.0166, 0.0191, 0.0174],\n",
       "          [0.9531, 0.9546, 0.9548,  ..., 0.9544, 0.9576, 0.9551]]),\n",
       "  1),\n",
       " (tensor([[0.9700, 0.9682, 0.9685,  ..., 0.9687, 0.9715, 0.9732],\n",
       "          [0.7370, 0.7353, 0.7353,  ..., 0.7346, 0.7375, 0.7390],\n",
       "          [0.9628, 0.9615, 0.9618,  ..., 0.9620, 0.9650, 0.9663],\n",
       "          ...,\n",
       "          [0.0164, 0.0189, 0.0184,  ..., 0.0166, 0.0191, 0.0174],\n",
       "          [0.9531, 0.9546, 0.9548,  ..., 0.9544, 0.9576, 0.9551],\n",
       "          [0.2213, 0.2235, 0.2233,  ..., 0.2213, 0.2243, 0.2213]]),\n",
       "  1),\n",
       " (tensor([[0.7370, 0.7353, 0.7353,  ..., 0.7346, 0.7375, 0.7390],\n",
       "          [0.9628, 0.9615, 0.9618,  ..., 0.9620, 0.9650, 0.9663],\n",
       "          [0.9777, 0.9767, 0.9769,  ..., 0.9772, 0.9802, 0.9811],\n",
       "          ...,\n",
       "          [0.9531, 0.9546, 0.9548,  ..., 0.9544, 0.9576, 0.9551],\n",
       "          [0.2213, 0.2235, 0.2233,  ..., 0.2213, 0.2243, 0.2213],\n",
       "          [0.5249, 0.5272, 0.5274,  ..., 0.5254, 0.5282, 0.5254]]),\n",
       "  1),\n",
       " (tensor([[0.9644, 0.9632, 0.9634,  ..., 0.9637, 0.9665, 0.9677],\n",
       "          [0.9786, 0.9777, 0.9779,  ..., 0.9782, 0.9810, 0.9820],\n",
       "          [0.3383, 0.3376, 0.3374,  ..., 0.3376, 0.3405, 0.3405],\n",
       "          ...,\n",
       "          [0.2547, 0.2569, 0.2566,  ..., 0.2547, 0.2576, 0.2547],\n",
       "          [0.5453, 0.5475, 0.5477,  ..., 0.5458, 0.5484, 0.5458],\n",
       "          [0.0012, 0.0036, 0.0036,  ..., 0.0019, 0.0040, 0.0021]]),\n",
       "  1),\n",
       " (tensor([[0.9786, 0.9777, 0.9779,  ..., 0.9782, 0.9810, 0.9820],\n",
       "          [0.3383, 0.3376, 0.3374,  ..., 0.3376, 0.3405, 0.3405],\n",
       "          [0.7828, 0.7813, 0.7811,  ..., 0.7809, 0.7835, 0.7842],\n",
       "          ...,\n",
       "          [0.5453, 0.5475, 0.5477,  ..., 0.5458, 0.5484, 0.5458],\n",
       "          [0.0012, 0.0036, 0.0036,  ..., 0.0019, 0.0040, 0.0021],\n",
       "          [0.3637, 0.3661, 0.3656,  ..., 0.3642, 0.3673, 0.3644]]),\n",
       "  1),\n",
       " (tensor([[0.9911, 0.9932, 0.9853,  ..., 0.9924, 0.9853, 0.9895],\n",
       "          [0.9892, 0.9911, 0.9834,  ..., 0.9903, 0.9832, 0.9874],\n",
       "          [0.9411, 0.9429, 0.9350,  ..., 0.9418, 0.9347, 0.9387],\n",
       "          ...,\n",
       "          [0.5995, 0.6016, 0.5939,  ..., 0.6005, 0.5937, 0.5971],\n",
       "          [0.8134, 0.8153, 0.8087,  ..., 0.8145, 0.8076, 0.8116],\n",
       "          [0.2371, 0.2389, 0.2316,  ..., 0.2387, 0.2321, 0.2355]]),\n",
       "  1),\n",
       " (tensor([[0.9892, 0.9911, 0.9834,  ..., 0.9903, 0.9832, 0.9874],\n",
       "          [0.9411, 0.9429, 0.9350,  ..., 0.9418, 0.9347, 0.9387],\n",
       "          [0.9482, 0.9500, 0.9424,  ..., 0.9492, 0.9421, 0.9463],\n",
       "          ...,\n",
       "          [0.8134, 0.8153, 0.8087,  ..., 0.8145, 0.8076, 0.8116],\n",
       "          [0.2371, 0.2389, 0.2316,  ..., 0.2387, 0.2321, 0.2355],\n",
       "          [0.3176, 0.3197, 0.3126,  ..., 0.3189, 0.3124, 0.3158]]),\n",
       "  1),\n",
       " (tensor([[0.9411, 0.9429, 0.9350,  ..., 0.9418, 0.9347, 0.9387],\n",
       "          [0.9482, 0.9500, 0.9424,  ..., 0.9492, 0.9421, 0.9463],\n",
       "          [0.9913, 0.9932, 0.9858,  ..., 0.9926, 0.9853, 0.9895],\n",
       "          ...,\n",
       "          [0.2371, 0.2389, 0.2316,  ..., 0.2387, 0.2321, 0.2355],\n",
       "          [0.3176, 0.3197, 0.3126,  ..., 0.3189, 0.3124, 0.3158],\n",
       "          [0.4003, 0.4021, 0.3945,  ..., 0.4016, 0.3939, 0.3976]]),\n",
       "  1),\n",
       " (tensor([[0.9464, 0.9483, 0.9406,  ..., 0.9475, 0.9404, 0.9446],\n",
       "          [0.9895, 0.9913, 0.9840,  ..., 0.9908, 0.9835, 0.9877],\n",
       "          [0.6365, 0.6386, 0.6304,  ..., 0.6375, 0.6302, 0.6346],\n",
       "          ...,\n",
       "          [0.3170, 0.3191, 0.3121,  ..., 0.3184, 0.3118, 0.3152],\n",
       "          [0.3995, 0.4014, 0.3937,  ..., 0.4008, 0.3932, 0.3969],\n",
       "          [0.9924, 0.9942, 0.9874,  ..., 0.9937, 0.9869, 0.9905]]),\n",
       "  1),\n",
       " (tensor([[0.9895, 0.9913, 0.9840,  ..., 0.9908, 0.9835, 0.9877],\n",
       "          [0.6365, 0.6386, 0.6304,  ..., 0.6375, 0.6302, 0.6346],\n",
       "          [0.7124, 0.7142, 0.7066,  ..., 0.7134, 0.7066, 0.7105],\n",
       "          ...,\n",
       "          [0.3995, 0.4014, 0.3937,  ..., 0.4008, 0.3932, 0.3969],\n",
       "          [0.9924, 0.9942, 0.9874,  ..., 0.9937, 0.9869, 0.9905],\n",
       "          [0.9924, 0.9942, 0.9877,  ..., 0.9932, 0.9866, 0.9903]]),\n",
       "  1),\n",
       " (tensor([[0.8654, 0.8653, 0.8644,  ..., 0.8750, 0.8737, 0.8772],\n",
       "          [0.7616, 0.7616, 0.7607,  ..., 0.7708, 0.7694, 0.7729],\n",
       "          [0.7290, 0.7290, 0.7280,  ..., 0.7389, 0.7373, 0.7411],\n",
       "          ...,\n",
       "          [0.0018, 0.0030, 0.0029,  ..., 0.0054, 0.0039, 0.0056],\n",
       "          [0.0308, 0.0325, 0.0323,  ..., 0.0341, 0.0330, 0.0342],\n",
       "          [0.0476, 0.0496, 0.0496,  ..., 0.0517, 0.0506, 0.0519]]),\n",
       "  1),\n",
       " (tensor([[0.8103, 0.8143, 0.8145,  ..., 0.8094, 0.8086, 0.8135],\n",
       "          [0.9871, 0.9938, 0.9933,  ..., 0.9954, 0.9946, 0.9979],\n",
       "          [0.3239, 0.3301, 0.3301,  ..., 0.3309, 0.3306, 0.3323],\n",
       "          ...,\n",
       "          [0.6935, 0.7010, 0.7016,  ..., 0.7029, 0.7024, 0.7040],\n",
       "          [0.7085, 0.7163, 0.7166,  ..., 0.7182, 0.7179, 0.7190],\n",
       "          [0.7413, 0.7491, 0.7493,  ..., 0.7507, 0.7501, 0.7515]]),\n",
       "  1),\n",
       " (tensor([[0.9871, 0.9938, 0.9933,  ..., 0.9954, 0.9946, 0.9979],\n",
       "          [0.3239, 0.3301, 0.3301,  ..., 0.3309, 0.3306, 0.3323],\n",
       "          [0.9289, 0.9361, 0.9359,  ..., 0.9407, 0.9399, 0.9420],\n",
       "          ...,\n",
       "          [0.7085, 0.7163, 0.7166,  ..., 0.7182, 0.7179, 0.7190],\n",
       "          [0.7413, 0.7491, 0.7493,  ..., 0.7507, 0.7501, 0.7515],\n",
       "          [0.8323, 0.8398, 0.8400,  ..., 0.8417, 0.8414, 0.8427]]),\n",
       "  1),\n",
       " (tensor([[0.3242, 0.3304, 0.3304,  ..., 0.3312, 0.3309, 0.3325],\n",
       "          [0.9296, 0.9369, 0.9366,  ..., 0.9414, 0.9406, 0.9428],\n",
       "          [0.0175, 0.0242, 0.0242,  ..., 0.0282, 0.0277, 0.0287],\n",
       "          ...,\n",
       "          [0.7419, 0.7497, 0.7499,  ..., 0.7513, 0.7507, 0.7521],\n",
       "          [0.8329, 0.8405, 0.8407,  ..., 0.8423, 0.8421, 0.8434],\n",
       "          [0.8944, 0.9025, 0.9028,  ..., 0.9060, 0.9057, 0.9068]]),\n",
       "  1),\n",
       " (tensor([[0.9296, 0.9369, 0.9366,  ..., 0.9414, 0.9406, 0.9428],\n",
       "          [0.0175, 0.0242, 0.0242,  ..., 0.0282, 0.0277, 0.0287],\n",
       "          [0.6538, 0.6610, 0.6610,  ..., 0.6632, 0.6632, 0.6648],\n",
       "          ...,\n",
       "          [0.8329, 0.8405, 0.8407,  ..., 0.8423, 0.8421, 0.8434],\n",
       "          [0.8944, 0.9025, 0.9028,  ..., 0.9060, 0.9057, 0.9068],\n",
       "          [0.9447, 0.9530, 0.9535,  ..., 0.9522, 0.9517, 0.9522]]),\n",
       "  1),\n",
       " (tensor([[0.2194, 0.2247, 0.2247,  ..., 0.2279, 0.2275, 0.2283],\n",
       "          [0.7249, 0.7307, 0.7307,  ..., 0.7324, 0.7324, 0.7337],\n",
       "          [0.2732, 0.2789, 0.2789,  ..., 0.2819, 0.2817, 0.2821],\n",
       "          ...,\n",
       "          [0.9161, 0.9225, 0.9227,  ..., 0.9253, 0.9251, 0.9259],\n",
       "          [0.9560, 0.9627, 0.9631,  ..., 0.9620, 0.9616, 0.9620],\n",
       "          [0.0049, 0.0107, 0.0087,  ..., 0.0096, 0.0087, 0.0079]]),\n",
       "  1),\n",
       " (tensor([[0.0020, 0.0081, 0.0061,  ..., 0.0047, 0.0067, 0.0099],\n",
       "          [0.1919, 0.1953, 0.1980,  ..., 0.1924, 0.1969, 0.2007],\n",
       "          [0.4661, 0.4663, 0.4717,  ..., 0.4654, 0.4676, 0.4701],\n",
       "          ...,\n",
       "          [0.4047, 0.4063, 0.4110,  ..., 0.4036, 0.4090, 0.4155],\n",
       "          [0.5638, 0.5643, 0.5683,  ..., 0.5625, 0.5647, 0.5665],\n",
       "          [0.7546, 0.7551, 0.7600,  ..., 0.7542, 0.7553, 0.7573]]),\n",
       "  1),\n",
       " (tensor([[0.1758, 0.1792, 0.1820,  ..., 0.1763, 0.1808, 0.1847],\n",
       "          [0.4554, 0.4556, 0.4612,  ..., 0.4547, 0.4570, 0.4595],\n",
       "          [0.7000, 0.7004, 0.7057,  ..., 0.6997, 0.7016, 0.7034],\n",
       "          ...,\n",
       "          [0.5551, 0.5556, 0.5597,  ..., 0.5537, 0.5560, 0.5579],\n",
       "          [0.7497, 0.7502, 0.7552,  ..., 0.7493, 0.7504, 0.7525],\n",
       "          [0.8719, 0.8721, 0.8778,  ..., 0.8710, 0.8726, 0.8742]]),\n",
       "  1),\n",
       " (tensor([[0.4554, 0.4556, 0.4612,  ..., 0.4547, 0.4570, 0.4595],\n",
       "          [0.7000, 0.7004, 0.7057,  ..., 0.6997, 0.7016, 0.7034],\n",
       "          [0.8377, 0.8375, 0.8435,  ..., 0.8370, 0.8389, 0.8405],\n",
       "          ...,\n",
       "          [0.7497, 0.7502, 0.7552,  ..., 0.7493, 0.7504, 0.7525],\n",
       "          [0.8719, 0.8721, 0.8778,  ..., 0.8710, 0.8726, 0.8742],\n",
       "          [0.3667, 0.3681, 0.3722,  ..., 0.3660, 0.3708, 0.3768]]),\n",
       "  1),\n",
       " (tensor([[0.7000, 0.7004, 0.7057,  ..., 0.6997, 0.7016, 0.7034],\n",
       "          [0.8377, 0.8375, 0.8435,  ..., 0.8370, 0.8389, 0.8405],\n",
       "          [0.9924, 0.9922, 0.9986,  ..., 0.9922, 0.9934, 0.9950],\n",
       "          ...,\n",
       "          [0.8719, 0.8721, 0.8778,  ..., 0.8710, 0.8726, 0.8742],\n",
       "          [0.3667, 0.3681, 0.3722,  ..., 0.3660, 0.3708, 0.3768],\n",
       "          [0.2828, 0.2842, 0.2888,  ..., 0.2819, 0.2872, 0.2931]]),\n",
       "  1),\n",
       " (tensor([[0.8377, 0.8375, 0.8435,  ..., 0.8370, 0.8389, 0.8405],\n",
       "          [0.9924, 0.9922, 0.9986,  ..., 0.9922, 0.9934, 0.9950],\n",
       "          [0.1180, 0.1242, 0.1235,  ..., 0.1199, 0.1224, 0.1261],\n",
       "          ...,\n",
       "          [0.3667, 0.3681, 0.3722,  ..., 0.3660, 0.3708, 0.3768],\n",
       "          [0.2828, 0.2842, 0.2888,  ..., 0.2819, 0.2872, 0.2931],\n",
       "          [0.8160, 0.8162, 0.8215,  ..., 0.8153, 0.8166, 0.8185]]),\n",
       "  1),\n",
       " (tensor([[0.4661, 0.4639, 0.4639,  ..., 0.4641, 0.4669, 0.4669],\n",
       "          [0.7807, 0.7788, 0.7779,  ..., 0.7779, 0.7818, 0.7808],\n",
       "          [0.9793, 0.9807, 0.9804,  ..., 0.9777, 0.9813, 0.9824],\n",
       "          ...,\n",
       "          [0.9691, 0.9674, 0.9663,  ..., 0.9663, 0.9702, 0.9684],\n",
       "          [0.3487, 0.3487, 0.3484,  ..., 0.3470, 0.3509, 0.3514],\n",
       "          [0.5115, 0.5127, 0.5127,  ..., 0.5105, 0.5136, 0.5146]]),\n",
       "  1),\n",
       " (tensor([[0.7807, 0.7788, 0.7779,  ..., 0.7779, 0.7818, 0.7808],\n",
       "          [0.9793, 0.9807, 0.9804,  ..., 0.9777, 0.9813, 0.9824],\n",
       "          [0.4226, 0.4223, 0.4217,  ..., 0.4206, 0.4240, 0.4245],\n",
       "          ...,\n",
       "          [0.3487, 0.3487, 0.3484,  ..., 0.3470, 0.3509, 0.3514],\n",
       "          [0.5115, 0.5127, 0.5127,  ..., 0.5105, 0.5136, 0.5146],\n",
       "          [0.8667, 0.8650, 0.8645,  ..., 0.8622, 0.8656, 0.8655]]),\n",
       "  1),\n",
       " (tensor([[0.9793, 0.9807, 0.9804,  ..., 0.9777, 0.9813, 0.9824],\n",
       "          [0.4226, 0.4223, 0.4217,  ..., 0.4206, 0.4240, 0.4245],\n",
       "          [0.6189, 0.6168, 0.6164,  ..., 0.6162, 0.6200, 0.6192],\n",
       "          ...,\n",
       "          [0.5115, 0.5127, 0.5127,  ..., 0.5105, 0.5136, 0.5146],\n",
       "          [0.8667, 0.8650, 0.8645,  ..., 0.8622, 0.8656, 0.8655],\n",
       "          [0.6382, 0.6360, 0.6355,  ..., 0.6352, 0.6388, 0.6383]]),\n",
       "  1),\n",
       " (tensor([[0.4226, 0.4223, 0.4217,  ..., 0.4206, 0.4240, 0.4245],\n",
       "          [0.6189, 0.6168, 0.6164,  ..., 0.6162, 0.6200, 0.6192],\n",
       "          [0.9886, 0.9874, 0.9866,  ..., 0.9858, 0.9896, 0.9882],\n",
       "          ...,\n",
       "          [0.8667, 0.8650, 0.8645,  ..., 0.8622, 0.8656, 0.8655],\n",
       "          [0.6382, 0.6360, 0.6355,  ..., 0.6352, 0.6388, 0.6383],\n",
       "          [0.8828, 0.8812, 0.8803,  ..., 0.8801, 0.8837, 0.8822]]),\n",
       "  1),\n",
       " (tensor([[0.6189, 0.6168, 0.6164,  ..., 0.6162, 0.6200, 0.6192],\n",
       "          [0.9886, 0.9874, 0.9866,  ..., 0.9858, 0.9896, 0.9882],\n",
       "          [0.0055, 0.0067, 0.0067,  ..., 0.0050, 0.0081, 0.0090],\n",
       "          ...,\n",
       "          [0.6382, 0.6360, 0.6355,  ..., 0.6352, 0.6388, 0.6383],\n",
       "          [0.8828, 0.8812, 0.8803,  ..., 0.8801, 0.8837, 0.8822],\n",
       "          [0.7607, 0.7588, 0.7578,  ..., 0.7578, 0.7612, 0.7602]]),\n",
       "  1),\n",
       " (tensor([[0.4884, 0.4872, 0.4874,  ..., 0.4907, 0.4907, 0.4897],\n",
       "          [0.5306, 0.5306, 0.5303,  ..., 0.5351, 0.5355, 0.5329],\n",
       "          [0.0041, 0.0042, 0.0054,  ..., 0.0255, 0.0273, 0.0241],\n",
       "          ...,\n",
       "          [0.9085, 0.9094, 0.9082,  ..., 0.9127, 0.9134, 0.9105],\n",
       "          [0.2953, 0.2934, 0.2957,  ..., 0.2985, 0.2997, 0.2956],\n",
       "          [0.8337, 0.8320, 0.8318,  ..., 0.8352, 0.8351, 0.8340]]),\n",
       "  1),\n",
       " (tensor([[0.5306, 0.5306, 0.5303,  ..., 0.5351, 0.5355, 0.5329],\n",
       "          [0.0041, 0.0042, 0.0054,  ..., 0.0255, 0.0273, 0.0241],\n",
       "          [0.8351, 0.8330, 0.8332,  ..., 0.8367, 0.8362, 0.8361],\n",
       "          ...,\n",
       "          [0.2953, 0.2934, 0.2957,  ..., 0.2985, 0.2997, 0.2956],\n",
       "          [0.8337, 0.8320, 0.8318,  ..., 0.8352, 0.8351, 0.8340],\n",
       "          [0.6526, 0.6507, 0.6514,  ..., 0.6542, 0.6548, 0.6523]]),\n",
       "  1),\n",
       " (tensor([[0.0041, 0.0042, 0.0054,  ..., 0.0255, 0.0273, 0.0241],\n",
       "          [0.8351, 0.8330, 0.8332,  ..., 0.8367, 0.8362, 0.8361],\n",
       "          [0.8577, 0.8556, 0.8556,  ..., 0.8593, 0.8589, 0.8583],\n",
       "          ...,\n",
       "          [0.8337, 0.8320, 0.8318,  ..., 0.8352, 0.8351, 0.8340],\n",
       "          [0.6526, 0.6507, 0.6514,  ..., 0.6542, 0.6548, 0.6523],\n",
       "          [0.7308, 0.7294, 0.7292,  ..., 0.7323, 0.7322, 0.7311]]),\n",
       "  1),\n",
       " (tensor([[0.7786, 0.7758, 0.7760,  ..., 0.7807, 0.7801, 0.7799],\n",
       "          [0.8089, 0.8062, 0.8062,  ..., 0.8111, 0.8105, 0.8097],\n",
       "          [0.8636, 0.8609, 0.8607,  ..., 0.8656, 0.8656, 0.8636],\n",
       "          ...,\n",
       "          [0.5336, 0.5311, 0.5320,  ..., 0.5358, 0.5365, 0.5332],\n",
       "          [0.6386, 0.6367, 0.6365,  ..., 0.6406, 0.6404, 0.6390],\n",
       "          [0.7891, 0.7868, 0.7866,  ..., 0.7911, 0.7911, 0.7899]]),\n",
       "  1),\n",
       " (tensor([[0.8089, 0.8062, 0.8062,  ..., 0.8111, 0.8105, 0.8097],\n",
       "          [0.8636, 0.8609, 0.8607,  ..., 0.8656, 0.8656, 0.8636],\n",
       "          [0.5865, 0.5842, 0.5838,  ..., 0.5885, 0.5881, 0.5873],\n",
       "          ...,\n",
       "          [0.6386, 0.6367, 0.6365,  ..., 0.6406, 0.6404, 0.6390],\n",
       "          [0.7891, 0.7868, 0.7866,  ..., 0.7911, 0.7911, 0.7899],\n",
       "          [0.7374, 0.7353, 0.7349,  ..., 0.7394, 0.7392, 0.7378]]),\n",
       "  1),\n",
       " (tensor([[0.3290, 0.3150, 0.3274,  ..., 0.3281, 0.3238, 0.3705],\n",
       "          [0.5811, 0.5646, 0.5766,  ..., 0.5768, 0.5732, 0.6211],\n",
       "          [0.8361, 0.8195, 0.8315,  ..., 0.8305, 0.8277, 0.8752],\n",
       "          ...,\n",
       "          [0.5073, 0.5105, 0.5067,  ..., 0.5222, 0.5185, 0.5632],\n",
       "          [0.0917, 0.0979, 0.0944,  ..., 0.1123, 0.1078, 0.1545],\n",
       "          [0.2382, 0.2429, 0.2392,  ..., 0.2545, 0.2511, 0.2950]]),\n",
       "  1),\n",
       " (tensor([[0.6662, 0.6531, 0.6626,  ..., 0.6628, 0.6600, 0.6981],\n",
       "          [0.8694, 0.8562, 0.8657,  ..., 0.8650, 0.8627, 0.9006],\n",
       "          [0.2972, 0.2803, 0.2946,  ..., 0.2961, 0.2914, 0.3405],\n",
       "          ...,\n",
       "          [0.2763, 0.2812, 0.2784,  ..., 0.2927, 0.2891, 0.3263],\n",
       "          [0.3930, 0.3968, 0.3938,  ..., 0.4060, 0.4033, 0.4383],\n",
       "          [0.0199, 0.0247, 0.0229,  ..., 0.0339, 0.0311, 0.0601]]),\n",
       "  1),\n",
       " (tensor([[0.8694, 0.8562, 0.8657,  ..., 0.8650, 0.8627, 0.9006],\n",
       "          [0.2972, 0.2803, 0.2946,  ..., 0.2961, 0.2914, 0.3405],\n",
       "          [0.6226, 0.6105, 0.6198,  ..., 0.6195, 0.6168, 0.6537],\n",
       "          ...,\n",
       "          [0.3930, 0.3968, 0.3938,  ..., 0.4060, 0.4033, 0.4383],\n",
       "          [0.0199, 0.0247, 0.0229,  ..., 0.0339, 0.0311, 0.0601],\n",
       "          [0.2543, 0.2602, 0.2573,  ..., 0.2723, 0.2686, 0.3134]]),\n",
       "  1),\n",
       " (tensor([[0.2972, 0.2803, 0.2946,  ..., 0.2961, 0.2914, 0.3405],\n",
       "          [0.6226, 0.6105, 0.6198,  ..., 0.6195, 0.6168, 0.6537],\n",
       "          [0.2611, 0.2461, 0.2604,  ..., 0.2619, 0.2571, 0.3040],\n",
       "          ...,\n",
       "          [0.0199, 0.0247, 0.0229,  ..., 0.0339, 0.0311, 0.0601],\n",
       "          [0.2543, 0.2602, 0.2573,  ..., 0.2723, 0.2686, 0.3134],\n",
       "          [0.5913, 0.5957, 0.5922,  ..., 0.6058, 0.6025, 0.6387]]),\n",
       "  1),\n",
       " (tensor([[0.6170, 0.6050, 0.6142,  ..., 0.6139, 0.6112, 0.6478],\n",
       "          [0.2588, 0.2439, 0.2580,  ..., 0.2595, 0.2548, 0.3013],\n",
       "          [0.6207, 0.6090, 0.6186,  ..., 0.6177, 0.6155, 0.6521],\n",
       "          ...,\n",
       "          [0.2520, 0.2579, 0.2549,  ..., 0.2698, 0.2662, 0.3106],\n",
       "          [0.5860, 0.5903, 0.5869,  ..., 0.6003, 0.5971, 0.6329],\n",
       "          [0.9220, 0.9259, 0.9227,  ..., 0.9335, 0.9315, 0.9650]]),\n",
       "  1),\n",
       " (tensor([[0.8489, 0.8481, 0.8456,  ..., 0.8463, 0.8469, 0.8473],\n",
       "          [0.8599, 0.8588, 0.8568,  ..., 0.8571, 0.8576, 0.8587],\n",
       "          [0.6940, 0.6931, 0.6902,  ..., 0.6914, 0.6920, 0.6919],\n",
       "          ...,\n",
       "          [0.0080, 0.0144, 0.0099,  ..., 0.0145, 0.0134, 0.0111],\n",
       "          [0.0515, 0.0579, 0.0539,  ..., 0.0606, 0.0603, 0.0576],\n",
       "          [0.2707, 0.2732, 0.2699,  ..., 0.2728, 0.2728, 0.2716]]),\n",
       "  1),\n",
       " (tensor([[0.8599, 0.8588, 0.8568,  ..., 0.8571, 0.8576, 0.8587],\n",
       "          [0.6940, 0.6931, 0.6902,  ..., 0.6914, 0.6920, 0.6919],\n",
       "          [0.8020, 0.8009, 0.7984,  ..., 0.7993, 0.7996, 0.7999],\n",
       "          ...,\n",
       "          [0.0515, 0.0579, 0.0539,  ..., 0.0606, 0.0603, 0.0576],\n",
       "          [0.2707, 0.2732, 0.2699,  ..., 0.2728, 0.2728, 0.2716],\n",
       "          [0.3943, 0.3941, 0.3901,  ..., 0.3929, 0.3924, 0.3917]]),\n",
       "  1),\n",
       " (tensor([[0.6940, 0.6931, 0.6902,  ..., 0.6914, 0.6920, 0.6919],\n",
       "          [0.8020, 0.8009, 0.7984,  ..., 0.7993, 0.7996, 0.7999],\n",
       "          [0.4688, 0.4682, 0.4639,  ..., 0.4667, 0.4662, 0.4655],\n",
       "          ...,\n",
       "          [0.2707, 0.2732, 0.2699,  ..., 0.2728, 0.2728, 0.2716],\n",
       "          [0.3943, 0.3941, 0.3901,  ..., 0.3929, 0.3924, 0.3917],\n",
       "          [0.4972, 0.4968, 0.4933,  ..., 0.4960, 0.4955, 0.4953]]),\n",
       "  1),\n",
       " (tensor([[0.8020, 0.8009, 0.7984,  ..., 0.7993, 0.7996, 0.7999],\n",
       "          [0.4688, 0.4682, 0.4639,  ..., 0.4667, 0.4662, 0.4655],\n",
       "          [0.6918, 0.6911, 0.6885,  ..., 0.6893, 0.6900, 0.6900],\n",
       "          ...,\n",
       "          [0.3943, 0.3941, 0.3901,  ..., 0.3929, 0.3924, 0.3917],\n",
       "          [0.4972, 0.4968, 0.4933,  ..., 0.4960, 0.4955, 0.4953],\n",
       "          [0.6579, 0.6574, 0.6546,  ..., 0.6556, 0.6560, 0.6558]]),\n",
       "  1),\n",
       " (tensor([[0.4688, 0.4682, 0.4639,  ..., 0.4667, 0.4662, 0.4655],\n",
       "          [0.6918, 0.6911, 0.6885,  ..., 0.6893, 0.6900, 0.6900],\n",
       "          [0.5448, 0.5441, 0.5409,  ..., 0.5430, 0.5428, 0.5423],\n",
       "          ...,\n",
       "          [0.4972, 0.4968, 0.4933,  ..., 0.4960, 0.4955, 0.4953],\n",
       "          [0.6579, 0.6574, 0.6546,  ..., 0.6556, 0.6560, 0.6558],\n",
       "          [0.6705, 0.6700, 0.6671,  ..., 0.6682, 0.6686, 0.6684]]),\n",
       "  1)]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(data)*0.8)\n",
    "test_size = len(data) - train_size\n",
    "train_set, test_set = torch.utils.data.random_split(data,[train_size, test_size])\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testa förhållandet 0 / 1 i train och test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5704113924050633\n",
      "0.4295886075949367\n"
     ]
    }
   ],
   "source": [
    "val_0 = 0\n",
    "val_1 = 0\n",
    "for i in range(len(train_set)):\n",
    "    if train_set[i][1] == 0:\n",
    "        val_0 += 1\n",
    "    if train_set[i][1] == 1:\n",
    "        val_1 += 1\n",
    "print((val_0)/(val_0+val_1))\n",
    "print((val_1)/(val_0+val_1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5993690851735016\n",
      "0.40063091482649843\n"
     ]
    }
   ],
   "source": [
    "val_0 = 0\n",
    "val_1 = 0\n",
    "for i in range(len(test_set)):\n",
    "    if test_set[i][1] == 0:\n",
    "        val_0 += 1\n",
    "    if test_set[i][1] == 1:\n",
    "        val_1 += 1\n",
    "print((val_0)/(val_0+val_1))\n",
    "print((val_1)/(val_0+val_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 20, 108])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for (X_train,y_train) in train_loader:\n",
    "    break\n",
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsample  = parameters[\"Sub sample\"]\n",
    "subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNMultilayerperceptron(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size=(subsample*108),output_size=2, layers=[220,84]):  # 120, 84\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, layers[0])\n",
    "        self.fc2 = nn.Linear(layers[0], layers[1])\n",
    "        self.fc2b = nn.Linear(layers[1], 500)\n",
    "        self.fc2c = nn.Linear(500, layers[1])\n",
    "        self.fc2d = nn.Linear(layers[1], layers[1])\n",
    "        self.fc3 = nn.Linear(layers[1], output_size)\n",
    "\n",
    "    def forward(self,X):\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = F.relu(self.fc2(X))\n",
    "        X = F.relu(self.fc2b(X))\n",
    "        X = F.relu(self.fc2c(X))\n",
    "        X = F.relu(self.fc2d(X))\n",
    "        X = self.fc3(X)\n",
    "\n",
    "        return F.log_softmax(X,dim=1) # PGA multiclass classification\n",
    "        #return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ANNMultilayerperceptron(\n",
       "  (fc1): Linear(in_features=2160, out_features=220, bias=True)\n",
       "  (fc2): Linear(in_features=220, out_features=84, bias=True)\n",
       "  (fc2b): Linear(in_features=84, out_features=500, bias=True)\n",
       "  (fc2c): Linear(in_features=500, out_features=84, bias=True)\n",
       "  (fc2d): Linear(in_features=84, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ANNMultilayerperceptron()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "#optimizer = torch.optim.Adadelta(model.parameters(), lr=0.001)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch: 30 Train Loss: 0.6794919967651367 Validation Loss: 0.6994456648826599\n",
      "Epoch 1 Batch: 30 Train Loss: 0.6772847175598145 Validation Loss: 0.7093688249588013\n",
      "Epoch 2 Batch: 30 Train Loss: 0.617722749710083 Validation Loss: 0.6706859469413757\n",
      "Epoch 3 Batch: 30 Train Loss: 0.6888092756271362 Validation Loss: 0.6885554790496826\n",
      "Epoch 4 Batch: 30 Train Loss: 0.6525634527206421 Validation Loss: 0.6534345746040344\n",
      "Epoch 5 Batch: 30 Train Loss: 0.6497920751571655 Validation Loss: 0.6857483983039856\n",
      "Epoch 6 Batch: 30 Train Loss: 0.6421738862991333 Validation Loss: 0.6731719374656677\n",
      "Epoch 7 Batch: 30 Train Loss: 0.6856144666671753 Validation Loss: 0.6971727609634399\n",
      "Epoch 8 Batch: 30 Train Loss: 0.6746363639831543 Validation Loss: 0.6442509293556213\n",
      "Epoch 9 Batch: 30 Train Loss: 0.6740750074386597 Validation Loss: 0.6643325090408325\n",
      "Epoch 10 Batch: 30 Train Loss: 0.656894862651825 Validation Loss: 0.6479921340942383\n",
      "Epoch 11 Batch: 30 Train Loss: 0.7626415491104126 Validation Loss: 0.6882886290550232\n",
      "Epoch 12 Batch: 30 Train Loss: 0.6613313555717468 Validation Loss: 0.7354711890220642\n",
      "Epoch 13 Batch: 30 Train Loss: 0.7092550992965698 Validation Loss: 0.7150411605834961\n",
      "Epoch 14 Batch: 30 Train Loss: 0.6964892148971558 Validation Loss: 0.6998435258865356\n",
      "Epoch 15 Batch: 30 Train Loss: 0.5922873616218567 Validation Loss: 0.681528627872467\n",
      "Epoch 16 Batch: 30 Train Loss: 0.7657670378684998 Validation Loss: 0.6380305886268616\n",
      "Epoch 17 Batch: 30 Train Loss: 0.6674069166183472 Validation Loss: 0.7527679800987244\n",
      "Epoch 18 Batch: 30 Train Loss: 0.6519725918769836 Validation Loss: 0.689576268196106\n",
      "Epoch 19 Batch: 30 Train Loss: 0.6788312792778015 Validation Loss: 0.6453567743301392\n",
      "Epoch 20 Batch: 30 Train Loss: 0.6807839870452881 Validation Loss: 0.6656988859176636\n",
      "Epoch 21 Batch: 30 Train Loss: 0.652861475944519 Validation Loss: 0.6340011358261108\n",
      "Epoch 22 Batch: 30 Train Loss: 0.6479175090789795 Validation Loss: 0.7107354402542114\n",
      "Epoch 23 Batch: 30 Train Loss: 0.7359852194786072 Validation Loss: 0.6389093399047852\n",
      "Epoch 24 Batch: 30 Train Loss: 0.6395429372787476 Validation Loss: 0.6255561709403992\n",
      "Epoch 25 Batch: 30 Train Loss: 0.7175212502479553 Validation Loss: 0.6927878260612488\n",
      "Epoch 26 Batch: 30 Train Loss: 0.6941879391670227 Validation Loss: 0.6470478177070618\n",
      "Epoch 27 Batch: 30 Train Loss: 0.786634087562561 Validation Loss: 0.6641479134559631\n",
      "Epoch 28 Batch: 30 Train Loss: 0.6693229079246521 Validation Loss: 0.57418292760849\n",
      "Epoch 29 Batch: 30 Train Loss: 0.6862976551055908 Validation Loss: 0.5944721698760986\n",
      "Epoch 30 Batch: 30 Train Loss: 0.7425321340560913 Validation Loss: 0.6388837099075317\n",
      "Epoch 31 Batch: 30 Train Loss: 0.7466496825218201 Validation Loss: 0.650232195854187\n",
      "Epoch 32 Batch: 30 Train Loss: 0.6164660453796387 Validation Loss: 0.7757408022880554\n",
      "Epoch 33 Batch: 30 Train Loss: 0.6601477861404419 Validation Loss: 0.5249054431915283\n",
      "Epoch 34 Batch: 30 Train Loss: 0.5826521515846252 Validation Loss: 0.78062903881073\n",
      "Epoch 35 Batch: 30 Train Loss: 0.6580930352210999 Validation Loss: 0.6121169328689575\n",
      "Epoch 36 Batch: 30 Train Loss: 0.6082733869552612 Validation Loss: 0.618550181388855\n",
      "Epoch 37 Batch: 30 Train Loss: 0.704116940498352 Validation Loss: 0.6263518333435059\n",
      "Epoch 38 Batch: 30 Train Loss: 0.7424217462539673 Validation Loss: 0.9382880330085754\n",
      "Epoch 39 Batch: 30 Train Loss: 0.5951232314109802 Validation Loss: 0.9772494435310364\n",
      "Epoch 40 Batch: 30 Train Loss: 0.5621435046195984 Validation Loss: 0.8046970367431641\n",
      "Epoch 41 Batch: 30 Train Loss: 0.6295803189277649 Validation Loss: 0.6220993995666504\n",
      "Epoch 42 Batch: 30 Train Loss: 0.5773888230323792 Validation Loss: 0.7568028569221497\n",
      "Epoch 43 Batch: 30 Train Loss: 0.6435879468917847 Validation Loss: 0.9311099052429199\n",
      "Epoch 44 Batch: 30 Train Loss: 0.6228111386299133 Validation Loss: 0.515373945236206\n",
      "Epoch 45 Batch: 30 Train Loss: 0.5773141980171204 Validation Loss: 0.9526052474975586\n",
      "Epoch 46 Batch: 30 Train Loss: 0.3887419104576111 Validation Loss: 0.8870890736579895\n",
      "Epoch 47 Batch: 30 Train Loss: 0.4024847149848938 Validation Loss: 0.9886364936828613\n",
      "Epoch 48 Batch: 30 Train Loss: 0.5050976872444153 Validation Loss: 0.8165085911750793\n",
      "Epoch 49 Batch: 30 Train Loss: 0.5238854289054871 Validation Loss: 0.7715262770652771\n",
      "Epoch 50 Batch: 30 Train Loss: 0.4698777198791504 Validation Loss: 0.4944602847099304\n",
      "Epoch 51 Batch: 30 Train Loss: 0.8797345161437988 Validation Loss: 0.6767860651016235\n",
      "Epoch 52 Batch: 30 Train Loss: 0.8192316889762878 Validation Loss: 0.8314531445503235\n",
      "Epoch 53 Batch: 30 Train Loss: 0.7509186863899231 Validation Loss: 0.6923233270645142\n",
      "Epoch 54 Batch: 30 Train Loss: 0.5085448622703552 Validation Loss: 1.1827455759048462\n",
      "Epoch 55 Batch: 30 Train Loss: 0.662589430809021 Validation Loss: 0.7995213866233826\n",
      "Epoch 56 Batch: 30 Train Loss: 0.5716320872306824 Validation Loss: 0.9616571664810181\n",
      "Epoch 57 Batch: 30 Train Loss: 0.5225869417190552 Validation Loss: 1.3550288677215576\n",
      "Epoch 58 Batch: 30 Train Loss: 0.5939041376113892 Validation Loss: 1.454756736755371\n",
      "Epoch 59 Batch: 30 Train Loss: 0.3958209156990051 Validation Loss: 0.9852707982063293\n",
      "Epoch 60 Batch: 30 Train Loss: 0.46632975339889526 Validation Loss: 0.7428401708602905\n",
      "Epoch 61 Batch: 30 Train Loss: 0.4586523175239563 Validation Loss: 1.2273166179656982\n",
      "Epoch 62 Batch: 30 Train Loss: 0.3435449004173279 Validation Loss: 0.446050226688385\n",
      "Epoch 63 Batch: 30 Train Loss: 0.40569692850112915 Validation Loss: 1.0612046718597412\n",
      "Epoch 64 Batch: 30 Train Loss: 0.7354196310043335 Validation Loss: 0.5842443704605103\n",
      "Epoch 65 Batch: 30 Train Loss: 0.4379780888557434 Validation Loss: 0.8909990191459656\n",
      "Epoch 66 Batch: 30 Train Loss: 0.46795162558555603 Validation Loss: 1.0186057090759277\n",
      "Epoch 67 Batch: 30 Train Loss: 0.47356709837913513 Validation Loss: 0.5149962306022644\n",
      "Epoch 68 Batch: 30 Train Loss: 0.3459315598011017 Validation Loss: 1.4208210706710815\n",
      "Epoch 69 Batch: 30 Train Loss: 0.5204538106918335 Validation Loss: 1.3406809568405151\n",
      "Epoch 70 Batch: 30 Train Loss: 0.5396114587783813 Validation Loss: 1.4644948244094849\n",
      "Epoch 71 Batch: 30 Train Loss: 0.27521759271621704 Validation Loss: 1.4615461826324463\n",
      "Epoch 72 Batch: 30 Train Loss: 0.25194114446640015 Validation Loss: 1.9422054290771484\n",
      "Epoch 73 Batch: 30 Train Loss: 0.2740511894226074 Validation Loss: 3.7316277027130127\n",
      "Epoch 74 Batch: 30 Train Loss: 0.14654074609279633 Validation Loss: 1.0722485780715942\n",
      "Epoch 75 Batch: 30 Train Loss: 0.2681581974029541 Validation Loss: 3.699676036834717\n",
      "Epoch 76 Batch: 30 Train Loss: 0.3146602511405945 Validation Loss: 0.9939922094345093\n",
      "Epoch 77 Batch: 30 Train Loss: 0.4106994569301605 Validation Loss: 0.6618316769599915\n",
      "Epoch 78 Batch: 30 Train Loss: 0.20679748058319092 Validation Loss: 1.1342777013778687\n",
      "Epoch 79 Batch: 30 Train Loss: 0.483377605676651 Validation Loss: 1.235187292098999\n",
      "Epoch 80 Batch: 30 Train Loss: 0.35702261328697205 Validation Loss: 1.7180989980697632\n",
      "Epoch 81 Batch: 30 Train Loss: 0.39694130420684814 Validation Loss: 1.7136424779891968\n",
      "Epoch 82 Batch: 30 Train Loss: 0.21706096827983856 Validation Loss: 3.7118258476257324\n",
      "Epoch 83 Batch: 30 Train Loss: 0.3337705731391907 Validation Loss: 2.678947925567627\n",
      "Epoch 84 Batch: 30 Train Loss: 0.1992100030183792 Validation Loss: 0.45444798469543457\n",
      "Epoch 85 Batch: 30 Train Loss: 0.24741026759147644 Validation Loss: 2.1179332733154297\n",
      "Epoch 86 Batch: 30 Train Loss: 0.19865559041500092 Validation Loss: 2.7030704021453857\n",
      "Epoch 87 Batch: 30 Train Loss: 0.030365850776433945 Validation Loss: 7.038081169128418\n",
      "Epoch 88 Batch: 30 Train Loss: 0.11935975402593613 Validation Loss: 2.6509463787078857\n",
      "Epoch 89 Batch: 30 Train Loss: 0.06297533214092255 Validation Loss: 0.3714437186717987\n",
      "Epoch 90 Batch: 30 Train Loss: 0.15416011214256287 Validation Loss: 1.336220383644104\n",
      "Epoch 91 Batch: 30 Train Loss: 0.5379770398139954 Validation Loss: 3.716435670852661\n",
      "Epoch 92 Batch: 30 Train Loss: 0.21608397364616394 Validation Loss: 3.2187552452087402\n",
      "Epoch 93 Batch: 30 Train Loss: 0.30867135524749756 Validation Loss: 2.8033370971679688\n",
      "Epoch 94 Batch: 30 Train Loss: 0.5719924569129944 Validation Loss: 0.7426167726516724\n",
      "Epoch 95 Batch: 30 Train Loss: 0.07555200159549713 Validation Loss: 4.779496669769287\n",
      "Epoch 96 Batch: 30 Train Loss: 0.28907033801078796 Validation Loss: 2.292048931121826\n",
      "Epoch 97 Batch: 30 Train Loss: 0.053603045642375946 Validation Loss: 3.351436138153076\n",
      "Epoch 98 Batch: 30 Train Loss: 0.14981719851493835 Validation Loss: 3.8753151893615723\n",
      "Epoch 99 Batch: 30 Train Loss: 0.206778883934021 Validation Loss: 4.206995964050293\n",
      "Epoch 100 Batch: 30 Train Loss: 0.1368614137172699 Validation Loss: 5.703990936279297\n",
      "Epoch 101 Batch: 30 Train Loss: 0.027820194140076637 Validation Loss: 1.6085233688354492\n",
      "Epoch 102 Batch: 30 Train Loss: 0.021328339353203773 Validation Loss: 1.8708336353302002\n",
      "Epoch 103 Batch: 30 Train Loss: 0.0285194031894207 Validation Loss: 5.858870506286621\n",
      "Epoch 104 Batch: 30 Train Loss: 0.030652333050966263 Validation Loss: 1.7452707290649414\n",
      "Epoch 105 Batch: 30 Train Loss: 0.014358818531036377 Validation Loss: 2.8629629611968994\n",
      "Epoch 106 Batch: 30 Train Loss: 0.03383391723036766 Validation Loss: 3.121432304382324\n",
      "Epoch 107 Batch: 30 Train Loss: 0.062328070402145386 Validation Loss: 1.6540863513946533\n",
      "Epoch 108 Batch: 30 Train Loss: 0.03208485618233681 Validation Loss: 2.394639253616333\n",
      "Epoch 109 Batch: 30 Train Loss: 0.07076488435268402 Validation Loss: 2.476757049560547\n",
      "Epoch 110 Batch: 30 Train Loss: 0.005218648351728916 Validation Loss: 5.202956676483154\n",
      "Epoch 111 Batch: 30 Train Loss: 0.08510349690914154 Validation Loss: 6.953624725341797\n",
      "Epoch 112 Batch: 30 Train Loss: 0.0776946023106575 Validation Loss: 4.164930820465088\n",
      "Epoch 113 Batch: 30 Train Loss: 0.0067066652700304985 Validation Loss: 4.236785411834717\n",
      "Epoch 114 Batch: 30 Train Loss: 0.017094101756811142 Validation Loss: 2.2563703060150146\n",
      "Epoch 115 Batch: 30 Train Loss: 0.0017637800192460418 Validation Loss: 1.671216607093811\n",
      "Epoch 116 Batch: 30 Train Loss: 0.039409905672073364 Validation Loss: 3.26776123046875\n",
      "Epoch 117 Batch: 30 Train Loss: 0.0018705392722040415 Validation Loss: 5.593124866485596\n",
      "Epoch 118 Batch: 30 Train Loss: 0.007768532726913691 Validation Loss: 2.212714433670044\n",
      "Epoch 119 Batch: 30 Train Loss: 0.02125565893948078 Validation Loss: 6.691839694976807\n",
      "Epoch 120 Batch: 30 Train Loss: 1.424401044845581 Validation Loss: 5.820772171020508\n",
      "Epoch 121 Batch: 30 Train Loss: 0.24133309721946716 Validation Loss: 3.76914644241333\n",
      "Epoch 122 Batch: 30 Train Loss: 0.06268966197967529 Validation Loss: 4.895066261291504\n",
      "Epoch 123 Batch: 30 Train Loss: 0.18767008185386658 Validation Loss: 1.9835952520370483\n",
      "Epoch 124 Batch: 30 Train Loss: 0.02370339259505272 Validation Loss: 5.394759654998779\n",
      "Epoch 125 Batch: 30 Train Loss: 0.05319774150848389 Validation Loss: 11.376326560974121\n",
      "Epoch 126 Batch: 30 Train Loss: 0.00772669306024909 Validation Loss: 4.238772392272949\n",
      "Epoch 127 Batch: 30 Train Loss: 0.0037954901345074177 Validation Loss: 3.5895702838897705\n",
      "Epoch 128 Batch: 30 Train Loss: 0.0018057801062241197 Validation Loss: 5.280861854553223\n",
      "Epoch 129 Batch: 30 Train Loss: 0.008640405721962452 Validation Loss: 5.978848457336426\n",
      "Epoch 130 Batch: 30 Train Loss: 0.009887291118502617 Validation Loss: 8.159074783325195\n",
      "Epoch 131 Batch: 30 Train Loss: 0.0020027682185173035 Validation Loss: 4.1433234214782715\n",
      "Epoch 132 Batch: 30 Train Loss: 0.0025968393310904503 Validation Loss: 13.070857048034668\n",
      "Epoch 133 Batch: 30 Train Loss: 0.008619118481874466 Validation Loss: 14.525845527648926\n",
      "Epoch 134 Batch: 30 Train Loss: 0.00934884324669838 Validation Loss: 17.213504791259766\n",
      "Epoch 135 Batch: 30 Train Loss: 0.008156259544193745 Validation Loss: 4.366713523864746\n",
      "Epoch 136 Batch: 30 Train Loss: 0.007872208021581173 Validation Loss: 4.182744979858398\n",
      "Epoch 137 Batch: 30 Train Loss: 0.0014287251979112625 Validation Loss: 4.560187339782715\n",
      "Epoch 138 Batch: 30 Train Loss: 0.009920981712639332 Validation Loss: 11.720185279846191\n",
      "Epoch 139 Batch: 30 Train Loss: 0.004790143575519323 Validation Loss: 2.878570079803467\n",
      "Epoch 140 Batch: 30 Train Loss: 0.001118819578550756 Validation Loss: 8.296624183654785\n",
      "Epoch 141 Batch: 30 Train Loss: 0.0008946236339397728 Validation Loss: 2.9857687950134277\n",
      "Epoch 142 Batch: 30 Train Loss: 0.001950760604813695 Validation Loss: 5.480809211730957\n",
      "Epoch 143 Batch: 30 Train Loss: 0.000982017838396132 Validation Loss: 6.790081977844238\n",
      "Epoch 144 Batch: 30 Train Loss: 0.0013265442103147507 Validation Loss: 5.909996509552002\n",
      "Epoch 145 Batch: 30 Train Loss: 0.0009134013089351356 Validation Loss: 12.000661849975586\n",
      "Epoch 146 Batch: 30 Train Loss: 0.0022832839749753475 Validation Loss: 7.129004001617432\n",
      "Epoch 147 Batch: 30 Train Loss: 0.00043261953396722674 Validation Loss: 1.760413408279419\n",
      "Epoch 148 Batch: 30 Train Loss: 0.0008400711230933666 Validation Loss: 2.766277551651001\n",
      "Epoch 149 Batch: 30 Train Loss: 0.002488268306478858 Validation Loss: 6.589787483215332\n",
      "Epoch 150 Batch: 30 Train Loss: 0.0008074878714978695 Validation Loss: 6.3764142990112305\n",
      "Epoch 151 Batch: 30 Train Loss: 0.0009882283629849553 Validation Loss: 6.245516777038574\n",
      "Epoch 152 Batch: 30 Train Loss: 0.00039837570511735976 Validation Loss: 11.34160327911377\n",
      "Epoch 153 Batch: 30 Train Loss: 0.00030378185329027474 Validation Loss: 6.032288074493408\n",
      "Epoch 154 Batch: 30 Train Loss: 0.0003676237538456917 Validation Loss: 0.5864031314849854\n",
      "Epoch 155 Batch: 30 Train Loss: 0.000883430999238044 Validation Loss: 6.5250091552734375\n",
      "Epoch 156 Batch: 30 Train Loss: 0.0003105714567936957 Validation Loss: 6.823098182678223\n",
      "Epoch 157 Batch: 30 Train Loss: 5.531236638489645e-06 Validation Loss: 11.626575469970703\n",
      "Epoch 158 Batch: 30 Train Loss: 6.557745655300096e-05 Validation Loss: 7.741568088531494\n",
      "Epoch 159 Batch: 30 Train Loss: 0.00027223562938161194 Validation Loss: 11.321405410766602\n",
      "Epoch 160 Batch: 30 Train Loss: 0.00040452665416523814 Validation Loss: 18.68535614013672\n",
      "Epoch 161 Batch: 30 Train Loss: 0.0005226201610639691 Validation Loss: 19.177927017211914\n",
      "Epoch 162 Batch: 30 Train Loss: 0.0003283997066318989 Validation Loss: 8.197742462158203\n",
      "Epoch 163 Batch: 30 Train Loss: 0.0002160034200642258 Validation Loss: 15.980509757995605\n",
      "Epoch 164 Batch: 30 Train Loss: 4.752500535687432e-05 Validation Loss: 7.792972564697266\n",
      "Epoch 165 Batch: 30 Train Loss: 0.00011149646161356941 Validation Loss: 8.830984115600586\n",
      "Epoch 166 Batch: 30 Train Loss: 0.00019737394177354872 Validation Loss: 9.323335647583008\n",
      "Epoch 167 Batch: 30 Train Loss: 0.0003109548706561327 Validation Loss: 4.493041515350342\n",
      "Epoch 168 Batch: 30 Train Loss: 0.0005150210345163941 Validation Loss: 11.106103897094727\n",
      "Epoch 169 Batch: 30 Train Loss: 0.0002956868556793779 Validation Loss: 2.642540454864502\n",
      "Epoch 170 Batch: 30 Train Loss: 0.00045720586786046624 Validation Loss: 9.213123321533203\n",
      "Epoch 171 Batch: 30 Train Loss: 0.000957395532168448 Validation Loss: 7.119969367980957\n",
      "Epoch 172 Batch: 30 Train Loss: 7.567079592263326e-05 Validation Loss: 12.331587791442871\n",
      "Epoch 173 Batch: 30 Train Loss: 0.00010530381405260414 Validation Loss: 9.34418773651123\n",
      "Epoch 174 Batch: 30 Train Loss: 0.0003144354559481144 Validation Loss: 17.323871612548828\n",
      "Epoch 175 Batch: 30 Train Loss: 0.00011384952813386917 Validation Loss: 7.9821953773498535\n",
      "Epoch 176 Batch: 30 Train Loss: 1.4036390781402588 Validation Loss: 1.327828049659729\n",
      "Epoch 177 Batch: 30 Train Loss: 0.6484119296073914 Validation Loss: 0.7714484333992004\n",
      "Epoch 178 Batch: 30 Train Loss: 0.5775232911109924 Validation Loss: 0.6901711225509644\n",
      "Epoch 179 Batch: 30 Train Loss: 0.6891995668411255 Validation Loss: 0.7529362440109253\n",
      "Epoch 180 Batch: 30 Train Loss: 0.8988226652145386 Validation Loss: 1.6348812580108643\n",
      "Epoch 181 Batch: 30 Train Loss: 0.8729833364486694 Validation Loss: 0.776565670967102\n",
      "Epoch 182 Batch: 30 Train Loss: 0.4899982511997223 Validation Loss: 4.8252272605896\n",
      "Epoch 183 Batch: 30 Train Loss: 0.2156980335712433 Validation Loss: 1.7998689413070679\n",
      "Epoch 184 Batch: 30 Train Loss: 0.8214235305786133 Validation Loss: 0.8703063130378723\n",
      "Epoch 185 Batch: 30 Train Loss: 0.13673029839992523 Validation Loss: 6.689946174621582\n",
      "Epoch 186 Batch: 30 Train Loss: 0.4333571791648865 Validation Loss: 0.9771382212638855\n",
      "Epoch 187 Batch: 30 Train Loss: 0.342808336019516 Validation Loss: 1.2520564794540405\n",
      "Epoch 188 Batch: 30 Train Loss: 0.27988821268081665 Validation Loss: 0.8726231455802917\n",
      "Epoch 189 Batch: 30 Train Loss: 0.3365178108215332 Validation Loss: 4.520131587982178\n",
      "Epoch 190 Batch: 30 Train Loss: 0.15882952511310577 Validation Loss: 3.2030930519104004\n",
      "Epoch 191 Batch: 30 Train Loss: 0.24041399359703064 Validation Loss: 2.913252353668213\n",
      "Epoch 192 Batch: 30 Train Loss: 0.1198665127158165 Validation Loss: 6.261045932769775\n",
      "Epoch 193 Batch: 30 Train Loss: 0.20245489478111267 Validation Loss: 1.5776050090789795\n",
      "Epoch 194 Batch: 30 Train Loss: 0.1133105531334877 Validation Loss: 1.990851640701294\n",
      "Epoch 195 Batch: 30 Train Loss: 0.013823886401951313 Validation Loss: 3.2261481285095215\n",
      "Epoch 196 Batch: 30 Train Loss: 0.05244548246264458 Validation Loss: 1.133561372756958\n",
      "Epoch 197 Batch: 30 Train Loss: 0.03950323536992073 Validation Loss: 2.169274091720581\n",
      "Epoch 198 Batch: 30 Train Loss: 0.047510869801044464 Validation Loss: 5.009499549865723\n",
      "Epoch 199 Batch: 30 Train Loss: 0.10263258218765259 Validation Loss: 2.263500452041626\n",
      "Epoch 200 Batch: 30 Train Loss: 0.13405831158161163 Validation Loss: 6.541191101074219\n",
      "Epoch 201 Batch: 30 Train Loss: 0.001843146630562842 Validation Loss: 5.411167144775391\n",
      "Epoch 202 Batch: 30 Train Loss: 0.025806110352277756 Validation Loss: 0.8548382520675659\n",
      "Epoch 203 Batch: 30 Train Loss: 0.12703433632850647 Validation Loss: 6.941503047943115\n",
      "Epoch 204 Batch: 30 Train Loss: 0.15403039753437042 Validation Loss: 0.3798099756240845\n",
      "Epoch 205 Batch: 30 Train Loss: 0.01775071583688259 Validation Loss: 3.039635181427002\n",
      "Epoch 206 Batch: 30 Train Loss: 0.0017485746648162603 Validation Loss: 9.191404342651367\n",
      "Epoch 207 Batch: 30 Train Loss: 0.019330119714140892 Validation Loss: 2.3756563663482666\n",
      "Epoch 208 Batch: 30 Train Loss: 0.012278033420443535 Validation Loss: 7.329891204833984\n",
      "Epoch 209 Batch: 30 Train Loss: 0.021642524749040604 Validation Loss: 4.048410892486572\n",
      "Epoch 210 Batch: 30 Train Loss: 0.004143393598496914 Validation Loss: 8.808588027954102\n",
      "Epoch 211 Batch: 30 Train Loss: 0.01272069476544857 Validation Loss: 9.825887680053711\n",
      "Epoch 212 Batch: 30 Train Loss: 0.004930432885885239 Validation Loss: 1.4951857328414917\n",
      "Epoch 213 Batch: 30 Train Loss: 0.004249208606779575 Validation Loss: 9.996737480163574\n",
      "Epoch 214 Batch: 30 Train Loss: 0.0005710509140044451 Validation Loss: 6.6318840980529785\n",
      "Epoch 215 Batch: 30 Train Loss: 0.00013728612975683063 Validation Loss: 9.688652038574219\n",
      "Epoch 216 Batch: 30 Train Loss: 0.0014959449181333184 Validation Loss: 3.6122028827667236\n",
      "Epoch 217 Batch: 30 Train Loss: 0.0027737088967114687 Validation Loss: 12.860069274902344\n",
      "Epoch 218 Batch: 30 Train Loss: 0.0004996791249141097 Validation Loss: 10.011136054992676\n",
      "Epoch 219 Batch: 30 Train Loss: 0.0013444209471344948 Validation Loss: 10.157910346984863\n",
      "Epoch 220 Batch: 30 Train Loss: 0.015360139310359955 Validation Loss: 8.485824584960938\n",
      "Epoch 221 Batch: 30 Train Loss: 0.00014624922187067568 Validation Loss: 2.2189910411834717\n",
      "Epoch 222 Batch: 30 Train Loss: 0.0005834735929965973 Validation Loss: 7.6662750244140625\n",
      "Epoch 223 Batch: 30 Train Loss: 0.0009235478937625885 Validation Loss: 4.270890235900879\n",
      "Epoch 224 Batch: 30 Train Loss: 0.0002472182095516473 Validation Loss: 1.9362694025039673\n",
      "Epoch 225 Batch: 30 Train Loss: 8.855314808897674e-05 Validation Loss: 10.844764709472656\n",
      "Epoch 226 Batch: 30 Train Loss: 0.00036739022471010685 Validation Loss: 5.803699493408203\n",
      "Epoch 227 Batch: 30 Train Loss: 0.00042809220030903816 Validation Loss: 3.9430809020996094\n",
      "Epoch 228 Batch: 30 Train Loss: 0.00036776260822080076 Validation Loss: 5.5163068771362305\n",
      "Epoch 229 Batch: 30 Train Loss: 0.00034786760807037354 Validation Loss: 6.7811994552612305\n",
      "Epoch 230 Batch: 30 Train Loss: 0.000525978859513998 Validation Loss: 5.5566887855529785\n",
      "Epoch 231 Batch: 30 Train Loss: 0.0007031176937744021 Validation Loss: 2.265106678009033\n",
      "Epoch 232 Batch: 30 Train Loss: 0.0008819783106446266 Validation Loss: 5.480252742767334\n",
      "Epoch 233 Batch: 30 Train Loss: 4.278149572201073e-05 Validation Loss: 5.299633979797363\n",
      "Epoch 234 Batch: 30 Train Loss: 3.874235517287161e-06 Validation Loss: 7.065939903259277\n",
      "Epoch 235 Batch: 30 Train Loss: 6.088039299356751e-05 Validation Loss: 6.415846824645996\n",
      "Epoch 236 Batch: 30 Train Loss: 0.00012377873645164073 Validation Loss: 6.4863152503967285\n",
      "Epoch 237 Batch: 30 Train Loss: 1.276696548302425e-05 Validation Loss: 8.41539478302002\n",
      "Epoch 238 Batch: 30 Train Loss: 0.00021996782743372023 Validation Loss: 12.781832695007324\n",
      "Epoch 239 Batch: 30 Train Loss: 0.00015319926023948938 Validation Loss: 5.632213592529297\n",
      "Epoch 240 Batch: 30 Train Loss: 5.926231824560091e-05 Validation Loss: 9.66569709777832\n",
      "Epoch 241 Batch: 30 Train Loss: 9.287688590120524e-05 Validation Loss: 4.3598504066467285\n",
      "Epoch 242 Batch: 30 Train Loss: 2.4341814423678443e-05 Validation Loss: 9.918330192565918\n",
      "Epoch 243 Batch: 30 Train Loss: 4.329365765443072e-05 Validation Loss: 4.881634712219238\n",
      "Epoch 244 Batch: 30 Train Loss: 3.966892109019682e-05 Validation Loss: 5.816793441772461\n",
      "Epoch 245 Batch: 30 Train Loss: 6.049781950423494e-05 Validation Loss: 13.481077194213867\n",
      "Epoch 246 Batch: 30 Train Loss: 5.431847966974601e-05 Validation Loss: 7.927872657775879\n",
      "Epoch 247 Batch: 30 Train Loss: 7.67017190810293e-05 Validation Loss: 2.9057748317718506\n",
      "Epoch 248 Batch: 30 Train Loss: 6.772898632334545e-05 Validation Loss: 11.851537704467773\n",
      "Epoch 249 Batch: 30 Train Loss: 6.367286550812423e-05 Validation Loss: 17.091527938842773\n",
      "Epoch 250 Batch: 30 Train Loss: 1.0359038242313545e-05 Validation Loss: 10.868276596069336\n",
      "Epoch 251 Batch: 30 Train Loss: 0.0001088202916434966 Validation Loss: 7.315570831298828\n",
      "Epoch 252 Batch: 30 Train Loss: 6.880964792799205e-05 Validation Loss: 18.297344207763672\n",
      "Epoch 253 Batch: 30 Train Loss: 0.00023219679133035243 Validation Loss: 4.717726707458496\n",
      "Epoch 254 Batch: 30 Train Loss: 6.596669118152931e-05 Validation Loss: 5.823068618774414\n",
      "Epoch 255 Batch: 30 Train Loss: 7.72467228671303e-06 Validation Loss: 9.462873458862305\n",
      "Epoch 256 Batch: 30 Train Loss: 3.3494914532639086e-05 Validation Loss: 7.130163669586182\n",
      "Epoch 257 Batch: 30 Train Loss: 1.0073011253552977e-05 Validation Loss: 8.822343826293945\n",
      "Epoch 258 Batch: 30 Train Loss: 1.1455604180810042e-05 Validation Loss: 9.469070434570312\n",
      "Epoch 259 Batch: 30 Train Loss: 2.092025533784181e-05 Validation Loss: 15.528295516967773\n",
      "Epoch 260 Batch: 30 Train Loss: 2.5555647880537435e-05 Validation Loss: 6.389182090759277\n",
      "Epoch 261 Batch: 30 Train Loss: 2.014621486523538e-06 Validation Loss: 7.439192295074463\n",
      "Epoch 262 Batch: 30 Train Loss: 1.9883551431121305e-05 Validation Loss: 6.933675289154053\n",
      "Epoch 263 Batch: 30 Train Loss: 1.380367211822886e-05 Validation Loss: 5.1212334632873535\n",
      "Epoch 264 Batch: 30 Train Loss: 1.1217101928195916e-05 Validation Loss: 7.405513763427734\n",
      "Epoch 265 Batch: 30 Train Loss: 7.72462590248324e-06 Validation Loss: 4.298347473144531\n",
      "Epoch 266 Batch: 30 Train Loss: 3.5720931919058785e-05 Validation Loss: 6.955599784851074\n",
      "Epoch 267 Batch: 30 Train Loss: 7.362068572547287e-05 Validation Loss: 22.58242416381836\n",
      "Epoch 268 Batch: 30 Train Loss: 8.010678357095458e-06 Validation Loss: 5.767431735992432\n",
      "Epoch 269 Batch: 30 Train Loss: 7.295382147276541e-06 Validation Loss: 9.692034721374512\n",
      "Epoch 270 Batch: 30 Train Loss: 1.1670344065350946e-05 Validation Loss: 13.198549270629883\n",
      "Epoch 271 Batch: 30 Train Loss: 3.550754991010763e-05 Validation Loss: 6.414863586425781\n",
      "Epoch 272 Batch: 30 Train Loss: 6.937860689504305e-06 Validation Loss: 8.222914695739746\n",
      "Epoch 273 Batch: 30 Train Loss: 8.558864465157967e-06 Validation Loss: 18.74392318725586\n",
      "Epoch 274 Batch: 30 Train Loss: 6.389550890162354e-06 Validation Loss: 10.583826065063477\n",
      "Epoch 275 Batch: 30 Train Loss: 5.292817149893381e-06 Validation Loss: 8.323403358459473\n",
      "Epoch 276 Batch: 30 Train Loss: 1.871505446615629e-05 Validation Loss: 15.140554428100586\n",
      "Epoch 277 Batch: 30 Train Loss: 1.6224024875555187e-05 Validation Loss: 5.962113380432129\n",
      "Epoch 278 Batch: 30 Train Loss: 5.7100637604889926e-06 Validation Loss: 10.3798189163208\n",
      "Epoch 279 Batch: 30 Train Loss: 2.9921116038167384e-06 Validation Loss: 8.370134353637695\n",
      "Epoch 280 Batch: 30 Train Loss: 5.435873390524648e-06 Validation Loss: 4.601683139801025\n",
      "Epoch 281 Batch: 30 Train Loss: 1.0680861123546492e-05 Validation Loss: 12.471307754516602\n",
      "Epoch 282 Batch: 30 Train Loss: 7.271747222148406e-07 Validation Loss: 5.097787380218506\n",
      "Epoch 283 Batch: 30 Train Loss: 2.098070126521634e-06 Validation Loss: 9.538223266601562\n",
      "Epoch 284 Batch: 30 Train Loss: 2.408003410891979e-06 Validation Loss: 8.850850105285645\n",
      "Epoch 285 Batch: 30 Train Loss: 1.9907888599846046e-06 Validation Loss: 3.278106212615967\n",
      "Epoch 286 Batch: 30 Train Loss: 2.0265565581212286e-07 Validation Loss: 10.04178237915039\n",
      "Epoch 287 Batch: 30 Train Loss: 2.9563746011262992e-06 Validation Loss: 13.123164176940918\n",
      "Epoch 288 Batch: 30 Train Loss: 9.465033144806512e-06 Validation Loss: 14.148994445800781\n",
      "Epoch 289 Batch: 30 Train Loss: 2.6821926439879462e-06 Validation Loss: 8.22630500793457\n",
      "Epoch 290 Batch: 30 Train Loss: 1.8238955590277328e-06 Validation Loss: 7.441252708435059\n",
      "Epoch 291 Batch: 30 Train Loss: 2.157672952307621e-06 Validation Loss: 17.040647506713867\n",
      "Epoch 292 Batch: 30 Train Loss: 4.07689185522031e-06 Validation Loss: 25.56827163696289\n",
      "Epoch 293 Batch: 30 Train Loss: 2.276885197716183e-06 Validation Loss: 7.912256717681885\n",
      "Epoch 294 Batch: 30 Train Loss: 1.6927663182286778e-06 Validation Loss: 12.138226509094238\n",
      "Epoch 295 Batch: 30 Train Loss: 2.0742359083669726e-06 Validation Loss: 13.644636154174805\n",
      "Epoch 296 Batch: 30 Train Loss: 5.602828423434403e-07 Validation Loss: 14.7755708694458\n",
      "Epoch 297 Batch: 30 Train Loss: 2.0265374587324914e-06 Validation Loss: 11.854482650756836\n",
      "Epoch 298 Batch: 30 Train Loss: 2.9206034923845436e-06 Validation Loss: 18.984539031982422\n",
      "Epoch 299 Batch: 30 Train Loss: 2.8848469355580164e-06 Validation Loss: 13.408361434936523\n",
      "Epoch 300 Batch: 30 Train Loss: 7.271743243109086e-07 Validation Loss: 13.057108879089355\n",
      "Epoch 301 Batch: 30 Train Loss: 1.3113017871546617e-07 Validation Loss: 18.92718505859375\n",
      "Epoch 302 Batch: 30 Train Loss: 1.5854800494707888e-06 Validation Loss: 13.275014877319336\n",
      "Epoch 303 Batch: 30 Train Loss: 1.5497197125569073e-07 Validation Loss: 18.585750579833984\n",
      "Epoch 304 Batch: 30 Train Loss: 2.1219148038653657e-06 Validation Loss: 9.745604515075684\n",
      "Epoch 305 Batch: 30 Train Loss: 4.6491578586937976e-07 Validation Loss: 14.885769844055176\n",
      "Epoch 306 Batch: 30 Train Loss: 8.106198947643861e-07 Validation Loss: 11.504201889038086\n",
      "Epoch 307 Batch: 30 Train Loss: 4.649155584957043e-07 Validation Loss: 6.439206600189209\n",
      "Epoch 308 Batch: 30 Train Loss: 3.6954844517822494e-07 Validation Loss: 7.9147748947143555\n",
      "Epoch 309 Batch: 30 Train Loss: 2.384185648907078e-08 Validation Loss: 14.644525527954102\n",
      "Epoch 310 Batch: 30 Train Loss: 4.053113116242457e-07 Validation Loss: 12.561634063720703\n",
      "Epoch 311 Batch: 30 Train Loss: 6.198864070938725e-07 Validation Loss: 32.88640594482422\n",
      "Epoch 312 Batch: 30 Train Loss: 4.887572799816553e-07 Validation Loss: 11.81248950958252\n",
      "Epoch 313 Batch: 30 Train Loss: 1.3113017871546617e-07 Validation Loss: 27.1781063079834\n",
      "Epoch 314 Batch: 30 Train Loss: 5.960463411724959e-08 Validation Loss: 16.671497344970703\n",
      "Epoch 315 Batch: 30 Train Loss: 3.576274707484117e-07 Validation Loss: 16.48973274230957\n",
      "Epoch 316 Batch: 30 Train Loss: 1.192092824453539e-08 Validation Loss: 24.357929229736328\n",
      "Epoch 317 Batch: 30 Train Loss: 4.52995038813242e-07 Validation Loss: 8.674503326416016\n",
      "Epoch 318 Batch: 30 Train Loss: 1.0728833643725011e-07 Validation Loss: 8.084928512573242\n",
      "Epoch 319 Batch: 30 Train Loss: 8.106222821879783e-07 Validation Loss: 6.393374919891357\n",
      "Epoch 320 Batch: 30 Train Loss: 1.120562274081749e-06 Validation Loss: 9.426379203796387\n",
      "Epoch 321 Batch: 30 Train Loss: 2.3841835172788706e-07 Validation Loss: 5.652239799499512\n",
      "Epoch 322 Batch: 30 Train Loss: 3.099438856679626e-07 Validation Loss: 19.13443374633789\n",
      "Epoch 323 Batch: 30 Train Loss: 8.344649415903405e-08 Validation Loss: 14.382281303405762\n",
      "Epoch 324 Batch: 30 Train Loss: 1.5497201388825488e-07 Validation Loss: 14.071334838867188\n",
      "Epoch 325 Batch: 30 Train Loss: 1.3113017871546617e-07 Validation Loss: 18.165733337402344\n",
      "Epoch 326 Batch: 30 Train Loss: 0.0 Validation Loss: 13.045328140258789\n",
      "Epoch 327 Batch: 30 Train Loss: 2.503392693142814e-07 Validation Loss: 13.20085334777832\n",
      "Epoch 328 Batch: 30 Train Loss: 1.668929030529398e-07 Validation Loss: 5.882375240325928\n",
      "Epoch 329 Batch: 30 Train Loss: 1.6689293147464923e-07 Validation Loss: 14.79168701171875\n",
      "Epoch 330 Batch: 30 Train Loss: 8.344649415903405e-08 Validation Loss: 4.023689270019531\n",
      "Epoch 331 Batch: 30 Train Loss: 4.887572799816553e-07 Validation Loss: 11.660581588745117\n",
      "Epoch 332 Batch: 30 Train Loss: 1.1920925402364446e-07 Validation Loss: 21.340341567993164\n",
      "Epoch 333 Batch: 30 Train Loss: 1.0728833643725011e-07 Validation Loss: 12.121421813964844\n",
      "Epoch 334 Batch: 30 Train Loss: 2.2649746256320213e-07 Validation Loss: 18.60848617553711\n",
      "Epoch 335 Batch: 30 Train Loss: 3.576278118089249e-08 Validation Loss: 10.277031898498535\n",
      "Epoch 336 Batch: 30 Train Loss: 5.960463411724959e-08 Validation Loss: 20.145816802978516\n",
      "Epoch 337 Batch: 30 Train Loss: 4.76837058727142e-08 Validation Loss: 6.587468147277832\n",
      "Epoch 338 Batch: 30 Train Loss: 1.192092824453539e-08 Validation Loss: 20.69236183166504\n",
      "Epoch 339 Batch: 30 Train Loss: 1.0728832933182275e-07 Validation Loss: 13.582260131835938\n",
      "Epoch 340 Batch: 30 Train Loss: 9.536741885085576e-08 Validation Loss: 17.169784545898438\n",
      "Epoch 341 Batch: 30 Train Loss: 4.768370942542788e-08 Validation Loss: 5.733330726623535\n",
      "Epoch 342 Batch: 30 Train Loss: 1.1920926823449918e-07 Validation Loss: 12.739572525024414\n",
      "Epoch 343 Batch: 30 Train Loss: 4.76837058727142e-08 Validation Loss: 12.470460891723633\n",
      "Epoch 344 Batch: 30 Train Loss: 3.576278118089249e-08 Validation Loss: 35.982521057128906\n",
      "Epoch 345 Batch: 30 Train Loss: 8.344649415903405e-08 Validation Loss: 14.857028007507324\n",
      "Epoch 346 Batch: 30 Train Loss: 9.53674117454284e-08 Validation Loss: 9.582365036010742\n",
      "Epoch 347 Batch: 30 Train Loss: 3.576278473360617e-08 Validation Loss: 36.133201599121094\n",
      "Epoch 348 Batch: 30 Train Loss: 3.576278473360617e-08 Validation Loss: 22.04050064086914\n",
      "Epoch 349 Batch: 30 Train Loss: 4.76837058727142e-08 Validation Loss: 13.7581205368042\n",
      "Epoch 350 Batch: 30 Train Loss: 4.768370942542788e-08 Validation Loss: 11.55830192565918\n",
      "Epoch 351 Batch: 30 Train Loss: 3.576278473360617e-08 Validation Loss: 10.23271656036377\n",
      "Epoch 352 Batch: 30 Train Loss: 5.960463411724959e-08 Validation Loss: 13.020395278930664\n",
      "Epoch 353 Batch: 30 Train Loss: 3.576278473360617e-08 Validation Loss: 11.076056480407715\n",
      "Epoch 354 Batch: 30 Train Loss: 1.192092824453539e-08 Validation Loss: 12.898513793945312\n",
      "Epoch 355 Batch: 30 Train Loss: 1.192092824453539e-08 Validation Loss: 9.158686637878418\n",
      "Epoch 356 Batch: 30 Train Loss: 1.192092824453539e-08 Validation Loss: 16.078922271728516\n",
      "Epoch 357 Batch: 30 Train Loss: 1.1920923270736239e-07 Validation Loss: 17.337764739990234\n",
      "Epoch 358 Batch: 30 Train Loss: 1.192092824453539e-08 Validation Loss: 17.690425872802734\n",
      "Epoch 359 Batch: 30 Train Loss: 0.0 Validation Loss: 18.627079010009766\n",
      "Epoch 360 Batch: 30 Train Loss: 0.0 Validation Loss: 12.137574195861816\n",
      "Epoch 361 Batch: 30 Train Loss: 0.0 Validation Loss: 19.619766235351562\n",
      "Epoch 362 Batch: 30 Train Loss: 3.576278473360617e-08 Validation Loss: 14.951603889465332\n",
      "Epoch 363 Batch: 30 Train Loss: 0.0 Validation Loss: 14.475852966308594\n",
      "Epoch 364 Batch: 30 Train Loss: 1.192092824453539e-08 Validation Loss: 15.95001220703125\n",
      "Epoch 365 Batch: 30 Train Loss: 0.0 Validation Loss: 12.01351261138916\n",
      "Epoch 366 Batch: 30 Train Loss: 3.576278473360617e-08 Validation Loss: 26.609447479248047\n",
      "Epoch 367 Batch: 30 Train Loss: 0.0 Validation Loss: 30.225811004638672\n",
      "Epoch 368 Batch: 30 Train Loss: 2.384185648907078e-08 Validation Loss: 25.068634033203125\n",
      "Epoch 369 Batch: 30 Train Loss: 9.536739753457368e-08 Validation Loss: 8.647611618041992\n",
      "Epoch 370 Batch: 30 Train Loss: 0.0 Validation Loss: 40.306793212890625\n",
      "Epoch 371 Batch: 30 Train Loss: 1.192092824453539e-08 Validation Loss: 8.453189849853516\n",
      "Epoch 372 Batch: 30 Train Loss: 0.0 Validation Loss: 22.759164810180664\n",
      "Epoch 373 Batch: 30 Train Loss: 0.0 Validation Loss: 20.12648582458496\n",
      "Epoch 374 Batch: 30 Train Loss: 0.0 Validation Loss: 16.71444320678711\n",
      "Epoch 375 Batch: 30 Train Loss: 0.0 Validation Loss: 8.281941413879395\n",
      "Epoch 376 Batch: 30 Train Loss: 1.192092824453539e-08 Validation Loss: 9.460622787475586\n",
      "Epoch 377 Batch: 30 Train Loss: 1.192092824453539e-08 Validation Loss: 9.106801986694336\n",
      "Epoch 378 Batch: 30 Train Loss: 0.0 Validation Loss: 16.643386840820312\n",
      "Epoch 379 Batch: 30 Train Loss: 1.192092824453539e-08 Validation Loss: 39.29007339477539\n",
      "Epoch 380 Batch: 30 Train Loss: 0.0 Validation Loss: 20.6219482421875\n",
      "Epoch 381 Batch: 30 Train Loss: 0.0 Validation Loss: 15.590846061706543\n",
      "Epoch 382 Batch: 30 Train Loss: 0.0 Validation Loss: 18.868432998657227\n",
      "Epoch 383 Batch: 30 Train Loss: 0.0 Validation Loss: 19.329133987426758\n",
      "Epoch 384 Batch: 30 Train Loss: 0.0 Validation Loss: 26.194677352905273\n",
      "Epoch 385 Batch: 30 Train Loss: 0.0 Validation Loss: 6.127882957458496\n",
      "Epoch 386 Batch: 30 Train Loss: 0.0 Validation Loss: 12.53951644897461\n",
      "Epoch 387 Batch: 30 Train Loss: 0.0 Validation Loss: 28.08489418029785\n",
      "Epoch 388 Batch: 30 Train Loss: 0.0 Validation Loss: 13.16100025177002\n",
      "Epoch 389 Batch: 30 Train Loss: 0.0 Validation Loss: 12.371936798095703\n",
      "Epoch 390 Batch: 30 Train Loss: 0.0 Validation Loss: 20.436243057250977\n",
      "Epoch 391 Batch: 30 Train Loss: 0.0 Validation Loss: 29.45492935180664\n",
      "Epoch 392 Batch: 30 Train Loss: 0.0 Validation Loss: 16.886154174804688\n",
      "Epoch 393 Batch: 30 Train Loss: 1.192092824453539e-08 Validation Loss: 9.235451698303223\n",
      "Epoch 394 Batch: 30 Train Loss: 0.0 Validation Loss: 16.74642562866211\n",
      "Epoch 395 Batch: 30 Train Loss: 0.0 Validation Loss: 22.049274444580078\n",
      "Epoch 396 Batch: 30 Train Loss: 0.0 Validation Loss: 21.809572219848633\n",
      "Epoch 397 Batch: 30 Train Loss: 0.0 Validation Loss: 12.83942699432373\n",
      "Epoch 398 Batch: 30 Train Loss: 0.0 Validation Loss: 13.543597221374512\n",
      "Epoch 399 Batch: 30 Train Loss: 0.0 Validation Loss: 2.6181302070617676\n",
      "Epoch 400 Batch: 30 Train Loss: 0.0 Validation Loss: 22.91975212097168\n",
      "Epoch 401 Batch: 30 Train Loss: 0.0 Validation Loss: 16.370487213134766\n",
      "Epoch 402 Batch: 30 Train Loss: 0.0 Validation Loss: 15.581209182739258\n",
      "Epoch 403 Batch: 30 Train Loss: 0.0 Validation Loss: 19.319894790649414\n",
      "Epoch 404 Batch: 30 Train Loss: 0.0 Validation Loss: 6.490149021148682\n",
      "Epoch 405 Batch: 30 Train Loss: 0.0 Validation Loss: 27.6810359954834\n",
      "Epoch 406 Batch: 30 Train Loss: 0.0 Validation Loss: 13.248987197875977\n",
      "Epoch 407 Batch: 30 Train Loss: 0.0 Validation Loss: 31.130813598632812\n",
      "Epoch 408 Batch: 30 Train Loss: 0.0 Validation Loss: 11.602773666381836\n",
      "Epoch 409 Batch: 30 Train Loss: 0.0 Validation Loss: 13.86584758758545\n",
      "Epoch 410 Batch: 30 Train Loss: 0.0 Validation Loss: 13.616175651550293\n",
      "Epoch 411 Batch: 30 Train Loss: 0.0 Validation Loss: 18.092304229736328\n",
      "Epoch 412 Batch: 30 Train Loss: 0.0 Validation Loss: 20.66830825805664\n",
      "Epoch 413 Batch: 30 Train Loss: 0.0 Validation Loss: 19.51559829711914\n",
      "Epoch 414 Batch: 30 Train Loss: 0.0 Validation Loss: 17.97029685974121\n",
      "Epoch 415 Batch: 30 Train Loss: 0.0 Validation Loss: 38.05353546142578\n",
      "Epoch 416 Batch: 30 Train Loss: 0.0 Validation Loss: 12.851194381713867\n",
      "Epoch 417 Batch: 30 Train Loss: 0.0 Validation Loss: 11.900827407836914\n",
      "Epoch 418 Batch: 30 Train Loss: 0.0 Validation Loss: 9.549168586730957\n",
      "Epoch 419 Batch: 30 Train Loss: 0.0 Validation Loss: 28.295358657836914\n",
      "Epoch 420 Batch: 30 Train Loss: 0.0 Validation Loss: 22.432785034179688\n",
      "Epoch 421 Batch: 30 Train Loss: 0.0 Validation Loss: 43.63746643066406\n",
      "Epoch 422 Batch: 30 Train Loss: 0.0 Validation Loss: 27.084579467773438\n",
      "Epoch 423 Batch: 30 Train Loss: 2.6978867053985596 Validation Loss: 2.21928334236145\n",
      "Epoch 424 Batch: 30 Train Loss: 0.46155136823654175 Validation Loss: 1.0065150260925293\n",
      "Epoch 425 Batch: 30 Train Loss: 0.4505693316459656 Validation Loss: 1.2620360851287842\n",
      "Epoch 426 Batch: 30 Train Loss: 0.36193224787712097 Validation Loss: 1.2935796976089478\n",
      "Epoch 427 Batch: 30 Train Loss: 0.6043669581413269 Validation Loss: 1.1435667276382446\n",
      "Epoch 428 Batch: 30 Train Loss: 0.519486665725708 Validation Loss: 0.9829527139663696\n",
      "Epoch 429 Batch: 30 Train Loss: 0.1868383139371872 Validation Loss: 1.9546535015106201\n",
      "Epoch 430 Batch: 30 Train Loss: 0.3741288483142853 Validation Loss: 0.9163438677787781\n",
      "Epoch 431 Batch: 30 Train Loss: 0.3843846023082733 Validation Loss: 2.7385475635528564\n",
      "Epoch 432 Batch: 30 Train Loss: 0.6577240228652954 Validation Loss: 0.9524770975112915\n",
      "Epoch 433 Batch: 30 Train Loss: 0.2558566927909851 Validation Loss: 1.542595624923706\n",
      "Epoch 434 Batch: 30 Train Loss: 0.44755619764328003 Validation Loss: 1.1527186632156372\n",
      "Epoch 435 Batch: 30 Train Loss: 0.02727966569364071 Validation Loss: 7.315421104431152\n",
      "Epoch 436 Batch: 30 Train Loss: 0.1574576050043106 Validation Loss: 1.6903539896011353\n",
      "Epoch 437 Batch: 30 Train Loss: 0.04043365642428398 Validation Loss: 4.093446254730225\n",
      "Epoch 438 Batch: 30 Train Loss: 0.19053104519844055 Validation Loss: 6.218504905700684\n",
      "Epoch 439 Batch: 30 Train Loss: 0.09626475721597672 Validation Loss: 2.2168147563934326\n",
      "Epoch 440 Batch: 30 Train Loss: 0.014892359264194965 Validation Loss: 2.2883026599884033\n",
      "Epoch 441 Batch: 30 Train Loss: 0.14543521404266357 Validation Loss: 1.0052509307861328\n",
      "Epoch 442 Batch: 30 Train Loss: 0.030475249513983727 Validation Loss: 10.580085754394531\n",
      "Epoch 443 Batch: 30 Train Loss: 0.021169880405068398 Validation Loss: 6.0650835037231445\n",
      "Epoch 444 Batch: 30 Train Loss: 0.004620063118636608 Validation Loss: 5.776453018188477\n",
      "Epoch 445 Batch: 30 Train Loss: 0.0012207141844555736 Validation Loss: 4.715285778045654\n",
      "Epoch 446 Batch: 30 Train Loss: 0.0004882777575403452 Validation Loss: 6.202884197235107\n",
      "Epoch 447 Batch: 30 Train Loss: 0.02006242796778679 Validation Loss: 16.21395492553711\n",
      "Epoch 448 Batch: 30 Train Loss: 0.0008330456912517548 Validation Loss: 7.139265537261963\n",
      "Epoch 449 Batch: 30 Train Loss: 0.0045423004776239395 Validation Loss: 7.514286994934082\n",
      "Epoch 450 Batch: 30 Train Loss: 0.0028571677394211292 Validation Loss: 6.3783674240112305\n",
      "Epoch 451 Batch: 30 Train Loss: 0.0010596876963973045 Validation Loss: 3.9299156665802\n",
      "Epoch 452 Batch: 30 Train Loss: 8.990903006633744e-05 Validation Loss: 2.156921863555908\n",
      "Epoch 453 Batch: 30 Train Loss: 0.00021462883159983903 Validation Loss: 2.605314016342163\n",
      "Epoch 454 Batch: 30 Train Loss: 0.0020880568772554398 Validation Loss: 5.630288600921631\n",
      "Epoch 455 Batch: 30 Train Loss: 0.004376969765871763 Validation Loss: 0.7548726201057434\n",
      "Epoch 456 Batch: 30 Train Loss: 0.0003010673390235752 Validation Loss: 6.847082614898682\n",
      "Epoch 457 Batch: 30 Train Loss: 3.6128414649283513e-05 Validation Loss: 15.859786987304688\n",
      "Epoch 458 Batch: 30 Train Loss: 6.68104476062581e-05 Validation Loss: 8.421781539916992\n",
      "Epoch 459 Batch: 30 Train Loss: 0.00046569807454943657 Validation Loss: 6.156863689422607\n",
      "Epoch 460 Batch: 30 Train Loss: 0.0005357288173399866 Validation Loss: 8.748922348022461\n",
      "Epoch 461 Batch: 30 Train Loss: 0.0003828086773864925 Validation Loss: 7.968827724456787\n",
      "Epoch 462 Batch: 30 Train Loss: 0.0002246023650513962 Validation Loss: 14.618894577026367\n",
      "Epoch 463 Batch: 30 Train Loss: 0.00015982901095412672 Validation Loss: 9.088578224182129\n",
      "Epoch 464 Batch: 30 Train Loss: 0.0002121551224263385 Validation Loss: 13.657061576843262\n",
      "Epoch 465 Batch: 30 Train Loss: 0.00019112552399747074 Validation Loss: 6.369234561920166\n",
      "Epoch 466 Batch: 30 Train Loss: 0.0003626322722993791 Validation Loss: 5.664965629577637\n",
      "Epoch 467 Batch: 30 Train Loss: 0.0008857765351422131 Validation Loss: 7.842965602874756\n",
      "Epoch 468 Batch: 30 Train Loss: 0.0001040900606312789 Validation Loss: 6.572801113128662\n",
      "Epoch 469 Batch: 30 Train Loss: 1.1324517799948808e-05 Validation Loss: 9.425114631652832\n",
      "Epoch 470 Batch: 30 Train Loss: 6.0598034906433895e-05 Validation Loss: 15.184100151062012\n",
      "Epoch 471 Batch: 30 Train Loss: 5.960395355941728e-06 Validation Loss: 7.54740047454834\n",
      "Epoch 472 Batch: 30 Train Loss: 5.559100100072101e-05 Validation Loss: 21.492477416992188\n",
      "Epoch 473 Batch: 30 Train Loss: 0.00010962664237013087 Validation Loss: 4.647298812866211\n",
      "Epoch 474 Batch: 30 Train Loss: 8.16119572846219e-05 Validation Loss: 5.20810079574585\n",
      "Epoch 475 Batch: 30 Train Loss: 7.202521373983473e-05 Validation Loss: 12.895291328430176\n",
      "Epoch 476 Batch: 30 Train Loss: 0.0003968105302192271 Validation Loss: 8.207646369934082\n",
      "Epoch 477 Batch: 30 Train Loss: 2.553313424868975e-05 Validation Loss: 19.405277252197266\n",
      "Epoch 478 Batch: 30 Train Loss: 2.4781042156973854e-05 Validation Loss: 3.702624559402466\n",
      "Epoch 479 Batch: 30 Train Loss: 1.6570004390814574e-06 Validation Loss: 13.746299743652344\n",
      "Epoch 480 Batch: 30 Train Loss: 9.224138921126723e-05 Validation Loss: 0.7046454548835754\n",
      "Epoch 481 Batch: 30 Train Loss: 0.0001459295308450237 Validation Loss: 12.93897819519043\n",
      "Epoch 482 Batch: 30 Train Loss: 1.8213813746115193e-05 Validation Loss: 8.754287719726562\n",
      "Epoch 483 Batch: 30 Train Loss: 2.1659088815795258e-05 Validation Loss: 10.722251892089844\n",
      "Epoch 484 Batch: 30 Train Loss: 2.503378482288099e-06 Validation Loss: 17.984546661376953\n",
      "Epoch 485 Batch: 30 Train Loss: 4.6928729716455564e-05 Validation Loss: 12.182806968688965\n",
      "Epoch 486 Batch: 30 Train Loss: 7.758384163025767e-05 Validation Loss: 4.310891151428223\n",
      "Epoch 487 Batch: 30 Train Loss: 0.00010782631579786539 Validation Loss: 6.158405303955078\n",
      "Epoch 488 Batch: 30 Train Loss: 9.318533557234332e-05 Validation Loss: 15.156176567077637\n",
      "Epoch 489 Batch: 30 Train Loss: 8.18950775283156e-06 Validation Loss: 9.49229621887207\n",
      "Epoch 490 Batch: 30 Train Loss: 1.94654112419812e-05 Validation Loss: 13.689607620239258\n",
      "Epoch 491 Batch: 30 Train Loss: 4.286419425625354e-05 Validation Loss: 14.264631271362305\n",
      "Epoch 492 Batch: 30 Train Loss: 5.960287580819568e-06 Validation Loss: 5.485471725463867\n",
      "Epoch 493 Batch: 30 Train Loss: 3.45552725775633e-05 Validation Loss: 9.888726234436035\n",
      "Epoch 494 Batch: 30 Train Loss: 1.311297978645598e-06 Validation Loss: 14.620485305786133\n",
      "Epoch 495 Batch: 30 Train Loss: 3.523354098433629e-05 Validation Loss: 6.185247898101807\n",
      "Epoch 496 Batch: 30 Train Loss: 9.492650860920548e-05 Validation Loss: 11.983858108520508\n",
      "Epoch 497 Batch: 30 Train Loss: 1.7881313851830782e-06 Validation Loss: 17.991039276123047\n",
      "Epoch 498 Batch: 30 Train Loss: 2.41620400629472e-05 Validation Loss: 11.736967086791992\n",
      "Epoch 499 Batch: 30 Train Loss: 1.0835901775863022e-05 Validation Loss: 8.529511451721191\n",
      "Epoch 500 Batch: 30 Train Loss: 1.1563258794922149e-06 Validation Loss: 3.143213987350464\n",
      "Epoch 501 Batch: 30 Train Loss: 1.311295591222006e-06 Validation Loss: 19.875410079956055\n",
      "Epoch 502 Batch: 30 Train Loss: 5.6742765082162805e-06 Validation Loss: 18.078189849853516\n",
      "Epoch 503 Batch: 30 Train Loss: 3.2901436952670338e-06 Validation Loss: 1.6711877584457397\n",
      "Epoch 504 Batch: 30 Train Loss: 6.198778919497272e-06 Validation Loss: 17.617782592773438\n",
      "Epoch 505 Batch: 30 Train Loss: 1.4149298294796608e-05 Validation Loss: 8.90722370147705\n",
      "Epoch 506 Batch: 30 Train Loss: 3.063633585043135e-06 Validation Loss: 6.480108737945557\n",
      "Epoch 507 Batch: 30 Train Loss: 5.769633389718365e-06 Validation Loss: 9.781686782836914\n",
      "Epoch 508 Batch: 30 Train Loss: 3.600091531552607e-06 Validation Loss: 11.066240310668945\n",
      "Epoch 509 Batch: 30 Train Loss: 5.960455382592045e-07 Validation Loss: 16.758365631103516\n",
      "Epoch 510 Batch: 30 Train Loss: 3.075580025324598e-06 Validation Loss: 7.2935285568237305\n",
      "Epoch 511 Batch: 30 Train Loss: 2.443778384986217e-06 Validation Loss: 25.987707138061523\n",
      "Epoch 512 Batch: 30 Train Loss: 3.612803266150877e-05 Validation Loss: 3.1767325401306152\n",
      "Epoch 513 Batch: 30 Train Loss: 1.6808469354145927e-06 Validation Loss: 24.35235023498535\n",
      "Epoch 514 Batch: 30 Train Loss: 1.3029144611209631e-05 Validation Loss: 13.564048767089844\n",
      "Epoch 515 Batch: 30 Train Loss: 2.21727668758831e-06 Validation Loss: 3.1194424629211426\n",
      "Epoch 516 Batch: 30 Train Loss: 7.152554815093026e-08 Validation Loss: 25.34521484375\n",
      "Epoch 517 Batch: 30 Train Loss: 0.0 Validation Loss: 21.613330841064453\n",
      "Epoch 518 Batch: 30 Train Loss: 1.6689287463123037e-07 Validation Loss: 6.354133605957031\n",
      "Epoch 519 Batch: 30 Train Loss: 2.9563659609266324e-06 Validation Loss: 9.075315475463867\n",
      "Epoch 520 Batch: 30 Train Loss: 3.9219457903527655e-06 Validation Loss: 4.742093563079834\n",
      "Epoch 521 Batch: 30 Train Loss: 8.225421197494143e-07 Validation Loss: 16.903079986572266\n",
      "Epoch 522 Batch: 30 Train Loss: 1.335141746494628e-06 Validation Loss: 7.944755554199219\n",
      "Epoch 523 Batch: 30 Train Loss: 3.0278927170002135e-06 Validation Loss: 6.529466152191162\n",
      "Epoch 524 Batch: 30 Train Loss: 1.6093210888357135e-06 Validation Loss: 16.046985626220703\n",
      "Epoch 525 Batch: 30 Train Loss: 1.2755341458614566e-06 Validation Loss: 18.096996307373047\n",
      "Epoch 526 Batch: 30 Train Loss: 8.463824769933126e-07 Validation Loss: 19.62250518798828\n",
      "Epoch 527 Batch: 30 Train Loss: 1.6450810562673723e-06 Validation Loss: 17.016033172607422\n",
      "Epoch 528 Batch: 30 Train Loss: 2.1815158106619492e-06 Validation Loss: 13.871030807495117\n",
      "Epoch 529 Batch: 30 Train Loss: 1.9073468138230965e-07 Validation Loss: 11.218877792358398\n",
      "Epoch 530 Batch: 30 Train Loss: 7.152556236178498e-08 Validation Loss: 7.5943450927734375\n",
      "Epoch 531 Batch: 30 Train Loss: 7.033323754512821e-07 Validation Loss: 12.661067962646484\n",
      "Epoch 532 Batch: 30 Train Loss: 3.2424438813905e-06 Validation Loss: 22.98692512512207\n",
      "Epoch 533 Batch: 30 Train Loss: 2.384183090953229e-07 Validation Loss: 26.47185707092285\n",
      "Epoch 534 Batch: 30 Train Loss: 1.0848025340237655e-06 Validation Loss: 10.150429725646973\n",
      "Epoch 535 Batch: 30 Train Loss: 1.0728833643725011e-07 Validation Loss: 9.386176109313965\n",
      "Epoch 536 Batch: 30 Train Loss: 7.98699716142437e-07 Validation Loss: 13.737963676452637\n",
      "Epoch 537 Batch: 30 Train Loss: 2.2530325622938108e-06 Validation Loss: 7.376086235046387\n",
      "Epoch 538 Batch: 30 Train Loss: 3.3378583452758903e-07 Validation Loss: 13.201678276062012\n",
      "Epoch 539 Batch: 30 Train Loss: 1.192092824453539e-08 Validation Loss: 2.8350155353546143\n",
      "Epoch 540 Batch: 30 Train Loss: 6.198864070938725e-07 Validation Loss: 8.658807754516602\n",
      "Epoch 541 Batch: 30 Train Loss: 1.9073470980401908e-07 Validation Loss: 11.691296577453613\n",
      "Epoch 542 Batch: 30 Train Loss: 1.3112974102114094e-06 Validation Loss: 18.44487190246582\n",
      "Epoch 543 Batch: 30 Train Loss: 2.384185471271394e-08 Validation Loss: 15.180383682250977\n",
      "Epoch 544 Batch: 30 Train Loss: 2.3841843699301535e-07 Validation Loss: 12.151815414428711\n",
      "Epoch 545 Batch: 30 Train Loss: 3.099438004028343e-07 Validation Loss: 15.265355110168457\n",
      "Epoch 546 Batch: 30 Train Loss: 3.0874739422870334e-06 Validation Loss: 22.510568618774414\n",
      "Epoch 547 Batch: 30 Train Loss: 1.192092824453539e-08 Validation Loss: 11.66401481628418\n",
      "Epoch 548 Batch: 30 Train Loss: 1.7881384906104358e-07 Validation Loss: 14.489067077636719\n",
      "Epoch 549 Batch: 30 Train Loss: 3.9339010982075706e-07 Validation Loss: 14.832858085632324\n",
      "Epoch 550 Batch: 30 Train Loss: 3.099436867159966e-07 Validation Loss: 5.870507717132568\n",
      "Epoch 551 Batch: 30 Train Loss: 1.1920921849650767e-07 Validation Loss: 19.86998176574707\n",
      "Epoch 552 Batch: 30 Train Loss: 2.7418104764365125e-07 Validation Loss: 12.891921997070312\n",
      "Epoch 553 Batch: 30 Train Loss: 3.576278118089249e-08 Validation Loss: 23.179733276367188\n",
      "Epoch 554 Batch: 30 Train Loss: 3.576278118089249e-08 Validation Loss: 16.42911720275879\n",
      "Epoch 555 Batch: 30 Train Loss: 3.576276412786683e-07 Validation Loss: 9.712298393249512\n",
      "Epoch 556 Batch: 30 Train Loss: 4.2915252151942695e-07 Validation Loss: 15.939729690551758\n",
      "Epoch 557 Batch: 30 Train Loss: 0.0 Validation Loss: 17.54929542541504\n",
      "Epoch 558 Batch: 30 Train Loss: 4.76837058727142e-08 Validation Loss: 19.963714599609375\n",
      "Epoch 559 Batch: 30 Train Loss: 5.960463411724959e-08 Validation Loss: 5.215771675109863\n",
      "Epoch 560 Batch: 30 Train Loss: 1.9073468138230965e-07 Validation Loss: 30.6949405670166\n",
      "Epoch 561 Batch: 30 Train Loss: 0.0 Validation Loss: 9.638372421264648\n",
      "Epoch 562 Batch: 30 Train Loss: 2.0265562739041343e-07 Validation Loss: 14.96923828125\n",
      "Epoch 563 Batch: 30 Train Loss: 5.960463411724959e-08 Validation Loss: 19.601478576660156\n",
      "Epoch 564 Batch: 30 Train Loss: 0.0 Validation Loss: 17.434141159057617\n",
      "Epoch 565 Batch: 30 Train Loss: 2.6226027216580405e-07 Validation Loss: 10.73871898651123\n",
      "Epoch 566 Batch: 30 Train Loss: 3.576278118089249e-08 Validation Loss: 18.443618774414062\n",
      "Epoch 567 Batch: 30 Train Loss: 7.152556236178498e-08 Validation Loss: 19.506772994995117\n",
      "Epoch 568 Batch: 30 Train Loss: 7.152554815093026e-08 Validation Loss: 7.6332688331604\n",
      "Epoch 569 Batch: 30 Train Loss: 1.6689293147464923e-07 Validation Loss: 14.127050399780273\n",
      "Epoch 570 Batch: 30 Train Loss: 0.0 Validation Loss: 6.147216796875\n",
      "Epoch 571 Batch: 30 Train Loss: 8.344648705360669e-08 Validation Loss: 15.54157543182373\n",
      "Epoch 572 Batch: 30 Train Loss: 1.0728833643725011e-07 Validation Loss: 10.492647171020508\n",
      "Epoch 573 Batch: 30 Train Loss: 1.192092824453539e-08 Validation Loss: 10.022069931030273\n",
      "Epoch 574 Batch: 30 Train Loss: 4.291525783628458e-07 Validation Loss: 22.128643035888672\n",
      "Epoch 575 Batch: 30 Train Loss: 1.311301218720473e-07 Validation Loss: 16.49520492553711\n",
      "Epoch 576 Batch: 30 Train Loss: 2.384185648907078e-08 Validation Loss: 17.970184326171875\n",
      "Epoch 577 Batch: 30 Train Loss: 1.1920926823449918e-07 Validation Loss: 13.384561538696289\n",
      "Epoch 578 Batch: 30 Train Loss: 0.0 Validation Loss: 20.207874298095703\n",
      "Epoch 579 Batch: 30 Train Loss: 0.0 Validation Loss: 16.77851676940918\n",
      "Epoch 580 Batch: 30 Train Loss: 2.384185471271394e-08 Validation Loss: 39.586097717285156\n",
      "Epoch 581 Batch: 30 Train Loss: 2.384185471271394e-08 Validation Loss: 13.79589557647705\n",
      "Epoch 582 Batch: 30 Train Loss: 1.192092824453539e-08 Validation Loss: 8.12761116027832\n",
      "Epoch 583 Batch: 30 Train Loss: 1.192092824453539e-08 Validation Loss: 9.861735343933105\n",
      "Epoch 584 Batch: 30 Train Loss: 1.192092824453539e-08 Validation Loss: 17.980850219726562\n",
      "Epoch 585 Batch: 30 Train Loss: 0.0 Validation Loss: 20.8992977142334\n",
      "Epoch 586 Batch: 30 Train Loss: 1.192092824453539e-08 Validation Loss: 15.902917861938477\n",
      "Epoch 587 Batch: 30 Train Loss: 0.0 Validation Loss: 20.555011749267578\n",
      "Epoch 588 Batch: 30 Train Loss: 0.0 Validation Loss: 21.488758087158203\n",
      "Epoch 589 Batch: 30 Train Loss: 1.192092824453539e-08 Validation Loss: 22.848011016845703\n",
      "Epoch 590 Batch: 30 Train Loss: 0.0 Validation Loss: 33.39257049560547\n",
      "Epoch 591 Batch: 30 Train Loss: 2.384185648907078e-08 Validation Loss: 26.912311553955078\n",
      "Epoch 592 Batch: 30 Train Loss: 0.0 Validation Loss: 19.082082748413086\n",
      "Epoch 593 Batch: 30 Train Loss: 1.192092824453539e-08 Validation Loss: 16.10293197631836\n",
      "Epoch 594 Batch: 30 Train Loss: 0.0 Validation Loss: 20.23739242553711\n",
      "Epoch 595 Batch: 30 Train Loss: 3.576278473360617e-08 Validation Loss: 13.77635383605957\n",
      "Epoch 596 Batch: 30 Train Loss: 0.0 Validation Loss: 16.684398651123047\n",
      "Epoch 597 Batch: 30 Train Loss: 1.192092824453539e-08 Validation Loss: 15.202230453491211\n",
      "Epoch 598 Batch: 30 Train Loss: 0.0 Validation Loss: 12.825839042663574\n",
      "Epoch 599 Batch: 30 Train Loss: 1.192092824453539e-08 Validation Loss: 20.093297958374023\n",
      "Epoch 600 Batch: 30 Train Loss: 0.0 Validation Loss: 7.130685329437256\n",
      "Epoch 601 Batch: 30 Train Loss: 1.192092824453539e-08 Validation Loss: 16.871219635009766\n",
      "Epoch 602 Batch: 30 Train Loss: 2.384185648907078e-08 Validation Loss: 19.56085968017578\n",
      "Epoch 603 Batch: 30 Train Loss: 0.0 Validation Loss: 13.708247184753418\n",
      "Epoch 604 Batch: 30 Train Loss: 0.0 Validation Loss: 17.093997955322266\n",
      "Epoch 605 Batch: 30 Train Loss: 0.0 Validation Loss: 19.3153018951416\n",
      "Epoch 606 Batch: 30 Train Loss: 0.0 Validation Loss: 18.943191528320312\n",
      "Epoch 607 Batch: 30 Train Loss: 0.0 Validation Loss: 37.821746826171875\n",
      "Epoch 608 Batch: 30 Train Loss: 0.0 Validation Loss: 15.362083435058594\n",
      "Epoch 609 Batch: 30 Train Loss: 1.192092824453539e-08 Validation Loss: 42.81089401245117\n",
      "Epoch 610 Batch: 30 Train Loss: 0.0 Validation Loss: 9.706168174743652\n",
      "Epoch 611 Batch: 30 Train Loss: 0.0 Validation Loss: 24.69350814819336\n",
      "Epoch 612 Batch: 30 Train Loss: 0.0 Validation Loss: 19.894067764282227\n",
      "Epoch 613 Batch: 30 Train Loss: 0.0 Validation Loss: 19.438013076782227\n",
      "Epoch 614 Batch: 30 Train Loss: 0.0 Validation Loss: 20.513301849365234\n",
      "Epoch 615 Batch: 30 Train Loss: 0.0 Validation Loss: 18.721668243408203\n",
      "Epoch 616 Batch: 30 Train Loss: 0.0 Validation Loss: 18.455524444580078\n",
      "Epoch 617 Batch: 30 Train Loss: 1.192092824453539e-08 Validation Loss: 16.92337989807129\n",
      "Epoch 618 Batch: 30 Train Loss: 0.0 Validation Loss: 40.58428955078125\n",
      "Epoch 619 Batch: 30 Train Loss: 0.0 Validation Loss: 11.118765830993652\n",
      "Epoch 620 Batch: 30 Train Loss: 1.192092824453539e-08 Validation Loss: 20.308691024780273\n",
      "Epoch 621 Batch: 30 Train Loss: 0.0 Validation Loss: 4.628670692443848\n",
      "Epoch 622 Batch: 30 Train Loss: 0.0 Validation Loss: 24.267501831054688\n",
      "Epoch 623 Batch: 30 Train Loss: 0.0 Validation Loss: 9.851906776428223\n",
      "Epoch 624 Batch: 30 Train Loss: 0.0 Validation Loss: 47.87839889526367\n",
      "Epoch 625 Batch: 30 Train Loss: 0.0 Validation Loss: 17.000972747802734\n",
      "Epoch 626 Batch: 30 Train Loss: 0.0 Validation Loss: 12.87492847442627\n",
      "Epoch 627 Batch: 30 Train Loss: 0.0 Validation Loss: 27.501209259033203\n",
      "Epoch 628 Batch: 30 Train Loss: 0.0 Validation Loss: 27.417211532592773\n",
      "Epoch 629 Batch: 30 Train Loss: 0.0 Validation Loss: 17.117124557495117\n",
      "Epoch 630 Batch: 30 Train Loss: 0.0 Validation Loss: 13.26812744140625\n",
      "Epoch 631 Batch: 30 Train Loss: 0.0 Validation Loss: 19.20328140258789\n",
      "Epoch 632 Batch: 30 Train Loss: 0.0 Validation Loss: 20.083913803100586\n",
      "Epoch 633 Batch: 30 Train Loss: 0.0 Validation Loss: 12.642425537109375\n",
      "Epoch 634 Batch: 30 Train Loss: 0.0 Validation Loss: 14.792098999023438\n",
      "Epoch 635 Batch: 30 Train Loss: 0.0 Validation Loss: 32.24736404418945\n",
      "Epoch 636 Batch: 30 Train Loss: 0.0 Validation Loss: 11.785700798034668\n",
      "Epoch 637 Batch: 30 Train Loss: 0.0 Validation Loss: 31.157901763916016\n",
      "Epoch 638 Batch: 30 Train Loss: 0.0 Validation Loss: 16.35579490661621\n",
      "Epoch 639 Batch: 30 Train Loss: 0.0 Validation Loss: 30.383264541625977\n",
      "Epoch 640 Batch: 30 Train Loss: 0.0 Validation Loss: 18.850711822509766\n",
      "Epoch 641 Batch: 30 Train Loss: 0.0 Validation Loss: 9.262750625610352\n",
      "Epoch 642 Batch: 30 Train Loss: 0.0 Validation Loss: 5.959641456604004\n",
      "Epoch 643 Batch: 30 Train Loss: 0.0 Validation Loss: 21.16412353515625\n",
      "Epoch 644 Batch: 30 Train Loss: 0.0 Validation Loss: 18.301681518554688\n",
      "Epoch 645 Batch: 30 Train Loss: 0.0 Validation Loss: 27.700336456298828\n",
      "Epoch 646 Batch: 30 Train Loss: 0.0 Validation Loss: 29.079845428466797\n",
      "Epoch 647 Batch: 30 Train Loss: 0.0 Validation Loss: 36.00377655029297\n",
      "Epoch 648 Batch: 30 Train Loss: 0.0 Validation Loss: 37.77553176879883\n",
      "Epoch 649 Batch: 30 Train Loss: 0.0 Validation Loss: 13.326388359069824\n",
      "Epoch 650 Batch: 30 Train Loss: 0.0 Validation Loss: 17.033992767333984\n",
      "Epoch 651 Batch: 30 Train Loss: 0.0 Validation Loss: 14.242185592651367\n",
      "Epoch 652 Batch: 30 Train Loss: 0.0 Validation Loss: 6.6225266456604\n",
      "Epoch 653 Batch: 30 Train Loss: 0.0 Validation Loss: 27.0513858795166\n",
      "Epoch 654 Batch: 30 Train Loss: 0.0 Validation Loss: 13.237139701843262\n",
      "Epoch 655 Batch: 30 Train Loss: 0.0 Validation Loss: 37.15363693237305\n",
      "Epoch 656 Batch: 30 Train Loss: 0.0 Validation Loss: 14.896222114562988\n",
      "Epoch 657 Batch: 30 Train Loss: 0.0 Validation Loss: 17.64596939086914\n",
      "Epoch 658 Batch: 30 Train Loss: 0.0 Validation Loss: 18.451717376708984\n",
      "Epoch 659 Batch: 30 Train Loss: 0.0 Validation Loss: 20.353206634521484\n",
      "Epoch 660 Batch: 30 Train Loss: 0.0 Validation Loss: 14.726541519165039\n",
      "Epoch 661 Batch: 30 Train Loss: 0.0 Validation Loss: 20.562082290649414\n",
      "Epoch 662 Batch: 30 Train Loss: 0.0 Validation Loss: 35.15976333618164\n",
      "Epoch 663 Batch: 30 Train Loss: 0.0 Validation Loss: 15.503129959106445\n",
      "Epoch 664 Batch: 30 Train Loss: 0.0 Validation Loss: 9.900993347167969\n",
      "Epoch 665 Batch: 30 Train Loss: 0.0 Validation Loss: 23.26621437072754\n",
      "Epoch 666 Batch: 30 Train Loss: 0.0 Validation Loss: 31.691974639892578\n",
      "Epoch 667 Batch: 30 Train Loss: 0.0 Validation Loss: 12.35595703125\n",
      "Epoch 668 Batch: 30 Train Loss: 0.0 Validation Loss: 23.658954620361328\n",
      "Epoch 669 Batch: 30 Train Loss: 0.0 Validation Loss: 23.570301055908203\n",
      "Epoch 670 Batch: 30 Train Loss: 0.0 Validation Loss: 10.951078414916992\n",
      "Epoch 671 Batch: 30 Train Loss: 0.0 Validation Loss: 30.02312660217285\n",
      "Epoch 672 Batch: 30 Train Loss: 0.0 Validation Loss: 18.69547462463379\n",
      "Epoch 673 Batch: 30 Train Loss: 0.0 Validation Loss: 25.66501808166504\n",
      "Epoch 674 Batch: 30 Train Loss: 0.0 Validation Loss: 16.80538558959961\n",
      "Epoch 675 Batch: 30 Train Loss: 0.0 Validation Loss: 6.605341911315918\n",
      "Epoch 676 Batch: 30 Train Loss: 0.0 Validation Loss: 26.371444702148438\n",
      "Epoch 677 Batch: 30 Train Loss: 0.0 Validation Loss: 24.19330406188965\n",
      "Epoch 678 Batch: 30 Train Loss: 0.0 Validation Loss: 12.705683708190918\n",
      "Epoch 679 Batch: 30 Train Loss: 0.0 Validation Loss: 23.586811065673828\n",
      "Epoch 680 Batch: 30 Train Loss: 0.0 Validation Loss: 14.29347038269043\n",
      "Epoch 681 Batch: 30 Train Loss: 0.0 Validation Loss: 21.054500579833984\n",
      "Epoch 682 Batch: 30 Train Loss: 0.0 Validation Loss: 13.263647079467773\n",
      "Epoch 683 Batch: 30 Train Loss: 0.0 Validation Loss: 26.607019424438477\n",
      "Epoch 684 Batch: 30 Train Loss: 0.0 Validation Loss: 25.60611915588379\n",
      "Epoch 685 Batch: 30 Train Loss: 0.0 Validation Loss: 14.908975601196289\n",
      "Epoch 686 Batch: 30 Train Loss: 0.0 Validation Loss: 16.503036499023438\n",
      "Epoch 687 Batch: 30 Train Loss: 0.0 Validation Loss: 10.237138748168945\n",
      "Epoch 688 Batch: 30 Train Loss: 0.0 Validation Loss: 28.041545867919922\n",
      "Epoch 689 Batch: 30 Train Loss: 0.0 Validation Loss: 15.946993827819824\n",
      "Epoch 690 Batch: 30 Train Loss: 0.0 Validation Loss: 29.481494903564453\n",
      "Epoch 691 Batch: 30 Train Loss: 0.0 Validation Loss: 16.478588104248047\n",
      "Epoch 692 Batch: 30 Train Loss: 0.0 Validation Loss: 16.801979064941406\n",
      "Epoch 693 Batch: 30 Train Loss: 0.0 Validation Loss: 6.117865085601807\n",
      "Epoch 694 Batch: 30 Train Loss: 0.0 Validation Loss: 11.835550308227539\n",
      "Epoch 695 Batch: 30 Train Loss: 0.0 Validation Loss: 14.805822372436523\n",
      "Epoch 696 Batch: 30 Train Loss: 0.0 Validation Loss: 27.016998291015625\n",
      "Epoch 697 Batch: 30 Train Loss: 0.0 Validation Loss: 9.8529634475708\n",
      "Epoch 698 Batch: 30 Train Loss: 0.0 Validation Loss: 15.769831657409668\n",
      "Epoch 699 Batch: 30 Train Loss: 0.0 Validation Loss: 32.47301483154297\n",
      "Epoch 700 Batch: 30 Train Loss: 0.0 Validation Loss: 22.917051315307617\n",
      "Epoch 701 Batch: 30 Train Loss: 0.0 Validation Loss: 19.064678192138672\n",
      "Epoch 702 Batch: 30 Train Loss: 0.0 Validation Loss: 20.998760223388672\n",
      "Epoch 703 Batch: 30 Train Loss: 0.0 Validation Loss: 16.452573776245117\n",
      "Epoch 704 Batch: 30 Train Loss: 0.0 Validation Loss: 9.736435890197754\n",
      "Epoch 705 Batch: 30 Train Loss: 0.0 Validation Loss: 20.671260833740234\n",
      "Epoch 706 Batch: 30 Train Loss: 0.0 Validation Loss: 21.45449447631836\n",
      "Epoch 707 Batch: 30 Train Loss: 0.0 Validation Loss: 29.621801376342773\n",
      "Epoch 708 Batch: 30 Train Loss: 0.0 Validation Loss: 11.253168106079102\n",
      "Epoch 709 Batch: 30 Train Loss: 0.0 Validation Loss: 21.79952621459961\n",
      "Epoch 710 Batch: 30 Train Loss: 0.0 Validation Loss: 25.701772689819336\n",
      "Epoch 711 Batch: 30 Train Loss: 0.0 Validation Loss: 9.259655952453613\n",
      "Epoch 712 Batch: 30 Train Loss: 0.0 Validation Loss: 12.208229064941406\n",
      "Epoch 713 Batch: 30 Train Loss: 0.0 Validation Loss: 18.205421447753906\n",
      "Epoch 714 Batch: 30 Train Loss: 0.0 Validation Loss: 23.4639949798584\n",
      "Epoch 715 Batch: 30 Train Loss: 0.0 Validation Loss: 22.238216400146484\n",
      "Epoch 716 Batch: 30 Train Loss: 0.0 Validation Loss: 13.6763334274292\n",
      "Epoch 717 Batch: 30 Train Loss: 0.0 Validation Loss: 22.285888671875\n",
      "Epoch 718 Batch: 30 Train Loss: 0.0 Validation Loss: 18.103282928466797\n",
      "Epoch 719 Batch: 30 Train Loss: 0.0 Validation Loss: 33.45780944824219\n",
      "Epoch 720 Batch: 30 Train Loss: 0.0 Validation Loss: 16.162067413330078\n",
      "Epoch 721 Batch: 30 Train Loss: 0.0 Validation Loss: 24.24102210998535\n",
      "Epoch 722 Batch: 30 Train Loss: 0.0 Validation Loss: 18.09144401550293\n",
      "Epoch 723 Batch: 30 Train Loss: 0.0 Validation Loss: 20.97435188293457\n",
      "Epoch 724 Batch: 30 Train Loss: 0.0 Validation Loss: 21.495271682739258\n",
      "Epoch 725 Batch: 30 Train Loss: 0.0 Validation Loss: 29.74920082092285\n",
      "Epoch 726 Batch: 30 Train Loss: 0.0 Validation Loss: 14.089544296264648\n",
      "Epoch 727 Batch: 30 Train Loss: 0.0 Validation Loss: 27.007732391357422\n",
      "Epoch 728 Batch: 30 Train Loss: 0.0 Validation Loss: 18.889114379882812\n",
      "Epoch 729 Batch: 30 Train Loss: 0.0 Validation Loss: 24.819332122802734\n",
      "Epoch 730 Batch: 30 Train Loss: 0.0 Validation Loss: 21.15504264831543\n",
      "Epoch 731 Batch: 30 Train Loss: 0.0 Validation Loss: 20.145750045776367\n",
      "Epoch 732 Batch: 30 Train Loss: 0.0 Validation Loss: 32.966827392578125\n",
      "Epoch 733 Batch: 30 Train Loss: 0.0 Validation Loss: 32.267738342285156\n",
      "Epoch 734 Batch: 30 Train Loss: 0.0 Validation Loss: 9.911145210266113\n",
      "Epoch 735 Batch: 30 Train Loss: 0.0 Validation Loss: 21.36933135986328\n",
      "Epoch 736 Batch: 30 Train Loss: 0.0 Validation Loss: 14.903470039367676\n",
      "Epoch 737 Batch: 30 Train Loss: 0.0 Validation Loss: 23.58504867553711\n",
      "Epoch 738 Batch: 30 Train Loss: 0.0 Validation Loss: 30.10013198852539\n",
      "Epoch 739 Batch: 30 Train Loss: 0.0 Validation Loss: 24.021526336669922\n",
      "Epoch 740 Batch: 30 Train Loss: 0.0 Validation Loss: 19.80465316772461\n",
      "Epoch 741 Batch: 30 Train Loss: 0.0 Validation Loss: 14.039077758789062\n",
      "Epoch 742 Batch: 30 Train Loss: 0.0 Validation Loss: 12.476761817932129\n",
      "Epoch 743 Batch: 30 Train Loss: 0.0 Validation Loss: 12.970324516296387\n",
      "Epoch 744 Batch: 30 Train Loss: 0.0 Validation Loss: 31.366785049438477\n",
      "Epoch 745 Batch: 30 Train Loss: 0.0 Validation Loss: 19.939172744750977\n",
      "Epoch 746 Batch: 30 Train Loss: 0.0 Validation Loss: 8.250359535217285\n",
      "Epoch 747 Batch: 30 Train Loss: 0.0 Validation Loss: 20.910415649414062\n",
      "Epoch 748 Batch: 30 Train Loss: 0.0 Validation Loss: 62.435760498046875\n",
      "Epoch 749 Batch: 30 Train Loss: 0.0 Validation Loss: 23.565879821777344\n",
      "Epoch 750 Batch: 30 Train Loss: 0.0 Validation Loss: 15.731164932250977\n",
      "Epoch 751 Batch: 30 Train Loss: 0.0 Validation Loss: 12.910901069641113\n",
      "Epoch 752 Batch: 30 Train Loss: 0.0 Validation Loss: 32.13030242919922\n",
      "Epoch 753 Batch: 30 Train Loss: 0.0 Validation Loss: 9.4027738571167\n",
      "Epoch 754 Batch: 30 Train Loss: 0.0 Validation Loss: 25.415096282958984\n",
      "Epoch 755 Batch: 30 Train Loss: 0.0 Validation Loss: 25.924724578857422\n",
      "Epoch 756 Batch: 30 Train Loss: 0.0 Validation Loss: 12.322662353515625\n",
      "Epoch 757 Batch: 30 Train Loss: 0.0 Validation Loss: 36.876914978027344\n",
      "Epoch 758 Batch: 30 Train Loss: 0.0 Validation Loss: 18.382183074951172\n",
      "Epoch 759 Batch: 30 Train Loss: 0.0 Validation Loss: 19.048294067382812\n",
      "Epoch 760 Batch: 30 Train Loss: 0.0 Validation Loss: 13.668489456176758\n",
      "Epoch 761 Batch: 30 Train Loss: 0.0 Validation Loss: 10.18181037902832\n",
      "Epoch 762 Batch: 30 Train Loss: 0.0 Validation Loss: 7.505436897277832\n",
      "Epoch 763 Batch: 30 Train Loss: 0.0 Validation Loss: 12.663657188415527\n",
      "Epoch 764 Batch: 30 Train Loss: 0.0 Validation Loss: 29.084606170654297\n",
      "Epoch 765 Batch: 30 Train Loss: 0.0 Validation Loss: 25.66188621520996\n",
      "Epoch 766 Batch: 30 Train Loss: 0.0 Validation Loss: 22.77219009399414\n",
      "Epoch 767 Batch: 30 Train Loss: 0.0 Validation Loss: 9.781816482543945\n",
      "Epoch 768 Batch: 30 Train Loss: 0.0 Validation Loss: 12.846506118774414\n",
      "Epoch 769 Batch: 30 Train Loss: 0.0 Validation Loss: 23.89865493774414\n",
      "Epoch 770 Batch: 30 Train Loss: 0.0 Validation Loss: 20.6102237701416\n",
      "Epoch 771 Batch: 30 Train Loss: 0.0 Validation Loss: 31.448511123657227\n",
      "Epoch 772 Batch: 30 Train Loss: 0.0 Validation Loss: 8.626008987426758\n",
      "Epoch 773 Batch: 30 Train Loss: 0.0 Validation Loss: 17.721445083618164\n",
      "Epoch 774 Batch: 30 Train Loss: 0.0 Validation Loss: 9.665486335754395\n",
      "Epoch 775 Batch: 30 Train Loss: 0.0 Validation Loss: 13.272790908813477\n",
      "Epoch 776 Batch: 30 Train Loss: 0.0 Validation Loss: 9.828082084655762\n",
      "Epoch 777 Batch: 30 Train Loss: 0.0 Validation Loss: 13.39195728302002\n",
      "Epoch 778 Batch: 30 Train Loss: 0.0 Validation Loss: 18.960203170776367\n",
      "Epoch 779 Batch: 30 Train Loss: 0.0 Validation Loss: 17.827383041381836\n",
      "Epoch 780 Batch: 30 Train Loss: 0.0 Validation Loss: 21.24496078491211\n",
      "Epoch 781 Batch: 30 Train Loss: 0.0 Validation Loss: 12.248135566711426\n",
      "Epoch 782 Batch: 30 Train Loss: 0.0 Validation Loss: 10.126142501831055\n",
      "Epoch 783 Batch: 30 Train Loss: 0.0 Validation Loss: 20.697818756103516\n",
      "Epoch 784 Batch: 30 Train Loss: 0.0 Validation Loss: 22.641132354736328\n",
      "Epoch 785 Batch: 30 Train Loss: 0.0 Validation Loss: 33.41202926635742\n",
      "Epoch 786 Batch: 30 Train Loss: 0.0 Validation Loss: 13.792376518249512\n",
      "Epoch 787 Batch: 30 Train Loss: 0.0 Validation Loss: 23.05519676208496\n",
      "Epoch 788 Batch: 30 Train Loss: 0.0 Validation Loss: 20.484771728515625\n",
      "Epoch 789 Batch: 30 Train Loss: 0.0 Validation Loss: 24.883625030517578\n",
      "Epoch 790 Batch: 30 Train Loss: 0.0 Validation Loss: 19.816362380981445\n",
      "Epoch 791 Batch: 30 Train Loss: 0.0 Validation Loss: 21.550504684448242\n",
      "Epoch 792 Batch: 30 Train Loss: 0.0 Validation Loss: 19.057653427124023\n",
      "Epoch 793 Batch: 30 Train Loss: 0.0 Validation Loss: 15.519024848937988\n",
      "Epoch 794 Batch: 30 Train Loss: 0.0 Validation Loss: 25.70119285583496\n",
      "Epoch 795 Batch: 30 Train Loss: 0.0 Validation Loss: 21.472679138183594\n",
      "Epoch 796 Batch: 30 Train Loss: 0.0 Validation Loss: 25.277894973754883\n",
      "Epoch 797 Batch: 30 Train Loss: 0.0 Validation Loss: 26.869897842407227\n",
      "Epoch 798 Batch: 30 Train Loss: 0.0 Validation Loss: 15.143728256225586\n",
      "Epoch 799 Batch: 30 Train Loss: 0.0 Validation Loss: 43.524131774902344\n",
      "Epoch 800 Batch: 30 Train Loss: 0.0 Validation Loss: 18.128795623779297\n",
      "Epoch 801 Batch: 30 Train Loss: 0.0 Validation Loss: 21.145339965820312\n",
      "Epoch 802 Batch: 30 Train Loss: 0.0 Validation Loss: 36.16896057128906\n",
      "Epoch 803 Batch: 30 Train Loss: 0.0 Validation Loss: 28.482952117919922\n",
      "Epoch 804 Batch: 30 Train Loss: 0.0 Validation Loss: 10.044981002807617\n",
      "Epoch 805 Batch: 30 Train Loss: 0.0 Validation Loss: 16.036457061767578\n",
      "Epoch 806 Batch: 30 Train Loss: 0.0 Validation Loss: 15.304719924926758\n",
      "Epoch 807 Batch: 30 Train Loss: 0.0 Validation Loss: 25.478422164916992\n",
      "Epoch 808 Batch: 30 Train Loss: 0.0 Validation Loss: 27.011322021484375\n",
      "Epoch 809 Batch: 30 Train Loss: 0.0 Validation Loss: 16.758729934692383\n",
      "Epoch 810 Batch: 30 Train Loss: 0.0 Validation Loss: 21.091339111328125\n",
      "Epoch 811 Batch: 30 Train Loss: 0.0 Validation Loss: 31.337017059326172\n",
      "Epoch 812 Batch: 30 Train Loss: 0.0 Validation Loss: 28.9040584564209\n",
      "Epoch 813 Batch: 30 Train Loss: 0.0 Validation Loss: 17.61060905456543\n",
      "Epoch 814 Batch: 30 Train Loss: 0.0 Validation Loss: 19.893962860107422\n",
      "Epoch 815 Batch: 30 Train Loss: 0.0 Validation Loss: 11.912991523742676\n",
      "Epoch 816 Batch: 30 Train Loss: 0.0 Validation Loss: 21.625720977783203\n",
      "Epoch 817 Batch: 30 Train Loss: 0.0 Validation Loss: 15.799446105957031\n",
      "Epoch 818 Batch: 30 Train Loss: 0.0 Validation Loss: 11.274248123168945\n",
      "Epoch 819 Batch: 30 Train Loss: 0.0 Validation Loss: 18.96230697631836\n",
      "Epoch 820 Batch: 30 Train Loss: 0.0 Validation Loss: 62.42335891723633\n",
      "Epoch 821 Batch: 30 Train Loss: 0.0 Validation Loss: 28.909143447875977\n",
      "Epoch 822 Batch: 30 Train Loss: 0.0 Validation Loss: 17.856767654418945\n",
      "Epoch 823 Batch: 30 Train Loss: 0.0 Validation Loss: 22.866945266723633\n",
      "Epoch 824 Batch: 30 Train Loss: 0.0 Validation Loss: 22.346088409423828\n",
      "Epoch 825 Batch: 30 Train Loss: 0.0 Validation Loss: 17.911123275756836\n",
      "Epoch 826 Batch: 30 Train Loss: 0.0 Validation Loss: 22.959875106811523\n",
      "Epoch 827 Batch: 30 Train Loss: 0.0 Validation Loss: 42.778526306152344\n",
      "Epoch 828 Batch: 30 Train Loss: 0.0 Validation Loss: 27.053226470947266\n",
      "Epoch 829 Batch: 30 Train Loss: 0.0 Validation Loss: 17.960777282714844\n",
      "Epoch 830 Batch: 30 Train Loss: 0.0 Validation Loss: 13.298001289367676\n",
      "Epoch 831 Batch: 30 Train Loss: 0.0 Validation Loss: 7.678868770599365\n",
      "Epoch 832 Batch: 30 Train Loss: 0.0 Validation Loss: 20.02001953125\n",
      "Epoch 833 Batch: 30 Train Loss: 0.0 Validation Loss: 15.875595092773438\n",
      "Epoch 834 Batch: 30 Train Loss: 0.0 Validation Loss: 6.838915824890137\n",
      "Epoch 835 Batch: 30 Train Loss: 0.0 Validation Loss: 3.892129421234131\n",
      "Epoch 836 Batch: 30 Train Loss: 0.0 Validation Loss: 18.483461380004883\n",
      "Epoch 837 Batch: 30 Train Loss: 0.0 Validation Loss: 14.256494522094727\n",
      "Epoch 838 Batch: 30 Train Loss: 0.0 Validation Loss: 31.233373641967773\n",
      "Epoch 839 Batch: 30 Train Loss: 0.0 Validation Loss: 26.36321449279785\n",
      "Epoch 840 Batch: 30 Train Loss: 0.0 Validation Loss: 26.781845092773438\n",
      "Epoch 841 Batch: 30 Train Loss: 0.0 Validation Loss: 26.809885025024414\n",
      "Epoch 842 Batch: 30 Train Loss: 0.0 Validation Loss: 24.499248504638672\n",
      "Epoch 843 Batch: 30 Train Loss: 0.0 Validation Loss: 20.216291427612305\n",
      "Epoch 844 Batch: 30 Train Loss: 0.0 Validation Loss: 18.205556869506836\n",
      "Epoch 845 Batch: 30 Train Loss: 0.0 Validation Loss: 20.864946365356445\n",
      "Epoch 846 Batch: 30 Train Loss: 0.0 Validation Loss: 10.803335189819336\n",
      "Epoch 847 Batch: 30 Train Loss: 0.0 Validation Loss: 24.737957000732422\n",
      "Epoch 848 Batch: 30 Train Loss: 0.0 Validation Loss: 33.288787841796875\n",
      "Epoch 849 Batch: 30 Train Loss: 0.0 Validation Loss: 30.854969024658203\n",
      "Epoch 850 Batch: 30 Train Loss: 0.0 Validation Loss: 19.594112396240234\n",
      "Epoch 851 Batch: 30 Train Loss: 0.0 Validation Loss: 31.213668823242188\n",
      "Epoch 852 Batch: 30 Train Loss: 0.0 Validation Loss: 15.152441024780273\n",
      "Epoch 853 Batch: 30 Train Loss: 0.0 Validation Loss: 40.304222106933594\n",
      "Epoch 854 Batch: 30 Train Loss: 0.0 Validation Loss: 50.12996292114258\n",
      "Epoch 855 Batch: 30 Train Loss: 0.0 Validation Loss: 27.07329750061035\n",
      "Epoch 856 Batch: 30 Train Loss: 0.0 Validation Loss: 14.218568801879883\n",
      "Epoch 857 Batch: 30 Train Loss: 0.0 Validation Loss: 13.8993501663208\n",
      "Epoch 858 Batch: 30 Train Loss: 0.0 Validation Loss: 32.6260986328125\n",
      "Epoch 859 Batch: 30 Train Loss: 0.0 Validation Loss: 10.682466506958008\n",
      "Epoch 860 Batch: 30 Train Loss: 0.0 Validation Loss: 22.253244400024414\n",
      "Epoch 861 Batch: 30 Train Loss: 0.0 Validation Loss: 19.913471221923828\n",
      "Epoch 862 Batch: 30 Train Loss: 0.0 Validation Loss: 18.11821937561035\n",
      "Epoch 863 Batch: 30 Train Loss: 0.0 Validation Loss: 15.444686889648438\n",
      "Epoch 864 Batch: 30 Train Loss: 0.0 Validation Loss: 19.388723373413086\n",
      "Epoch 865 Batch: 30 Train Loss: 0.0 Validation Loss: 15.18951416015625\n",
      "Epoch 866 Batch: 30 Train Loss: 0.0 Validation Loss: 46.90264892578125\n",
      "Epoch 867 Batch: 30 Train Loss: 0.0 Validation Loss: 20.056156158447266\n",
      "Epoch 868 Batch: 30 Train Loss: 0.0 Validation Loss: 36.519432067871094\n",
      "Epoch 869 Batch: 30 Train Loss: 0.0 Validation Loss: 21.46092987060547\n",
      "Epoch 870 Batch: 30 Train Loss: 0.0 Validation Loss: 18.101215362548828\n",
      "Epoch 871 Batch: 30 Train Loss: 0.0 Validation Loss: 24.47124481201172\n",
      "Epoch 872 Batch: 30 Train Loss: 0.0 Validation Loss: 19.447423934936523\n",
      "Epoch 873 Batch: 30 Train Loss: 0.0 Validation Loss: 26.504150390625\n",
      "Epoch 874 Batch: 30 Train Loss: 0.0 Validation Loss: 9.881767272949219\n",
      "Epoch 875 Batch: 30 Train Loss: 0.0 Validation Loss: 24.889108657836914\n",
      "Epoch 876 Batch: 30 Train Loss: 0.0 Validation Loss: 14.97447681427002\n",
      "Epoch 877 Batch: 30 Train Loss: 0.0 Validation Loss: 20.284595489501953\n",
      "Epoch 878 Batch: 30 Train Loss: 0.0 Validation Loss: 10.819900512695312\n",
      "Epoch 879 Batch: 30 Train Loss: 0.0 Validation Loss: 10.96854305267334\n",
      "Epoch 880 Batch: 30 Train Loss: 0.0 Validation Loss: 18.612079620361328\n",
      "Epoch 881 Batch: 30 Train Loss: 0.0 Validation Loss: 27.238876342773438\n",
      "Epoch 882 Batch: 30 Train Loss: 0.0 Validation Loss: 28.070241928100586\n",
      "Epoch 883 Batch: 30 Train Loss: 0.0 Validation Loss: 14.644502639770508\n",
      "Epoch 884 Batch: 30 Train Loss: 0.0 Validation Loss: 16.791030883789062\n",
      "Epoch 885 Batch: 30 Train Loss: 0.0 Validation Loss: 21.684534072875977\n",
      "Epoch 886 Batch: 30 Train Loss: 0.0 Validation Loss: 36.095062255859375\n",
      "Epoch 887 Batch: 30 Train Loss: 0.0 Validation Loss: 29.160634994506836\n",
      "Epoch 888 Batch: 30 Train Loss: 0.0 Validation Loss: 24.654333114624023\n",
      "Epoch 889 Batch: 30 Train Loss: 0.0 Validation Loss: 24.235628128051758\n",
      "Epoch 890 Batch: 30 Train Loss: 0.0 Validation Loss: 35.59528350830078\n",
      "Epoch 891 Batch: 30 Train Loss: 0.0 Validation Loss: 41.785865783691406\n",
      "Epoch 892 Batch: 30 Train Loss: 0.0 Validation Loss: 8.63986873626709\n",
      "Epoch 893 Batch: 30 Train Loss: 0.0 Validation Loss: 20.640226364135742\n",
      "Epoch 894 Batch: 30 Train Loss: 0.0 Validation Loss: 24.781673431396484\n",
      "Epoch 895 Batch: 30 Train Loss: 0.0 Validation Loss: 23.24803924560547\n",
      "Epoch 896 Batch: 30 Train Loss: 0.0 Validation Loss: 13.63464641571045\n",
      "Epoch 897 Batch: 30 Train Loss: 0.0 Validation Loss: 15.054415702819824\n",
      "Epoch 898 Batch: 30 Train Loss: 0.0 Validation Loss: 27.544300079345703\n",
      "Epoch 899 Batch: 30 Train Loss: 0.0 Validation Loss: 26.276992797851562\n",
      "Epoch 900 Batch: 30 Train Loss: 0.0 Validation Loss: 6.320799827575684\n",
      "Epoch 901 Batch: 30 Train Loss: 0.0 Validation Loss: 12.792093276977539\n",
      "Epoch 902 Batch: 30 Train Loss: 0.0 Validation Loss: 32.597206115722656\n",
      "Epoch 903 Batch: 30 Train Loss: 0.0 Validation Loss: 24.255746841430664\n",
      "Epoch 904 Batch: 30 Train Loss: 0.0 Validation Loss: 17.41243553161621\n",
      "Epoch 905 Batch: 30 Train Loss: 0.0 Validation Loss: 27.70919418334961\n",
      "Epoch 906 Batch: 30 Train Loss: 0.0 Validation Loss: 14.708908081054688\n",
      "Epoch 907 Batch: 30 Train Loss: 0.0 Validation Loss: 2.6244587898254395\n",
      "Epoch 908 Batch: 30 Train Loss: 0.0 Validation Loss: 12.905627250671387\n",
      "Epoch 909 Batch: 30 Train Loss: 0.0 Validation Loss: 22.455339431762695\n",
      "Epoch 910 Batch: 30 Train Loss: 0.0 Validation Loss: 15.475481033325195\n",
      "Epoch 911 Batch: 30 Train Loss: 0.0 Validation Loss: 31.505752563476562\n",
      "Epoch 912 Batch: 30 Train Loss: 0.0 Validation Loss: 32.52210235595703\n",
      "Epoch 913 Batch: 30 Train Loss: 0.0 Validation Loss: 21.458168029785156\n",
      "Epoch 914 Batch: 30 Train Loss: 0.0 Validation Loss: 30.05350685119629\n",
      "Epoch 915 Batch: 30 Train Loss: 0.0 Validation Loss: 19.479402542114258\n",
      "Epoch 916 Batch: 30 Train Loss: 0.0 Validation Loss: 27.936620712280273\n",
      "Epoch 917 Batch: 30 Train Loss: 0.0 Validation Loss: 13.646937370300293\n",
      "Epoch 918 Batch: 30 Train Loss: 0.0 Validation Loss: 27.811023712158203\n",
      "Epoch 919 Batch: 30 Train Loss: 0.0 Validation Loss: 18.322961807250977\n",
      "Epoch 920 Batch: 30 Train Loss: 0.0 Validation Loss: 20.34830093383789\n",
      "Epoch 921 Batch: 30 Train Loss: 0.0 Validation Loss: 24.814464569091797\n",
      "Epoch 922 Batch: 30 Train Loss: 0.0 Validation Loss: 30.695812225341797\n",
      "Epoch 923 Batch: 30 Train Loss: 0.0 Validation Loss: 32.07781982421875\n",
      "Epoch 924 Batch: 30 Train Loss: 0.0 Validation Loss: 0.0\n",
      "Epoch 925 Batch: 30 Train Loss: 0.0 Validation Loss: 18.030662536621094\n",
      "Epoch 926 Batch: 30 Train Loss: 0.0 Validation Loss: 18.428878784179688\n",
      "Epoch 927 Batch: 30 Train Loss: 0.0 Validation Loss: 22.490690231323242\n",
      "Epoch 928 Batch: 30 Train Loss: 0.0 Validation Loss: 28.63165855407715\n",
      "Epoch 929 Batch: 30 Train Loss: 0.0 Validation Loss: 12.567303657531738\n",
      "Epoch 930 Batch: 30 Train Loss: 0.0 Validation Loss: 24.326261520385742\n",
      "Epoch 931 Batch: 30 Train Loss: 0.0 Validation Loss: 15.94972038269043\n",
      "Epoch 932 Batch: 30 Train Loss: 0.0 Validation Loss: 10.861150741577148\n",
      "Epoch 933 Batch: 30 Train Loss: 0.0 Validation Loss: 30.063013076782227\n",
      "Epoch 934 Batch: 30 Train Loss: 0.0 Validation Loss: 25.738643646240234\n",
      "Epoch 935 Batch: 30 Train Loss: 0.0 Validation Loss: 10.188220024108887\n",
      "Epoch 936 Batch: 30 Train Loss: 0.0 Validation Loss: 19.13540267944336\n",
      "Epoch 937 Batch: 30 Train Loss: 0.0 Validation Loss: 26.976465225219727\n",
      "Epoch 938 Batch: 30 Train Loss: 0.0 Validation Loss: 12.482656478881836\n",
      "Epoch 939 Batch: 30 Train Loss: 0.0 Validation Loss: 16.507699966430664\n",
      "Epoch 940 Batch: 30 Train Loss: 0.0 Validation Loss: 17.297561645507812\n",
      "Epoch 941 Batch: 30 Train Loss: 0.0 Validation Loss: 19.82938003540039\n",
      "Epoch 942 Batch: 30 Train Loss: 0.0 Validation Loss: 31.025287628173828\n",
      "Epoch 943 Batch: 30 Train Loss: 0.0 Validation Loss: 18.219202041625977\n",
      "Epoch 944 Batch: 30 Train Loss: 0.0 Validation Loss: 12.229780197143555\n",
      "Epoch 945 Batch: 30 Train Loss: 0.0 Validation Loss: 17.472105026245117\n",
      "Epoch 946 Batch: 30 Train Loss: 0.0 Validation Loss: 20.381973266601562\n",
      "Epoch 947 Batch: 30 Train Loss: 0.0 Validation Loss: 15.60669231414795\n",
      "Epoch 948 Batch: 30 Train Loss: 0.0 Validation Loss: 28.100271224975586\n",
      "Epoch 949 Batch: 30 Train Loss: 0.0 Validation Loss: 18.76647186279297\n",
      "Epoch 950 Batch: 30 Train Loss: 0.0 Validation Loss: 47.580604553222656\n",
      "Epoch 951 Batch: 30 Train Loss: 0.0 Validation Loss: 7.023444175720215\n",
      "Epoch 952 Batch: 30 Train Loss: 0.0 Validation Loss: 25.60149574279785\n",
      "Epoch 953 Batch: 30 Train Loss: 0.0 Validation Loss: 16.878698348999023\n",
      "Epoch 954 Batch: 30 Train Loss: 0.0 Validation Loss: 26.054149627685547\n",
      "Epoch 955 Batch: 30 Train Loss: 0.0 Validation Loss: 16.060022354125977\n",
      "Epoch 956 Batch: 30 Train Loss: 0.0 Validation Loss: 7.801619529724121\n",
      "Epoch 957 Batch: 30 Train Loss: 0.0 Validation Loss: 28.617694854736328\n",
      "Epoch 958 Batch: 30 Train Loss: 0.0 Validation Loss: 30.481781005859375\n",
      "Epoch 959 Batch: 30 Train Loss: 0.0 Validation Loss: 24.687498092651367\n",
      "Epoch 960 Batch: 30 Train Loss: 0.0 Validation Loss: 17.171123504638672\n",
      "Epoch 961 Batch: 30 Train Loss: 0.0 Validation Loss: 20.921138763427734\n",
      "Epoch 962 Batch: 30 Train Loss: 0.0 Validation Loss: 24.927940368652344\n",
      "Epoch 963 Batch: 30 Train Loss: 0.0 Validation Loss: 32.26164627075195\n",
      "Epoch 964 Batch: 30 Train Loss: 0.0 Validation Loss: 36.8282470703125\n",
      "Epoch 965 Batch: 30 Train Loss: 0.0 Validation Loss: 53.87556076049805\n",
      "Epoch 966 Batch: 30 Train Loss: 0.0 Validation Loss: 36.13789749145508\n",
      "Epoch 967 Batch: 30 Train Loss: 0.0 Validation Loss: 21.56071662902832\n",
      "Epoch 968 Batch: 30 Train Loss: 0.0 Validation Loss: 11.421175003051758\n",
      "Epoch 969 Batch: 30 Train Loss: 0.0 Validation Loss: 30.07034683227539\n",
      "Epoch 970 Batch: 30 Train Loss: 0.0 Validation Loss: 13.076375961303711\n",
      "Epoch 971 Batch: 30 Train Loss: 0.0 Validation Loss: 20.403995513916016\n",
      "Epoch 972 Batch: 30 Train Loss: 0.0 Validation Loss: 22.051624298095703\n",
      "Epoch 973 Batch: 30 Train Loss: 0.0 Validation Loss: 38.97015380859375\n",
      "Epoch 974 Batch: 30 Train Loss: 0.0 Validation Loss: 25.841655731201172\n",
      "Epoch 975 Batch: 30 Train Loss: 0.0 Validation Loss: 19.9000186920166\n",
      "Epoch 976 Batch: 30 Train Loss: 0.0 Validation Loss: 17.326570510864258\n",
      "Epoch 977 Batch: 30 Train Loss: 0.0 Validation Loss: 20.75332260131836\n",
      "Epoch 978 Batch: 30 Train Loss: 0.0 Validation Loss: 19.756553649902344\n",
      "Epoch 979 Batch: 30 Train Loss: 0.0 Validation Loss: 18.356502532958984\n",
      "Epoch 980 Batch: 30 Train Loss: 0.0 Validation Loss: 18.919275283813477\n",
      "Epoch 981 Batch: 30 Train Loss: 0.0 Validation Loss: 18.10332679748535\n",
      "Epoch 982 Batch: 30 Train Loss: 0.0 Validation Loss: 16.251169204711914\n",
      "Epoch 983 Batch: 30 Train Loss: 0.0 Validation Loss: 28.53692054748535\n",
      "Epoch 984 Batch: 30 Train Loss: 0.0 Validation Loss: 26.88686180114746\n",
      "Epoch 985 Batch: 30 Train Loss: 0.0 Validation Loss: 15.875518798828125\n",
      "Epoch 986 Batch: 30 Train Loss: 0.0 Validation Loss: 19.2230224609375\n",
      "Epoch 987 Batch: 30 Train Loss: 0.0 Validation Loss: 19.253446578979492\n",
      "Epoch 988 Batch: 30 Train Loss: 0.0 Validation Loss: 29.980487823486328\n",
      "Epoch 989 Batch: 30 Train Loss: 0.0 Validation Loss: 12.293607711791992\n",
      "Epoch 990 Batch: 30 Train Loss: 0.0 Validation Loss: 18.786943435668945\n",
      "Epoch 991 Batch: 30 Train Loss: 0.0 Validation Loss: 12.414167404174805\n",
      "Epoch 992 Batch: 30 Train Loss: 0.0 Validation Loss: 18.805191040039062\n",
      "Epoch 993 Batch: 30 Train Loss: 0.0 Validation Loss: 2.848118543624878\n",
      "Epoch 994 Batch: 30 Train Loss: 0.0 Validation Loss: 24.005104064941406\n",
      "Epoch 995 Batch: 30 Train Loss: 0.0 Validation Loss: 28.105417251586914\n",
      "Epoch 996 Batch: 30 Train Loss: 0.0 Validation Loss: 15.462331771850586\n",
      "Epoch 997 Batch: 30 Train Loss: 0.0 Validation Loss: 24.549022674560547\n",
      "Epoch 998 Batch: 30 Train Loss: 0.0 Validation Loss: 14.214424133300781\n",
      "Epoch 999 Batch: 30 Train Loss: 0.0 Validation Loss: 19.673633575439453\n"
     ]
    }
   ],
   "source": [
    "from ignite.metrics import Precision, Recall\n",
    "from ignite.metrics import Precision ### LÄGG TILL IGNITE\n",
    "train_precision = Precision()\n",
    "train_recall = Recall()\n",
    "\n",
    "test_precision = Precision()\n",
    "test_recall = Recall()\n",
    "# https://pytorch.org/ignite/metrics.html\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_correct = []\n",
    "test_correct = []\n",
    "\n",
    "# For loop epochs \n",
    "\n",
    "for i in range(epochs):\n",
    "    trn_correct = 0\n",
    "    tst_correct = 0 \n",
    "\n",
    "    # Train\n",
    "\n",
    "    for b, (X_train, y_train) in enumerate(train_loader):\n",
    "        \n",
    "        # Skip iteration if batch size not equal to stated dim\n",
    "        \n",
    "            \n",
    "        #print(X_train.shape, y_train.shape) \n",
    "        \n",
    "        b += 1\n",
    "        \n",
    "        y_pred = model(X_train.view(batch_size, -1))  # Flatten input\n",
    "        lossTrain = criterion(y_pred, y_train)\n",
    "\n",
    "        predicted = torch.max(y_pred.data,1)[1]\n",
    "\n",
    "        #calculate precision and recall\n",
    "        train_precision.update((y_pred, y_train))\n",
    "        train_recall.update((y_pred, y_train))\n",
    "      \n",
    "\n",
    "        batch_corr = (predicted == y_train).sum()\n",
    "        trn_correct += batch_corr\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        lossTrain.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #if b%2 == 0:\n",
    "           #print(f\"Epoch {i} Batch: {b} Train Loss: {lossTrain.item()}\")\n",
    "\n",
    "    train_losses.append(lossTrain.data.item())\n",
    "    train_correct.append(trn_correct)\n",
    "\n",
    "    # Test\n",
    "    with torch.no_grad():\n",
    "        for b, (X_test,y_test) in enumerate(test_loader):\n",
    "            y_val = model(X_train.view(batch_size, -1))\n",
    "\n",
    "            predicted = torch.max(y_val.data,1)[1]\n",
    "\n",
    "            #calculate precision and recall\n",
    "            test_precision.update((y_val, y_test))\n",
    "            test_recall.update((y_val, y_test))\n",
    "            \n",
    "            loss = criterion(y_val, y_test)\n",
    "            test_losses.append(loss)\n",
    "            test_correct.append(trn_correct)\n",
    "\n",
    "        if b%5 == 0:\n",
    "            print(f\"Epoch {i} Batch: {b} Train Loss: {lossTrain.item()} Validation Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9564, 0.9747], dtype=torch.float64)\n",
      "tensor([0.9816, 0.9406], dtype=torch.float64)\n",
      "tensor([0.5980, 0.3987], dtype=torch.float64)\n",
      "tensor([0.5865, 0.4102], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(train_precision.compute())\n",
    "print(train_recall.compute())\n",
    "print(test_precision.compute())\n",
    "print(test_recall.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f2a381af130>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(train_losses, label=\"Train losses\")\n",
    "#plt.plot(test_losses, label= \"Test losses\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict a state of a single sample from test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sample 3 selected with state 0. Model predict state is 0\n",
      "Random sample 228 selected with state 1. Model predict state is 0\n",
      "Random sample 238 selected with state 1. Model predict state is 0\n",
      "Random sample 58 selected with state 1. Model predict state is 1\n",
      "Random sample 21 selected with state 1. Model predict state is 0\n",
      "Random sample 200 selected with state 0. Model predict state is 0\n",
      "Random sample 143 selected with state 0. Model predict state is 0\n",
      "Random sample 237 selected with state 0. Model predict state is 1\n",
      "Random sample 85 selected with state 1. Model predict state is 0\n",
      "Random sample 30 selected with state 0. Model predict state is 0\n",
      "Random sample 118 selected with state 0. Model predict state is 1\n",
      "Random sample 253 selected with state 0. Model predict state is 0\n",
      "Random sample 249 selected with state 0. Model predict state is 0\n",
      "Random sample 22 selected with state 0. Model predict state is 0\n",
      "Random sample 271 selected with state 0. Model predict state is 0\n",
      "Random sample 149 selected with state 1. Model predict state is 1\n",
      "Random sample 259 selected with state 1. Model predict state is 1\n",
      "Random sample 284 selected with state 0. Model predict state is 1\n",
      "Random sample 223 selected with state 0. Model predict state is 0\n",
      "Random sample 33 selected with state 1. Model predict state is 0\n",
      "Random sample 92 selected with state 1. Model predict state is 0\n",
      "Random sample 12 selected with state 0. Model predict state is 0\n",
      "Random sample 248 selected with state 1. Model predict state is 0\n",
      "Random sample 132 selected with state 1. Model predict state is 0\n",
      "Random sample 143 selected with state 0. Model predict state is 0\n",
      "Random sample 162 selected with state 1. Model predict state is 1\n",
      "Random sample 172 selected with state 1. Model predict state is 1\n",
      "Random sample 86 selected with state 0. Model predict state is 1\n",
      "Random sample 135 selected with state 1. Model predict state is 0\n",
      "Random sample 12 selected with state 0. Model predict state is 0\n",
      "Random sample 45 selected with state 1. Model predict state is 1\n",
      "Random sample 243 selected with state 0. Model predict state is 1\n",
      "Random sample 231 selected with state 1. Model predict state is 0\n",
      "Random sample 166 selected with state 0. Model predict state is 0\n",
      "Random sample 211 selected with state 0. Model predict state is 0\n",
      "Random sample 88 selected with state 0. Model predict state is 1\n",
      "Random sample 23 selected with state 0. Model predict state is 0\n",
      "Random sample 285 selected with state 0. Model predict state is 0\n",
      "Random sample 147 selected with state 1. Model predict state is 1\n",
      "Random sample 127 selected with state 1. Model predict state is 0\n",
      "Random sample 50 selected with state 1. Model predict state is 0\n",
      "Random sample 23 selected with state 0. Model predict state is 0\n",
      "Random sample 145 selected with state 0. Model predict state is 1\n",
      "Random sample 121 selected with state 0. Model predict state is 1\n",
      "Random sample 314 selected with state 0. Model predict state is 0\n",
      "Random sample 287 selected with state 0. Model predict state is 1\n",
      "Random sample 238 selected with state 1. Model predict state is 0\n",
      "Random sample 91 selected with state 1. Model predict state is 1\n",
      "Random sample 60 selected with state 1. Model predict state is 1\n",
      "Random sample 66 selected with state 0. Model predict state is 0\n",
      "Random sample 183 selected with state 0. Model predict state is 1\n",
      "Random sample 57 selected with state 1. Model predict state is 0\n",
      "Random sample 54 selected with state 1. Model predict state is 0\n",
      "Random sample 261 selected with state 0. Model predict state is 1\n",
      "Random sample 289 selected with state 0. Model predict state is 1\n",
      "Random sample 75 selected with state 0. Model predict state is 0\n",
      "Random sample 133 selected with state 0. Model predict state is 0\n",
      "Random sample 262 selected with state 1. Model predict state is 0\n",
      "Random sample 193 selected with state 0. Model predict state is 0\n",
      "Random sample 141 selected with state 1. Model predict state is 0\n",
      "Random sample 89 selected with state 1. Model predict state is 0\n",
      "Random sample 206 selected with state 1. Model predict state is 0\n",
      "Random sample 1 selected with state 1. Model predict state is 1\n",
      "Random sample 295 selected with state 1. Model predict state is 1\n",
      "Random sample 207 selected with state 0. Model predict state is 1\n",
      "Random sample 94 selected with state 0. Model predict state is 1\n",
      "Random sample 85 selected with state 1. Model predict state is 0\n",
      "Random sample 265 selected with state 1. Model predict state is 1\n",
      "Random sample 272 selected with state 0. Model predict state is 0\n",
      "Random sample 46 selected with state 1. Model predict state is 0\n",
      "Random sample 83 selected with state 0. Model predict state is 0\n",
      "Random sample 133 selected with state 0. Model predict state is 0\n",
      "Random sample 12 selected with state 0. Model predict state is 0\n",
      "Random sample 225 selected with state 0. Model predict state is 1\n",
      "Random sample 109 selected with state 1. Model predict state is 0\n",
      "Random sample 103 selected with state 0. Model predict state is 1\n",
      "Random sample 164 selected with state 0. Model predict state is 1\n",
      "Random sample 285 selected with state 0. Model predict state is 0\n",
      "Random sample 202 selected with state 0. Model predict state is 0\n",
      "Random sample 128 selected with state 1. Model predict state is 0\n",
      "Random sample 35 selected with state 0. Model predict state is 0\n",
      "Random sample 269 selected with state 0. Model predict state is 0\n",
      "Random sample 143 selected with state 0. Model predict state is 0\n",
      "Random sample 261 selected with state 0. Model predict state is 1\n",
      "Random sample 71 selected with state 1. Model predict state is 1\n",
      "Random sample 180 selected with state 0. Model predict state is 1\n",
      "Random sample 295 selected with state 1. Model predict state is 1\n",
      "Random sample 21 selected with state 1. Model predict state is 0\n",
      "Random sample 87 selected with state 0. Model predict state is 0\n",
      "Random sample 76 selected with state 0. Model predict state is 0\n",
      "Random sample 248 selected with state 1. Model predict state is 0\n",
      "Random sample 107 selected with state 0. Model predict state is 1\n",
      "Random sample 212 selected with state 0. Model predict state is 0\n",
      "Random sample 61 selected with state 1. Model predict state is 0\n",
      "Random sample 114 selected with state 1. Model predict state is 1\n",
      "Random sample 150 selected with state 1. Model predict state is 1\n",
      "Random sample 313 selected with state 0. Model predict state is 0\n",
      "Random sample 118 selected with state 0. Model predict state is 1\n",
      "Random sample 114 selected with state 1. Model predict state is 1\n",
      "Random sample 21 selected with state 1. Model predict state is 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for _ in range(100):\n",
    "    # Select random sample\n",
    "    i = np.random.randint(1,len(test_set))\n",
    "    x = test_set[i][0]\n",
    "    y = test_set[i][1]\n",
    "\n",
    "\n",
    "    # Evaluate on sample\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        new_pred =model(x.view(1,-1))\n",
    "        pred_int = int(torch.max(new_pred.data,1)[1])\n",
    "    print(f\"Random sample {i} selected with state {y}. Model predict state is {pred_int}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict a state of a single sample from train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sample 1087 selected with state 0. Model predict state is 0\n",
      "Random sample 480 selected with state 1. Model predict state is 1\n",
      "Random sample 120 selected with state 1. Model predict state is 1\n",
      "Random sample 1216 selected with state 1. Model predict state is 1\n",
      "Random sample 650 selected with state 1. Model predict state is 1\n",
      "Random sample 109 selected with state 1. Model predict state is 1\n",
      "Random sample 1135 selected with state 0. Model predict state is 0\n",
      "Random sample 1033 selected with state 1. Model predict state is 1\n",
      "Random sample 131 selected with state 1. Model predict state is 1\n",
      "Random sample 1160 selected with state 0. Model predict state is 0\n",
      "Random sample 619 selected with state 0. Model predict state is 0\n",
      "Random sample 430 selected with state 1. Model predict state is 1\n",
      "Random sample 1250 selected with state 0. Model predict state is 0\n",
      "Random sample 456 selected with state 0. Model predict state is 0\n",
      "Random sample 728 selected with state 1. Model predict state is 1\n",
      "Random sample 1093 selected with state 1. Model predict state is 1\n",
      "Random sample 266 selected with state 1. Model predict state is 1\n",
      "Random sample 1136 selected with state 0. Model predict state is 0\n",
      "Random sample 238 selected with state 0. Model predict state is 0\n",
      "Random sample 690 selected with state 0. Model predict state is 0\n",
      "Random sample 842 selected with state 1. Model predict state is 1\n",
      "Random sample 755 selected with state 0. Model predict state is 0\n",
      "Random sample 677 selected with state 1. Model predict state is 1\n",
      "Random sample 1047 selected with state 0. Model predict state is 0\n",
      "Random sample 747 selected with state 1. Model predict state is 1\n",
      "Random sample 55 selected with state 1. Model predict state is 1\n",
      "Random sample 24 selected with state 0. Model predict state is 0\n",
      "Random sample 498 selected with state 1. Model predict state is 1\n",
      "Random sample 1037 selected with state 1. Model predict state is 1\n",
      "Random sample 661 selected with state 0. Model predict state is 0\n",
      "Random sample 767 selected with state 0. Model predict state is 0\n",
      "Random sample 274 selected with state 0. Model predict state is 0\n",
      "Random sample 602 selected with state 0. Model predict state is 0\n",
      "Random sample 594 selected with state 1. Model predict state is 1\n",
      "Random sample 943 selected with state 1. Model predict state is 1\n",
      "Random sample 891 selected with state 1. Model predict state is 1\n",
      "Random sample 903 selected with state 0. Model predict state is 0\n",
      "Random sample 913 selected with state 1. Model predict state is 1\n",
      "Random sample 799 selected with state 1. Model predict state is 1\n",
      "Random sample 390 selected with state 0. Model predict state is 0\n",
      "Random sample 153 selected with state 0. Model predict state is 0\n",
      "Random sample 714 selected with state 1. Model predict state is 1\n",
      "Random sample 471 selected with state 0. Model predict state is 0\n",
      "Random sample 583 selected with state 1. Model predict state is 1\n",
      "Random sample 252 selected with state 1. Model predict state is 1\n",
      "Random sample 847 selected with state 0. Model predict state is 0\n",
      "Random sample 224 selected with state 1. Model predict state is 1\n",
      "Random sample 539 selected with state 1. Model predict state is 1\n",
      "Random sample 485 selected with state 0. Model predict state is 0\n",
      "Random sample 172 selected with state 0. Model predict state is 0\n",
      "Random sample 947 selected with state 0. Model predict state is 0\n",
      "Random sample 1107 selected with state 1. Model predict state is 1\n",
      "Random sample 978 selected with state 1. Model predict state is 1\n",
      "Random sample 498 selected with state 1. Model predict state is 1\n",
      "Random sample 152 selected with state 0. Model predict state is 0\n",
      "Random sample 669 selected with state 0. Model predict state is 0\n",
      "Random sample 408 selected with state 0. Model predict state is 0\n",
      "Random sample 246 selected with state 1. Model predict state is 1\n",
      "Random sample 600 selected with state 0. Model predict state is 0\n",
      "Random sample 98 selected with state 0. Model predict state is 0\n",
      "Random sample 262 selected with state 1. Model predict state is 1\n",
      "Random sample 426 selected with state 0. Model predict state is 0\n",
      "Random sample 1238 selected with state 1. Model predict state is 1\n",
      "Random sample 939 selected with state 1. Model predict state is 1\n",
      "Random sample 1026 selected with state 0. Model predict state is 0\n",
      "Random sample 748 selected with state 0. Model predict state is 0\n",
      "Random sample 488 selected with state 0. Model predict state is 0\n",
      "Random sample 1214 selected with state 0. Model predict state is 0\n",
      "Random sample 1188 selected with state 0. Model predict state is 0\n",
      "Random sample 1244 selected with state 0. Model predict state is 0\n",
      "Random sample 779 selected with state 1. Model predict state is 1\n",
      "Random sample 25 selected with state 0. Model predict state is 0\n",
      "Random sample 133 selected with state 0. Model predict state is 0\n",
      "Random sample 153 selected with state 0. Model predict state is 0\n",
      "Random sample 363 selected with state 1. Model predict state is 1\n",
      "Random sample 914 selected with state 0. Model predict state is 0\n",
      "Random sample 308 selected with state 0. Model predict state is 0\n",
      "Random sample 221 selected with state 0. Model predict state is 0\n",
      "Random sample 1118 selected with state 1. Model predict state is 1\n",
      "Random sample 399 selected with state 0. Model predict state is 0\n",
      "Random sample 860 selected with state 0. Model predict state is 0\n",
      "Random sample 1173 selected with state 0. Model predict state is 0\n",
      "Random sample 732 selected with state 0. Model predict state is 0\n",
      "Random sample 173 selected with state 0. Model predict state is 0\n",
      "Random sample 924 selected with state 1. Model predict state is 1\n",
      "Random sample 165 selected with state 0. Model predict state is 0\n",
      "Random sample 1044 selected with state 1. Model predict state is 1\n",
      "Random sample 7 selected with state 1. Model predict state is 1\n",
      "Random sample 210 selected with state 0. Model predict state is 0\n",
      "Random sample 242 selected with state 0. Model predict state is 0\n",
      "Random sample 236 selected with state 0. Model predict state is 0\n",
      "Random sample 467 selected with state 1. Model predict state is 1\n",
      "Random sample 417 selected with state 0. Model predict state is 0\n",
      "Random sample 303 selected with state 0. Model predict state is 0\n",
      "Random sample 1030 selected with state 0. Model predict state is 0\n",
      "Random sample 1070 selected with state 1. Model predict state is 1\n",
      "Random sample 120 selected with state 1. Model predict state is 1\n",
      "Random sample 1022 selected with state 0. Model predict state is 0\n",
      "Random sample 542 selected with state 0. Model predict state is 0\n",
      "Random sample 540 selected with state 0. Model predict state is 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for _ in range(100):\n",
    "    # Select random sample\n",
    "    i = np.random.randint(1,len(train_set))\n",
    "    x = train_set[i][0]\n",
    "    y = train_set[i][1]\n",
    "\n",
    "\n",
    "    # Evaluate on sample\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        new_pred =model(x.view(1,-1))\n",
    "        pred_int = int(torch.max(new_pred.data,1)[1])\n",
    "    print(f\"Random sample {i} selected with state {y}. Model predict state is {pred_int}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3a5e77589088e21c3c0d4cdd96e34c3b18a8ad86691c2f192753c63150bee9a2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
