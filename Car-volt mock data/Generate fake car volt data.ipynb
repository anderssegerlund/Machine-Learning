{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22bef119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a60668e",
   "metadata": {},
   "source": [
    "## Generate fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22b892c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bc3e88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    x_min = x.min()\n",
    "    x_max = x.max()\n",
    "    \n",
    "    x = (x-x_min)/(x_max -x_min)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbb1c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3144c776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "time_steps = 2\n",
    "n_cells = 4 # Amount of cells in pack\n",
    "x = np.zeros([time_steps,n_cells])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,2)\n",
    "\n",
    "failed_cell = 1 # Which cell is the failing cell\n",
    "is_fail = True# Does the pack include a failed cell\n",
    "volt_stochastic = 0.9\n",
    "for i in range(time_steps):\n",
    "    sigma = 0.0 # How much is the fluctuation\n",
    "    median = random.uniform(2, 4)\n",
    "    volt = np.random.normal(loc=median, scale=sigma, size=n_cells)\n",
    "    if is_fail:\n",
    "        r = random.random()\n",
    "        if r < volt_stochastic:\n",
    "            stochstic_amplifier = random.randint(1,10)\n",
    "            \n",
    "            y = (1-i/stochstic_amplifier)\n",
    "            #print(y)\n",
    "            volt[failed_cell]*=y\n",
    "    ax[1].scatter(i,y)\n",
    "    x[i,:] = volt\n",
    "            \n",
    "    ax[0].plot(volt, label=i)\n",
    "#plt.ylim(0,16)\n",
    "#plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22bbba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequence(time_steps=2, n_cells=4, failed_cell=1, is_fail=True, sigma=0.0, volt_stochastic = 0.9, normalize_x=False):\n",
    "    x = np.zeros([time_steps,n_cells])\n",
    "    for i in range(time_steps):\n",
    "        median = random.uniform(2, 4)\n",
    "        volt = np.random.normal(loc=median, scale=sigma, size=n_cells)\n",
    "        if is_fail:\n",
    "            r = random.random()\n",
    "            if r < volt_stochastic:\n",
    "                stochstic_amplifier = random.randint(1,10) # 100, 1000\n",
    "\n",
    "                y = (1-i/stochstic_amplifier)\n",
    "                #print(y)\n",
    "                volt[failed_cell]*=y\n",
    "        x[i,:] = volt\n",
    "    if normalize_x:\n",
    "        x = normalize(x)\n",
    "    if is_fail:\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 0\n",
    "    return x, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0f7fba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps=2 \n",
    "n_cells=4\n",
    "failed_cell=1 \n",
    "is_fail=True \n",
    "sigma=0.0 \n",
    "volt_stochastic = 0.9 \n",
    "normalize_x=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e93b1ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros([time_steps,n_cells])\n",
    "for i in range(time_steps):\n",
    "    median = random.uniform(2, 4)\n",
    "    volt = np.random.normal(loc=median, scale=sigma, size=n_cells)\n",
    "    if is_fail:\n",
    "        r = random.random()\n",
    "        if r < volt_stochastic:\n",
    "            stochstic_amplifier = random.randint(1,10) # 100, 1000\n",
    "\n",
    "            y = (1-i/stochstic_amplifier)\n",
    "            #print(y)\n",
    "            volt[failed_cell]*=y\n",
    "    x[i,:] = volt\n",
    "if normalize_x:\n",
    "    x = normalize(x)\n",
    "if is_fail:\n",
    "    label = 1\n",
    "else:\n",
    "    label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "50e65f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.214761002501522"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volt.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f4c2a492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8370062871437267"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volt.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab29315d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31d1894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d59d4c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4d3cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0353fbf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57e14190",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 2\n",
    "n_cells =4\n",
    "x,y = create_sequence(time_steps=time_steps, n_cells=n_cells, failed_cell=2, is_fail=True, sigma=0.0, volt_stochastic = 0.9, normalize_x=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "808b7e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x.T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9e7fba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(n_healthy=2, n_fails=2,time_steps=time_steps):\n",
    "    data = []\n",
    "    \n",
    "    data_info ={\"index_1\":[],\n",
    "           \"index_0\": []} \n",
    "    \n",
    "    for i in range(n_healthy+n_fails):\n",
    "        if i >= n_healthy:\n",
    "            fail=True\n",
    "        else:\n",
    "            fail=False\n",
    "        x,y = create_sequence(time_steps=time_steps, n_cells=n_cells, failed_cell=2, is_fail=fail, sigma=0.0, volt_stochastic = 0.9, normalize_x=True)\n",
    "        data.append((torch.FloatTensor(x),y))\n",
    "        if y == 1:\n",
    "            data_info[\"index_1\"].append(i)\n",
    "            \n",
    "        if y == 0:\n",
    "            data_info[\"index_0\"].append(i)\n",
    "            \n",
    "    return (data, data_info)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4b9bbe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, data_info = create_data(n_healthy=10, n_fails=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4b43a806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index_1': [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
       " 'index_0': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "499e55fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_info[\"index_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f1b532a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy.random' has no attribute 'choices'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jw/61jjk4jd69sbkflvlpjrlv1h0000gn/T/ipykernel_25990/2876408256.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"index_1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy.random' has no attribute 'choices'"
     ]
    }
   ],
   "source": [
    "np.random.choices(data_info[\"index_1\"], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e790af04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 18, 16, 14, 11, 12, 17, 15, 19, 13]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9c07452b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa7db6d2790>]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(data[10][0][0].T) #Plocka ut n√§st sista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "11fbf41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[10][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9160ec3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa7c0f37d60>,\n",
       " <matplotlib.lines.Line2D at 0x7fa7c0f37dc0>]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4cbdc160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_samples = random.sample(data_info[\"index_1\"],10)\n",
    "fig, ax = plt.subplots(5)\n",
    "for i in range(5):\n",
    "    ax[i].plot(data[random_samples[i]][0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "63a9511a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "578c7e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " (tensor([[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]),\n",
       "  0),\n",
       " ...]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "97d8b6ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jw/61jjk4jd69sbkflvlpjrlv1h0000gn/T/ipykernel_25990/91768756.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "[item for item in a if item[0] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e08baca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e07f0ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f16b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74fe2511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# Kolla antalet 0 och 1 i datasettet\n",
    "n_0 = 0\n",
    "n_1 = 0\n",
    "for i in range(len(data)):\n",
    "    if data[i][1] == 1:\n",
    "        n_1 +=1\n",
    "    if data[i][1] == 0:\n",
    "        n_0 +=1\n",
    "    \n",
    "    \n",
    "print(n_0)\n",
    "print(n_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e500b877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479de800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27b2454d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a2bf0168",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(data)*0.8)\n",
    "test_size = len(data) - train_size\n",
    "train_set, test_set = torch.utils.data.random_split(data,[train_size, test_size])\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e61c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c395ceb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e0ab2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a2f4bee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNMultilayerperceptron(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size=(time_steps*n_cells),output_size=2, layers=[220,84]):  # 120, 84\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, layers[0])\n",
    "        self.fc2 = nn.Linear(layers[0], layers[1])\n",
    "        self.fc2b = nn.Linear(layers[1], 500)\n",
    "        self.fc2c = nn.Linear(500, layers[1])\n",
    "        self.fc2d = nn.Linear(layers[1], layers[1])\n",
    "        self.fc3 = nn.Linear(layers[1], output_size)\n",
    "\n",
    "    def forward(self,X):\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = F.relu(self.fc2(X))\n",
    "        X = F.relu(self.fc2b(X))\n",
    "        X = F.relu(self.fc2c(X))\n",
    "        X = F.relu(self.fc2d(X))\n",
    "        X = self.fc3(X)\n",
    "\n",
    "        return F.log_softmax(X,dim=1) # PGA multiclass classification\n",
    "        #return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3ddbf0dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ANNMultilayerperceptron(\n",
       "  (fc1): Linear(in_features=8, out_features=220, bias=True)\n",
       "  (fc2): Linear(in_features=220, out_features=84, bias=True)\n",
       "  (fc2b): Linear(in_features=84, out_features=500, bias=True)\n",
       "  (fc2c): Linear(in_features=500, out_features=84, bias=True)\n",
       "  (fc2d): Linear(in_features=84, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ANNMultilayerperceptron()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715f24d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc52a08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8497c260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e65d3518",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "#optimizer = torch.optim.Adadelta(model.parameters(), lr=0.001)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cd05a0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch: 1 Train Loss: 0.7206144332885742\n",
      "Epoch 0 Batch: 2 Train Loss: 0.7260395288467407\n",
      "Epoch 0 Batch: 3 Train Loss: 0.721284031867981\n",
      "Epoch 0 Batch: 4 Train Loss: 0.6992607116699219\n",
      "Epoch 0 Batch: 5 Train Loss: 0.7109556198120117\n",
      "Epoch 0 Batch: 6 Train Loss: 0.7113783359527588\n",
      "Epoch 0 Batch: 7 Train Loss: 0.704898476600647\n",
      "Epoch 0 Batch: 8 Train Loss: 0.7023300528526306\n",
      "Epoch 0 Batch: 9 Train Loss: 0.7031729221343994\n",
      "Epoch 0 Batch: 10 Train Loss: 0.6956555247306824\n",
      "Epoch 0 Batch: 11 Train Loss: 0.6970252990722656\n",
      "Epoch 0 Batch: 12 Train Loss: 0.6946864724159241\n",
      "Epoch 0 Batch: 13 Train Loss: 0.6919562220573425\n",
      "Epoch 0 Batch: 14 Train Loss: 0.6900253891944885\n",
      "Epoch 0 Batch: 15 Train Loss: 0.6878281235694885\n",
      "Epoch 0 Batch: 16 Train Loss: 0.6855993866920471\n",
      "Epoch 0 Batch: 17 Train Loss: 0.6830440759658813\n",
      "Epoch 0 Batch: 18 Train Loss: 0.6784285306930542\n",
      "Epoch 0 Batch: 19 Train Loss: 0.6788907051086426\n",
      "Epoch 0 Batch: 20 Train Loss: 0.6773935556411743\n",
      "Epoch 0 Batch: 21 Train Loss: 0.6796251535415649\n",
      "Epoch 0 Batch: 22 Train Loss: 0.678038477897644\n",
      "Epoch 0 Batch: 23 Train Loss: 0.6654952764511108\n",
      "Epoch 0 Batch: 24 Train Loss: 0.6745535731315613\n",
      "Epoch 0 Batch: 25 Train Loss: 0.6601144075393677\n",
      "Epoch 0 Batch: 26 Train Loss: 0.685905396938324\n",
      "Epoch 0 Batch: 27 Train Loss: 0.6700514554977417\n",
      "Epoch 0 Batch: 28 Train Loss: 0.668806254863739\n",
      "Epoch 0 Batch: 29 Train Loss: 0.6495104432106018\n",
      "Epoch 0 Batch: 30 Train Loss: 0.6468947529792786\n",
      "Epoch 0 Batch: 31 Train Loss: 0.66367506980896\n",
      "Epoch 0 Batch: 32 Train Loss: 0.6398104429244995\n",
      "Epoch 0 Batch: 33 Train Loss: 0.6363961100578308\n",
      "Epoch 0 Batch: 34 Train Loss: 0.6332522630691528\n",
      "Epoch 0 Batch: 35 Train Loss: 0.6561951637268066\n",
      "Epoch 0 Batch: 36 Train Loss: 0.6545116901397705\n",
      "Epoch 0 Batch: 37 Train Loss: 0.6241027116775513\n",
      "Epoch 0 Batch: 38 Train Loss: 0.6516523361206055\n",
      "Epoch 0 Batch: 39 Train Loss: 0.6147518754005432\n",
      "Epoch 0 Batch: 40 Train Loss: 0.6088519096374512\n",
      "Epoch 0 Batch: 41 Train Loss: 0.6083869338035583\n",
      "Epoch 0 Batch: 42 Train Loss: 0.6427512764930725\n",
      "Epoch 0 Batch: 43 Train Loss: 0.6410373449325562\n",
      "Epoch 0 Batch: 44 Train Loss: 0.595533549785614\n",
      "Epoch 0 Batch: 45 Train Loss: 0.6115929484367371\n",
      "Epoch 0 Batch: 46 Train Loss: 0.6083565950393677\n",
      "Epoch 0 Batch: 47 Train Loss: 0.6017090082168579\n",
      "Epoch 0 Batch: 48 Train Loss: 0.5992975234985352\n",
      "Epoch 0 Batch: 49 Train Loss: 0.5960853099822998\n",
      "Epoch 0 Batch: 50 Train Loss: 0.5928741693496704\n",
      "Epoch 0 Batch: 51 Train Loss: 0.5908973813056946\n",
      "Epoch 0 Batch: 52 Train Loss: 0.5566235780715942\n",
      "Epoch 0 Batch: 53 Train Loss: 0.5809869170188904\n",
      "Epoch 0 Batch: 54 Train Loss: 0.5422880053520203\n",
      "Epoch 0 Batch: 55 Train Loss: 0.6397444009780884\n",
      "Epoch 0 Batch: 56 Train Loss: 0.6007936596870422\n",
      "Epoch 0 Batch: 57 Train Loss: 0.5184298157691956\n",
      "Epoch 0 Batch: 58 Train Loss: 0.5128064155578613\n",
      "Epoch 0 Batch: 59 Train Loss: 0.5051730871200562\n",
      "Epoch 0 Batch: 60 Train Loss: 0.5443485379219055\n",
      "Epoch 0 Batch: 61 Train Loss: 0.5412432551383972\n",
      "Epoch 0 Batch: 62 Train Loss: 0.5381821393966675\n",
      "Epoch 0 Batch: 63 Train Loss: 0.47385716438293457\n",
      "Epoch 0 Batch: 64 Train Loss: 0.5203289985656738\n",
      "Epoch 0 Batch: 65 Train Loss: 0.563305139541626\n",
      "Epoch 0 Batch: 66 Train Loss: 0.4486914277076721\n",
      "Epoch 0 Batch: 67 Train Loss: 0.49673429131507874\n",
      "Epoch 0 Batch: 68 Train Loss: 0.5608893632888794\n",
      "Epoch 0 Batch: 69 Train Loss: 0.4168727993965149\n",
      "Epoch 0 Batch: 70 Train Loss: 0.40659770369529724\n",
      "Epoch 0 Batch: 71 Train Loss: 0.39306992292404175\n",
      "Epoch 0 Batch: 72 Train Loss: 0.5364065766334534\n",
      "Epoch 0 Batch: 73 Train Loss: 0.43855637311935425\n",
      "Epoch 0 Batch: 74 Train Loss: 0.7025171518325806\n",
      "Epoch 0 Batch: 75 Train Loss: 0.5172306299209595\n",
      "Epoch 0 Batch: 76 Train Loss: 0.3436858057975769\n",
      "Epoch 0 Batch: 77 Train Loss: 0.42922115325927734\n",
      "Epoch 0 Batch: 78 Train Loss: 0.6229574084281921\n",
      "Epoch 0 Batch: 79 Train Loss: 0.3207095265388489\n",
      "Epoch 0 Batch: 80 Train Loss: 0.40920907258987427\n",
      "Epoch 0 Batch: 81 Train Loss: 0.28306299448013306\n",
      "Epoch 0 Batch: 82 Train Loss: 0.39342695474624634\n",
      "Epoch 0 Batch: 83 Train Loss: 0.38314351439476013\n",
      "Epoch 0 Batch: 84 Train Loss: 0.2689834535121918\n",
      "Epoch 0 Batch: 85 Train Loss: 0.392566442489624\n",
      "Epoch 0 Batch: 86 Train Loss: 0.24726073443889618\n",
      "Epoch 0 Batch: 87 Train Loss: 0.2399446666240692\n",
      "Epoch 0 Batch: 88 Train Loss: 0.21371705830097198\n",
      "Epoch 0 Batch: 89 Train Loss: 0.3579457104206085\n",
      "Epoch 0 Batch: 90 Train Loss: 0.3462800979614258\n",
      "Epoch 0 Batch: 91 Train Loss: 0.372964084148407\n",
      "Epoch 0 Batch: 92 Train Loss: 0.352152943611145\n",
      "Epoch 0 Batch: 93 Train Loss: 0.17365260422229767\n",
      "Epoch 0 Batch: 94 Train Loss: 0.5293237566947937\n",
      "Epoch 0 Batch: 95 Train Loss: 0.3219517171382904\n",
      "Epoch 0 Batch: 96 Train Loss: 0.14060695469379425\n",
      "Epoch 0 Batch: 97 Train Loss: 0.14753358066082\n",
      "Epoch 0 Batch: 98 Train Loss: 0.3359471261501312\n",
      "Epoch 0 Batch: 99 Train Loss: 0.5383824706077576\n",
      "Epoch 0 Batch: 100 Train Loss: 0.1216253787279129\n",
      "Epoch 0 Batch: 101 Train Loss: 0.5211299657821655\n",
      "Epoch 0 Batch: 102 Train Loss: 0.3116113543510437\n",
      "Epoch 0 Batch: 103 Train Loss: 0.5909696221351624\n",
      "Epoch 0 Batch: 104 Train Loss: 0.3119902014732361\n",
      "Epoch 0 Batch: 105 Train Loss: 0.10586939007043839\n",
      "Epoch 0 Batch: 106 Train Loss: 0.1068546399474144\n",
      "Epoch 0 Batch: 107 Train Loss: 0.1005646139383316\n",
      "Epoch 0 Batch: 108 Train Loss: 0.552462100982666\n",
      "Epoch 0 Batch: 109 Train Loss: 0.6019255518913269\n",
      "Epoch 0 Batch: 110 Train Loss: 0.09450088441371918\n",
      "Epoch 0 Batch: 111 Train Loss: 0.5518792271614075\n",
      "Epoch 0 Batch: 112 Train Loss: 0.3430076837539673\n",
      "Epoch 0 Batch: 113 Train Loss: 0.5669194459915161\n",
      "Epoch 0 Batch: 114 Train Loss: 0.31943628191947937\n",
      "Epoch 0 Batch: 115 Train Loss: 0.10237789154052734\n",
      "Epoch 0 Batch: 116 Train Loss: 0.5534811019897461\n",
      "Epoch 0 Batch: 117 Train Loss: 0.3294059634208679\n",
      "Epoch 0 Batch: 118 Train Loss: 0.1017884835600853\n",
      "Epoch 0 Batch: 119 Train Loss: 0.5249248147010803\n",
      "Epoch 0 Batch: 120 Train Loss: 0.5348527431488037\n",
      "Epoch 0 Batch: 121 Train Loss: 0.3040767312049866\n",
      "Epoch 0 Batch: 122 Train Loss: 0.31288012862205505\n",
      "Epoch 0 Batch: 123 Train Loss: 0.3051757216453552\n",
      "Epoch 0 Batch: 124 Train Loss: 0.5530773401260376\n",
      "Epoch 0 Batch: 125 Train Loss: 0.5430005788803101\n",
      "Epoch 0 Batch: 126 Train Loss: 0.1075226217508316\n",
      "Epoch 0 Batch: 127 Train Loss: 0.1093006506562233\n",
      "Epoch 0 Batch: 128 Train Loss: 0.12295065820217133\n",
      "Epoch 0 Batch: 129 Train Loss: 0.5089651942253113\n",
      "Epoch 0 Batch: 130 Train Loss: 0.3379065990447998\n",
      "Epoch 0 Batch: 131 Train Loss: 0.11475606262683868\n",
      "Epoch 0 Batch: 132 Train Loss: 0.32336267828941345\n",
      "Epoch 0 Batch: 133 Train Loss: 0.11348412930965424\n",
      "Epoch 0 Batch: 134 Train Loss: 0.11188938468694687\n",
      "Epoch 0 Batch: 135 Train Loss: 0.34721800684928894\n",
      "Epoch 0 Batch: 136 Train Loss: 0.33484283089637756\n",
      "Epoch 0 Batch: 137 Train Loss: 0.11750788986682892\n",
      "Epoch 0 Batch: 138 Train Loss: 0.5175307989120483\n",
      "Epoch 0 Batch: 139 Train Loss: 0.09443925321102142\n",
      "Epoch 0 Batch: 140 Train Loss: 0.3046707510948181\n",
      "Epoch 0 Batch: 141 Train Loss: 0.5029927492141724\n",
      "Epoch 0 Batch: 142 Train Loss: 0.5288593769073486\n",
      "Epoch 0 Batch: 143 Train Loss: 0.3094133138656616\n",
      "Epoch 0 Batch: 144 Train Loss: 0.32511070370674133\n",
      "Epoch 0 Batch: 145 Train Loss: 0.09467482566833496\n",
      "Epoch 0 Batch: 146 Train Loss: 0.7513689994812012\n",
      "Epoch 0 Batch: 147 Train Loss: 0.3121638894081116\n",
      "Epoch 0 Batch: 148 Train Loss: 0.29755979776382446\n",
      "Epoch 0 Batch: 149 Train Loss: 0.5189636945724487\n",
      "Epoch 0 Batch: 150 Train Loss: 0.0991382971405983\n",
      "Epoch 0 Batch: 151 Train Loss: 0.09619562327861786\n",
      "Epoch 0 Batch: 152 Train Loss: 0.5295171737670898\n",
      "Epoch 0 Batch: 153 Train Loss: 0.3045137822628021\n",
      "Epoch 0 Batch: 154 Train Loss: 0.10807999223470688\n",
      "Epoch 0 Batch: 155 Train Loss: 0.0892317146062851\n",
      "Epoch 0 Batch: 156 Train Loss: 0.538608193397522\n",
      "Epoch 0 Batch: 157 Train Loss: 0.7052263617515564\n",
      "Epoch 0 Batch: 158 Train Loss: 0.09667162597179413\n",
      "Epoch 0 Batch: 159 Train Loss: 0.10479049384593964\n",
      "Epoch 0 Batch: 160 Train Loss: 0.09663231670856476\n",
      "Epoch 0 Batch: 161 Train Loss: 0.521985650062561\n",
      "Epoch 0 Batch: 162 Train Loss: 0.09502185881137848\n",
      "Epoch 0 Batch: 163 Train Loss: 0.09779156744480133\n",
      "Epoch 0 Batch: 164 Train Loss: 0.49180665612220764\n",
      "Epoch 0 Batch: 165 Train Loss: 0.31036871671676636\n",
      "Epoch 0 Batch: 166 Train Loss: 0.4835829734802246\n",
      "Epoch 0 Batch: 167 Train Loss: 0.0940900668501854\n",
      "Epoch 0 Batch: 168 Train Loss: 0.4707861542701721\n",
      "Epoch 0 Batch: 169 Train Loss: 0.3104047477245331\n",
      "Epoch 0 Batch: 170 Train Loss: 0.5035930871963501\n",
      "Epoch 0 Batch: 171 Train Loss: 0.10098864883184433\n",
      "Epoch 0 Batch: 172 Train Loss: 0.519148051738739\n",
      "Epoch 0 Batch: 173 Train Loss: 0.4762668013572693\n",
      "Epoch 0 Batch: 174 Train Loss: 0.2937155067920685\n",
      "Epoch 0 Batch: 175 Train Loss: 0.5062254667282104\n",
      "Epoch 0 Batch: 176 Train Loss: 0.10446096956729889\n",
      "Epoch 0 Batch: 177 Train Loss: 0.10127425193786621\n",
      "Epoch 0 Batch: 178 Train Loss: 0.10435055196285248\n",
      "Epoch 0 Batch: 179 Train Loss: 0.881872832775116\n",
      "Epoch 0 Batch: 180 Train Loss: 0.7248163223266602\n",
      "Epoch 0 Batch: 181 Train Loss: 0.3024131655693054\n",
      "Epoch 0 Batch: 182 Train Loss: 0.10832077264785767\n",
      "Epoch 0 Batch: 183 Train Loss: 0.3083338737487793\n",
      "Epoch 0 Batch: 184 Train Loss: 0.10994064807891846\n",
      "Epoch 0 Batch: 185 Train Loss: 0.2662084996700287\n",
      "Epoch 0 Batch: 186 Train Loss: 0.45362037420272827\n",
      "Epoch 0 Batch: 187 Train Loss: 0.2723246216773987\n",
      "Epoch 0 Batch: 188 Train Loss: 0.28684133291244507\n",
      "Epoch 0 Batch: 189 Train Loss: 0.3105282187461853\n",
      "Epoch 0 Batch: 190 Train Loss: 0.4588959217071533\n",
      "Epoch 0 Batch: 191 Train Loss: 0.1085623949766159\n",
      "Epoch 0 Batch: 192 Train Loss: 0.10429741442203522\n",
      "Epoch 0 Batch: 193 Train Loss: 0.10295727103948593\n",
      "Epoch 0 Batch: 194 Train Loss: 0.47574082016944885\n",
      "Epoch 0 Batch: 195 Train Loss: 0.3346866965293884\n",
      "Epoch 0 Batch: 196 Train Loss: 0.10144519805908203\n",
      "Epoch 0 Batch: 197 Train Loss: 0.0969848558306694\n",
      "Epoch 0 Batch: 198 Train Loss: 0.46231159567832947\n",
      "Epoch 0 Batch: 199 Train Loss: 0.2873229682445526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch: 200 Train Loss: 0.33113807439804077\n",
      "Epoch 0 Batch: 201 Train Loss: 0.49818676710128784\n",
      "Epoch 0 Batch: 202 Train Loss: 0.28455689549446106\n",
      "Epoch 0 Batch: 203 Train Loss: 0.09301437437534332\n",
      "Epoch 0 Batch: 204 Train Loss: 0.3302096426486969\n",
      "Epoch 0 Batch: 205 Train Loss: 0.08957286179065704\n",
      "Epoch 0 Batch: 206 Train Loss: 0.6880839467048645\n",
      "Epoch 0 Batch: 207 Train Loss: 0.6602739095687866\n",
      "Epoch 0 Batch: 208 Train Loss: 0.09177723526954651\n",
      "Epoch 0 Batch: 209 Train Loss: 0.2719081938266754\n",
      "Epoch 0 Batch: 210 Train Loss: 0.09355104714632034\n",
      "Epoch 0 Batch: 211 Train Loss: 0.2588096559047699\n",
      "Epoch 0 Batch: 212 Train Loss: 0.25817376375198364\n",
      "Epoch 0 Batch: 213 Train Loss: 0.2868548631668091\n",
      "Epoch 0 Batch: 214 Train Loss: 0.43462935090065\n",
      "Epoch 0 Batch: 215 Train Loss: 0.09273195266723633\n",
      "Epoch 0 Batch: 216 Train Loss: 0.24564075469970703\n",
      "Epoch 0 Batch: 217 Train Loss: 0.2868092656135559\n",
      "Epoch 0 Batch: 218 Train Loss: 0.5398596525192261\n",
      "Epoch 0 Batch: 219 Train Loss: 0.41433411836624146\n",
      "Epoch 0 Batch: 220 Train Loss: 0.2504630982875824\n",
      "Epoch 0 Batch: 221 Train Loss: 0.49291378259658813\n",
      "Epoch 0 Batch: 222 Train Loss: 0.2725645899772644\n",
      "Epoch 0 Batch: 223 Train Loss: 0.09770067036151886\n",
      "Epoch 0 Batch: 224 Train Loss: 0.4271426200866699\n",
      "Epoch 0 Batch: 225 Train Loss: 0.24776038527488708\n",
      "Epoch 0 Batch: 226 Train Loss: 0.2641156017780304\n",
      "Epoch 0 Batch: 227 Train Loss: 0.3838476240634918\n",
      "Epoch 0 Batch: 228 Train Loss: 0.42715325951576233\n",
      "Epoch 0 Batch: 229 Train Loss: 0.42400917410850525\n",
      "Epoch 0 Batch: 230 Train Loss: 0.10498641431331635\n",
      "Epoch 0 Batch: 231 Train Loss: 0.38726919889450073\n",
      "Epoch 0 Batch: 232 Train Loss: 0.2555599510669708\n",
      "Epoch 0 Batch: 233 Train Loss: 0.23843035101890564\n",
      "Epoch 0 Batch: 234 Train Loss: 0.1026010736823082\n",
      "Epoch 0 Batch: 235 Train Loss: 0.10142066329717636\n",
      "Epoch 0 Batch: 236 Train Loss: 0.31505662202835083\n",
      "Epoch 0 Batch: 237 Train Loss: 0.2303699553012848\n",
      "Epoch 0 Batch: 238 Train Loss: 0.0911775529384613\n",
      "Epoch 0 Batch: 239 Train Loss: 0.09288132190704346\n",
      "Epoch 0 Batch: 240 Train Loss: 0.08538204431533813\n",
      "Epoch 0 Batch: 241 Train Loss: 0.23738959431648254\n",
      "Epoch 0 Batch: 242 Train Loss: 0.4256792962551117\n",
      "Epoch 0 Batch: 243 Train Loss: 0.21984398365020752\n",
      "Epoch 0 Batch: 244 Train Loss: 0.35598695278167725\n",
      "Epoch 0 Batch: 245 Train Loss: 0.07573338598012924\n",
      "Epoch 0 Batch: 246 Train Loss: 0.23010015487670898\n",
      "Epoch 0 Batch: 247 Train Loss: 0.27044054865837097\n",
      "Epoch 0 Batch: 248 Train Loss: 0.3343384563922882\n",
      "Epoch 0 Batch: 249 Train Loss: 0.1976293921470642\n",
      "Epoch 0 Batch: 250 Train Loss: 0.06797333061695099\n",
      "Epoch 0 Batch: 251 Train Loss: 0.2000749558210373\n",
      "Epoch 0 Batch: 252 Train Loss: 0.06565846502780914\n",
      "Epoch 0 Batch: 253 Train Loss: 0.2921651601791382\n",
      "Epoch 0 Batch: 254 Train Loss: 0.2972085475921631\n",
      "Epoch 0 Batch: 255 Train Loss: 0.06355907768011093\n",
      "Epoch 0 Batch: 256 Train Loss: 0.18771933019161224\n",
      "Epoch 0 Batch: 257 Train Loss: 0.4217008054256439\n",
      "Epoch 0 Batch: 258 Train Loss: 0.21873824298381805\n",
      "Epoch 0 Batch: 259 Train Loss: 0.061769790947437286\n",
      "Epoch 0 Batch: 260 Train Loss: 0.38996678590774536\n",
      "Epoch 0 Batch: 261 Train Loss: 0.45933476090431213\n",
      "Epoch 0 Batch: 262 Train Loss: 0.060409121215343475\n",
      "Epoch 0 Batch: 263 Train Loss: 0.294628769159317\n",
      "Epoch 0 Batch: 264 Train Loss: 0.27017608284950256\n",
      "Epoch 0 Batch: 265 Train Loss: 0.43121594190597534\n",
      "Epoch 0 Batch: 266 Train Loss: 0.06996874511241913\n",
      "Epoch 0 Batch: 267 Train Loss: 0.37404966354370117\n",
      "Epoch 0 Batch: 268 Train Loss: 0.3483474850654602\n",
      "Epoch 0 Batch: 269 Train Loss: 0.23748815059661865\n",
      "Epoch 0 Batch: 270 Train Loss: 0.16787488758563995\n",
      "Epoch 0 Batch: 271 Train Loss: 0.22629694640636444\n",
      "Epoch 0 Batch: 272 Train Loss: 0.2704900801181793\n",
      "Epoch 0 Batch: 273 Train Loss: 0.07922235876321793\n",
      "Epoch 0 Batch: 274 Train Loss: 0.6114427447319031\n",
      "Epoch 0 Batch: 275 Train Loss: 0.44435739517211914\n",
      "Epoch 0 Batch: 276 Train Loss: 0.08593583106994629\n",
      "Epoch 0 Batch: 277 Train Loss: 0.0803016647696495\n",
      "Epoch 0 Batch: 278 Train Loss: 0.09758066385984421\n",
      "Epoch 0 Batch: 279 Train Loss: 0.20034900307655334\n",
      "Epoch 0 Batch: 280 Train Loss: 0.26486220955848694\n",
      "Epoch 0 Batch: 281 Train Loss: 0.37428683042526245\n",
      "Epoch 0 Batch: 282 Train Loss: 0.16092848777770996\n",
      "Epoch 0 Batch: 283 Train Loss: 0.07182999700307846\n",
      "Epoch 0 Batch: 284 Train Loss: 0.5641167759895325\n",
      "Epoch 0 Batch: 285 Train Loss: 0.33136501908302307\n",
      "Epoch 0 Batch: 286 Train Loss: 0.1642632931470871\n",
      "Epoch 0 Batch: 287 Train Loss: 0.08923028409481049\n",
      "Epoch 0 Batch: 288 Train Loss: 0.07405879348516464\n",
      "Epoch 0 Batch: 289 Train Loss: 0.25653645396232605\n",
      "Epoch 0 Batch: 290 Train Loss: 0.14970891177654266\n",
      "Epoch 0 Batch: 291 Train Loss: 0.38484227657318115\n",
      "Epoch 0 Batch: 292 Train Loss: 0.19074152410030365\n",
      "Epoch 0 Batch: 293 Train Loss: 0.062072157859802246\n",
      "Epoch 0 Batch: 294 Train Loss: 0.1384543776512146\n",
      "Epoch 0 Batch: 295 Train Loss: 0.17485347390174866\n",
      "Epoch 0 Batch: 296 Train Loss: 0.21761469542980194\n",
      "Epoch 0 Batch: 297 Train Loss: 0.25633078813552856\n",
      "Epoch 0 Batch: 298 Train Loss: 0.05875339359045029\n",
      "Epoch 0 Batch: 299 Train Loss: 0.057324398308992386\n",
      "Epoch 0 Batch: 300 Train Loss: 0.11944109201431274\n",
      "Epoch 0 Batch: 301 Train Loss: 0.05325240641832352\n",
      "Epoch 0 Batch: 302 Train Loss: 0.05088106542825699\n",
      "Epoch 0 Batch: 303 Train Loss: 0.1931203454732895\n",
      "Epoch 0 Batch: 304 Train Loss: 0.1251598298549652\n",
      "Epoch 0 Batch: 305 Train Loss: 0.044531311839818954\n",
      "Epoch 0 Batch: 306 Train Loss: 0.04251705855131149\n",
      "Epoch 0 Batch: 307 Train Loss: 0.04738921299576759\n",
      "Epoch 0 Batch: 308 Train Loss: 0.04267969727516174\n",
      "Epoch 0 Batch: 309 Train Loss: 0.22954139113426208\n",
      "Epoch 0 Batch: 310 Train Loss: 0.03649350628256798\n",
      "Epoch 0 Batch: 311 Train Loss: 0.22141873836517334\n",
      "Epoch 0 Batch: 312 Train Loss: 0.09194066375494003\n",
      "Epoch 0 Batch: 313 Train Loss: 0.03589291125535965\n",
      "Epoch 0 Batch: 314 Train Loss: 0.09275688230991364\n",
      "Epoch 0 Batch: 315 Train Loss: 0.03323129564523697\n",
      "Epoch 0 Batch: 316 Train Loss: 0.4114886224269867\n",
      "Epoch 0 Batch: 317 Train Loss: 0.2183133065700531\n",
      "Epoch 0 Batch: 318 Train Loss: 0.2024448662996292\n",
      "Epoch 0 Batch: 319 Train Loss: 0.24188606441020966\n",
      "Epoch 0 Batch: 320 Train Loss: 0.02952706813812256\n",
      "Epoch 0 Batch: 321 Train Loss: 0.08118840306997299\n",
      "Epoch 0 Batch: 322 Train Loss: 0.034656278789043427\n",
      "Epoch 0 Batch: 323 Train Loss: 0.21835628151893616\n",
      "Epoch 0 Batch: 324 Train Loss: 0.0818277969956398\n",
      "Epoch 0 Batch: 325 Train Loss: 0.09938627481460571\n",
      "Epoch 0 Batch: 326 Train Loss: 0.03579952195286751\n",
      "Epoch 0 Batch: 327 Train Loss: 0.033965639770030975\n",
      "Epoch 0 Batch: 328 Train Loss: 0.029803112149238586\n",
      "Epoch 0 Batch: 329 Train Loss: 0.19602034986019135\n",
      "Epoch 0 Batch: 330 Train Loss: 0.28699609637260437\n",
      "Epoch 0 Batch: 331 Train Loss: 0.1119614839553833\n",
      "Epoch 0 Batch: 332 Train Loss: 0.24118168652057648\n",
      "Epoch 0 Batch: 333 Train Loss: 0.34760600328445435\n",
      "Epoch 0 Batch: 334 Train Loss: 0.29011303186416626\n",
      "Epoch 0 Batch: 335 Train Loss: 0.23032402992248535\n",
      "Epoch 0 Batch: 336 Train Loss: 0.12313538789749146\n",
      "Epoch 0 Batch: 337 Train Loss: 0.1391262263059616\n",
      "Epoch 0 Batch: 338 Train Loss: 0.36409085988998413\n",
      "Epoch 0 Batch: 339 Train Loss: 0.05627404525876045\n",
      "Epoch 0 Batch: 340 Train Loss: 0.05755072832107544\n",
      "Epoch 0 Batch: 341 Train Loss: 0.09335489571094513\n",
      "Epoch 0 Batch: 342 Train Loss: 0.2755682170391083\n",
      "Epoch 0 Batch: 343 Train Loss: 0.05859459191560745\n",
      "Epoch 0 Batch: 344 Train Loss: 0.09286272525787354\n",
      "Epoch 0 Batch: 345 Train Loss: 0.14466413855552673\n",
      "Epoch 0 Batch: 346 Train Loss: 0.05522281676530838\n",
      "Epoch 0 Batch: 347 Train Loss: 0.1526026725769043\n",
      "Epoch 0 Batch: 348 Train Loss: 0.08296920359134674\n",
      "Epoch 0 Batch: 349 Train Loss: 0.2149566411972046\n",
      "Epoch 0 Batch: 350 Train Loss: 0.3545367121696472\n",
      "Epoch 0 Batch: 351 Train Loss: 0.08342189341783524\n",
      "Epoch 0 Batch: 352 Train Loss: 0.17843231558799744\n",
      "Epoch 0 Batch: 353 Train Loss: 0.07204941660165787\n",
      "Epoch 0 Batch: 354 Train Loss: 0.039299994707107544\n",
      "Epoch 0 Batch: 355 Train Loss: 0.03977903723716736\n",
      "Epoch 0 Batch: 356 Train Loss: 0.13802595436573029\n",
      "Epoch 0 Batch: 357 Train Loss: 0.19592399895191193\n",
      "Epoch 0 Batch: 358 Train Loss: 0.03206031769514084\n",
      "Epoch 0 Batch: 359 Train Loss: 0.033783990889787674\n",
      "Epoch 0 Batch: 360 Train Loss: 0.060605037957429886\n",
      "Epoch 0 Batch: 361 Train Loss: 0.05748140811920166\n",
      "Epoch 0 Batch: 362 Train Loss: 0.2691807448863983\n",
      "Epoch 0 Batch: 363 Train Loss: 0.40366309881210327\n",
      "Epoch 0 Batch: 364 Train Loss: 0.02781430445611477\n",
      "Epoch 0 Batch: 365 Train Loss: 0.5164869427680969\n",
      "Epoch 0 Batch: 366 Train Loss: 0.39514416456222534\n",
      "Epoch 0 Batch: 367 Train Loss: 0.05783499404788017\n",
      "Epoch 0 Batch: 368 Train Loss: 0.034334175288677216\n",
      "Epoch 0 Batch: 369 Train Loss: 0.351561576128006\n",
      "Epoch 0 Batch: 370 Train Loss: 0.06540445238351822\n",
      "Epoch 0 Batch: 371 Train Loss: 0.34375494718551636\n",
      "Epoch 0 Batch: 372 Train Loss: 0.0432911142706871\n",
      "Epoch 0 Batch: 373 Train Loss: 0.07330246269702911\n",
      "Epoch 0 Batch: 374 Train Loss: 0.04661152511835098\n",
      "Epoch 0 Batch: 375 Train Loss: 0.38034573197364807\n",
      "Epoch 0 Batch: 376 Train Loss: 0.05792619660496712\n",
      "Epoch 0 Batch: 377 Train Loss: 0.22317445278167725\n",
      "Epoch 0 Batch: 378 Train Loss: 0.05144558101892471\n",
      "Epoch 0 Batch: 379 Train Loss: 0.09582076966762543\n",
      "Epoch 0 Batch: 380 Train Loss: 0.059624236077070236\n",
      "Epoch 0 Batch: 381 Train Loss: 0.06079179048538208\n",
      "Epoch 0 Batch: 382 Train Loss: 0.04096405953168869\n",
      "Epoch 0 Batch: 383 Train Loss: 0.03531179577112198\n",
      "Epoch 0 Batch: 384 Train Loss: 0.032693639397621155\n",
      "Epoch 0 Batch: 385 Train Loss: 0.04785362258553505\n",
      "Epoch 0 Batch: 386 Train Loss: 0.06905065476894379\n",
      "Epoch 0 Batch: 387 Train Loss: 0.04360325634479523\n",
      "Epoch 0 Batch: 388 Train Loss: 0.02196451835334301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch: 389 Train Loss: 0.11494763195514679\n",
      "Epoch 0 Batch: 390 Train Loss: 0.04408124461770058\n",
      "Epoch 0 Batch: 391 Train Loss: 0.017241861671209335\n",
      "Epoch 0 Batch: 392 Train Loss: 0.016527991741895676\n",
      "Epoch 0 Batch: 393 Train Loss: 0.014517875388264656\n",
      "Epoch 0 Batch: 394 Train Loss: 0.013782063499093056\n",
      "Epoch 0 Batch: 395 Train Loss: 0.3078324794769287\n",
      "Epoch 0 Batch: 396 Train Loss: 0.012227377854287624\n",
      "Epoch 0 Batch: 397 Train Loss: 0.690844714641571\n",
      "Epoch 0 Batch: 398 Train Loss: 0.45502883195877075\n",
      "Epoch 0 Batch: 399 Train Loss: 0.20921678841114044\n",
      "Epoch 0 Batch: 400 Train Loss: 0.039122555404901505\n",
      "Epoch 0 Batch: 401 Train Loss: 0.01851819083094597\n",
      "Epoch 0 Batch: 402 Train Loss: 0.038271479308605194\n",
      "Epoch 0 Batch: 403 Train Loss: 0.05149281769990921\n",
      "Epoch 0 Batch: 404 Train Loss: 0.023411735892295837\n",
      "Epoch 0 Batch: 405 Train Loss: 0.03774969279766083\n",
      "Epoch 0 Batch: 406 Train Loss: 0.05507909506559372\n",
      "Epoch 0 Batch: 407 Train Loss: 0.037308163940906525\n",
      "Epoch 0 Batch: 408 Train Loss: 0.0276032742112875\n",
      "Epoch 0 Batch: 409 Train Loss: 0.025726284831762314\n",
      "Epoch 0 Batch: 410 Train Loss: 0.1579500138759613\n",
      "Epoch 0 Batch: 411 Train Loss: 0.18322531878948212\n",
      "Epoch 0 Batch: 412 Train Loss: 0.039404429495334625\n",
      "Epoch 0 Batch: 413 Train Loss: 0.029606085270643234\n",
      "Epoch 0 Batch: 414 Train Loss: 0.40962308645248413\n",
      "Epoch 0 Batch: 415 Train Loss: 0.04442454129457474\n",
      "Epoch 0 Batch: 416 Train Loss: 0.049223393201828\n",
      "Epoch 0 Batch: 417 Train Loss: 0.07099731266498566\n",
      "Epoch 0 Batch: 418 Train Loss: 0.044773880392313004\n",
      "Epoch 0 Batch: 419 Train Loss: 0.04311863332986832\n",
      "Epoch 0 Batch: 420 Train Loss: 0.043199263513088226\n",
      "Epoch 0 Batch: 421 Train Loss: 0.03199802339076996\n",
      "Epoch 0 Batch: 422 Train Loss: 0.08187860995531082\n",
      "Epoch 0 Batch: 423 Train Loss: 0.0728844478726387\n",
      "Epoch 0 Batch: 424 Train Loss: 0.025906551629304886\n",
      "Epoch 0 Batch: 425 Train Loss: 0.14778704941272736\n",
      "Epoch 0 Batch: 426 Train Loss: 0.027919763699173927\n",
      "Epoch 0 Batch: 427 Train Loss: 0.047404468059539795\n",
      "Epoch 0 Batch: 428 Train Loss: 0.04136844724416733\n",
      "Epoch 0 Batch: 429 Train Loss: 0.4473830759525299\n",
      "Epoch 0 Batch: 430 Train Loss: 0.03838682919740677\n",
      "Epoch 0 Batch: 431 Train Loss: 0.034440916031599045\n",
      "Epoch 0 Batch: 432 Train Loss: 0.021993346512317657\n",
      "Epoch 0 Batch: 433 Train Loss: 0.21258755028247833\n",
      "Epoch 0 Batch: 434 Train Loss: 0.02918846346437931\n",
      "Epoch 0 Batch: 435 Train Loss: 0.021832305938005447\n",
      "Epoch 0 Batch: 436 Train Loss: 0.03414980322122574\n",
      "Epoch 0 Batch: 437 Train Loss: 0.019525570794939995\n",
      "Epoch 0 Batch: 438 Train Loss: 0.02727874182164669\n",
      "Epoch 0 Batch: 439 Train Loss: 0.034074462950229645\n",
      "Epoch 0 Batch: 440 Train Loss: 0.024422166869044304\n",
      "Epoch 0 Batch: 441 Train Loss: 0.0381513349711895\n",
      "Epoch 0 Batch: 442 Train Loss: 0.07591306418180466\n",
      "Epoch 0 Batch: 443 Train Loss: 0.01640046015381813\n",
      "Epoch 0 Batch: 444 Train Loss: 0.015404726378619671\n",
      "Epoch 0 Batch: 445 Train Loss: 0.2075628936290741\n",
      "Epoch 0 Batch: 446 Train Loss: 0.020909864455461502\n",
      "Epoch 0 Batch: 447 Train Loss: 0.026822185143828392\n",
      "Epoch 0 Batch: 448 Train Loss: 0.017225967720150948\n",
      "Epoch 0 Batch: 449 Train Loss: 0.01713266596198082\n",
      "Epoch 0 Batch: 450 Train Loss: 0.013511461205780506\n",
      "Epoch 0 Batch: 451 Train Loss: 0.4617970883846283\n",
      "Epoch 0 Batch: 452 Train Loss: 0.016552740707993507\n",
      "Epoch 0 Batch: 453 Train Loss: 0.018984198570251465\n",
      "Epoch 0 Batch: 454 Train Loss: 0.14615139365196228\n",
      "Epoch 0 Batch: 455 Train Loss: 0.015841525048017502\n",
      "Epoch 0 Batch: 456 Train Loss: 0.01640540361404419\n",
      "Epoch 0 Batch: 457 Train Loss: 0.04132453352212906\n",
      "Epoch 0 Batch: 458 Train Loss: 0.01632172241806984\n",
      "Epoch 0 Batch: 459 Train Loss: 0.016327686607837677\n",
      "Epoch 0 Batch: 460 Train Loss: 0.018398653715848923\n",
      "Epoch 0 Batch: 461 Train Loss: 0.033570222556591034\n",
      "Epoch 0 Batch: 462 Train Loss: 0.028713557869195938\n",
      "Epoch 0 Batch: 463 Train Loss: 0.014995324425399303\n",
      "Epoch 0 Batch: 464 Train Loss: 0.017083371058106422\n",
      "Epoch 0 Batch: 465 Train Loss: 0.013576211407780647\n",
      "Epoch 0 Batch: 466 Train Loss: 0.017473924905061722\n",
      "Epoch 0 Batch: 467 Train Loss: 0.01202342752367258\n",
      "Epoch 0 Batch: 468 Train Loss: 0.011600491590797901\n",
      "Epoch 0 Batch: 469 Train Loss: 0.01065453514456749\n",
      "Epoch 0 Batch: 470 Train Loss: 0.01761246658861637\n",
      "Epoch 0 Batch: 471 Train Loss: 0.00970064289867878\n",
      "Epoch 0 Batch: 472 Train Loss: 0.022207051515579224\n",
      "Epoch 0 Batch: 473 Train Loss: 0.021182529628276825\n",
      "Epoch 0 Batch: 474 Train Loss: 0.008148863911628723\n",
      "Epoch 0 Batch: 475 Train Loss: 0.007220583967864513\n",
      "Epoch 0 Batch: 476 Train Loss: 0.2578953504562378\n",
      "Epoch 0 Batch: 477 Train Loss: 0.0101889967918396\n",
      "Epoch 0 Batch: 478 Train Loss: 0.007527042180299759\n",
      "Epoch 0 Batch: 479 Train Loss: 0.007423407398164272\n",
      "Epoch 0 Batch: 480 Train Loss: 0.5272424817085266\n",
      "Epoch 0 Batch: 481 Train Loss: 0.011451558209955692\n",
      "Epoch 0 Batch: 482 Train Loss: 0.010978778824210167\n",
      "Epoch 0 Batch: 483 Train Loss: 0.01380786020308733\n",
      "Epoch 0 Batch: 484 Train Loss: 0.00959733035415411\n",
      "Epoch 0 Batch: 485 Train Loss: 0.009735560044646263\n",
      "Epoch 0 Batch: 486 Train Loss: 0.08771924674510956\n",
      "Epoch 0 Batch: 487 Train Loss: 0.011171834543347359\n",
      "Epoch 0 Batch: 488 Train Loss: 0.12062601000070572\n",
      "Epoch 0 Batch: 489 Train Loss: 0.23194512724876404\n",
      "Epoch 0 Batch: 490 Train Loss: 0.08593522757291794\n",
      "Epoch 0 Batch: 491 Train Loss: 0.02326779067516327\n",
      "Epoch 0 Batch: 492 Train Loss: 0.02431349642574787\n",
      "Epoch 0 Batch: 493 Train Loss: 0.36406946182250977\n",
      "Epoch 0 Batch: 494 Train Loss: 0.03265458345413208\n",
      "Epoch 0 Batch: 495 Train Loss: 0.04740085452795029\n",
      "Epoch 0 Batch: 496 Train Loss: 0.05663583427667618\n",
      "Epoch 0 Batch: 497 Train Loss: 0.05389906093478203\n",
      "Epoch 0 Batch: 498 Train Loss: 0.06714823842048645\n",
      "Epoch 0 Batch: 499 Train Loss: 0.045453742146492004\n",
      "Epoch 0 Batch: 500 Train Loss: 0.04170890152454376\n",
      "Epoch 0 Batch: 501 Train Loss: 0.03519336134195328\n",
      "Epoch 0 Batch: 502 Train Loss: 0.19879989326000214\n",
      "Epoch 0 Batch: 503 Train Loss: 0.026274073868989944\n",
      "Epoch 0 Batch: 504 Train Loss: 0.023821529000997543\n",
      "Epoch 0 Batch: 505 Train Loss: 0.019729655236005783\n",
      "Epoch 0 Batch: 506 Train Loss: 0.016169173642992973\n",
      "Epoch 0 Batch: 507 Train Loss: 0.04113655537366867\n",
      "Epoch 0 Batch: 508 Train Loss: 0.012453502975404263\n",
      "Epoch 0 Batch: 509 Train Loss: 0.02181217446923256\n",
      "Epoch 0 Batch: 510 Train Loss: 0.6186500787734985\n",
      "Epoch 0 Batch: 511 Train Loss: 0.013180078938603401\n",
      "Epoch 0 Batch: 512 Train Loss: 0.49730199575424194\n",
      "Epoch 0 Batch: 513 Train Loss: 0.01132774818688631\n",
      "Epoch 0 Batch: 514 Train Loss: 0.45492324233055115\n",
      "Epoch 0 Batch: 515 Train Loss: 0.018235329538583755\n",
      "Epoch 0 Batch: 516 Train Loss: 0.020230989903211594\n",
      "Epoch 0 Batch: 517 Train Loss: 0.0358181931078434\n",
      "Epoch 0 Batch: 518 Train Loss: 0.02464595064520836\n",
      "Epoch 0 Batch: 519 Train Loss: 0.026202181354165077\n",
      "Epoch 0 Batch: 520 Train Loss: 0.027843084186315536\n",
      "Epoch 0 Batch: 521 Train Loss: 0.0945248156785965\n",
      "Epoch 0 Batch: 522 Train Loss: 0.027630219236016273\n",
      "Epoch 0 Batch: 523 Train Loss: 0.02637599967420101\n",
      "Epoch 0 Batch: 524 Train Loss: 0.023676063865423203\n",
      "Epoch 0 Batch: 525 Train Loss: 0.02489905245602131\n",
      "Epoch 0 Batch: 526 Train Loss: 0.020364783704280853\n",
      "Epoch 0 Batch: 527 Train Loss: 0.01985856518149376\n",
      "Epoch 0 Batch: 528 Train Loss: 0.04584672302007675\n",
      "Epoch 0 Batch: 529 Train Loss: 0.057838428765535355\n",
      "Epoch 0 Batch: 530 Train Loss: 0.018956627696752548\n",
      "Epoch 0 Batch: 531 Train Loss: 0.01759422942996025\n",
      "Epoch 0 Batch: 532 Train Loss: 0.014437836594879627\n",
      "Epoch 0 Batch: 533 Train Loss: 0.012264711782336235\n",
      "Epoch 0 Batch: 534 Train Loss: 0.013407731428742409\n",
      "Epoch 0 Batch: 535 Train Loss: 0.010773125104606152\n",
      "Epoch 0 Batch: 536 Train Loss: 0.00956303346902132\n",
      "Epoch 0 Batch: 537 Train Loss: 0.044284239411354065\n",
      "Epoch 0 Batch: 538 Train Loss: 0.015017097815871239\n",
      "Epoch 0 Batch: 539 Train Loss: 0.14784125983715057\n",
      "Epoch 0 Batch: 540 Train Loss: 0.09487277269363403\n",
      "Epoch 0 Batch: 541 Train Loss: 0.01777336373925209\n",
      "Epoch 0 Batch: 542 Train Loss: 0.010605871677398682\n",
      "Epoch 0 Batch: 543 Train Loss: 0.15441304445266724\n",
      "Epoch 0 Batch: 544 Train Loss: 0.06605778634548187\n",
      "Epoch 0 Batch: 545 Train Loss: 0.01785779744386673\n",
      "Epoch 0 Batch: 546 Train Loss: 0.023760564625263214\n",
      "Epoch 0 Batch: 547 Train Loss: 0.04356389492750168\n",
      "Epoch 0 Batch: 548 Train Loss: 0.033453039824962616\n",
      "Epoch 0 Batch: 549 Train Loss: 0.03574422746896744\n",
      "Epoch 0 Batch: 550 Train Loss: 0.02900475263595581\n",
      "Epoch 0 Batch: 551 Train Loss: 0.029016125947237015\n",
      "Epoch 0 Batch: 552 Train Loss: 0.022948145866394043\n",
      "Epoch 0 Batch: 553 Train Loss: 0.04176827147603035\n",
      "Epoch 0 Batch: 554 Train Loss: 0.021270107477903366\n",
      "Epoch 0 Batch: 555 Train Loss: 0.028625745326280594\n",
      "Epoch 0 Batch: 556 Train Loss: 0.015073524788022041\n",
      "Epoch 0 Batch: 557 Train Loss: 0.014036739245057106\n",
      "Epoch 0 Batch: 558 Train Loss: 0.01270398497581482\n",
      "Epoch 0 Batch: 559 Train Loss: 0.012057611718773842\n",
      "Epoch 0 Batch: 560 Train Loss: 0.01090617198497057\n",
      "Epoch 0 Batch: 561 Train Loss: 0.10328201204538345\n",
      "Epoch 0 Batch: 562 Train Loss: 0.009816364385187626\n",
      "Epoch 0 Batch: 563 Train Loss: 0.47729143500328064\n",
      "Epoch 0 Batch: 564 Train Loss: 0.08041855692863464\n",
      "Epoch 0 Batch: 565 Train Loss: 0.010694758966565132\n",
      "Epoch 0 Batch: 566 Train Loss: 0.011043282225728035\n",
      "Epoch 0 Batch: 567 Train Loss: 0.012150894850492477\n",
      "Epoch 0 Batch: 568 Train Loss: 0.012705197557806969\n",
      "Epoch 0 Batch: 569 Train Loss: 0.013077346608042717\n",
      "Epoch 0 Batch: 570 Train Loss: 0.013001020066440105\n",
      "Epoch 0 Batch: 571 Train Loss: 0.01408771239221096\n",
      "Epoch 0 Batch: 572 Train Loss: 0.011288817971944809\n",
      "Epoch 0 Batch: 573 Train Loss: 0.012693616561591625\n",
      "Epoch 0 Batch: 574 Train Loss: 0.03123689815402031\n",
      "Epoch 0 Batch: 575 Train Loss: 0.011640814132988453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch: 576 Train Loss: 0.47005701065063477\n",
      "Epoch 0 Batch: 577 Train Loss: 0.013555832207202911\n",
      "Epoch 0 Batch: 578 Train Loss: 0.06476106494665146\n",
      "Epoch 0 Batch: 579 Train Loss: 0.013674517162144184\n",
      "Epoch 0 Batch: 580 Train Loss: 0.018504548817873\n",
      "Epoch 0 Batch: 581 Train Loss: 0.017020799219608307\n",
      "Epoch 0 Batch: 582 Train Loss: 0.4252558648586273\n",
      "Epoch 0 Batch: 583 Train Loss: 0.01964971236884594\n",
      "Epoch 0 Batch: 584 Train Loss: 0.020686063915491104\n",
      "Epoch 0 Batch: 585 Train Loss: 0.02612162195146084\n",
      "Epoch 0 Batch: 586 Train Loss: 0.02421160787343979\n",
      "Epoch 0 Batch: 587 Train Loss: 0.021764058619737625\n",
      "Epoch 0 Batch: 588 Train Loss: 0.022570915520191193\n",
      "Epoch 0 Batch: 589 Train Loss: 0.0200799573212862\n",
      "Epoch 0 Batch: 590 Train Loss: 0.01802625134587288\n",
      "Epoch 0 Batch: 591 Train Loss: 0.018315985798835754\n",
      "Epoch 0 Batch: 592 Train Loss: 0.013524949550628662\n",
      "Epoch 0 Batch: 593 Train Loss: 0.012807177379727364\n",
      "Epoch 0 Batch: 594 Train Loss: 0.011592808179557323\n",
      "Epoch 0 Batch: 595 Train Loss: 0.012693243101239204\n",
      "Epoch 0 Batch: 596 Train Loss: 0.009208355098962784\n",
      "Epoch 0 Batch: 597 Train Loss: 0.008389837108552456\n",
      "Epoch 0 Batch: 598 Train Loss: 0.008045358583331108\n",
      "Epoch 0 Batch: 599 Train Loss: 0.007409975863993168\n",
      "Epoch 0 Batch: 600 Train Loss: 0.1563902348279953\n",
      "Epoch 0 Batch: 601 Train Loss: 0.006074056960642338\n",
      "Epoch 0 Batch: 602 Train Loss: 0.006150316447019577\n",
      "Epoch 0 Batch: 603 Train Loss: 0.006893673446029425\n",
      "Epoch 0 Batch: 604 Train Loss: 0.006988526787608862\n",
      "Epoch 0 Batch: 605 Train Loss: 0.00614203792065382\n",
      "Epoch 0 Batch: 606 Train Loss: 0.007011630572378635\n",
      "Epoch 0 Batch: 607 Train Loss: 0.006417428143322468\n",
      "Epoch 0 Batch: 608 Train Loss: 0.5095815062522888\n",
      "Epoch 0 Batch: 609 Train Loss: 0.0740719810128212\n",
      "Epoch 0 Batch: 610 Train Loss: 0.09606096893548965\n",
      "Epoch 0 Batch: 611 Train Loss: 0.45696982741355896\n",
      "Epoch 0 Batch: 612 Train Loss: 0.011319249868392944\n",
      "Epoch 0 Batch: 613 Train Loss: 0.3931175172328949\n",
      "Epoch 0 Batch: 614 Train Loss: 0.02416805550456047\n",
      "Epoch 0 Batch: 615 Train Loss: 0.03016231581568718\n",
      "Epoch 0 Batch: 616 Train Loss: 0.03947368264198303\n",
      "Epoch 0 Batch: 617 Train Loss: 0.07686375826597214\n",
      "Epoch 0 Batch: 618 Train Loss: 0.06083809211850166\n",
      "Epoch 0 Batch: 619 Train Loss: 0.0486157163977623\n",
      "Epoch 0 Batch: 620 Train Loss: 0.03483835607767105\n",
      "Epoch 0 Batch: 621 Train Loss: 0.036379020661115646\n",
      "Epoch 0 Batch: 622 Train Loss: 0.018371500074863434\n",
      "Epoch 0 Batch: 623 Train Loss: 0.015058835037052631\n",
      "Epoch 0 Batch: 624 Train Loss: 0.014727172441780567\n",
      "Epoch 0 Batch: 625 Train Loss: 0.42174243927001953\n",
      "Epoch 0 Batch: 626 Train Loss: 0.011179270222783089\n",
      "Epoch 0 Batch: 627 Train Loss: 0.010178985074162483\n",
      "Epoch 0 Batch: 628 Train Loss: 0.008886291645467281\n",
      "Epoch 0 Batch: 629 Train Loss: 0.008876720443367958\n",
      "Epoch 0 Batch: 630 Train Loss: 0.5215445756912231\n",
      "Epoch 0 Batch: 631 Train Loss: 0.009160597808659077\n",
      "Epoch 0 Batch: 632 Train Loss: 0.009289062581956387\n",
      "Epoch 0 Batch: 633 Train Loss: 0.008611964993178844\n",
      "Epoch 0 Batch: 634 Train Loss: 0.0906379297375679\n",
      "Epoch 0 Batch: 635 Train Loss: 0.1374680995941162\n",
      "Epoch 0 Batch: 636 Train Loss: 0.011200929991900921\n",
      "Epoch 0 Batch: 637 Train Loss: 0.012387407012283802\n",
      "Epoch 0 Batch: 638 Train Loss: 0.08306463062763214\n",
      "Epoch 0 Batch: 639 Train Loss: 0.025604188442230225\n",
      "Epoch 0 Batch: 640 Train Loss: 0.016631796956062317\n",
      "Epoch 0 Batch: 641 Train Loss: 0.03007413074374199\n",
      "Epoch 0 Batch: 642 Train Loss: 0.014823299832642078\n",
      "Epoch 0 Batch: 643 Train Loss: 0.023733701556921005\n",
      "Epoch 0 Batch: 644 Train Loss: 0.02059072256088257\n",
      "Epoch 0 Batch: 645 Train Loss: 0.016177190467715263\n",
      "Epoch 0 Batch: 646 Train Loss: 0.06480520218610764\n",
      "Epoch 0 Batch: 647 Train Loss: 0.02273685485124588\n",
      "Epoch 0 Batch: 648 Train Loss: 0.022735554724931717\n",
      "Epoch 0 Batch: 649 Train Loss: 0.02117619849741459\n",
      "Epoch 0 Batch: 650 Train Loss: 0.015701357275247574\n",
      "Epoch 0 Batch: 651 Train Loss: 0.015333865769207478\n",
      "Epoch 0 Batch: 652 Train Loss: 0.014601899310946465\n",
      "Epoch 0 Batch: 653 Train Loss: 0.011488610878586769\n",
      "Epoch 0 Batch: 654 Train Loss: 0.010273647494614124\n",
      "Epoch 0 Batch: 655 Train Loss: 0.010180710814893246\n",
      "Epoch 0 Batch: 656 Train Loss: 0.009917529299855232\n",
      "Epoch 0 Batch: 657 Train Loss: 0.009306801483035088\n",
      "Epoch 0 Batch: 658 Train Loss: 0.009434801526367664\n",
      "Epoch 0 Batch: 659 Train Loss: 0.007798973470926285\n",
      "Epoch 0 Batch: 660 Train Loss: 0.007157701067626476\n",
      "Epoch 0 Batch: 661 Train Loss: 0.015003999695181847\n",
      "Epoch 0 Batch: 662 Train Loss: 0.006418485194444656\n",
      "Epoch 0 Batch: 663 Train Loss: 0.006056944839656353\n",
      "Epoch 0 Batch: 664 Train Loss: 0.005933552049100399\n",
      "Epoch 0 Batch: 665 Train Loss: 1.0382745265960693\n",
      "Epoch 0 Batch: 666 Train Loss: 0.00612275255843997\n",
      "Epoch 0 Batch: 667 Train Loss: 0.006402877159416676\n",
      "Epoch 0 Batch: 668 Train Loss: 0.0071162567473948\n",
      "Epoch 0 Batch: 669 Train Loss: 0.013805544003844261\n",
      "Epoch 0 Batch: 670 Train Loss: 0.007026588078588247\n",
      "Epoch 0 Batch: 671 Train Loss: 0.007025404367595911\n",
      "Epoch 0 Batch: 672 Train Loss: 0.00978041160851717\n",
      "Epoch 0 Batch: 673 Train Loss: 0.00860370323061943\n",
      "Epoch 0 Batch: 674 Train Loss: 0.509419322013855\n",
      "Epoch 0 Batch: 675 Train Loss: 0.4602961540222168\n",
      "Epoch 0 Batch: 676 Train Loss: 0.00926339440047741\n",
      "Epoch 0 Batch: 677 Train Loss: 0.011762152425944805\n",
      "Epoch 0 Batch: 678 Train Loss: 0.012371656484901905\n",
      "Epoch 0 Batch: 679 Train Loss: 0.11349651962518692\n",
      "Epoch 0 Batch: 680 Train Loss: 0.015630191192030907\n",
      "Epoch 0 Batch: 681 Train Loss: 0.018794279545545578\n",
      "Epoch 0 Batch: 682 Train Loss: 0.0181443989276886\n",
      "Epoch 0 Batch: 683 Train Loss: 0.01739223301410675\n",
      "Epoch 0 Batch: 684 Train Loss: 0.022695910185575485\n",
      "Epoch 0 Batch: 685 Train Loss: 0.036440443247556686\n",
      "Epoch 0 Batch: 686 Train Loss: 0.023747587576508522\n",
      "Epoch 0 Batch: 687 Train Loss: 0.36849498748779297\n",
      "Epoch 0 Batch: 688 Train Loss: 0.019961858168244362\n",
      "Epoch 0 Batch: 689 Train Loss: 0.03881259262561798\n",
      "Epoch 0 Batch: 690 Train Loss: 0.34895041584968567\n",
      "Epoch 0 Batch: 691 Train Loss: 0.0533270426094532\n",
      "Epoch 0 Batch: 692 Train Loss: 0.02423316240310669\n",
      "Epoch 0 Batch: 693 Train Loss: 0.0494864359498024\n",
      "Epoch 0 Batch: 694 Train Loss: 0.03936140611767769\n",
      "Epoch 0 Batch: 695 Train Loss: 0.026057768613100052\n",
      "Epoch 0 Batch: 696 Train Loss: 0.3404642641544342\n",
      "Epoch 0 Batch: 697 Train Loss: 0.020427023991942406\n",
      "Epoch 0 Batch: 698 Train Loss: 0.020636890083551407\n",
      "Epoch 0 Batch: 699 Train Loss: 0.029144058004021645\n",
      "Epoch 0 Batch: 700 Train Loss: 0.019965721294283867\n",
      "Epoch 0 Batch: 701 Train Loss: 0.02954886481165886\n",
      "Epoch 0 Batch: 702 Train Loss: 0.022770950570702553\n",
      "Epoch 0 Batch: 703 Train Loss: 0.016248760744929314\n",
      "Epoch 0 Batch: 704 Train Loss: 0.015563124790787697\n",
      "Epoch 0 Batch: 705 Train Loss: 0.013507338240742683\n",
      "Epoch 0 Batch: 706 Train Loss: 0.011901124380528927\n",
      "Epoch 0 Batch: 707 Train Loss: 0.010812904685735703\n",
      "Epoch 0 Batch: 708 Train Loss: 0.010935655795037746\n",
      "Epoch 0 Batch: 709 Train Loss: 0.009545451030135155\n",
      "Epoch 0 Batch: 710 Train Loss: 0.007501738611608744\n",
      "Epoch 0 Batch: 711 Train Loss: 0.006908939220011234\n",
      "Epoch 0 Batch: 712 Train Loss: 0.5479063987731934\n",
      "Epoch 0 Batch: 713 Train Loss: 0.006265832576900721\n",
      "Epoch 0 Batch: 714 Train Loss: 0.006227270234376192\n",
      "Epoch 0 Batch: 715 Train Loss: 0.0072455741465091705\n",
      "Epoch 0 Batch: 716 Train Loss: 0.060045480728149414\n",
      "Epoch 0 Batch: 717 Train Loss: 0.006118157412856817\n",
      "Epoch 0 Batch: 718 Train Loss: 0.007055341266095638\n",
      "Epoch 0 Batch: 719 Train Loss: 0.006298189517110586\n",
      "Epoch 0 Batch: 720 Train Loss: 0.006303885485976934\n",
      "Epoch 0 Batch: 721 Train Loss: 0.007513479329645634\n",
      "Epoch 0 Batch: 722 Train Loss: 0.11409062147140503\n",
      "Epoch 0 Batch: 723 Train Loss: 0.006430349312722683\n",
      "Epoch 0 Batch: 724 Train Loss: 0.05320409685373306\n",
      "Epoch 0 Batch: 725 Train Loss: 0.0073241060599684715\n",
      "Epoch 0 Batch: 726 Train Loss: 0.007576937321573496\n",
      "Epoch 0 Batch: 727 Train Loss: 0.5036019682884216\n",
      "Epoch 0 Batch: 728 Train Loss: 0.008065929636359215\n",
      "Epoch 0 Batch: 729 Train Loss: 0.010081619024276733\n",
      "Epoch 0 Batch: 730 Train Loss: 0.009554033167660236\n",
      "Epoch 0 Batch: 731 Train Loss: 0.011601652950048447\n",
      "Epoch 0 Batch: 732 Train Loss: 0.020027533173561096\n",
      "Epoch 0 Batch: 733 Train Loss: 0.012714853510260582\n",
      "Epoch 0 Batch: 734 Train Loss: 0.012730196118354797\n",
      "Epoch 0 Batch: 735 Train Loss: 0.012537973932921886\n",
      "Epoch 0 Batch: 736 Train Loss: 0.012088745832443237\n",
      "Epoch 0 Batch: 737 Train Loss: 0.012196233496069908\n",
      "Epoch 0 Batch: 738 Train Loss: 0.013005517423152924\n",
      "Epoch 0 Batch: 739 Train Loss: 0.01213956717401743\n",
      "Epoch 0 Batch: 740 Train Loss: 0.011694177985191345\n",
      "Epoch 0 Batch: 741 Train Loss: 0.010926349088549614\n",
      "Epoch 0 Batch: 742 Train Loss: 0.01223628781735897\n",
      "Epoch 0 Batch: 743 Train Loss: 0.010450467467308044\n",
      "Epoch 0 Batch: 744 Train Loss: 0.009653874672949314\n",
      "Epoch 0 Batch: 745 Train Loss: 0.01047874428331852\n",
      "Epoch 0 Batch: 746 Train Loss: 0.014582367613911629\n",
      "Epoch 0 Batch: 747 Train Loss: 0.00785654503852129\n",
      "Epoch 0 Batch: 748 Train Loss: 0.008909051306545734\n",
      "Epoch 0 Batch: 749 Train Loss: 0.008068417198956013\n",
      "Epoch 0 Batch: 750 Train Loss: 0.008351322263479233\n",
      "Epoch 0 Batch: 751 Train Loss: 0.010103602893650532\n",
      "Epoch 0 Batch: 752 Train Loss: 0.006632378790527582\n",
      "Epoch 0 Batch: 753 Train Loss: 0.007892986759543419\n",
      "Epoch 0 Batch: 754 Train Loss: 0.006263683084398508\n",
      "Epoch 0 Batch: 755 Train Loss: 0.006495083682239056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch: 756 Train Loss: 0.006324432790279388\n",
      "Epoch 0 Batch: 757 Train Loss: 0.5094923973083496\n",
      "Epoch 0 Batch: 758 Train Loss: 0.13593757152557373\n",
      "Epoch 0 Batch: 759 Train Loss: 0.009275917895138264\n",
      "Epoch 0 Batch: 760 Train Loss: 0.006892534904181957\n",
      "Epoch 0 Batch: 761 Train Loss: 0.007676216308027506\n",
      "Epoch 0 Batch: 762 Train Loss: 0.008418417535722256\n",
      "Epoch 0 Batch: 763 Train Loss: 0.008511248975992203\n",
      "Epoch 0 Batch: 764 Train Loss: 0.009029335342347622\n",
      "Epoch 0 Batch: 765 Train Loss: 0.009778882376849651\n",
      "Epoch 0 Batch: 766 Train Loss: 0.009009486995637417\n",
      "Epoch 0 Batch: 767 Train Loss: 0.4671076834201813\n",
      "Epoch 0 Batch: 768 Train Loss: 0.011493852362036705\n",
      "Epoch 0 Batch: 769 Train Loss: 0.012174286879599094\n",
      "Epoch 0 Batch: 770 Train Loss: 0.012093211524188519\n",
      "Epoch 0 Batch: 771 Train Loss: 0.45732420682907104\n",
      "Epoch 0 Batch: 772 Train Loss: 0.013713414780795574\n",
      "Epoch 0 Batch: 773 Train Loss: 0.01577884331345558\n",
      "Epoch 0 Batch: 774 Train Loss: 0.017107341438531876\n",
      "Epoch 0 Batch: 775 Train Loss: 0.01988360844552517\n",
      "Epoch 0 Batch: 776 Train Loss: 0.020974619314074516\n",
      "Epoch 0 Batch: 777 Train Loss: 0.02195555344223976\n",
      "Epoch 0 Batch: 778 Train Loss: 0.01683729514479637\n",
      "Epoch 0 Batch: 779 Train Loss: 0.017843682318925858\n",
      "Epoch 0 Batch: 780 Train Loss: 0.01130953710526228\n",
      "Epoch 0 Batch: 781 Train Loss: 0.013724623247981071\n",
      "Epoch 0 Batch: 782 Train Loss: 0.4537290930747986\n",
      "Epoch 0 Batch: 783 Train Loss: 0.011013672687113285\n",
      "Epoch 0 Batch: 784 Train Loss: 0.012060267850756645\n",
      "Epoch 0 Batch: 785 Train Loss: 0.026044106110930443\n",
      "Epoch 0 Batch: 786 Train Loss: 0.011620372533798218\n",
      "Epoch 0 Batch: 787 Train Loss: 0.011273575946688652\n",
      "Epoch 0 Batch: 788 Train Loss: 0.010959954932332039\n",
      "Epoch 0 Batch: 789 Train Loss: 0.01092267781496048\n",
      "Epoch 0 Batch: 790 Train Loss: 0.00930476002395153\n",
      "Epoch 0 Batch: 791 Train Loss: 0.008432449772953987\n",
      "Epoch 0 Batch: 792 Train Loss: 0.008637728169560432\n",
      "Epoch 0 Batch: 793 Train Loss: 0.00873592495918274\n",
      "Epoch 0 Batch: 794 Train Loss: 0.493461936712265\n",
      "Epoch 0 Batch: 795 Train Loss: 0.008551018312573433\n",
      "Epoch 0 Batch: 796 Train Loss: 0.011502280831336975\n",
      "Epoch 0 Batch: 797 Train Loss: 0.009022562764585018\n",
      "Epoch 0 Batch: 798 Train Loss: 0.00930416863411665\n",
      "Epoch 0 Batch: 799 Train Loss: 0.00930839590728283\n",
      "Epoch 0 Batch: 800 Train Loss: 0.008582575246691704\n",
      "Epoch 0 Batch: 801 Train Loss: 0.008310853503644466\n",
      "Epoch 0 Batch: 802 Train Loss: 0.13710850477218628\n",
      "Epoch 0 Batch: 803 Train Loss: 0.00941886380314827\n",
      "Epoch 0 Batch: 804 Train Loss: 0.009647508151829243\n",
      "Epoch 0 Batch: 805 Train Loss: 0.010406682267785072\n",
      "Epoch 0 Batch: 806 Train Loss: 0.010707695968449116\n",
      "Epoch 0 Batch: 807 Train Loss: 0.009901243261992931\n",
      "Epoch 0 Batch: 808 Train Loss: 0.011200045235455036\n",
      "Epoch 0 Batch: 809 Train Loss: 0.010044514201581478\n",
      "Epoch 0 Batch: 810 Train Loss: 0.009177098050713539\n",
      "Epoch 0 Batch: 811 Train Loss: 0.02548234723508358\n",
      "Epoch 0 Batch: 812 Train Loss: 0.010821813717484474\n",
      "Epoch 0 Batch: 813 Train Loss: 0.010418538935482502\n",
      "Epoch 0 Batch: 814 Train Loss: 0.009373175911605358\n",
      "Epoch 0 Batch: 815 Train Loss: 0.008798206225037575\n",
      "Epoch 0 Batch: 816 Train Loss: 0.008347873575985432\n",
      "Epoch 0 Batch: 817 Train Loss: 0.008797558024525642\n",
      "Epoch 0 Batch: 818 Train Loss: 0.007170965429395437\n",
      "Epoch 0 Batch: 819 Train Loss: 0.008337346836924553\n",
      "Epoch 0 Batch: 820 Train Loss: 0.007933266460895538\n",
      "Epoch 0 Batch: 821 Train Loss: 0.007588966283947229\n",
      "Epoch 0 Batch: 822 Train Loss: 0.006359227932989597\n",
      "Epoch 0 Batch: 823 Train Loss: 0.006077754311263561\n",
      "Epoch 0 Batch: 824 Train Loss: 0.006026652175933123\n",
      "Epoch 0 Batch: 825 Train Loss: 0.0064980885945260525\n",
      "Epoch 0 Batch: 826 Train Loss: 0.006091571878641844\n",
      "Epoch 0 Batch: 827 Train Loss: 0.0044840676710009575\n",
      "Epoch 0 Batch: 828 Train Loss: 0.5138530731201172\n",
      "Epoch 0 Batch: 829 Train Loss: 0.0058982474729418755\n",
      "Epoch 0 Batch: 830 Train Loss: 0.005581843201071024\n",
      "Epoch 0 Batch: 831 Train Loss: 0.005429102573543787\n",
      "Epoch 0 Batch: 832 Train Loss: 0.5011345744132996\n",
      "Epoch 0 Batch: 833 Train Loss: 0.006617248058319092\n",
      "Epoch 0 Batch: 834 Train Loss: 0.005913585890084505\n",
      "Epoch 0 Batch: 835 Train Loss: 0.0737399309873581\n",
      "Epoch 0 Batch: 836 Train Loss: 0.009318642318248749\n",
      "Epoch 0 Batch: 837 Train Loss: 0.007649709470570087\n",
      "Epoch 0 Batch: 838 Train Loss: 0.007642732001841068\n",
      "Epoch 0 Batch: 839 Train Loss: 0.010761714540421963\n",
      "Epoch 0 Batch: 840 Train Loss: 0.010956542566418648\n",
      "Epoch 0 Batch: 841 Train Loss: 0.012407917529344559\n",
      "Epoch 0 Batch: 842 Train Loss: 0.015598838217556477\n",
      "Epoch 0 Batch: 843 Train Loss: 0.010154985822737217\n",
      "Epoch 0 Batch: 844 Train Loss: 0.010717262513935566\n",
      "Epoch 0 Batch: 845 Train Loss: 0.008001713082194328\n",
      "Epoch 0 Batch: 846 Train Loss: 0.011102244257926941\n",
      "Epoch 0 Batch: 847 Train Loss: 0.010325879789888859\n",
      "Epoch 0 Batch: 848 Train Loss: 0.009557383134961128\n",
      "Epoch 0 Batch: 849 Train Loss: 0.006634312216192484\n",
      "Epoch 0 Batch: 850 Train Loss: 0.00864404160529375\n",
      "Epoch 0 Batch: 851 Train Loss: 0.4563297629356384\n",
      "Epoch 0 Batch: 852 Train Loss: 0.006088234484195709\n",
      "Epoch 0 Batch: 853 Train Loss: 0.010643573477864265\n",
      "Epoch 0 Batch: 854 Train Loss: 0.00624886155128479\n",
      "Epoch 0 Batch: 855 Train Loss: 0.44672101736068726\n",
      "Epoch 0 Batch: 856 Train Loss: 0.5410028100013733\n",
      "Epoch 0 Batch: 857 Train Loss: 0.012231947854161263\n",
      "Epoch 0 Batch: 858 Train Loss: 0.40429744124412537\n",
      "Epoch 0 Batch: 859 Train Loss: 0.018819764256477356\n",
      "Epoch 0 Batch: 860 Train Loss: 0.020378731191158295\n",
      "Epoch 0 Batch: 861 Train Loss: 0.029115736484527588\n",
      "Epoch 0 Batch: 862 Train Loss: 0.023392461240291595\n",
      "Epoch 0 Batch: 863 Train Loss: 0.3169822096824646\n",
      "Epoch 0 Batch: 864 Train Loss: 0.043681032955646515\n",
      "Epoch 0 Batch: 865 Train Loss: 0.055060796439647675\n",
      "Epoch 0 Batch: 866 Train Loss: 0.044769447296857834\n",
      "Epoch 0 Batch: 867 Train Loss: 0.04017513617873192\n",
      "Epoch 0 Batch: 868 Train Loss: 0.03269592300057411\n",
      "Epoch 0 Batch: 869 Train Loss: 0.02852283976972103\n",
      "Epoch 0 Batch: 870 Train Loss: 0.013790814206004143\n",
      "Epoch 0 Batch: 871 Train Loss: 0.013500258326530457\n",
      "Epoch 0 Batch: 872 Train Loss: 0.010070430114865303\n",
      "Epoch 0 Batch: 873 Train Loss: 0.011907119303941727\n",
      "Epoch 0 Batch: 874 Train Loss: 0.009264710359275341\n",
      "Epoch 0 Batch: 875 Train Loss: 0.008300403133034706\n",
      "Epoch 0 Batch: 876 Train Loss: 0.007557967212051153\n",
      "Epoch 0 Batch: 877 Train Loss: 0.007028243504464626\n",
      "Epoch 0 Batch: 878 Train Loss: 0.008540380746126175\n",
      "Epoch 0 Batch: 879 Train Loss: 0.03585083410143852\n",
      "Epoch 0 Batch: 880 Train Loss: 0.0060719698667526245\n",
      "Epoch 2 Batch: 1 Train Loss: 0.007122695446014404\n",
      "Epoch 2 Batch: 2 Train Loss: 0.007193276192992926\n",
      "Epoch 2 Batch: 3 Train Loss: 0.007116164080798626\n",
      "Epoch 2 Batch: 4 Train Loss: 0.006297001149505377\n",
      "Epoch 2 Batch: 5 Train Loss: 0.006510195787996054\n",
      "Epoch 2 Batch: 6 Train Loss: 0.006154592148959637\n",
      "Epoch 2 Batch: 7 Train Loss: 0.005934639368206263\n",
      "Epoch 2 Batch: 8 Train Loss: 0.0050663514994084835\n",
      "Epoch 2 Batch: 9 Train Loss: 0.005269430112093687\n",
      "Epoch 2 Batch: 10 Train Loss: 0.015047535300254822\n",
      "Epoch 2 Batch: 11 Train Loss: 0.005377986468374729\n",
      "Epoch 2 Batch: 12 Train Loss: 0.005545997526496649\n",
      "Epoch 2 Batch: 13 Train Loss: 0.005263817962259054\n",
      "Epoch 2 Batch: 14 Train Loss: 0.004998109303414822\n",
      "Epoch 2 Batch: 15 Train Loss: 0.005110264755785465\n",
      "Epoch 2 Batch: 16 Train Loss: 0.5490260720252991\n",
      "Epoch 2 Batch: 17 Train Loss: 0.0048409015871584415\n",
      "Epoch 2 Batch: 18 Train Loss: 0.004646042827516794\n",
      "Epoch 2 Batch: 19 Train Loss: 0.006039928644895554\n",
      "Epoch 2 Batch: 20 Train Loss: 0.0040519144386053085\n",
      "Epoch 2 Batch: 21 Train Loss: 0.005500669125467539\n",
      "Epoch 2 Batch: 22 Train Loss: 0.005659245885908604\n",
      "Epoch 2 Batch: 23 Train Loss: 0.005089848302304745\n",
      "Epoch 2 Batch: 24 Train Loss: 0.005653842817991972\n",
      "Epoch 2 Batch: 25 Train Loss: 0.0051583824679255486\n",
      "Epoch 2 Batch: 26 Train Loss: 0.005126622039824724\n",
      "Epoch 2 Batch: 27 Train Loss: 0.00514940544962883\n",
      "Epoch 2 Batch: 28 Train Loss: 0.005692328326404095\n",
      "Epoch 2 Batch: 29 Train Loss: 0.004977566655725241\n",
      "Epoch 2 Batch: 30 Train Loss: 0.0050095985643565655\n",
      "Epoch 2 Batch: 31 Train Loss: 0.004849919117987156\n",
      "Epoch 2 Batch: 32 Train Loss: 0.0742885023355484\n",
      "Epoch 2 Batch: 33 Train Loss: 0.5236509442329407\n",
      "Epoch 2 Batch: 34 Train Loss: 0.006047305651009083\n",
      "Epoch 2 Batch: 35 Train Loss: 0.006240407936275005\n",
      "Epoch 2 Batch: 36 Train Loss: 0.007334936410188675\n",
      "Epoch 2 Batch: 37 Train Loss: 0.005569823086261749\n",
      "Epoch 2 Batch: 38 Train Loss: 0.008362045511603355\n",
      "Epoch 2 Batch: 39 Train Loss: 0.007035703863948584\n",
      "Epoch 2 Batch: 40 Train Loss: 0.009304193779826164\n",
      "Epoch 2 Batch: 41 Train Loss: 0.006678333971649408\n",
      "Epoch 2 Batch: 42 Train Loss: 0.009175055660307407\n",
      "Epoch 2 Batch: 43 Train Loss: 0.010203240439295769\n",
      "Epoch 2 Batch: 44 Train Loss: 0.008206425234675407\n",
      "Epoch 2 Batch: 45 Train Loss: 0.008676925674080849\n",
      "Epoch 2 Batch: 46 Train Loss: 0.007640835829079151\n",
      "Epoch 2 Batch: 47 Train Loss: 0.008492430672049522\n",
      "Epoch 2 Batch: 48 Train Loss: 0.009011032059788704\n",
      "Epoch 2 Batch: 49 Train Loss: 0.007904388010501862\n",
      "Epoch 2 Batch: 50 Train Loss: 0.008471263572573662\n",
      "Epoch 2 Batch: 51 Train Loss: 0.48781466484069824\n",
      "Epoch 2 Batch: 52 Train Loss: 0.007453733589500189\n",
      "Epoch 2 Batch: 53 Train Loss: 0.4753072261810303\n",
      "Epoch 2 Batch: 54 Train Loss: 0.009122364223003387\n",
      "Epoch 2 Batch: 55 Train Loss: 0.010225656442344189\n",
      "Epoch 2 Batch: 56 Train Loss: 0.01252947747707367\n",
      "Epoch 2 Batch: 57 Train Loss: 0.015206868760287762\n",
      "Epoch 2 Batch: 58 Train Loss: 0.014215049333870411\n",
      "Epoch 2 Batch: 59 Train Loss: 0.015224546194076538\n",
      "Epoch 2 Batch: 60 Train Loss: 0.018539218232035637\n",
      "Epoch 2 Batch: 61 Train Loss: 0.013716822490096092\n",
      "Epoch 2 Batch: 62 Train Loss: 0.4864063858985901\n",
      "Epoch 2 Batch: 63 Train Loss: 0.43527358770370483\n",
      "Epoch 2 Batch: 64 Train Loss: 0.012922883033752441\n",
      "Epoch 2 Batch: 65 Train Loss: 0.01635109633207321\n",
      "Epoch 2 Batch: 66 Train Loss: 0.02006877213716507\n",
      "Epoch 2 Batch: 67 Train Loss: 0.017542269080877304\n",
      "Epoch 2 Batch: 68 Train Loss: 0.015593352727591991\n",
      "Epoch 2 Batch: 69 Train Loss: 0.015068848617374897\n",
      "Epoch 2 Batch: 70 Train Loss: 0.013409353792667389\n",
      "Epoch 2 Batch: 71 Train Loss: 0.01328536868095398\n",
      "Epoch 2 Batch: 72 Train Loss: 0.015304377302527428\n",
      "Epoch 2 Batch: 73 Train Loss: 0.010188378393650055\n",
      "Epoch 2 Batch: 74 Train Loss: 0.012221477925777435\n",
      "Epoch 2 Batch: 75 Train Loss: 0.01246644463390112\n",
      "Epoch 2 Batch: 76 Train Loss: 0.010191189125180244\n",
      "Epoch 2 Batch: 77 Train Loss: 0.009570363909006119\n",
      "Epoch 2 Batch: 78 Train Loss: 0.009307085536420345\n",
      "Epoch 2 Batch: 79 Train Loss: 0.007805895060300827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch: 80 Train Loss: 0.006115652155131102\n",
      "Epoch 2 Batch: 81 Train Loss: 0.006234679836779833\n",
      "Epoch 2 Batch: 82 Train Loss: 0.006202994380146265\n",
      "Epoch 2 Batch: 83 Train Loss: 0.006551031023263931\n",
      "Epoch 2 Batch: 84 Train Loss: 0.006121722050011158\n",
      "Epoch 2 Batch: 85 Train Loss: 0.005635919515043497\n",
      "Epoch 2 Batch: 86 Train Loss: 0.005197201389819384\n",
      "Epoch 2 Batch: 87 Train Loss: 0.005316448863595724\n",
      "Epoch 2 Batch: 88 Train Loss: 0.004553086124360561\n",
      "Epoch 2 Batch: 89 Train Loss: 0.004928188864141703\n",
      "Epoch 2 Batch: 90 Train Loss: 0.5294123291969299\n",
      "Epoch 2 Batch: 91 Train Loss: 0.004805128090083599\n",
      "Epoch 2 Batch: 92 Train Loss: 0.00534666096791625\n",
      "Epoch 2 Batch: 93 Train Loss: 0.005344551056623459\n",
      "Epoch 2 Batch: 94 Train Loss: 0.004803952760994434\n",
      "Epoch 2 Batch: 95 Train Loss: 0.005601775832474232\n",
      "Epoch 2 Batch: 96 Train Loss: 0.005303514655679464\n",
      "Epoch 2 Batch: 97 Train Loss: 0.00457503879442811\n",
      "Epoch 2 Batch: 98 Train Loss: 0.005720752757042646\n",
      "Epoch 2 Batch: 99 Train Loss: 0.005691749509423971\n",
      "Epoch 2 Batch: 100 Train Loss: 0.00493465643376112\n",
      "Epoch 2 Batch: 101 Train Loss: 0.004755002912133932\n",
      "Epoch 2 Batch: 102 Train Loss: 0.005269912537187338\n",
      "Epoch 2 Batch: 103 Train Loss: 0.5213587284088135\n",
      "Epoch 2 Batch: 104 Train Loss: 0.004850134253501892\n",
      "Epoch 2 Batch: 105 Train Loss: 0.004592317156493664\n",
      "Epoch 2 Batch: 106 Train Loss: 0.005695884115993977\n",
      "Epoch 2 Batch: 107 Train Loss: 0.5013464093208313\n",
      "Epoch 2 Batch: 108 Train Loss: 0.008578251115977764\n",
      "Epoch 2 Batch: 109 Train Loss: 0.0074716173112392426\n",
      "Epoch 2 Batch: 110 Train Loss: 0.5276476144790649\n",
      "Epoch 2 Batch: 111 Train Loss: 0.007708903402090073\n",
      "Epoch 2 Batch: 112 Train Loss: 0.008665520697832108\n",
      "Epoch 2 Batch: 113 Train Loss: 0.5120723843574524\n",
      "Epoch 2 Batch: 114 Train Loss: 0.013025025837123394\n",
      "Epoch 2 Batch: 115 Train Loss: 0.01229151152074337\n",
      "Epoch 2 Batch: 116 Train Loss: 0.012353346683084965\n",
      "Epoch 2 Batch: 117 Train Loss: 0.013753945007920265\n",
      "Epoch 2 Batch: 118 Train Loss: 0.015094248577952385\n",
      "Epoch 2 Batch: 119 Train Loss: 0.013855010271072388\n",
      "Epoch 2 Batch: 120 Train Loss: 0.01694192923605442\n",
      "Epoch 2 Batch: 121 Train Loss: 0.01683848537504673\n",
      "Epoch 2 Batch: 122 Train Loss: 0.018979966640472412\n",
      "Epoch 2 Batch: 123 Train Loss: 0.4088338315486908\n",
      "Epoch 2 Batch: 124 Train Loss: 0.01659000664949417\n",
      "Epoch 2 Batch: 125 Train Loss: 0.015014837495982647\n",
      "Epoch 2 Batch: 126 Train Loss: 0.018574584275484085\n",
      "Epoch 2 Batch: 127 Train Loss: 0.020336728543043137\n",
      "Epoch 2 Batch: 128 Train Loss: 0.014514562673866749\n",
      "Epoch 2 Batch: 129 Train Loss: 0.01957661285996437\n",
      "Epoch 2 Batch: 130 Train Loss: 0.016875075176358223\n",
      "Epoch 2 Batch: 131 Train Loss: 0.42909449338912964\n",
      "Epoch 2 Batch: 132 Train Loss: 0.01755632646381855\n",
      "Epoch 2 Batch: 133 Train Loss: 0.017311273142695427\n",
      "Epoch 2 Batch: 134 Train Loss: 0.42188137769699097\n",
      "Epoch 2 Batch: 135 Train Loss: 0.020476965233683586\n",
      "Epoch 2 Batch: 136 Train Loss: 0.018819456920027733\n",
      "Epoch 2 Batch: 137 Train Loss: 0.01881830021739006\n",
      "Epoch 2 Batch: 138 Train Loss: 0.019601279869675636\n",
      "Epoch 2 Batch: 139 Train Loss: 0.4094041883945465\n",
      "Epoch 2 Batch: 140 Train Loss: 0.021972687914967537\n",
      "Epoch 2 Batch: 141 Train Loss: 0.022558167576789856\n",
      "Epoch 2 Batch: 142 Train Loss: 0.022228065878152847\n",
      "Epoch 2 Batch: 143 Train Loss: 0.022223221138119698\n",
      "Epoch 2 Batch: 144 Train Loss: 0.01998770236968994\n",
      "Epoch 2 Batch: 145 Train Loss: 0.021751847118139267\n",
      "Epoch 2 Batch: 146 Train Loss: 0.40410980582237244\n",
      "Epoch 2 Batch: 147 Train Loss: 0.026484018191695213\n",
      "Epoch 2 Batch: 148 Train Loss: 0.01799311302602291\n",
      "Epoch 2 Batch: 149 Train Loss: 0.020976634696125984\n",
      "Epoch 2 Batch: 150 Train Loss: 0.021963659673929214\n",
      "Epoch 2 Batch: 151 Train Loss: 0.018947366625070572\n",
      "Epoch 2 Batch: 152 Train Loss: 0.016959629952907562\n",
      "Epoch 2 Batch: 153 Train Loss: 0.01801854558289051\n",
      "Epoch 2 Batch: 154 Train Loss: 0.017599618062376976\n",
      "Epoch 2 Batch: 155 Train Loss: 0.014334961771965027\n",
      "Epoch 2 Batch: 156 Train Loss: 0.014452795498073101\n",
      "Epoch 2 Batch: 157 Train Loss: 0.015088727697730064\n",
      "Epoch 2 Batch: 158 Train Loss: 0.008850539103150368\n",
      "Epoch 2 Batch: 159 Train Loss: 0.014096846804022789\n",
      "Epoch 2 Batch: 160 Train Loss: 0.012693608179688454\n",
      "Epoch 2 Batch: 161 Train Loss: 0.012125052511692047\n",
      "Epoch 2 Batch: 162 Train Loss: 0.46384698152542114\n",
      "Epoch 2 Batch: 163 Train Loss: 0.010160312987864017\n",
      "Epoch 2 Batch: 164 Train Loss: 0.008896245621144772\n",
      "Epoch 2 Batch: 165 Train Loss: 0.010116545483469963\n",
      "Epoch 2 Batch: 166 Train Loss: 0.009758385829627514\n",
      "Epoch 2 Batch: 167 Train Loss: 0.011052378453314304\n",
      "Epoch 2 Batch: 168 Train Loss: 0.010403802618384361\n",
      "Epoch 2 Batch: 169 Train Loss: 0.008391770534217358\n",
      "Epoch 2 Batch: 170 Train Loss: 0.007848708890378475\n",
      "Epoch 2 Batch: 171 Train Loss: 0.008695858530700207\n",
      "Epoch 2 Batch: 172 Train Loss: 0.00926331989467144\n",
      "Epoch 2 Batch: 173 Train Loss: 0.00802845973521471\n",
      "Epoch 2 Batch: 174 Train Loss: 0.008705832064151764\n",
      "Epoch 2 Batch: 175 Train Loss: 0.006853108294308186\n",
      "Epoch 2 Batch: 176 Train Loss: 0.007188943214714527\n",
      "Epoch 2 Batch: 177 Train Loss: 0.008026777766644955\n",
      "Epoch 2 Batch: 178 Train Loss: 0.007373063825070858\n",
      "Epoch 2 Batch: 179 Train Loss: 0.007389049045741558\n",
      "Epoch 2 Batch: 180 Train Loss: 0.006888786796480417\n",
      "Epoch 2 Batch: 181 Train Loss: 0.006622182670980692\n",
      "Epoch 2 Batch: 182 Train Loss: 0.006439580116420984\n",
      "Epoch 2 Batch: 183 Train Loss: 0.028211679309606552\n",
      "Epoch 2 Batch: 184 Train Loss: 0.005753724370151758\n",
      "Epoch 2 Batch: 185 Train Loss: 0.005718199070543051\n",
      "Epoch 2 Batch: 186 Train Loss: 0.005733446218073368\n",
      "Epoch 2 Batch: 187 Train Loss: 0.0057245828211307526\n",
      "Epoch 2 Batch: 188 Train Loss: 0.0052329422906041145\n",
      "Epoch 2 Batch: 189 Train Loss: 0.0051023876294493675\n",
      "Epoch 2 Batch: 190 Train Loss: 0.005360408686101437\n",
      "Epoch 2 Batch: 191 Train Loss: 0.00513157295063138\n",
      "Epoch 2 Batch: 192 Train Loss: 0.004526213277131319\n",
      "Epoch 2 Batch: 193 Train Loss: 0.004697853233665228\n",
      "Epoch 2 Batch: 194 Train Loss: 0.005103062838315964\n",
      "Epoch 2 Batch: 195 Train Loss: 0.004850567784160376\n",
      "Epoch 2 Batch: 196 Train Loss: 0.003794037504121661\n",
      "Epoch 2 Batch: 197 Train Loss: 0.004179010167717934\n",
      "Epoch 2 Batch: 198 Train Loss: 0.004129091743379831\n",
      "Epoch 2 Batch: 199 Train Loss: 0.5344358682632446\n",
      "Epoch 2 Batch: 200 Train Loss: 0.004266089294105768\n",
      "Epoch 2 Batch: 201 Train Loss: 0.004025482106953859\n",
      "Epoch 2 Batch: 202 Train Loss: 0.004654631018638611\n",
      "Epoch 2 Batch: 203 Train Loss: 0.5172287821769714\n",
      "Epoch 2 Batch: 204 Train Loss: 0.5078679919242859\n",
      "Epoch 2 Batch: 205 Train Loss: 0.0051041701808571815\n",
      "Epoch 2 Batch: 206 Train Loss: 0.006514911539852619\n",
      "Epoch 2 Batch: 207 Train Loss: 0.006431105546653271\n",
      "Epoch 2 Batch: 208 Train Loss: 0.006155567243695259\n",
      "Epoch 2 Batch: 209 Train Loss: 0.010350825265049934\n",
      "Epoch 2 Batch: 210 Train Loss: 0.5178848505020142\n",
      "Epoch 2 Batch: 211 Train Loss: 0.42978644371032715\n",
      "Epoch 2 Batch: 212 Train Loss: 0.008660340681672096\n",
      "Epoch 2 Batch: 213 Train Loss: 0.011448035016655922\n",
      "Epoch 2 Batch: 214 Train Loss: 0.013104429468512535\n",
      "Epoch 2 Batch: 215 Train Loss: 0.019834792241454124\n",
      "Epoch 2 Batch: 216 Train Loss: 0.018595965579152107\n",
      "Epoch 2 Batch: 217 Train Loss: 0.021862979978322983\n",
      "Epoch 2 Batch: 218 Train Loss: 0.024584252387285233\n",
      "Epoch 2 Batch: 219 Train Loss: 0.024505948647856712\n",
      "Epoch 2 Batch: 220 Train Loss: 0.011467529460787773\n",
      "Epoch 2 Batch: 221 Train Loss: 0.38352471590042114\n",
      "Epoch 2 Batch: 222 Train Loss: 0.014245152473449707\n",
      "Epoch 2 Batch: 223 Train Loss: 0.015411129221320152\n",
      "Epoch 2 Batch: 224 Train Loss: 0.014777541160583496\n",
      "Epoch 2 Batch: 225 Train Loss: 0.4731954038143158\n",
      "Epoch 2 Batch: 226 Train Loss: 0.01460902951657772\n",
      "Epoch 2 Batch: 227 Train Loss: 0.020535919815301895\n",
      "Epoch 2 Batch: 228 Train Loss: 0.01649409532546997\n",
      "Epoch 2 Batch: 229 Train Loss: 0.01920122280716896\n",
      "Epoch 2 Batch: 230 Train Loss: 0.0113118477165699\n",
      "Epoch 2 Batch: 231 Train Loss: 0.020867180079221725\n",
      "Epoch 2 Batch: 232 Train Loss: 0.01568114385008812\n",
      "Epoch 2 Batch: 233 Train Loss: 0.013929435983300209\n",
      "Epoch 2 Batch: 234 Train Loss: 0.015681417658925056\n",
      "Epoch 2 Batch: 235 Train Loss: 0.013130547478795052\n",
      "Epoch 2 Batch: 236 Train Loss: 0.012516575865447521\n",
      "Epoch 2 Batch: 237 Train Loss: 0.012020282447338104\n",
      "Epoch 2 Batch: 238 Train Loss: 0.013323036022484303\n",
      "Epoch 2 Batch: 239 Train Loss: 0.01028065849095583\n",
      "Epoch 2 Batch: 240 Train Loss: 0.012831360101699829\n",
      "Epoch 2 Batch: 241 Train Loss: 0.4832523465156555\n",
      "Epoch 2 Batch: 242 Train Loss: 0.01067583542317152\n",
      "Epoch 2 Batch: 243 Train Loss: 0.012662464752793312\n",
      "Epoch 2 Batch: 244 Train Loss: 0.012978225946426392\n",
      "Epoch 2 Batch: 245 Train Loss: 0.011751734651625156\n",
      "Epoch 2 Batch: 246 Train Loss: 0.011499050073325634\n",
      "Epoch 2 Batch: 247 Train Loss: 0.44267138838768005\n",
      "Epoch 2 Batch: 248 Train Loss: 0.01054484210908413\n",
      "Epoch 2 Batch: 249 Train Loss: 0.01209894847124815\n",
      "Epoch 2 Batch: 250 Train Loss: 0.011005994863808155\n",
      "Epoch 2 Batch: 251 Train Loss: 0.01184250507503748\n",
      "Epoch 2 Batch: 252 Train Loss: 0.011627479456365108\n",
      "Epoch 2 Batch: 253 Train Loss: 0.01129251066595316\n",
      "Epoch 2 Batch: 254 Train Loss: 0.008445937186479568\n",
      "Epoch 2 Batch: 255 Train Loss: 0.010559454560279846\n",
      "Epoch 2 Batch: 256 Train Loss: 0.009795772843062878\n",
      "Epoch 2 Batch: 257 Train Loss: 0.00956263579428196\n",
      "Epoch 2 Batch: 258 Train Loss: 0.010032583959400654\n",
      "Epoch 2 Batch: 259 Train Loss: 0.010238382034003735\n",
      "Epoch 2 Batch: 260 Train Loss: 0.48742976784706116\n",
      "Epoch 2 Batch: 261 Train Loss: 0.009207591414451599\n",
      "Epoch 2 Batch: 262 Train Loss: 0.009074445813894272\n",
      "Epoch 2 Batch: 263 Train Loss: 0.010048828087747097\n",
      "Epoch 2 Batch: 264 Train Loss: 0.9232034683227539\n",
      "Epoch 2 Batch: 265 Train Loss: 0.011431972496211529\n",
      "Epoch 2 Batch: 266 Train Loss: 0.009715434163808823\n",
      "Epoch 2 Batch: 267 Train Loss: 0.01166963204741478\n",
      "Epoch 2 Batch: 268 Train Loss: 0.014377472922205925\n",
      "Epoch 2 Batch: 269 Train Loss: 0.44179290533065796\n",
      "Epoch 2 Batch: 270 Train Loss: 0.01609038934111595\n",
      "Epoch 2 Batch: 271 Train Loss: 0.017418144270777702\n",
      "Epoch 2 Batch: 272 Train Loss: 0.018131792545318604\n",
      "Epoch 2 Batch: 273 Train Loss: 0.013213180005550385\n",
      "Epoch 2 Batch: 274 Train Loss: 0.019390003755688667\n",
      "Epoch 2 Batch: 275 Train Loss: 0.015655774623155594\n",
      "Epoch 2 Batch: 276 Train Loss: 0.0159151591360569\n",
      "Epoch 2 Batch: 277 Train Loss: 0.015915822237730026\n",
      "Epoch 2 Batch: 278 Train Loss: 0.018476339057087898\n",
      "Epoch 2 Batch: 279 Train Loss: 0.01761850342154503\n",
      "Epoch 2 Batch: 280 Train Loss: 0.017533916980028152\n",
      "Epoch 2 Batch: 281 Train Loss: 0.012842396274209023\n",
      "Epoch 2 Batch: 282 Train Loss: 0.016057627275586128\n",
      "Epoch 2 Batch: 283 Train Loss: 0.015317117795348167\n",
      "Epoch 2 Batch: 284 Train Loss: 0.011877456679940224\n",
      "Epoch 2 Batch: 285 Train Loss: 0.013814610429108143\n",
      "Epoch 2 Batch: 286 Train Loss: 0.01346536260098219\n",
      "Epoch 2 Batch: 287 Train Loss: 0.013136036694049835\n",
      "Epoch 2 Batch: 288 Train Loss: 0.013448553159832954\n",
      "Epoch 2 Batch: 289 Train Loss: 0.018630702048540115\n",
      "Epoch 2 Batch: 290 Train Loss: 0.008946755900979042\n",
      "Epoch 2 Batch: 291 Train Loss: 0.010051550343632698\n",
      "Epoch 2 Batch: 292 Train Loss: 0.00995459035038948\n",
      "Epoch 2 Batch: 293 Train Loss: 0.013061043806374073\n",
      "Epoch 2 Batch: 294 Train Loss: 0.45728224515914917\n",
      "Epoch 2 Batch: 295 Train Loss: 0.008458335883915424\n",
      "Epoch 2 Batch: 296 Train Loss: 0.010363169014453888\n",
      "Epoch 2 Batch: 297 Train Loss: 0.00706430384889245\n",
      "Epoch 2 Batch: 298 Train Loss: 0.023974206298589706\n",
      "Epoch 2 Batch: 299 Train Loss: 0.00985648948699236\n",
      "Epoch 2 Batch: 300 Train Loss: 0.44830775260925293\n",
      "Epoch 2 Batch: 301 Train Loss: 0.011285621672868729\n",
      "Epoch 2 Batch: 302 Train Loss: 0.01182600762695074\n",
      "Epoch 2 Batch: 303 Train Loss: 0.011865356005728245\n",
      "Epoch 2 Batch: 304 Train Loss: 0.012519843876361847\n",
      "Epoch 2 Batch: 305 Train Loss: 0.012708243913948536\n",
      "Epoch 2 Batch: 306 Train Loss: 0.010242357850074768\n",
      "Epoch 2 Batch: 307 Train Loss: 0.08474544435739517\n",
      "Epoch 2 Batch: 308 Train Loss: 0.009552625939249992\n",
      "Epoch 2 Batch: 309 Train Loss: 0.01434636116027832\n",
      "Epoch 2 Batch: 310 Train Loss: 0.013279279693961143\n",
      "Epoch 2 Batch: 311 Train Loss: 0.01610807701945305\n",
      "Epoch 2 Batch: 312 Train Loss: 0.01321574579924345\n",
      "Epoch 2 Batch: 313 Train Loss: 0.016373248770833015\n",
      "Epoch 2 Batch: 314 Train Loss: 0.013549720868468285\n",
      "Epoch 2 Batch: 315 Train Loss: 0.0140885841101408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch: 316 Train Loss: 0.01557684876024723\n",
      "Epoch 2 Batch: 317 Train Loss: 0.015046986751258373\n",
      "Epoch 2 Batch: 318 Train Loss: 0.013312840834259987\n",
      "Epoch 2 Batch: 319 Train Loss: 0.4646011292934418\n",
      "Epoch 2 Batch: 320 Train Loss: 0.012340033426880836\n",
      "Epoch 2 Batch: 321 Train Loss: 0.012609121389687061\n",
      "Epoch 2 Batch: 322 Train Loss: 0.011470114812254906\n",
      "Epoch 2 Batch: 323 Train Loss: 0.013139801099896431\n",
      "Epoch 2 Batch: 324 Train Loss: 0.012891612946987152\n",
      "Epoch 2 Batch: 325 Train Loss: 0.011026700958609581\n",
      "Epoch 2 Batch: 326 Train Loss: 0.01191749144345522\n",
      "Epoch 2 Batch: 327 Train Loss: 0.012949420139193535\n",
      "Epoch 2 Batch: 328 Train Loss: 0.011514350771903992\n",
      "Epoch 2 Batch: 329 Train Loss: 0.010585291311144829\n",
      "Epoch 2 Batch: 330 Train Loss: 0.4613000750541687\n",
      "Epoch 2 Batch: 331 Train Loss: 0.00999913178384304\n",
      "Epoch 2 Batch: 332 Train Loss: 0.011300044134259224\n",
      "Epoch 2 Batch: 333 Train Loss: 0.011298896744847298\n",
      "Epoch 2 Batch: 334 Train Loss: 0.011112253181636333\n",
      "Epoch 2 Batch: 335 Train Loss: 0.011215722188353539\n",
      "Epoch 2 Batch: 336 Train Loss: 0.010778933763504028\n",
      "Epoch 2 Batch: 337 Train Loss: 0.01015943568199873\n",
      "Epoch 2 Batch: 338 Train Loss: 0.008455001749098301\n",
      "Epoch 2 Batch: 339 Train Loss: 0.008924273774027824\n",
      "Epoch 2 Batch: 340 Train Loss: 0.00903130229562521\n",
      "Epoch 2 Batch: 341 Train Loss: 0.007819881662726402\n",
      "Epoch 2 Batch: 342 Train Loss: 0.009596087969839573\n",
      "Epoch 2 Batch: 343 Train Loss: 0.47099965810775757\n",
      "Epoch 2 Batch: 344 Train Loss: 0.009115112945437431\n",
      "Epoch 2 Batch: 345 Train Loss: 0.00785702746361494\n",
      "Epoch 2 Batch: 346 Train Loss: 0.008840912953019142\n",
      "Epoch 2 Batch: 347 Train Loss: 0.009125442244112492\n",
      "Epoch 2 Batch: 348 Train Loss: 0.009746125899255276\n",
      "Epoch 2 Batch: 349 Train Loss: 0.008327395655214787\n",
      "Epoch 2 Batch: 350 Train Loss: 0.008016038686037064\n",
      "Epoch 2 Batch: 351 Train Loss: 0.00953522976487875\n",
      "Epoch 2 Batch: 352 Train Loss: 0.46198731660842896\n",
      "Epoch 2 Batch: 353 Train Loss: 0.4537394940853119\n",
      "Epoch 2 Batch: 354 Train Loss: 0.009899834170937538\n",
      "Epoch 2 Batch: 355 Train Loss: 0.011307385750114918\n",
      "Epoch 2 Batch: 356 Train Loss: 0.011796792037785053\n",
      "Epoch 2 Batch: 357 Train Loss: 0.012483106926083565\n",
      "Epoch 2 Batch: 358 Train Loss: 0.01213718019425869\n",
      "Epoch 2 Batch: 359 Train Loss: 0.014518747106194496\n",
      "Epoch 2 Batch: 360 Train Loss: 0.012587325647473335\n",
      "Epoch 2 Batch: 361 Train Loss: 0.013761721551418304\n",
      "Epoch 2 Batch: 362 Train Loss: 0.015374106355011463\n",
      "Epoch 2 Batch: 363 Train Loss: 0.016813216730952263\n",
      "Epoch 2 Batch: 364 Train Loss: 0.012385905720293522\n",
      "Epoch 2 Batch: 365 Train Loss: 0.42057013511657715\n",
      "Epoch 2 Batch: 366 Train Loss: 0.012222827412188053\n",
      "Epoch 2 Batch: 367 Train Loss: 0.011703869327902794\n",
      "Epoch 2 Batch: 368 Train Loss: 0.014136908575892448\n",
      "Epoch 2 Batch: 369 Train Loss: 0.015089323744177818\n",
      "Epoch 2 Batch: 370 Train Loss: 0.015410011634230614\n",
      "Epoch 2 Batch: 371 Train Loss: 0.009392288513481617\n",
      "Epoch 2 Batch: 372 Train Loss: 0.010337477549910545\n",
      "Epoch 2 Batch: 373 Train Loss: 0.014626244083046913\n",
      "Epoch 2 Batch: 374 Train Loss: 0.011950897984206676\n",
      "Epoch 2 Batch: 375 Train Loss: 0.5087003111839294\n",
      "Epoch 2 Batch: 376 Train Loss: 0.009465599432587624\n",
      "Epoch 2 Batch: 377 Train Loss: 0.011368902400135994\n",
      "Epoch 2 Batch: 378 Train Loss: 0.010599778965115547\n",
      "Epoch 2 Batch: 379 Train Loss: 0.014002439565956593\n",
      "Epoch 2 Batch: 380 Train Loss: 0.012820635922253132\n",
      "Epoch 2 Batch: 381 Train Loss: 0.013313250616192818\n",
      "Epoch 2 Batch: 382 Train Loss: 0.011362580582499504\n",
      "Epoch 2 Batch: 383 Train Loss: 0.007413336541503668\n",
      "Epoch 2 Batch: 384 Train Loss: 0.009199797175824642\n",
      "Epoch 2 Batch: 385 Train Loss: 0.0095854876562953\n",
      "Epoch 2 Batch: 386 Train Loss: 0.009283897466957569\n",
      "Epoch 2 Batch: 387 Train Loss: 0.009038754738867283\n",
      "Epoch 2 Batch: 388 Train Loss: 0.009642962366342545\n",
      "Epoch 2 Batch: 389 Train Loss: 0.004276424180716276\n",
      "Epoch 2 Batch: 390 Train Loss: 0.006851057521998882\n",
      "Epoch 2 Batch: 391 Train Loss: 0.009445288218557835\n",
      "Epoch 2 Batch: 392 Train Loss: 0.4691641330718994\n",
      "Epoch 2 Batch: 393 Train Loss: 0.46616706252098083\n",
      "Epoch 2 Batch: 394 Train Loss: 0.008067791350185871\n",
      "Epoch 2 Batch: 395 Train Loss: 0.008045737631618977\n",
      "Epoch 2 Batch: 396 Train Loss: 0.009070769883692265\n",
      "Epoch 2 Batch: 397 Train Loss: 0.009514901787042618\n",
      "Epoch 2 Batch: 398 Train Loss: 0.007830058224499226\n",
      "Epoch 2 Batch: 399 Train Loss: 0.010821832343935966\n",
      "Epoch 2 Batch: 400 Train Loss: 0.010247396305203438\n",
      "Epoch 2 Batch: 401 Train Loss: 0.011164294555783272\n",
      "Epoch 2 Batch: 402 Train Loss: 0.009801378473639488\n",
      "Epoch 2 Batch: 403 Train Loss: 0.011184625327587128\n",
      "Epoch 2 Batch: 404 Train Loss: 0.011460615321993828\n",
      "Epoch 2 Batch: 405 Train Loss: 0.01344333030283451\n",
      "Epoch 2 Batch: 406 Train Loss: 0.011516815051436424\n",
      "Epoch 2 Batch: 407 Train Loss: 0.012024464085698128\n",
      "Epoch 2 Batch: 408 Train Loss: 0.007931499741971493\n",
      "Epoch 2 Batch: 409 Train Loss: 0.5169167518615723\n",
      "Epoch 2 Batch: 410 Train Loss: 0.009246744215488434\n",
      "Epoch 2 Batch: 411 Train Loss: 0.00882248766720295\n",
      "Epoch 2 Batch: 412 Train Loss: 0.00878182053565979\n",
      "Epoch 2 Batch: 413 Train Loss: 0.009507688693702221\n",
      "Epoch 2 Batch: 414 Train Loss: 0.008191598579287529\n",
      "Epoch 2 Batch: 415 Train Loss: 0.00800606794655323\n",
      "Epoch 2 Batch: 416 Train Loss: 0.010537238791584969\n",
      "Epoch 2 Batch: 417 Train Loss: 0.009078304283320904\n",
      "Epoch 2 Batch: 418 Train Loss: 0.4493665099143982\n",
      "Epoch 2 Batch: 419 Train Loss: 0.009223481640219688\n",
      "Epoch 2 Batch: 420 Train Loss: 0.010200484655797482\n",
      "Epoch 2 Batch: 421 Train Loss: 0.008435225114226341\n",
      "Epoch 2 Batch: 422 Train Loss: 0.01115640439093113\n",
      "Epoch 2 Batch: 423 Train Loss: 0.009123357944190502\n",
      "Epoch 2 Batch: 424 Train Loss: 0.010468977503478527\n",
      "Epoch 2 Batch: 425 Train Loss: 0.009821966290473938\n",
      "Epoch 2 Batch: 426 Train Loss: 0.012389438226819038\n",
      "Epoch 2 Batch: 427 Train Loss: 0.010881668888032436\n",
      "Epoch 2 Batch: 428 Train Loss: 0.5040425658226013\n",
      "Epoch 2 Batch: 429 Train Loss: 0.009831053204834461\n",
      "Epoch 2 Batch: 430 Train Loss: 0.011307032778859138\n",
      "Epoch 2 Batch: 431 Train Loss: 0.010088928043842316\n",
      "Epoch 2 Batch: 432 Train Loss: 0.010422646068036556\n",
      "Epoch 2 Batch: 433 Train Loss: 0.009934647008776665\n",
      "Epoch 2 Batch: 434 Train Loss: 0.007394914515316486\n",
      "Epoch 2 Batch: 435 Train Loss: 0.44218820333480835\n",
      "Epoch 2 Batch: 436 Train Loss: 0.010548138990998268\n",
      "Epoch 2 Batch: 437 Train Loss: 0.4339469075202942\n",
      "Epoch 2 Batch: 438 Train Loss: 0.011252637952566147\n",
      "Epoch 2 Batch: 439 Train Loss: 0.013793839141726494\n",
      "Epoch 2 Batch: 440 Train Loss: 0.010054746642708778\n",
      "Epoch 2 Batch: 441 Train Loss: 0.009477642364799976\n",
      "Epoch 2 Batch: 442 Train Loss: 0.016706451773643494\n",
      "Epoch 2 Batch: 443 Train Loss: 0.018262183293700218\n",
      "Epoch 2 Batch: 444 Train Loss: 0.012041610665619373\n",
      "Epoch 2 Batch: 445 Train Loss: 0.01742372289299965\n",
      "Epoch 2 Batch: 446 Train Loss: 0.016175422817468643\n",
      "Epoch 2 Batch: 447 Train Loss: 0.013742903247475624\n",
      "Epoch 2 Batch: 448 Train Loss: 0.013389483094215393\n",
      "Epoch 2 Batch: 449 Train Loss: 0.010000312700867653\n",
      "Epoch 2 Batch: 450 Train Loss: 0.015468415804207325\n",
      "Epoch 2 Batch: 451 Train Loss: 0.4762810170650482\n",
      "Epoch 2 Batch: 452 Train Loss: 0.015014772303402424\n",
      "Epoch 2 Batch: 453 Train Loss: 0.014138678088784218\n",
      "Epoch 2 Batch: 454 Train Loss: 0.012241911143064499\n",
      "Epoch 2 Batch: 455 Train Loss: 0.41255998611450195\n",
      "Epoch 2 Batch: 456 Train Loss: 0.013661189004778862\n",
      "Epoch 2 Batch: 457 Train Loss: 0.012996220961213112\n",
      "Epoch 2 Batch: 458 Train Loss: 0.013058723881840706\n",
      "Epoch 2 Batch: 459 Train Loss: 0.01430318783968687\n",
      "Epoch 2 Batch: 460 Train Loss: 0.015398643910884857\n",
      "Epoch 2 Batch: 461 Train Loss: 0.01247201208025217\n",
      "Epoch 2 Batch: 462 Train Loss: 0.00900741945952177\n",
      "Epoch 2 Batch: 463 Train Loss: 0.013006975874304771\n",
      "Epoch 2 Batch: 464 Train Loss: 0.014646196737885475\n",
      "Epoch 2 Batch: 465 Train Loss: 0.013261191546916962\n",
      "Epoch 2 Batch: 466 Train Loss: 0.01310547161847353\n",
      "Epoch 2 Batch: 467 Train Loss: 0.013186277821660042\n",
      "Epoch 2 Batch: 468 Train Loss: 0.01574583537876606\n",
      "Epoch 2 Batch: 469 Train Loss: 0.009928026236593723\n",
      "Epoch 2 Batch: 470 Train Loss: 0.013095025904476643\n",
      "Epoch 2 Batch: 471 Train Loss: 0.0064865415915846825\n",
      "Epoch 2 Batch: 472 Train Loss: 0.010779089294373989\n",
      "Epoch 2 Batch: 473 Train Loss: 0.009506948292255402\n",
      "Epoch 2 Batch: 474 Train Loss: 0.008810427039861679\n",
      "Epoch 2 Batch: 475 Train Loss: 0.007911395281553268\n",
      "Epoch 2 Batch: 476 Train Loss: 0.008439174853265285\n",
      "Epoch 2 Batch: 477 Train Loss: 0.008202934637665749\n",
      "Epoch 2 Batch: 478 Train Loss: 0.00779903307557106\n",
      "Epoch 2 Batch: 479 Train Loss: 0.006352263502776623\n",
      "Epoch 2 Batch: 480 Train Loss: 0.0067372978664934635\n",
      "Epoch 2 Batch: 481 Train Loss: 0.007571608759462833\n",
      "Epoch 2 Batch: 482 Train Loss: 0.006916806101799011\n",
      "Epoch 2 Batch: 483 Train Loss: 0.005788292270153761\n",
      "Epoch 2 Batch: 484 Train Loss: 0.006549638696014881\n",
      "Epoch 2 Batch: 485 Train Loss: 0.005074006039649248\n",
      "Epoch 2 Batch: 486 Train Loss: 0.005521836690604687\n",
      "Epoch 2 Batch: 487 Train Loss: 0.005077166482806206\n",
      "Epoch 2 Batch: 488 Train Loss: 0.004871496465057135\n",
      "Epoch 2 Batch: 489 Train Loss: 0.005055100657045841\n",
      "Epoch 2 Batch: 490 Train Loss: 0.004497201181948185\n",
      "Epoch 2 Batch: 491 Train Loss: 0.0037941276095807552\n",
      "Epoch 2 Batch: 492 Train Loss: 0.5312691330909729\n",
      "Epoch 2 Batch: 493 Train Loss: 0.005021777935326099\n",
      "Epoch 2 Batch: 494 Train Loss: 0.005942427087575197\n",
      "Epoch 2 Batch: 495 Train Loss: 0.5653766393661499\n",
      "Epoch 2 Batch: 496 Train Loss: 0.004416469018906355\n",
      "Epoch 2 Batch: 497 Train Loss: 0.006028448697179556\n",
      "Epoch 2 Batch: 498 Train Loss: 0.016228336840867996\n",
      "Epoch 2 Batch: 499 Train Loss: 0.005270793568342924\n",
      "Epoch 2 Batch: 500 Train Loss: 0.005783969536423683\n",
      "Epoch 2 Batch: 501 Train Loss: 0.004464088473469019\n",
      "Epoch 2 Batch: 502 Train Loss: 0.007292238529771566\n",
      "Epoch 2 Batch: 503 Train Loss: 0.006651925388723612\n",
      "Epoch 2 Batch: 504 Train Loss: 0.00616700341925025\n",
      "Epoch 2 Batch: 505 Train Loss: 0.007201951928436756\n",
      "Epoch 2 Batch: 506 Train Loss: 0.007523075677454472\n",
      "Epoch 2 Batch: 507 Train Loss: 0.007291163317859173\n",
      "Epoch 2 Batch: 508 Train Loss: 0.006180626340210438\n",
      "Epoch 2 Batch: 509 Train Loss: 0.007245778106153011\n",
      "Epoch 2 Batch: 510 Train Loss: 0.007420176174491644\n",
      "Epoch 2 Batch: 511 Train Loss: 0.005574118811637163\n",
      "Epoch 2 Batch: 512 Train Loss: 0.0061787492595613\n",
      "Epoch 2 Batch: 513 Train Loss: 0.006873405072838068\n",
      "Epoch 2 Batch: 514 Train Loss: 0.49141058325767517\n",
      "Epoch 2 Batch: 515 Train Loss: 0.006451484747231007\n",
      "Epoch 2 Batch: 516 Train Loss: 0.006229977123439312\n",
      "Epoch 2 Batch: 517 Train Loss: 0.007175459526479244\n",
      "Epoch 2 Batch: 518 Train Loss: 0.006433551665395498\n",
      "Epoch 2 Batch: 519 Train Loss: 0.006743519101291895\n",
      "Epoch 2 Batch: 520 Train Loss: 0.007813616655766964\n",
      "Epoch 2 Batch: 521 Train Loss: 0.00711738970130682\n",
      "Epoch 2 Batch: 522 Train Loss: 0.007461221423000097\n",
      "Epoch 2 Batch: 523 Train Loss: 0.008171385154128075\n",
      "Epoch 2 Batch: 524 Train Loss: 0.007135777734220028\n",
      "Epoch 2 Batch: 525 Train Loss: 0.006477590650320053\n",
      "Epoch 2 Batch: 526 Train Loss: 0.007109801284968853\n",
      "Epoch 2 Batch: 527 Train Loss: 0.006436921656131744\n",
      "Epoch 2 Batch: 528 Train Loss: 0.005390041507780552\n",
      "Epoch 2 Batch: 529 Train Loss: 0.006311082746833563\n",
      "Epoch 2 Batch: 530 Train Loss: 0.00639538187533617\n",
      "Epoch 2 Batch: 531 Train Loss: 0.007435097359120846\n",
      "Epoch 2 Batch: 532 Train Loss: 0.0064085619524121284\n",
      "Epoch 2 Batch: 533 Train Loss: 0.0062273647636175156\n",
      "Epoch 2 Batch: 534 Train Loss: 0.0060420893132686615\n",
      "Epoch 2 Batch: 535 Train Loss: 0.005347096361219883\n",
      "Epoch 2 Batch: 536 Train Loss: 0.004976523574441671\n",
      "Epoch 2 Batch: 537 Train Loss: 0.005323971621692181\n",
      "Epoch 2 Batch: 538 Train Loss: 0.00504219951108098\n",
      "Epoch 2 Batch: 539 Train Loss: 0.5062424540519714\n",
      "Epoch 2 Batch: 540 Train Loss: 0.006019272841513157\n",
      "Epoch 2 Batch: 541 Train Loss: 0.0046630967408418655\n",
      "Epoch 2 Batch: 542 Train Loss: 0.0050155287608504295\n",
      "Epoch 2 Batch: 543 Train Loss: 0.0050476184114813805\n",
      "Epoch 2 Batch: 544 Train Loss: 0.004396095406264067\n",
      "Epoch 2 Batch: 545 Train Loss: 0.006372705101966858\n",
      "Epoch 2 Batch: 546 Train Loss: 0.005606723017990589\n",
      "Epoch 2 Batch: 547 Train Loss: 0.0036278418265283108\n",
      "Epoch 2 Batch: 548 Train Loss: 0.0036517945118248463\n",
      "Epoch 2 Batch: 549 Train Loss: 0.005994129925966263\n",
      "Epoch 2 Batch: 550 Train Loss: 0.0043390048667788506\n",
      "Epoch 2 Batch: 551 Train Loss: 0.005055562127381563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch: 552 Train Loss: 0.005397534463554621\n",
      "Epoch 2 Batch: 553 Train Loss: 0.006128713954240084\n",
      "Epoch 2 Batch: 554 Train Loss: 0.004431693349033594\n",
      "Epoch 2 Batch: 555 Train Loss: 0.005926233250647783\n",
      "Epoch 2 Batch: 556 Train Loss: 0.0050352634862065315\n",
      "Epoch 2 Batch: 557 Train Loss: 0.0037870858795940876\n",
      "Epoch 2 Batch: 558 Train Loss: 0.0041586896404623985\n",
      "Epoch 2 Batch: 559 Train Loss: 0.005126572214066982\n",
      "Epoch 2 Batch: 560 Train Loss: 0.00464978814125061\n",
      "Epoch 2 Batch: 561 Train Loss: 0.0041956594213843346\n",
      "Epoch 2 Batch: 562 Train Loss: 0.004089906811714172\n",
      "Epoch 2 Batch: 563 Train Loss: 0.004320940468460321\n",
      "Epoch 2 Batch: 564 Train Loss: 0.004875755403190851\n",
      "Epoch 2 Batch: 565 Train Loss: 0.004166923463344574\n",
      "Epoch 2 Batch: 566 Train Loss: 0.003416307270526886\n",
      "Epoch 2 Batch: 567 Train Loss: 0.00416071992367506\n",
      "Epoch 2 Batch: 568 Train Loss: 0.004338868893682957\n",
      "Epoch 2 Batch: 569 Train Loss: 0.004244337789714336\n",
      "Epoch 2 Batch: 570 Train Loss: 0.004109574481844902\n",
      "Epoch 2 Batch: 571 Train Loss: 0.604814350605011\n",
      "Epoch 2 Batch: 572 Train Loss: 0.003099238034337759\n",
      "Epoch 2 Batch: 573 Train Loss: 0.0032010474242269993\n",
      "Epoch 2 Batch: 574 Train Loss: 0.003331441432237625\n",
      "Epoch 2 Batch: 575 Train Loss: 0.586249828338623\n",
      "Epoch 2 Batch: 576 Train Loss: 0.003988218493759632\n",
      "Epoch 2 Batch: 577 Train Loss: 0.5178002119064331\n",
      "Epoch 2 Batch: 578 Train Loss: 0.04832196980714798\n",
      "Epoch 2 Batch: 579 Train Loss: 0.007118051405996084\n",
      "Epoch 2 Batch: 580 Train Loss: 0.47426968812942505\n",
      "Epoch 2 Batch: 581 Train Loss: 0.008627685718238354\n",
      "Epoch 2 Batch: 582 Train Loss: 0.009722160175442696\n",
      "Epoch 2 Batch: 583 Train Loss: 0.01197478361427784\n",
      "Epoch 2 Batch: 584 Train Loss: 0.013311857357621193\n",
      "Epoch 2 Batch: 585 Train Loss: 0.38538461923599243\n",
      "Epoch 2 Batch: 586 Train Loss: 0.023886725306510925\n",
      "Epoch 2 Batch: 587 Train Loss: 0.07856514304876328\n",
      "Epoch 2 Batch: 588 Train Loss: 0.06057148426771164\n",
      "Epoch 2 Batch: 589 Train Loss: 0.04115544259548187\n",
      "Epoch 2 Batch: 590 Train Loss: 0.027727996930480003\n",
      "Epoch 2 Batch: 591 Train Loss: 0.020095694810152054\n",
      "Epoch 2 Batch: 592 Train Loss: 0.01143929548561573\n",
      "Epoch 2 Batch: 593 Train Loss: 0.4183625280857086\n",
      "Epoch 2 Batch: 594 Train Loss: 0.01295317243784666\n",
      "Epoch 2 Batch: 595 Train Loss: 0.013498609885573387\n",
      "Epoch 2 Batch: 596 Train Loss: 0.009881418198347092\n",
      "Epoch 2 Batch: 597 Train Loss: 0.008403809741139412\n",
      "Epoch 2 Batch: 598 Train Loss: 0.009668765589594841\n",
      "Epoch 2 Batch: 599 Train Loss: 0.008421273902058601\n",
      "Epoch 2 Batch: 600 Train Loss: 0.00843053124845028\n",
      "Epoch 2 Batch: 601 Train Loss: 0.00808231346309185\n",
      "Epoch 2 Batch: 602 Train Loss: 0.009050268679857254\n",
      "Epoch 2 Batch: 603 Train Loss: 0.008216110058128834\n",
      "Epoch 2 Batch: 604 Train Loss: 0.006840738467872143\n",
      "Epoch 2 Batch: 605 Train Loss: 0.006428939290344715\n",
      "Epoch 2 Batch: 606 Train Loss: 0.5131394863128662\n",
      "Epoch 2 Batch: 607 Train Loss: 0.006987735629081726\n",
      "Epoch 2 Batch: 608 Train Loss: 0.00667190458625555\n",
      "Epoch 2 Batch: 609 Train Loss: 0.007637140341103077\n",
      "Epoch 2 Batch: 610 Train Loss: 0.00760660832747817\n",
      "Epoch 2 Batch: 611 Train Loss: 0.007352435030043125\n",
      "Epoch 2 Batch: 612 Train Loss: 0.11896742880344391\n",
      "Epoch 2 Batch: 613 Train Loss: 0.018168633803725243\n",
      "Epoch 2 Batch: 614 Train Loss: 0.007575242780148983\n",
      "Epoch 2 Batch: 615 Train Loss: 0.008210645988583565\n",
      "Epoch 2 Batch: 616 Train Loss: 0.007236828096210957\n",
      "Epoch 2 Batch: 617 Train Loss: 0.007770574651658535\n",
      "Epoch 2 Batch: 618 Train Loss: 0.010531865991652012\n",
      "Epoch 2 Batch: 619 Train Loss: 0.4511559009552002\n",
      "Epoch 2 Batch: 620 Train Loss: 0.012195063754916191\n",
      "Epoch 2 Batch: 621 Train Loss: 0.010419615544378757\n",
      "Epoch 2 Batch: 622 Train Loss: 0.01127602718770504\n",
      "Epoch 2 Batch: 623 Train Loss: 0.010163294151425362\n",
      "Epoch 2 Batch: 624 Train Loss: 0.014618190005421638\n",
      "Epoch 2 Batch: 625 Train Loss: 0.01501835323870182\n",
      "Epoch 2 Batch: 626 Train Loss: 0.01244314294308424\n",
      "Epoch 2 Batch: 627 Train Loss: 0.014370748773217201\n",
      "Epoch 2 Batch: 628 Train Loss: 0.013499690219759941\n",
      "Epoch 2 Batch: 629 Train Loss: 0.012583242729306221\n",
      "Epoch 2 Batch: 630 Train Loss: 0.014175569638609886\n",
      "Epoch 2 Batch: 631 Train Loss: 0.4786669611930847\n",
      "Epoch 2 Batch: 632 Train Loss: 0.01329796202480793\n",
      "Epoch 2 Batch: 633 Train Loss: 0.47630739212036133\n",
      "Epoch 2 Batch: 634 Train Loss: 0.016902172937989235\n",
      "Epoch 2 Batch: 635 Train Loss: 0.01568850316107273\n",
      "Epoch 2 Batch: 636 Train Loss: 0.01690288446843624\n",
      "Epoch 2 Batch: 637 Train Loss: 0.01707823947072029\n",
      "Epoch 2 Batch: 638 Train Loss: 0.013090352527797222\n",
      "Epoch 2 Batch: 639 Train Loss: 0.016309117898344994\n",
      "Epoch 2 Batch: 640 Train Loss: 0.44614213705062866\n",
      "Epoch 2 Batch: 641 Train Loss: 0.01663323864340782\n",
      "Epoch 2 Batch: 642 Train Loss: 0.01462913304567337\n",
      "Epoch 2 Batch: 643 Train Loss: 0.014837603084743023\n",
      "Epoch 2 Batch: 644 Train Loss: 0.018192825838923454\n",
      "Epoch 2 Batch: 645 Train Loss: 0.01658487506210804\n",
      "Epoch 2 Batch: 646 Train Loss: 0.013658585026860237\n",
      "Epoch 2 Batch: 647 Train Loss: 0.016847847029566765\n",
      "Epoch 2 Batch: 648 Train Loss: 0.42869535088539124\n",
      "Epoch 2 Batch: 649 Train Loss: 0.01651952974498272\n",
      "Epoch 2 Batch: 650 Train Loss: 0.4261323809623718\n",
      "Epoch 2 Batch: 651 Train Loss: 0.015849029645323753\n",
      "Epoch 2 Batch: 652 Train Loss: 0.016490917652845383\n",
      "Epoch 2 Batch: 653 Train Loss: 0.016942506656050682\n",
      "Epoch 2 Batch: 654 Train Loss: 0.015233431942760944\n",
      "Epoch 2 Batch: 655 Train Loss: 0.017369911074638367\n",
      "Epoch 2 Batch: 656 Train Loss: 0.017330443486571312\n",
      "Epoch 2 Batch: 657 Train Loss: 0.016533294692635536\n",
      "Epoch 2 Batch: 658 Train Loss: 0.016883518546819687\n",
      "Epoch 2 Batch: 659 Train Loss: 0.016503866761922836\n",
      "Epoch 2 Batch: 660 Train Loss: 0.013925011269748211\n",
      "Epoch 2 Batch: 661 Train Loss: 0.014983324334025383\n",
      "Epoch 2 Batch: 662 Train Loss: 0.015203784219920635\n",
      "Epoch 2 Batch: 663 Train Loss: 0.012679291889071465\n",
      "Epoch 2 Batch: 664 Train Loss: 0.014047299511730671\n",
      "Epoch 2 Batch: 665 Train Loss: 0.012490429915487766\n",
      "Epoch 2 Batch: 666 Train Loss: 0.011074230074882507\n",
      "Epoch 2 Batch: 667 Train Loss: 0.013411049731075764\n",
      "Epoch 2 Batch: 668 Train Loss: 0.012109343893826008\n",
      "Epoch 2 Batch: 669 Train Loss: 0.012117752805352211\n",
      "Epoch 2 Batch: 670 Train Loss: 0.010468311607837677\n",
      "Epoch 2 Batch: 671 Train Loss: 0.010702402330935001\n",
      "Epoch 2 Batch: 672 Train Loss: 0.4580489993095398\n",
      "Epoch 2 Batch: 673 Train Loss: 0.009235898032784462\n",
      "Epoch 2 Batch: 674 Train Loss: 0.006080390885472298\n",
      "Epoch 2 Batch: 675 Train Loss: 0.007824487052857876\n",
      "Epoch 2 Batch: 676 Train Loss: 0.008791016414761543\n",
      "Epoch 2 Batch: 677 Train Loss: 0.008664419874548912\n",
      "Epoch 2 Batch: 678 Train Loss: 0.008813629858195782\n",
      "Epoch 2 Batch: 679 Train Loss: 0.010059493593871593\n",
      "Epoch 2 Batch: 680 Train Loss: 0.00985055509954691\n",
      "Epoch 2 Batch: 681 Train Loss: 0.009294470772147179\n",
      "Epoch 2 Batch: 682 Train Loss: 0.5020993947982788\n",
      "Epoch 2 Batch: 683 Train Loss: 0.4636150300502777\n",
      "Epoch 2 Batch: 684 Train Loss: 0.008782027289271355\n",
      "Epoch 2 Batch: 685 Train Loss: 0.00861274916678667\n",
      "Epoch 2 Batch: 686 Train Loss: 0.009411165490746498\n",
      "Epoch 2 Batch: 687 Train Loss: 0.011065716855227947\n",
      "Epoch 2 Batch: 688 Train Loss: 0.010452523827552795\n",
      "Epoch 2 Batch: 689 Train Loss: 0.4396544396877289\n",
      "Epoch 2 Batch: 690 Train Loss: 0.011916403658688068\n",
      "Epoch 2 Batch: 691 Train Loss: 0.011478854343295097\n",
      "Epoch 2 Batch: 692 Train Loss: 0.012524617835879326\n",
      "Epoch 2 Batch: 693 Train Loss: 0.010133346542716026\n",
      "Epoch 2 Batch: 694 Train Loss: 0.012564755976200104\n",
      "Epoch 2 Batch: 695 Train Loss: 0.012745740823447704\n",
      "Epoch 2 Batch: 696 Train Loss: 0.01313270628452301\n",
      "Epoch 2 Batch: 697 Train Loss: 0.012059889733791351\n",
      "Epoch 2 Batch: 698 Train Loss: 0.013424883596599102\n",
      "Epoch 2 Batch: 699 Train Loss: 0.01326555572450161\n",
      "Epoch 2 Batch: 700 Train Loss: 0.012609516270458698\n",
      "Epoch 2 Batch: 701 Train Loss: 0.015930907800793648\n",
      "Epoch 2 Batch: 702 Train Loss: 0.011247333139181137\n",
      "Epoch 2 Batch: 703 Train Loss: 0.012963411398231983\n",
      "Epoch 2 Batch: 704 Train Loss: 0.010255912318825722\n",
      "Epoch 2 Batch: 705 Train Loss: 0.011192199774086475\n",
      "Epoch 2 Batch: 706 Train Loss: 0.012249687686562538\n",
      "Epoch 2 Batch: 707 Train Loss: 0.010539340786635876\n",
      "Epoch 2 Batch: 708 Train Loss: 0.01010130811482668\n",
      "Epoch 2 Batch: 709 Train Loss: 0.008188867941498756\n",
      "Epoch 2 Batch: 710 Train Loss: 0.009094826877117157\n",
      "Epoch 2 Batch: 711 Train Loss: 0.006984208710491657\n",
      "Epoch 2 Batch: 712 Train Loss: 0.00688337255269289\n",
      "Epoch 2 Batch: 713 Train Loss: 0.4594663977622986\n",
      "Epoch 2 Batch: 714 Train Loss: 0.4573643207550049\n",
      "Epoch 2 Batch: 715 Train Loss: 0.006911547388881445\n",
      "Epoch 2 Batch: 716 Train Loss: 0.009127611294388771\n",
      "Epoch 2 Batch: 717 Train Loss: 0.008842730894684792\n",
      "Epoch 2 Batch: 718 Train Loss: 0.00845976360142231\n",
      "Epoch 2 Batch: 719 Train Loss: 0.0096964780241251\n",
      "Epoch 2 Batch: 720 Train Loss: 0.009374204091727734\n",
      "Epoch 2 Batch: 721 Train Loss: 0.009455780498683453\n",
      "Epoch 2 Batch: 722 Train Loss: 0.4274504780769348\n",
      "Epoch 2 Batch: 723 Train Loss: 0.0122760571539402\n",
      "Epoch 2 Batch: 724 Train Loss: 0.009985385462641716\n",
      "Epoch 2 Batch: 725 Train Loss: 0.014500933699309826\n",
      "Epoch 2 Batch: 726 Train Loss: 0.015586811117827892\n",
      "Epoch 2 Batch: 727 Train Loss: 0.012012002058327198\n",
      "Epoch 2 Batch: 728 Train Loss: 0.01014463696628809\n",
      "Epoch 2 Batch: 729 Train Loss: 0.011425279080867767\n",
      "Epoch 2 Batch: 730 Train Loss: 0.010021470487117767\n",
      "Epoch 2 Batch: 731 Train Loss: 0.010562465526163578\n",
      "Epoch 2 Batch: 732 Train Loss: 0.01099940575659275\n",
      "Epoch 2 Batch: 733 Train Loss: 0.012670300900936127\n",
      "Epoch 2 Batch: 734 Train Loss: 0.013593019917607307\n",
      "Epoch 2 Batch: 735 Train Loss: 0.012004267424345016\n",
      "Epoch 2 Batch: 736 Train Loss: 0.009898331016302109\n",
      "Epoch 2 Batch: 737 Train Loss: 0.008977271616458893\n",
      "Epoch 2 Batch: 738 Train Loss: 0.012452347204089165\n",
      "Epoch 2 Batch: 739 Train Loss: 0.009890356101095676\n",
      "Epoch 2 Batch: 740 Train Loss: 0.010367346927523613\n",
      "Epoch 2 Batch: 741 Train Loss: 0.00961123313754797\n",
      "Epoch 2 Batch: 742 Train Loss: 0.009684113785624504\n",
      "Epoch 2 Batch: 743 Train Loss: 0.009722455404698849\n",
      "Epoch 2 Batch: 744 Train Loss: 0.005945889744907618\n",
      "Epoch 2 Batch: 745 Train Loss: 0.005696521140635014\n",
      "Epoch 2 Batch: 746 Train Loss: 0.005465204827487469\n",
      "Epoch 2 Batch: 747 Train Loss: 0.006743924226611853\n",
      "Epoch 2 Batch: 748 Train Loss: 0.006478058639913797\n",
      "Epoch 2 Batch: 749 Train Loss: 0.004179763607680798\n",
      "Epoch 2 Batch: 750 Train Loss: 0.004708955530077219\n",
      "Epoch 2 Batch: 751 Train Loss: 0.006999456789344549\n",
      "Epoch 2 Batch: 752 Train Loss: 0.005825904197990894\n",
      "Epoch 2 Batch: 753 Train Loss: 0.005118446424603462\n",
      "Epoch 2 Batch: 754 Train Loss: 0.004899225197732449\n",
      "Epoch 2 Batch: 755 Train Loss: 0.006538269575685263\n",
      "Epoch 2 Batch: 756 Train Loss: 0.004014285746961832\n",
      "Epoch 2 Batch: 757 Train Loss: 0.005588645581156015\n",
      "Epoch 2 Batch: 758 Train Loss: 0.0035420344211161137\n",
      "Epoch 2 Batch: 759 Train Loss: 0.003624721895903349\n",
      "Epoch 2 Batch: 760 Train Loss: 0.004604412242770195\n",
      "Epoch 2 Batch: 761 Train Loss: 0.004665724467486143\n",
      "Epoch 2 Batch: 762 Train Loss: 0.003913694527000189\n",
      "Epoch 2 Batch: 763 Train Loss: 0.0053329141810536385\n",
      "Epoch 2 Batch: 764 Train Loss: 0.004420160315930843\n",
      "Epoch 2 Batch: 765 Train Loss: 0.003924048505723476\n",
      "Epoch 2 Batch: 766 Train Loss: 0.00399458222091198\n",
      "Epoch 2 Batch: 767 Train Loss: 0.004026863723993301\n",
      "Epoch 2 Batch: 768 Train Loss: 0.6266450881958008\n",
      "Epoch 2 Batch: 769 Train Loss: 0.003272059140726924\n",
      "Epoch 2 Batch: 770 Train Loss: 0.0034538484178483486\n",
      "Epoch 2 Batch: 771 Train Loss: 0.003980354871600866\n",
      "Epoch 2 Batch: 772 Train Loss: 0.0035924550611525774\n",
      "Epoch 2 Batch: 773 Train Loss: 0.004244324751198292\n",
      "Epoch 2 Batch: 774 Train Loss: 0.0028392556123435497\n",
      "Epoch 2 Batch: 775 Train Loss: 0.003163387533277273\n",
      "Epoch 2 Batch: 776 Train Loss: 0.00400694040581584\n",
      "Epoch 2 Batch: 777 Train Loss: 0.004008409101516008\n",
      "Epoch 2 Batch: 778 Train Loss: 0.004000319633632898\n",
      "Epoch 2 Batch: 779 Train Loss: 0.004005451686680317\n",
      "Epoch 2 Batch: 780 Train Loss: 0.0031675659120082855\n",
      "Epoch 2 Batch: 781 Train Loss: 0.0034120685886591673\n",
      "Epoch 2 Batch: 782 Train Loss: 0.004384822212159634\n",
      "Epoch 2 Batch: 783 Train Loss: 0.0035899034701287746\n",
      "Epoch 2 Batch: 784 Train Loss: 0.0032810536213219166\n",
      "Epoch 2 Batch: 785 Train Loss: 0.007909690961241722\n",
      "Epoch 2 Batch: 786 Train Loss: 0.003512956202030182\n",
      "Epoch 2 Batch: 787 Train Loss: 0.003918756265193224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch: 788 Train Loss: 0.004517714027315378\n",
      "Epoch 2 Batch: 789 Train Loss: 0.002988760592415929\n",
      "Epoch 2 Batch: 790 Train Loss: 0.003365822834894061\n",
      "Epoch 2 Batch: 791 Train Loss: 0.002204697346314788\n",
      "Epoch 2 Batch: 792 Train Loss: 0.0034756262321025133\n",
      "Epoch 2 Batch: 793 Train Loss: 0.0027930079959332943\n",
      "Epoch 2 Batch: 794 Train Loss: 0.0029448973946273327\n",
      "Epoch 2 Batch: 795 Train Loss: 0.0025302921421825886\n",
      "Epoch 2 Batch: 796 Train Loss: 0.0030935797840356827\n",
      "Epoch 2 Batch: 797 Train Loss: 0.003254211274906993\n",
      "Epoch 2 Batch: 798 Train Loss: 0.07302802056074142\n",
      "Epoch 2 Batch: 799 Train Loss: 0.0029407928232103586\n",
      "Epoch 2 Batch: 800 Train Loss: 0.0030322102829813957\n",
      "Epoch 2 Batch: 801 Train Loss: 0.6041494607925415\n",
      "Epoch 2 Batch: 802 Train Loss: 0.003726387396454811\n",
      "Epoch 2 Batch: 803 Train Loss: 0.5873326659202576\n",
      "Epoch 2 Batch: 804 Train Loss: 0.00461231404915452\n",
      "Epoch 2 Batch: 805 Train Loss: 0.005812336690723896\n",
      "Epoch 2 Batch: 806 Train Loss: 0.5488557815551758\n",
      "Epoch 2 Batch: 807 Train Loss: 0.006249965168535709\n",
      "Epoch 2 Batch: 808 Train Loss: 0.006627608090639114\n",
      "Epoch 2 Batch: 809 Train Loss: 0.007315223105251789\n",
      "Epoch 2 Batch: 810 Train Loss: 0.007666673511266708\n",
      "Epoch 2 Batch: 811 Train Loss: 0.009112907573580742\n",
      "Epoch 2 Batch: 812 Train Loss: 0.010073167271912098\n",
      "Epoch 2 Batch: 813 Train Loss: 0.012514503672719002\n",
      "Epoch 2 Batch: 814 Train Loss: 0.009678935632109642\n",
      "Epoch 2 Batch: 815 Train Loss: 0.012985830195248127\n",
      "Epoch 2 Batch: 816 Train Loss: 0.01021360419690609\n",
      "Epoch 2 Batch: 817 Train Loss: 0.011536647565662861\n",
      "Epoch 2 Batch: 818 Train Loss: 0.010741526260972023\n",
      "Epoch 2 Batch: 819 Train Loss: 0.4414258599281311\n",
      "Epoch 2 Batch: 820 Train Loss: 0.01364090759307146\n",
      "Epoch 2 Batch: 821 Train Loss: 0.013957761228084564\n",
      "Epoch 2 Batch: 822 Train Loss: 0.01139687281101942\n",
      "Epoch 2 Batch: 823 Train Loss: 0.013440785929560661\n",
      "Epoch 2 Batch: 824 Train Loss: 0.4419759213924408\n",
      "Epoch 2 Batch: 825 Train Loss: 0.013654041104018688\n",
      "Epoch 2 Batch: 826 Train Loss: 0.011702927760779858\n",
      "Epoch 2 Batch: 827 Train Loss: 0.01716996356844902\n",
      "Epoch 2 Batch: 828 Train Loss: 0.017064321786165237\n",
      "Epoch 2 Batch: 829 Train Loss: 0.014169690199196339\n",
      "Epoch 2 Batch: 830 Train Loss: 0.01566437818109989\n",
      "Epoch 2 Batch: 831 Train Loss: 0.012032311409711838\n",
      "Epoch 2 Batch: 832 Train Loss: 0.015224252827465534\n",
      "Epoch 2 Batch: 833 Train Loss: 0.016491981223225594\n",
      "Epoch 2 Batch: 834 Train Loss: 0.016028426587581635\n",
      "Epoch 2 Batch: 835 Train Loss: 0.014059221372008324\n",
      "Epoch 2 Batch: 836 Train Loss: 0.013134008273482323\n",
      "Epoch 2 Batch: 837 Train Loss: 0.012630872428417206\n",
      "Epoch 2 Batch: 838 Train Loss: 0.44143134355545044\n",
      "Epoch 2 Batch: 839 Train Loss: 0.013712970539927483\n",
      "Epoch 2 Batch: 840 Train Loss: 0.012400830164551735\n",
      "Epoch 2 Batch: 841 Train Loss: 0.012227589264512062\n",
      "Epoch 2 Batch: 842 Train Loss: 0.010664674453437328\n",
      "Epoch 2 Batch: 843 Train Loss: 0.013483256101608276\n",
      "Epoch 2 Batch: 844 Train Loss: 0.012820886448025703\n",
      "Epoch 2 Batch: 845 Train Loss: 0.45736414194107056\n",
      "Epoch 2 Batch: 846 Train Loss: 0.011425655335187912\n",
      "Epoch 2 Batch: 847 Train Loss: 0.011617935262620449\n",
      "Epoch 2 Batch: 848 Train Loss: 0.012949487194418907\n",
      "Epoch 2 Batch: 849 Train Loss: 0.01268957369029522\n",
      "Epoch 2 Batch: 850 Train Loss: 0.011523500084877014\n",
      "Epoch 2 Batch: 851 Train Loss: 0.011286775581538677\n",
      "Epoch 2 Batch: 852 Train Loss: 0.008795817382633686\n",
      "Epoch 2 Batch: 853 Train Loss: 0.010999574325978756\n",
      "Epoch 2 Batch: 854 Train Loss: 0.011276924051344395\n",
      "Epoch 2 Batch: 855 Train Loss: 0.01175304502248764\n",
      "Epoch 2 Batch: 856 Train Loss: 0.01131970901042223\n",
      "Epoch 2 Batch: 857 Train Loss: 0.007612315472215414\n",
      "Epoch 2 Batch: 858 Train Loss: 0.010564829222857952\n",
      "Epoch 2 Batch: 859 Train Loss: 0.4651705324649811\n",
      "Epoch 2 Batch: 860 Train Loss: 0.010503840632736683\n",
      "Epoch 2 Batch: 861 Train Loss: 0.009525886736810207\n",
      "Epoch 2 Batch: 862 Train Loss: 0.009292994625866413\n",
      "Epoch 2 Batch: 863 Train Loss: 0.009084383957087994\n",
      "Epoch 2 Batch: 864 Train Loss: 0.01047718059271574\n",
      "Epoch 2 Batch: 865 Train Loss: 0.009961983188986778\n",
      "Epoch 2 Batch: 866 Train Loss: 0.008158395066857338\n",
      "Epoch 2 Batch: 867 Train Loss: 0.010897481814026833\n",
      "Epoch 2 Batch: 868 Train Loss: 0.008916626684367657\n",
      "Epoch 2 Batch: 869 Train Loss: 0.009006818756461143\n",
      "Epoch 2 Batch: 870 Train Loss: 0.009036825969815254\n",
      "Epoch 2 Batch: 871 Train Loss: 0.008676039054989815\n",
      "Epoch 2 Batch: 872 Train Loss: 0.007426714990288019\n",
      "Epoch 2 Batch: 873 Train Loss: 0.008200319483876228\n",
      "Epoch 2 Batch: 874 Train Loss: 0.5031846761703491\n",
      "Epoch 2 Batch: 875 Train Loss: 0.008100097067654133\n",
      "Epoch 2 Batch: 876 Train Loss: 0.008415229618549347\n",
      "Epoch 2 Batch: 877 Train Loss: 0.0077196210622787476\n",
      "Epoch 2 Batch: 878 Train Loss: 0.49334460496902466\n",
      "Epoch 2 Batch: 879 Train Loss: 0.007899900898337364\n",
      "Epoch 2 Batch: 880 Train Loss: 0.009372913278639317\n",
      "Epoch 4 Batch: 1 Train Loss: 0.008251798339188099\n",
      "Epoch 4 Batch: 2 Train Loss: 0.008026067167520523\n",
      "Epoch 4 Batch: 3 Train Loss: 0.007797278463840485\n",
      "Epoch 4 Batch: 4 Train Loss: 0.008804512210190296\n",
      "Epoch 4 Batch: 5 Train Loss: 0.008490021340548992\n",
      "Epoch 4 Batch: 6 Train Loss: 0.00825001485645771\n",
      "Epoch 4 Batch: 7 Train Loss: 0.005827342160046101\n",
      "Epoch 4 Batch: 8 Train Loss: 0.00822846032679081\n",
      "Epoch 4 Batch: 9 Train Loss: 0.007455452345311642\n",
      "Epoch 4 Batch: 10 Train Loss: 0.008168012835085392\n",
      "Epoch 4 Batch: 11 Train Loss: 0.5417648553848267\n",
      "Epoch 4 Batch: 12 Train Loss: 0.0065802112221717834\n",
      "Epoch 4 Batch: 13 Train Loss: 0.4751167297363281\n",
      "Epoch 4 Batch: 14 Train Loss: 0.007093510590493679\n",
      "Epoch 4 Batch: 15 Train Loss: 0.007807782851159573\n",
      "Epoch 4 Batch: 16 Train Loss: 0.007252743933349848\n",
      "Epoch 4 Batch: 17 Train Loss: 0.007371869869530201\n",
      "Epoch 4 Batch: 18 Train Loss: 0.009161827154457569\n",
      "Epoch 4 Batch: 19 Train Loss: 0.008816735818982124\n",
      "Epoch 4 Batch: 20 Train Loss: 0.006773635745048523\n",
      "Epoch 4 Batch: 21 Train Loss: 0.008477735333144665\n",
      "Epoch 4 Batch: 22 Train Loss: 0.00884905457496643\n",
      "Epoch 4 Batch: 23 Train Loss: 0.008329885080456734\n",
      "Epoch 4 Batch: 24 Train Loss: 0.4558325707912445\n",
      "Epoch 4 Batch: 25 Train Loss: 0.009136596694588661\n",
      "Epoch 4 Batch: 26 Train Loss: 0.009560339152812958\n",
      "Epoch 4 Batch: 27 Train Loss: 0.007899312302470207\n",
      "Epoch 4 Batch: 28 Train Loss: 0.4419390559196472\n",
      "Epoch 4 Batch: 29 Train Loss: 0.011268671602010727\n",
      "Epoch 4 Batch: 30 Train Loss: 0.012536125257611275\n",
      "Epoch 4 Batch: 31 Train Loss: 0.006958091165870428\n",
      "Epoch 4 Batch: 32 Train Loss: 0.011966787278652191\n",
      "Epoch 4 Batch: 33 Train Loss: 0.008066557347774506\n",
      "Epoch 4 Batch: 34 Train Loss: 0.010827292688190937\n",
      "Epoch 4 Batch: 35 Train Loss: 0.49335789680480957\n",
      "Epoch 4 Batch: 36 Train Loss: 0.011385173536837101\n",
      "Epoch 4 Batch: 37 Train Loss: 0.010885646566748619\n",
      "Epoch 4 Batch: 38 Train Loss: 0.012146534398198128\n",
      "Epoch 4 Batch: 39 Train Loss: 0.013372182846069336\n",
      "Epoch 4 Batch: 40 Train Loss: 0.0135525893419981\n",
      "Epoch 4 Batch: 41 Train Loss: 0.015521705150604248\n",
      "Epoch 4 Batch: 42 Train Loss: 0.009800685569643974\n",
      "Epoch 4 Batch: 43 Train Loss: 0.012572352774441242\n",
      "Epoch 4 Batch: 44 Train Loss: 0.014326649717986584\n",
      "Epoch 4 Batch: 45 Train Loss: 0.014872714877128601\n",
      "Epoch 4 Batch: 46 Train Loss: 0.011071136221289635\n",
      "Epoch 4 Batch: 47 Train Loss: 0.013508826494216919\n",
      "Epoch 4 Batch: 48 Train Loss: 0.011446363292634487\n",
      "Epoch 4 Batch: 49 Train Loss: 0.011728943325579166\n",
      "Epoch 4 Batch: 50 Train Loss: 0.011660367250442505\n",
      "Epoch 4 Batch: 51 Train Loss: 0.009740667417645454\n",
      "Epoch 4 Batch: 52 Train Loss: 0.010588251985609531\n",
      "Epoch 4 Batch: 53 Train Loss: 0.009262438863515854\n",
      "Epoch 4 Batch: 54 Train Loss: 0.010756103321909904\n",
      "Epoch 4 Batch: 55 Train Loss: 0.007873324677348137\n",
      "Epoch 4 Batch: 56 Train Loss: 0.010356591083109379\n",
      "Epoch 4 Batch: 57 Train Loss: 0.44987982511520386\n",
      "Epoch 4 Batch: 58 Train Loss: 0.007181392051279545\n",
      "Epoch 4 Batch: 59 Train Loss: 0.010820111259818077\n",
      "Epoch 4 Batch: 60 Train Loss: 0.008426018059253693\n",
      "Epoch 4 Batch: 61 Train Loss: 0.00904964841902256\n",
      "Epoch 4 Batch: 62 Train Loss: 0.011028721928596497\n",
      "Epoch 4 Batch: 63 Train Loss: 0.00810777023434639\n",
      "Epoch 4 Batch: 64 Train Loss: 0.009227666072547436\n",
      "Epoch 4 Batch: 65 Train Loss: 0.01038316823542118\n",
      "Epoch 4 Batch: 66 Train Loss: 0.008507882244884968\n",
      "Epoch 4 Batch: 67 Train Loss: 0.007119092158973217\n",
      "Epoch 4 Batch: 68 Train Loss: 0.009260294958949089\n",
      "Epoch 4 Batch: 69 Train Loss: 0.007440079934895039\n",
      "Epoch 4 Batch: 70 Train Loss: 0.008824970573186874\n",
      "Epoch 4 Batch: 71 Train Loss: 0.45991629362106323\n",
      "Epoch 4 Batch: 72 Train Loss: 0.007177695631980896\n",
      "Epoch 4 Batch: 73 Train Loss: 0.00954059325158596\n",
      "Epoch 4 Batch: 74 Train Loss: 0.008532351814210415\n",
      "Epoch 4 Batch: 75 Train Loss: 0.007524539716541767\n",
      "Epoch 4 Batch: 76 Train Loss: 0.00857926718890667\n",
      "Epoch 4 Batch: 77 Train Loss: 0.00739858765155077\n",
      "Epoch 4 Batch: 78 Train Loss: 0.006195423658937216\n",
      "Epoch 4 Batch: 79 Train Loss: 0.007826076820492744\n",
      "Epoch 4 Batch: 80 Train Loss: 0.007733434438705444\n",
      "Epoch 4 Batch: 81 Train Loss: 0.008834782987833023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Batch: 82 Train Loss: 0.00587550550699234\n",
      "Epoch 4 Batch: 83 Train Loss: 0.007945643737912178\n",
      "Epoch 4 Batch: 84 Train Loss: 0.009536398574709892\n",
      "Epoch 4 Batch: 85 Train Loss: 0.006561527960002422\n",
      "Epoch 4 Batch: 86 Train Loss: 0.006404203362762928\n",
      "Epoch 4 Batch: 87 Train Loss: 0.006149954162538052\n",
      "Epoch 4 Batch: 88 Train Loss: 0.007153999991714954\n",
      "Epoch 4 Batch: 89 Train Loss: 0.005498270038515329\n",
      "Epoch 4 Batch: 90 Train Loss: 0.007194823585450649\n",
      "Epoch 4 Batch: 91 Train Loss: 0.005114406812936068\n",
      "Epoch 4 Batch: 92 Train Loss: 0.006321945693343878\n",
      "Epoch 4 Batch: 93 Train Loss: 0.004836197942495346\n",
      "Epoch 4 Batch: 94 Train Loss: 0.006423745304346085\n",
      "Epoch 4 Batch: 95 Train Loss: 0.004563518334180117\n",
      "Epoch 4 Batch: 96 Train Loss: 0.005212224088609219\n",
      "Epoch 4 Batch: 97 Train Loss: 0.005551463924348354\n",
      "Epoch 4 Batch: 98 Train Loss: 0.5764002203941345\n",
      "Epoch 4 Batch: 99 Train Loss: 0.005438012070953846\n",
      "Epoch 4 Batch: 100 Train Loss: 0.005141331348568201\n",
      "Epoch 4 Batch: 101 Train Loss: 0.005471940152347088\n",
      "Epoch 4 Batch: 102 Train Loss: 0.005263549741357565\n",
      "Epoch 4 Batch: 103 Train Loss: 0.006750712636858225\n",
      "Epoch 4 Batch: 104 Train Loss: 0.004245229531079531\n",
      "Epoch 4 Batch: 105 Train Loss: 0.014852562919259071\n",
      "Epoch 4 Batch: 106 Train Loss: 0.5567693710327148\n",
      "Epoch 4 Batch: 107 Train Loss: 0.005283988546580076\n",
      "Epoch 4 Batch: 108 Train Loss: 0.006342320237308741\n",
      "Epoch 4 Batch: 109 Train Loss: 0.005280672572553158\n",
      "Epoch 4 Batch: 110 Train Loss: 0.006553576327860355\n",
      "Epoch 4 Batch: 111 Train Loss: 0.0066948579624295235\n",
      "Epoch 4 Batch: 112 Train Loss: 0.005491630174219608\n",
      "Epoch 4 Batch: 113 Train Loss: 0.006046845577657223\n",
      "Epoch 4 Batch: 114 Train Loss: 0.006698067300021648\n",
      "Epoch 4 Batch: 115 Train Loss: 0.006589951459318399\n",
      "Epoch 4 Batch: 116 Train Loss: 0.00672284048050642\n",
      "Epoch 4 Batch: 117 Train Loss: 0.0072994837537407875\n",
      "Epoch 4 Batch: 118 Train Loss: 0.0066193812526762486\n",
      "Epoch 4 Batch: 119 Train Loss: 0.00739249074831605\n",
      "Epoch 4 Batch: 120 Train Loss: 0.0038674678653478622\n",
      "Epoch 4 Batch: 121 Train Loss: 0.007133112754672766\n",
      "Epoch 4 Batch: 122 Train Loss: 0.007374147418886423\n",
      "Epoch 4 Batch: 123 Train Loss: 0.9835683107376099\n",
      "Epoch 4 Batch: 124 Train Loss: 0.00725338002666831\n",
      "Epoch 4 Batch: 125 Train Loss: 0.00822133757174015\n",
      "Epoch 4 Batch: 126 Train Loss: 0.00624594371765852\n",
      "Epoch 4 Batch: 127 Train Loss: 0.00911242701113224\n",
      "Epoch 4 Batch: 128 Train Loss: 0.008606093004345894\n",
      "Epoch 4 Batch: 129 Train Loss: 0.008743411861360073\n",
      "Epoch 4 Batch: 130 Train Loss: 0.008960886858403683\n",
      "Epoch 4 Batch: 131 Train Loss: 0.011738630011677742\n",
      "Epoch 4 Batch: 132 Train Loss: 0.010328682139515877\n",
      "Epoch 4 Batch: 133 Train Loss: 0.007570466957986355\n",
      "Epoch 4 Batch: 134 Train Loss: 0.00881313905119896\n",
      "Epoch 4 Batch: 135 Train Loss: 0.4862651824951172\n",
      "Epoch 4 Batch: 136 Train Loss: 0.009666369296610355\n",
      "Epoch 4 Batch: 137 Train Loss: 0.011482012458145618\n",
      "Epoch 4 Batch: 138 Train Loss: 0.4427264630794525\n",
      "Epoch 4 Batch: 139 Train Loss: 0.010799238458275795\n",
      "Epoch 4 Batch: 140 Train Loss: 0.4641210436820984\n",
      "Epoch 4 Batch: 141 Train Loss: 0.013938618823885918\n",
      "Epoch 4 Batch: 142 Train Loss: 0.012783737853169441\n",
      "Epoch 4 Batch: 143 Train Loss: 0.41698408126831055\n",
      "Epoch 4 Batch: 144 Train Loss: 0.016716474667191505\n",
      "Epoch 4 Batch: 145 Train Loss: 0.013753555715084076\n",
      "Epoch 4 Batch: 146 Train Loss: 0.013363519683480263\n",
      "Epoch 4 Batch: 147 Train Loss: 0.014515248127281666\n",
      "Epoch 4 Batch: 148 Train Loss: 0.020504994317889214\n",
      "Epoch 4 Batch: 149 Train Loss: 0.019684158265590668\n",
      "Epoch 4 Batch: 150 Train Loss: 0.02416989952325821\n",
      "Epoch 4 Batch: 151 Train Loss: 0.018819671124219894\n",
      "Epoch 4 Batch: 152 Train Loss: 0.019019251689314842\n",
      "Epoch 4 Batch: 153 Train Loss: 0.020963342860341072\n",
      "Epoch 4 Batch: 154 Train Loss: 0.013830850832164288\n",
      "Epoch 4 Batch: 155 Train Loss: 0.020169589668512344\n",
      "Epoch 4 Batch: 156 Train Loss: 0.017345048487186432\n",
      "Epoch 4 Batch: 157 Train Loss: 0.01954483427107334\n",
      "Epoch 4 Batch: 158 Train Loss: 0.015695638954639435\n",
      "Epoch 4 Batch: 159 Train Loss: 0.019117463380098343\n",
      "Epoch 4 Batch: 160 Train Loss: 0.01641087420284748\n",
      "Epoch 4 Batch: 161 Train Loss: 0.015157113783061504\n",
      "Epoch 4 Batch: 162 Train Loss: 0.016695253551006317\n",
      "Epoch 4 Batch: 163 Train Loss: 0.015036473050713539\n",
      "Epoch 4 Batch: 164 Train Loss: 0.01304788701236248\n",
      "Epoch 4 Batch: 165 Train Loss: 0.012378336861729622\n",
      "Epoch 4 Batch: 166 Train Loss: 0.011762925423681736\n",
      "Epoch 4 Batch: 167 Train Loss: 0.011832484975457191\n",
      "Epoch 4 Batch: 168 Train Loss: 0.0102318050339818\n",
      "Epoch 4 Batch: 169 Train Loss: 0.010628234595060349\n",
      "Epoch 4 Batch: 170 Train Loss: 0.008000051602721214\n",
      "Epoch 4 Batch: 171 Train Loss: 0.007891051471233368\n",
      "Epoch 4 Batch: 172 Train Loss: 0.009648175910115242\n",
      "Epoch 4 Batch: 173 Train Loss: 0.006862299982458353\n",
      "Epoch 4 Batch: 174 Train Loss: 0.006540751550346613\n",
      "Epoch 4 Batch: 175 Train Loss: 0.007093148771673441\n",
      "Epoch 4 Batch: 176 Train Loss: 0.007726309355348349\n",
      "Epoch 4 Batch: 177 Train Loss: 0.007153126411139965\n",
      "Epoch 4 Batch: 178 Train Loss: 0.0066823274828493595\n",
      "Epoch 4 Batch: 179 Train Loss: 0.006754696369171143\n",
      "Epoch 4 Batch: 180 Train Loss: 0.005861335899680853\n",
      "Epoch 4 Batch: 181 Train Loss: 0.006072340067476034\n",
      "Epoch 4 Batch: 182 Train Loss: 0.005828137509524822\n",
      "Epoch 4 Batch: 183 Train Loss: 0.004573427140712738\n",
      "Epoch 4 Batch: 184 Train Loss: 0.005672638304531574\n",
      "Epoch 4 Batch: 185 Train Loss: 0.007016050163656473\n",
      "Epoch 4 Batch: 186 Train Loss: 0.0049592843279242516\n",
      "Epoch 4 Batch: 187 Train Loss: 0.0033902544528245926\n",
      "Epoch 4 Batch: 188 Train Loss: 0.004798348527401686\n",
      "Epoch 4 Batch: 189 Train Loss: 0.004399861209094524\n",
      "Epoch 4 Batch: 190 Train Loss: 0.00412814412266016\n",
      "Epoch 4 Batch: 191 Train Loss: 0.004111710004508495\n",
      "Epoch 4 Batch: 192 Train Loss: 0.00412312988191843\n",
      "Epoch 4 Batch: 193 Train Loss: 0.0037846446502953768\n",
      "Epoch 4 Batch: 194 Train Loss: 0.003920021001249552\n",
      "Epoch 4 Batch: 195 Train Loss: 0.003577195107936859\n",
      "Epoch 4 Batch: 196 Train Loss: 0.003919999115169048\n",
      "Epoch 4 Batch: 197 Train Loss: 0.0035021223593503237\n",
      "Epoch 4 Batch: 198 Train Loss: 0.002716608578339219\n",
      "Epoch 4 Batch: 199 Train Loss: 0.0037324584554880857\n",
      "Epoch 4 Batch: 200 Train Loss: 0.003742727916687727\n",
      "Epoch 4 Batch: 201 Train Loss: 0.0029705080669373274\n",
      "Epoch 4 Batch: 202 Train Loss: 0.0026214313693344593\n",
      "Epoch 4 Batch: 203 Train Loss: 0.003295864211395383\n",
      "Epoch 4 Batch: 204 Train Loss: 0.0027043502777814865\n",
      "Epoch 4 Batch: 205 Train Loss: 0.0024509942159056664\n",
      "Epoch 4 Batch: 206 Train Loss: 0.002826143754646182\n",
      "Epoch 4 Batch: 207 Train Loss: 0.002685070503503084\n",
      "Epoch 4 Batch: 208 Train Loss: 0.0030417623929679394\n",
      "Epoch 4 Batch: 209 Train Loss: 0.0028192936442792416\n",
      "Epoch 4 Batch: 210 Train Loss: 0.002684773877263069\n",
      "Epoch 4 Batch: 211 Train Loss: 0.002323189517483115\n",
      "Epoch 4 Batch: 212 Train Loss: 0.0025827432982623577\n",
      "Epoch 4 Batch: 213 Train Loss: 0.0022351625375449657\n",
      "Epoch 4 Batch: 214 Train Loss: 0.0026340654585510492\n",
      "Epoch 4 Batch: 215 Train Loss: 0.0022372915409505367\n",
      "Epoch 4 Batch: 216 Train Loss: 0.004209747537970543\n",
      "Epoch 4 Batch: 217 Train Loss: 0.008437410928308964\n",
      "Epoch 4 Batch: 218 Train Loss: 0.0020803925581276417\n",
      "Epoch 4 Batch: 219 Train Loss: 0.002543975133448839\n",
      "Epoch 4 Batch: 220 Train Loss: 0.592521071434021\n",
      "Epoch 4 Batch: 221 Train Loss: 0.0023887213319540024\n",
      "Epoch 4 Batch: 222 Train Loss: 0.0025143534876406193\n",
      "Epoch 4 Batch: 223 Train Loss: 0.6120908260345459\n",
      "Epoch 4 Batch: 224 Train Loss: 0.02099919132888317\n",
      "Epoch 4 Batch: 225 Train Loss: 0.0035731345415115356\n",
      "Epoch 4 Batch: 226 Train Loss: 0.003402597503736615\n",
      "Epoch 4 Batch: 227 Train Loss: 0.004048869479447603\n",
      "Epoch 4 Batch: 228 Train Loss: 0.004468264989554882\n",
      "Epoch 4 Batch: 229 Train Loss: 0.004646014422178268\n",
      "Epoch 4 Batch: 230 Train Loss: 0.5260741114616394\n",
      "Epoch 4 Batch: 231 Train Loss: 0.0053102257661521435\n",
      "Epoch 4 Batch: 232 Train Loss: 0.005932398606091738\n",
      "Epoch 4 Batch: 233 Train Loss: 0.006397480610758066\n",
      "Epoch 4 Batch: 234 Train Loss: 0.0047316416166722775\n",
      "Epoch 4 Batch: 235 Train Loss: 0.0073923892341554165\n",
      "Epoch 4 Batch: 236 Train Loss: 0.00631329882889986\n",
      "Epoch 4 Batch: 237 Train Loss: 0.008039931766688824\n",
      "Epoch 4 Batch: 238 Train Loss: 0.007152671925723553\n",
      "Epoch 4 Batch: 239 Train Loss: 0.47491732239723206\n",
      "Epoch 4 Batch: 240 Train Loss: 0.4967535436153412\n",
      "Epoch 4 Batch: 241 Train Loss: 0.010372564196586609\n",
      "Epoch 4 Batch: 242 Train Loss: 0.010102669708430767\n",
      "Epoch 4 Batch: 243 Train Loss: 0.011224622838199139\n",
      "Epoch 4 Batch: 244 Train Loss: 0.01574670895934105\n",
      "Epoch 4 Batch: 245 Train Loss: 0.010785548947751522\n",
      "Epoch 4 Batch: 246 Train Loss: 0.014070344157516956\n",
      "Epoch 4 Batch: 247 Train Loss: 0.45103392004966736\n",
      "Epoch 4 Batch: 248 Train Loss: 0.015715572983026505\n",
      "Epoch 4 Batch: 249 Train Loss: 0.016659777611494064\n",
      "Epoch 4 Batch: 250 Train Loss: 0.015209103934466839\n",
      "Epoch 4 Batch: 251 Train Loss: 0.015771429985761642\n",
      "Epoch 4 Batch: 252 Train Loss: 0.016179420053958893\n",
      "Epoch 4 Batch: 253 Train Loss: 0.01920628920197487\n",
      "Epoch 4 Batch: 254 Train Loss: 0.016293523833155632\n",
      "Epoch 4 Batch: 255 Train Loss: 0.016074109822511673\n",
      "Epoch 4 Batch: 256 Train Loss: 0.01826549507677555\n",
      "Epoch 4 Batch: 257 Train Loss: 0.017593836411833763\n",
      "Epoch 4 Batch: 258 Train Loss: 0.01569356396794319\n",
      "Epoch 4 Batch: 259 Train Loss: 0.0157149787992239\n",
      "Epoch 4 Batch: 260 Train Loss: 0.012909917160868645\n",
      "Epoch 4 Batch: 261 Train Loss: 0.015260368585586548\n",
      "Epoch 4 Batch: 262 Train Loss: 0.015044030733406544\n",
      "Epoch 4 Batch: 263 Train Loss: 0.011573354713618755\n",
      "Epoch 4 Batch: 264 Train Loss: 0.011512622237205505\n",
      "Epoch 4 Batch: 265 Train Loss: 0.010659082792699337\n",
      "Epoch 4 Batch: 266 Train Loss: 0.013021321967244148\n",
      "Epoch 4 Batch: 267 Train Loss: 0.011390932835638523\n",
      "Epoch 4 Batch: 268 Train Loss: 0.44951000809669495\n",
      "Epoch 4 Batch: 269 Train Loss: 0.01175327692180872\n",
      "Epoch 4 Batch: 270 Train Loss: 0.011687267571687698\n",
      "Epoch 4 Batch: 271 Train Loss: 0.011566582135856152\n",
      "Epoch 4 Batch: 272 Train Loss: 0.010992949828505516\n",
      "Epoch 4 Batch: 273 Train Loss: 0.01097172312438488\n",
      "Epoch 4 Batch: 274 Train Loss: 0.010424268431961536\n",
      "Epoch 4 Batch: 275 Train Loss: 0.009498583152890205\n",
      "Epoch 4 Batch: 276 Train Loss: 0.010153083130717278\n",
      "Epoch 4 Batch: 277 Train Loss: 0.01009491179138422\n",
      "Epoch 4 Batch: 278 Train Loss: 0.010278183035552502\n",
      "Epoch 4 Batch: 279 Train Loss: 0.008340662345290184\n",
      "Epoch 4 Batch: 280 Train Loss: 0.009889497421681881\n",
      "Epoch 4 Batch: 281 Train Loss: 0.007584079168736935\n",
      "Epoch 4 Batch: 282 Train Loss: 0.00779313687235117\n",
      "Epoch 4 Batch: 283 Train Loss: 0.0072791860438883305\n",
      "Epoch 4 Batch: 284 Train Loss: 0.007248842157423496\n",
      "Epoch 4 Batch: 285 Train Loss: 0.007184184156358242\n",
      "Epoch 4 Batch: 286 Train Loss: 0.006693732924759388\n",
      "Epoch 4 Batch: 287 Train Loss: 0.005839765537530184\n",
      "Epoch 4 Batch: 288 Train Loss: 0.006480411626398563\n",
      "Epoch 4 Batch: 289 Train Loss: 0.005832691676914692\n",
      "Epoch 4 Batch: 290 Train Loss: 0.0054193600080907345\n",
      "Epoch 4 Batch: 291 Train Loss: 0.005941455718129873\n",
      "Epoch 4 Batch: 292 Train Loss: 0.537195086479187\n",
      "Epoch 4 Batch: 293 Train Loss: 0.005716246087104082\n",
      "Epoch 4 Batch: 294 Train Loss: 0.005960978101938963\n",
      "Epoch 4 Batch: 295 Train Loss: 0.006509418599307537\n",
      "Epoch 4 Batch: 296 Train Loss: 0.006371804978698492\n",
      "Epoch 4 Batch: 297 Train Loss: 0.005274688359349966\n",
      "Epoch 4 Batch: 298 Train Loss: 0.004735384602099657\n",
      "Epoch 4 Batch: 299 Train Loss: 0.006201265379786491\n",
      "Epoch 4 Batch: 300 Train Loss: 0.5058838725090027\n",
      "Epoch 4 Batch: 301 Train Loss: 0.005739097949117422\n",
      "Epoch 4 Batch: 302 Train Loss: 0.006216199137270451\n",
      "Epoch 4 Batch: 303 Train Loss: 0.5144269466400146\n",
      "Epoch 4 Batch: 304 Train Loss: 0.00614057295024395\n",
      "Epoch 4 Batch: 305 Train Loss: 0.008081287145614624\n",
      "Epoch 4 Batch: 306 Train Loss: 0.008269283920526505\n",
      "Epoch 4 Batch: 307 Train Loss: 0.007961252704262733\n",
      "Epoch 4 Batch: 308 Train Loss: 0.008882002905011177\n",
      "Epoch 4 Batch: 309 Train Loss: 0.008582002483308315\n",
      "Epoch 4 Batch: 310 Train Loss: 0.4774412214756012\n",
      "Epoch 4 Batch: 311 Train Loss: 0.009216784499585629\n",
      "Epoch 4 Batch: 312 Train Loss: 0.009747834876179695\n",
      "Epoch 4 Batch: 313 Train Loss: 0.45930489897727966\n",
      "Epoch 4 Batch: 314 Train Loss: 0.012038184329867363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Batch: 315 Train Loss: 0.010642129927873611\n",
      "Epoch 4 Batch: 316 Train Loss: 0.011135146021842957\n",
      "Epoch 4 Batch: 317 Train Loss: 0.01485928613692522\n",
      "Epoch 4 Batch: 318 Train Loss: 0.014457473531365395\n",
      "Epoch 4 Batch: 319 Train Loss: 0.014973469078540802\n",
      "Epoch 4 Batch: 320 Train Loss: 0.015328660607337952\n",
      "Epoch 4 Batch: 321 Train Loss: 0.011780396103858948\n",
      "Epoch 4 Batch: 322 Train Loss: 0.01088559441268444\n",
      "Epoch 4 Batch: 323 Train Loss: 0.013685840182006359\n",
      "Epoch 4 Batch: 324 Train Loss: 0.01560142356902361\n",
      "Epoch 4 Batch: 325 Train Loss: 0.015053664334118366\n",
      "Epoch 4 Batch: 326 Train Loss: 0.013891613110899925\n",
      "Epoch 4 Batch: 327 Train Loss: 0.4544221758842468\n",
      "Epoch 4 Batch: 328 Train Loss: 0.01248745433986187\n",
      "Epoch 4 Batch: 329 Train Loss: 0.01565631851553917\n",
      "Epoch 4 Batch: 330 Train Loss: 0.013876902870833874\n",
      "Epoch 4 Batch: 331 Train Loss: 0.42389434576034546\n",
      "Epoch 4 Batch: 332 Train Loss: 0.012628707103431225\n",
      "Epoch 4 Batch: 333 Train Loss: 0.015927007421851158\n",
      "Epoch 4 Batch: 334 Train Loss: 0.013911480084061623\n",
      "Epoch 4 Batch: 335 Train Loss: 0.015394004061818123\n",
      "Epoch 4 Batch: 336 Train Loss: 0.013185376301407814\n",
      "Epoch 4 Batch: 337 Train Loss: 0.014922944828867912\n",
      "Epoch 4 Batch: 338 Train Loss: 0.015114469453692436\n",
      "Epoch 4 Batch: 339 Train Loss: 0.011920125223696232\n",
      "Epoch 4 Batch: 340 Train Loss: 0.015445500612258911\n",
      "Epoch 4 Batch: 341 Train Loss: 0.013793952763080597\n",
      "Epoch 4 Batch: 342 Train Loss: 0.013727854005992413\n",
      "Epoch 4 Batch: 343 Train Loss: 0.010537504218518734\n",
      "Epoch 4 Batch: 344 Train Loss: 0.010958305560052395\n",
      "Epoch 4 Batch: 345 Train Loss: 0.012706978246569633\n",
      "Epoch 4 Batch: 346 Train Loss: 0.011864451691508293\n",
      "Epoch 4 Batch: 347 Train Loss: 0.47021931409835815\n",
      "Epoch 4 Batch: 348 Train Loss: 0.011452280916273594\n",
      "Epoch 4 Batch: 349 Train Loss: 0.011987067759037018\n",
      "Epoch 4 Batch: 350 Train Loss: 0.009303081780672073\n",
      "Epoch 4 Batch: 351 Train Loss: 0.007308456115424633\n",
      "Epoch 4 Batch: 352 Train Loss: 0.008691455237567425\n",
      "Epoch 4 Batch: 353 Train Loss: 0.010655872523784637\n",
      "Epoch 4 Batch: 354 Train Loss: 0.4690522253513336\n",
      "Epoch 4 Batch: 355 Train Loss: 0.4537487030029297\n",
      "Epoch 4 Batch: 356 Train Loss: 0.01184982992708683\n",
      "Epoch 4 Batch: 357 Train Loss: 0.0076688313856720924\n",
      "Epoch 4 Batch: 358 Train Loss: 0.013290716335177422\n",
      "Epoch 4 Batch: 359 Train Loss: 0.013629399240016937\n",
      "Epoch 4 Batch: 360 Train Loss: 0.01395698357373476\n",
      "Epoch 4 Batch: 361 Train Loss: 0.012711862102150917\n",
      "Epoch 4 Batch: 362 Train Loss: 0.014279303140938282\n",
      "Epoch 4 Batch: 363 Train Loss: 0.014061731286346912\n",
      "Epoch 4 Batch: 364 Train Loss: 0.014277649112045765\n",
      "Epoch 4 Batch: 365 Train Loss: 0.012736165896058083\n",
      "Epoch 4 Batch: 366 Train Loss: 0.013931895606219769\n",
      "Epoch 4 Batch: 367 Train Loss: 0.44018539786338806\n",
      "Epoch 4 Batch: 368 Train Loss: 0.012819143012166023\n",
      "Epoch 4 Batch: 369 Train Loss: 0.44351592659950256\n",
      "Epoch 4 Batch: 370 Train Loss: 0.013453684747219086\n",
      "Epoch 4 Batch: 371 Train Loss: 0.012209891341626644\n",
      "Epoch 4 Batch: 372 Train Loss: 0.8432818651199341\n",
      "Epoch 4 Batch: 373 Train Loss: 0.014119291678071022\n",
      "Epoch 4 Batch: 374 Train Loss: 0.01747739501297474\n",
      "Epoch 4 Batch: 375 Train Loss: 0.0208410806953907\n",
      "Epoch 4 Batch: 376 Train Loss: 0.020170748233795166\n",
      "Epoch 4 Batch: 377 Train Loss: 0.0229655671864748\n",
      "Epoch 4 Batch: 378 Train Loss: 0.39918044209480286\n",
      "Epoch 4 Batch: 379 Train Loss: 0.022570239380002022\n",
      "Epoch 4 Batch: 380 Train Loss: 0.02088114805519581\n",
      "Epoch 4 Batch: 381 Train Loss: 0.026571625843644142\n",
      "Epoch 4 Batch: 382 Train Loss: 0.026915201917290688\n",
      "Epoch 4 Batch: 383 Train Loss: 0.02144772931933403\n",
      "Epoch 4 Batch: 384 Train Loss: 0.3938165009021759\n",
      "Epoch 4 Batch: 385 Train Loss: 0.018840868026018143\n",
      "Epoch 4 Batch: 386 Train Loss: 0.024511508643627167\n",
      "Epoch 4 Batch: 387 Train Loss: 0.02801838517189026\n",
      "Epoch 4 Batch: 388 Train Loss: 0.02397526428103447\n",
      "Epoch 4 Batch: 389 Train Loss: 0.02380313351750374\n",
      "Epoch 4 Batch: 390 Train Loss: 0.025346791371703148\n",
      "Epoch 4 Batch: 391 Train Loss: 0.38734960556030273\n",
      "Epoch 4 Batch: 392 Train Loss: 0.0240081325173378\n",
      "Epoch 4 Batch: 393 Train Loss: 0.02512170374393463\n",
      "Epoch 4 Batch: 394 Train Loss: 0.02204747125506401\n",
      "Epoch 4 Batch: 395 Train Loss: 0.023180430755019188\n",
      "Epoch 4 Batch: 396 Train Loss: 0.3873042166233063\n",
      "Epoch 4 Batch: 397 Train Loss: 0.021495485678315163\n",
      "Epoch 4 Batch: 398 Train Loss: 0.018987225368618965\n",
      "Epoch 4 Batch: 399 Train Loss: 0.41775640845298767\n",
      "Epoch 4 Batch: 400 Train Loss: 0.022286858409643173\n",
      "Epoch 4 Batch: 401 Train Loss: 0.3816213011741638\n",
      "Epoch 4 Batch: 402 Train Loss: 0.020543385297060013\n",
      "Epoch 4 Batch: 403 Train Loss: 0.37146297097206116\n",
      "Epoch 4 Batch: 404 Train Loss: 0.026259660720825195\n",
      "Epoch 4 Batch: 405 Train Loss: 0.020804448053240776\n",
      "Epoch 4 Batch: 406 Train Loss: 0.030656013637781143\n",
      "Epoch 4 Batch: 407 Train Loss: 0.3582307994365692\n",
      "Epoch 4 Batch: 408 Train Loss: 0.02622159756720066\n",
      "Epoch 4 Batch: 409 Train Loss: 0.034133777022361755\n",
      "Epoch 4 Batch: 410 Train Loss: 0.0250701941549778\n",
      "Epoch 4 Batch: 411 Train Loss: 0.03501720726490021\n",
      "Epoch 4 Batch: 412 Train Loss: 0.03064984641969204\n",
      "Epoch 4 Batch: 413 Train Loss: 0.032067008316516876\n",
      "Epoch 4 Batch: 414 Train Loss: 0.01808684505522251\n",
      "Epoch 4 Batch: 415 Train Loss: 0.02253330871462822\n",
      "Epoch 4 Batch: 416 Train Loss: 0.02699722908437252\n",
      "Epoch 4 Batch: 417 Train Loss: 0.022332124412059784\n",
      "Epoch 4 Batch: 418 Train Loss: 0.02437824010848999\n",
      "Epoch 4 Batch: 419 Train Loss: 0.019987937062978745\n",
      "Epoch 4 Batch: 420 Train Loss: 0.020216558128595352\n",
      "Epoch 4 Batch: 421 Train Loss: 0.016384592279791832\n",
      "Epoch 4 Batch: 422 Train Loss: 0.016708621755242348\n",
      "Epoch 4 Batch: 423 Train Loss: 0.011994985863566399\n",
      "Epoch 4 Batch: 424 Train Loss: 0.3958016037940979\n",
      "Epoch 4 Batch: 425 Train Loss: 0.016542430967092514\n",
      "Epoch 4 Batch: 426 Train Loss: 0.015078434720635414\n",
      "Epoch 4 Batch: 427 Train Loss: 0.012467196211218834\n",
      "Epoch 4 Batch: 428 Train Loss: 0.013153312727808952\n",
      "Epoch 4 Batch: 429 Train Loss: 0.010570899583399296\n",
      "Epoch 4 Batch: 430 Train Loss: 0.015362371690571308\n",
      "Epoch 4 Batch: 431 Train Loss: 0.014654691331088543\n",
      "Epoch 4 Batch: 432 Train Loss: 0.015047241933643818\n",
      "Epoch 4 Batch: 433 Train Loss: 0.013463023118674755\n",
      "Epoch 4 Batch: 434 Train Loss: 0.01046718005090952\n",
      "Epoch 4 Batch: 435 Train Loss: 0.008971931412816048\n",
      "Epoch 4 Batch: 436 Train Loss: 0.00871356762945652\n",
      "Epoch 4 Batch: 437 Train Loss: 0.008174905553460121\n",
      "Epoch 4 Batch: 438 Train Loss: 0.007194695062935352\n",
      "Epoch 4 Batch: 439 Train Loss: 0.009506876580417156\n",
      "Epoch 4 Batch: 440 Train Loss: 0.00839504785835743\n",
      "Epoch 4 Batch: 441 Train Loss: 0.006954300217330456\n",
      "Epoch 4 Batch: 442 Train Loss: 0.00626097759231925\n",
      "Epoch 4 Batch: 443 Train Loss: 0.006271379999816418\n",
      "Epoch 4 Batch: 444 Train Loss: 0.005536912940442562\n",
      "Epoch 4 Batch: 445 Train Loss: 0.46266570687294006\n",
      "Epoch 4 Batch: 446 Train Loss: 0.006417851895093918\n",
      "Epoch 4 Batch: 447 Train Loss: 0.007529045455157757\n",
      "Epoch 4 Batch: 448 Train Loss: 0.0056053949519991875\n",
      "Epoch 4 Batch: 449 Train Loss: 0.007762822322547436\n",
      "Epoch 4 Batch: 450 Train Loss: 0.006488659884780645\n",
      "Epoch 4 Batch: 451 Train Loss: 0.008157344534993172\n",
      "Epoch 4 Batch: 452 Train Loss: 0.005328295286744833\n",
      "Epoch 4 Batch: 453 Train Loss: 0.006101556122303009\n",
      "Epoch 4 Batch: 454 Train Loss: 0.008760393597185612\n",
      "Epoch 4 Batch: 455 Train Loss: 0.006757517345249653\n",
      "Epoch 4 Batch: 456 Train Loss: 0.5600985288619995\n",
      "Epoch 4 Batch: 457 Train Loss: 0.006610158830881119\n",
      "Epoch 4 Batch: 458 Train Loss: 0.00799497589468956\n",
      "Epoch 4 Batch: 459 Train Loss: 0.00606965646147728\n",
      "Epoch 4 Batch: 460 Train Loss: 0.007572778966277838\n",
      "Epoch 4 Batch: 461 Train Loss: 0.008802585303783417\n",
      "Epoch 4 Batch: 462 Train Loss: 0.008785062469542027\n",
      "Epoch 4 Batch: 463 Train Loss: 0.007777763996273279\n",
      "Epoch 4 Batch: 464 Train Loss: 0.006513369269669056\n",
      "Epoch 4 Batch: 465 Train Loss: 0.00805851723998785\n",
      "Epoch 4 Batch: 466 Train Loss: 0.006848796270787716\n",
      "Epoch 4 Batch: 467 Train Loss: 0.006754872389137745\n",
      "Epoch 4 Batch: 468 Train Loss: 0.007134974002838135\n",
      "Epoch 4 Batch: 469 Train Loss: 0.4732971787452698\n",
      "Epoch 4 Batch: 470 Train Loss: 0.006262047681957483\n",
      "Epoch 4 Batch: 471 Train Loss: 0.007421462796628475\n",
      "Epoch 4 Batch: 472 Train Loss: 0.008623277768492699\n",
      "Epoch 4 Batch: 473 Train Loss: 0.007129599805921316\n",
      "Epoch 4 Batch: 474 Train Loss: 0.007235606200993061\n",
      "Epoch 4 Batch: 475 Train Loss: 0.5352333188056946\n",
      "Epoch 4 Batch: 476 Train Loss: 0.008143095299601555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Batch: 477 Train Loss: 0.0073365261778235435\n",
      "Epoch 4 Batch: 478 Train Loss: 0.007580970413982868\n",
      "Epoch 4 Batch: 479 Train Loss: 0.008386889472603798\n",
      "Epoch 4 Batch: 480 Train Loss: 0.007863141596317291\n",
      "Epoch 4 Batch: 481 Train Loss: 0.007947414182126522\n",
      "Epoch 4 Batch: 482 Train Loss: 0.008098159916698933\n",
      "Epoch 4 Batch: 483 Train Loss: 0.009331243112683296\n",
      "Epoch 4 Batch: 484 Train Loss: 0.014774657785892487\n",
      "Epoch 4 Batch: 485 Train Loss: 0.009358825162053108\n",
      "Epoch 4 Batch: 486 Train Loss: 0.01365751214325428\n",
      "Epoch 4 Batch: 487 Train Loss: 0.008166155777871609\n",
      "Epoch 4 Batch: 488 Train Loss: 0.009847353212535381\n",
      "Epoch 4 Batch: 489 Train Loss: 0.006547875702381134\n",
      "Epoch 4 Batch: 490 Train Loss: 0.007834351621568203\n",
      "Epoch 4 Batch: 491 Train Loss: 0.00904439389705658\n",
      "Epoch 4 Batch: 492 Train Loss: 0.008413090370595455\n",
      "Epoch 4 Batch: 493 Train Loss: 0.009048636071383953\n",
      "Epoch 4 Batch: 494 Train Loss: 0.0077638812363147736\n",
      "Epoch 4 Batch: 495 Train Loss: 0.007740039378404617\n",
      "Epoch 4 Batch: 496 Train Loss: 0.009449304081499577\n",
      "Epoch 4 Batch: 497 Train Loss: 0.009182094596326351\n",
      "Epoch 4 Batch: 498 Train Loss: 0.006485443562269211\n",
      "Epoch 4 Batch: 499 Train Loss: 0.007080102805048227\n",
      "Epoch 4 Batch: 500 Train Loss: 0.007243948522955179\n",
      "Epoch 4 Batch: 501 Train Loss: 0.006661576684564352\n",
      "Epoch 4 Batch: 502 Train Loss: 0.007148669566959143\n",
      "Epoch 4 Batch: 503 Train Loss: 0.005094569176435471\n",
      "Epoch 4 Batch: 504 Train Loss: 0.006379136350005865\n",
      "Epoch 4 Batch: 505 Train Loss: 0.004632056690752506\n",
      "Epoch 4 Batch: 506 Train Loss: 0.005834943614900112\n",
      "Epoch 4 Batch: 507 Train Loss: 0.005362941883504391\n",
      "Epoch 4 Batch: 508 Train Loss: 0.005896718241274357\n",
      "Epoch 4 Batch: 509 Train Loss: 0.005443505011498928\n",
      "Epoch 4 Batch: 510 Train Loss: 0.004219447262585163\n",
      "Epoch 4 Batch: 511 Train Loss: 0.0038446064572781324\n",
      "Epoch 4 Batch: 512 Train Loss: 1.0698604583740234\n",
      "Epoch 4 Batch: 513 Train Loss: 0.005124832037836313\n",
      "Epoch 4 Batch: 514 Train Loss: 0.006311011966317892\n",
      "Epoch 4 Batch: 515 Train Loss: 0.00947935413569212\n",
      "Epoch 4 Batch: 516 Train Loss: 0.005666575860232115\n",
      "Epoch 4 Batch: 517 Train Loss: 0.006760238204151392\n",
      "Epoch 4 Batch: 518 Train Loss: 0.48227787017822266\n",
      "Epoch 4 Batch: 519 Train Loss: 0.006676717661321163\n",
      "Epoch 4 Batch: 520 Train Loss: 0.00754782697185874\n",
      "Epoch 4 Batch: 521 Train Loss: 0.008011429570615292\n",
      "Epoch 4 Batch: 522 Train Loss: 0.007493368349969387\n",
      "Epoch 4 Batch: 523 Train Loss: 0.008286634460091591\n",
      "Epoch 4 Batch: 524 Train Loss: 0.00885448232293129\n",
      "Epoch 4 Batch: 525 Train Loss: 0.010556251741945744\n",
      "Epoch 4 Batch: 526 Train Loss: 0.008898355066776276\n",
      "Epoch 4 Batch: 527 Train Loss: 0.009739959612488747\n",
      "Epoch 4 Batch: 528 Train Loss: 0.010358599945902824\n",
      "Epoch 4 Batch: 529 Train Loss: 0.007160757668316364\n",
      "Epoch 4 Batch: 530 Train Loss: 0.008999992161989212\n",
      "Epoch 4 Batch: 531 Train Loss: 0.01082424633204937\n",
      "Epoch 4 Batch: 532 Train Loss: 0.01054986659437418\n",
      "Epoch 4 Batch: 533 Train Loss: 0.0055648284032940865\n",
      "Epoch 4 Batch: 534 Train Loss: 0.009240128099918365\n",
      "Epoch 4 Batch: 535 Train Loss: 0.0075510418973863125\n",
      "Epoch 4 Batch: 536 Train Loss: 0.008194549940526485\n",
      "Epoch 4 Batch: 537 Train Loss: 0.007477837614715099\n",
      "Epoch 4 Batch: 538 Train Loss: 0.009993897750973701\n",
      "Epoch 4 Batch: 539 Train Loss: 0.007719635032117367\n",
      "Epoch 4 Batch: 540 Train Loss: 0.00799024011939764\n",
      "Epoch 4 Batch: 541 Train Loss: 0.007756548933684826\n",
      "Epoch 4 Batch: 542 Train Loss: 0.005467688664793968\n",
      "Epoch 4 Batch: 543 Train Loss: 0.007192100398242474\n",
      "Epoch 4 Batch: 544 Train Loss: 0.007071985397487879\n",
      "Epoch 4 Batch: 545 Train Loss: 0.006333872675895691\n",
      "Epoch 4 Batch: 546 Train Loss: 0.005039296578615904\n",
      "Epoch 4 Batch: 547 Train Loss: 0.006338872015476227\n",
      "Epoch 4 Batch: 548 Train Loss: 0.004525249358266592\n",
      "Epoch 4 Batch: 549 Train Loss: 0.0070312777534127235\n",
      "Epoch 4 Batch: 550 Train Loss: 0.08439796417951584\n",
      "Epoch 4 Batch: 551 Train Loss: 0.48403310775756836\n",
      "Epoch 4 Batch: 552 Train Loss: 0.006435629911720753\n",
      "Epoch 4 Batch: 553 Train Loss: 0.007533153984695673\n",
      "Epoch 4 Batch: 554 Train Loss: 1.0164353847503662\n",
      "Epoch 4 Batch: 555 Train Loss: 0.45452943444252014\n",
      "Epoch 4 Batch: 556 Train Loss: 0.010127370245754719\n",
      "Epoch 4 Batch: 557 Train Loss: 0.012877749279141426\n",
      "Epoch 4 Batch: 558 Train Loss: 0.014448544010519981\n",
      "Epoch 4 Batch: 559 Train Loss: 0.019134145230054855\n",
      "Epoch 4 Batch: 560 Train Loss: 0.01955876313149929\n",
      "Epoch 4 Batch: 561 Train Loss: 0.01891864836215973\n",
      "Epoch 4 Batch: 562 Train Loss: 0.024985844269394875\n",
      "Epoch 4 Batch: 563 Train Loss: 0.02113860845565796\n",
      "Epoch 4 Batch: 564 Train Loss: 0.025357935577630997\n",
      "Epoch 4 Batch: 565 Train Loss: 0.022543082013726234\n",
      "Epoch 4 Batch: 566 Train Loss: 0.018038872629404068\n",
      "Epoch 4 Batch: 567 Train Loss: 0.019518902525305748\n",
      "Epoch 4 Batch: 568 Train Loss: 0.8312420845031738\n",
      "Epoch 4 Batch: 569 Train Loss: 0.020650027319788933\n",
      "Epoch 4 Batch: 570 Train Loss: 0.021058497950434685\n",
      "Epoch 4 Batch: 571 Train Loss: 0.020551618188619614\n",
      "Epoch 4 Batch: 572 Train Loss: 0.018429405987262726\n",
      "Epoch 4 Batch: 573 Train Loss: 0.02133001759648323\n",
      "Epoch 4 Batch: 574 Train Loss: 0.017999548465013504\n",
      "Epoch 4 Batch: 575 Train Loss: 0.019825030118227005\n",
      "Epoch 4 Batch: 576 Train Loss: 0.015667414292693138\n",
      "Epoch 4 Batch: 577 Train Loss: 0.018895532935857773\n",
      "Epoch 4 Batch: 578 Train Loss: 0.0164406169205904\n",
      "Epoch 4 Batch: 579 Train Loss: 0.017737364396452904\n",
      "Epoch 4 Batch: 580 Train Loss: 0.015361080877482891\n",
      "Epoch 4 Batch: 581 Train Loss: 0.42635035514831543\n",
      "Epoch 4 Batch: 582 Train Loss: 0.01633528433740139\n",
      "Epoch 4 Batch: 583 Train Loss: 0.016020232811570168\n",
      "Epoch 4 Batch: 584 Train Loss: 0.012642884626984596\n",
      "Epoch 4 Batch: 585 Train Loss: 0.012583298608660698\n",
      "Epoch 4 Batch: 586 Train Loss: 0.015256819315254688\n",
      "Epoch 4 Batch: 587 Train Loss: 0.012003762647509575\n",
      "Epoch 4 Batch: 588 Train Loss: 0.012926079332828522\n",
      "Epoch 4 Batch: 589 Train Loss: 0.011594000272452831\n",
      "Epoch 4 Batch: 590 Train Loss: 0.012238173745572567\n",
      "Epoch 4 Batch: 591 Train Loss: 0.011681627482175827\n",
      "Epoch 4 Batch: 592 Train Loss: 0.009928581304848194\n",
      "Epoch 4 Batch: 593 Train Loss: 0.009690029546618462\n",
      "Epoch 4 Batch: 594 Train Loss: 0.010422232560813427\n",
      "Epoch 4 Batch: 595 Train Loss: 0.01133735291659832\n",
      "Epoch 4 Batch: 596 Train Loss: 0.009745815768837929\n",
      "Epoch 4 Batch: 597 Train Loss: 0.009224338456988335\n",
      "Epoch 4 Batch: 598 Train Loss: 0.009224221110343933\n",
      "Epoch 4 Batch: 599 Train Loss: 0.008483903482556343\n",
      "Epoch 4 Batch: 600 Train Loss: 0.03303401172161102\n",
      "Epoch 4 Batch: 601 Train Loss: 0.007428796496242285\n",
      "Epoch 4 Batch: 602 Train Loss: 0.008920513093471527\n",
      "Epoch 4 Batch: 603 Train Loss: 0.009135709144175053\n",
      "Epoch 4 Batch: 604 Train Loss: 0.008826220408082008\n",
      "Epoch 4 Batch: 605 Train Loss: 0.007530407048761845\n",
      "Epoch 4 Batch: 606 Train Loss: 0.008094033226370811\n",
      "Epoch 4 Batch: 607 Train Loss: 0.008508597500622272\n",
      "Epoch 4 Batch: 608 Train Loss: 0.007690258324146271\n",
      "Epoch 4 Batch: 609 Train Loss: 0.008105472661554813\n",
      "Epoch 4 Batch: 610 Train Loss: 0.007277691271156073\n",
      "Epoch 4 Batch: 611 Train Loss: 0.0064284042455255985\n",
      "Epoch 4 Batch: 612 Train Loss: 0.006646439433097839\n",
      "Epoch 4 Batch: 613 Train Loss: 0.0060320207849144936\n",
      "Epoch 4 Batch: 614 Train Loss: 0.006260955240577459\n",
      "Epoch 4 Batch: 615 Train Loss: 0.004213361069560051\n",
      "Epoch 4 Batch: 616 Train Loss: 0.02171987295150757\n",
      "Epoch 4 Batch: 617 Train Loss: 0.006361458450555801\n",
      "Epoch 4 Batch: 618 Train Loss: 0.005767744965851307\n",
      "Epoch 4 Batch: 619 Train Loss: 0.005151510704308748\n",
      "Epoch 4 Batch: 620 Train Loss: 0.004752780310809612\n",
      "Epoch 4 Batch: 621 Train Loss: 0.006140626966953278\n",
      "Epoch 4 Batch: 622 Train Loss: 0.004098518751561642\n",
      "Epoch 4 Batch: 623 Train Loss: 0.005902583245187998\n",
      "Epoch 4 Batch: 624 Train Loss: 0.00552974920719862\n",
      "Epoch 4 Batch: 625 Train Loss: 0.00493232486769557\n",
      "Epoch 4 Batch: 626 Train Loss: 0.005410001613199711\n",
      "Epoch 4 Batch: 627 Train Loss: 0.004157034680247307\n",
      "Epoch 4 Batch: 628 Train Loss: 0.004706203006207943\n",
      "Epoch 4 Batch: 629 Train Loss: 0.5260371565818787\n",
      "Epoch 4 Batch: 630 Train Loss: 0.5427766442298889\n",
      "Epoch 4 Batch: 631 Train Loss: 0.5348483324050903\n",
      "Epoch 4 Batch: 632 Train Loss: 0.006098310928791761\n",
      "Epoch 4 Batch: 633 Train Loss: 0.006181688513606787\n",
      "Epoch 4 Batch: 634 Train Loss: 0.007629318628460169\n",
      "Epoch 4 Batch: 635 Train Loss: 0.007616755552589893\n",
      "Epoch 4 Batch: 636 Train Loss: 0.008768838830292225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Batch: 637 Train Loss: 0.009560264647006989\n",
      "Epoch 4 Batch: 638 Train Loss: 0.009927205741405487\n",
      "Epoch 4 Batch: 639 Train Loss: 0.010241634212434292\n",
      "Epoch 4 Batch: 640 Train Loss: 0.008439334109425545\n",
      "Epoch 4 Batch: 641 Train Loss: 0.009702911600470543\n",
      "Epoch 4 Batch: 642 Train Loss: 0.008813779801130295\n",
      "Epoch 4 Batch: 643 Train Loss: 0.009995242580771446\n",
      "Epoch 4 Batch: 644 Train Loss: 0.011162573471665382\n",
      "Epoch 4 Batch: 645 Train Loss: 0.009786268696188927\n",
      "Epoch 4 Batch: 646 Train Loss: 0.010207719169557095\n",
      "Epoch 4 Batch: 647 Train Loss: 0.009743648581206799\n",
      "Epoch 4 Batch: 648 Train Loss: 0.009622490033507347\n",
      "Epoch 4 Batch: 649 Train Loss: 0.01055431179702282\n",
      "Epoch 4 Batch: 650 Train Loss: 0.010350385680794716\n",
      "Epoch 4 Batch: 651 Train Loss: 0.008999449200928211\n",
      "Epoch 4 Batch: 652 Train Loss: 0.008774657733738422\n",
      "Epoch 4 Batch: 653 Train Loss: 0.4705159664154053\n",
      "Epoch 4 Batch: 654 Train Loss: 0.006552876438945532\n",
      "Epoch 4 Batch: 655 Train Loss: 0.008833745494484901\n",
      "Epoch 4 Batch: 656 Train Loss: 0.008148254826664925\n",
      "Epoch 4 Batch: 657 Train Loss: 0.008594563230872154\n",
      "Epoch 4 Batch: 658 Train Loss: 0.47590112686157227\n",
      "Epoch 4 Batch: 659 Train Loss: 0.010710255242884159\n",
      "Epoch 4 Batch: 660 Train Loss: 0.008342132903635502\n",
      "Epoch 4 Batch: 661 Train Loss: 0.011593464761972427\n",
      "Epoch 4 Batch: 662 Train Loss: 0.010209428146481514\n",
      "Epoch 4 Batch: 663 Train Loss: 0.008381051011383533\n",
      "Epoch 4 Batch: 664 Train Loss: 0.01100966241210699\n",
      "Epoch 4 Batch: 665 Train Loss: 0.01154155470430851\n",
      "Epoch 4 Batch: 666 Train Loss: 0.010733095929026604\n",
      "Epoch 4 Batch: 667 Train Loss: 0.46347203850746155\n",
      "Epoch 4 Batch: 668 Train Loss: 0.009322875179350376\n",
      "Epoch 4 Batch: 669 Train Loss: 0.012292042374610901\n",
      "Epoch 4 Batch: 670 Train Loss: 0.008873981423676014\n",
      "Epoch 4 Batch: 671 Train Loss: 0.012820775620639324\n",
      "Epoch 4 Batch: 672 Train Loss: 0.44629210233688354\n",
      "Epoch 4 Batch: 673 Train Loss: 0.010680342093110085\n",
      "Epoch 4 Batch: 674 Train Loss: 0.012445452623069286\n",
      "Epoch 4 Batch: 675 Train Loss: 0.012784494087100029\n",
      "Epoch 4 Batch: 676 Train Loss: 0.011734208092093468\n",
      "Epoch 4 Batch: 677 Train Loss: 0.013167178258299828\n",
      "Epoch 4 Batch: 678 Train Loss: 0.013336211442947388\n",
      "Epoch 4 Batch: 679 Train Loss: 0.013245709240436554\n",
      "Epoch 4 Batch: 680 Train Loss: 0.014497330412268639\n",
      "Epoch 4 Batch: 681 Train Loss: 0.014350381679832935\n",
      "Epoch 4 Batch: 682 Train Loss: 0.012696568854153156\n",
      "Epoch 4 Batch: 683 Train Loss: 0.013868488371372223\n",
      "Epoch 4 Batch: 684 Train Loss: 0.011948256753385067\n",
      "Epoch 4 Batch: 685 Train Loss: 0.012914126738905907\n",
      "Epoch 4 Batch: 686 Train Loss: 0.012542201206088066\n",
      "Epoch 4 Batch: 687 Train Loss: 0.009624497964978218\n",
      "Epoch 4 Batch: 688 Train Loss: 0.008183272555470467\n",
      "Epoch 4 Batch: 689 Train Loss: 0.011382075026631355\n",
      "Epoch 4 Batch: 690 Train Loss: 0.0076256706379354\n",
      "Epoch 4 Batch: 691 Train Loss: 0.00935293361544609\n",
      "Epoch 4 Batch: 692 Train Loss: 0.4716457426548004\n",
      "Epoch 4 Batch: 693 Train Loss: 0.010062533430755138\n",
      "Epoch 4 Batch: 694 Train Loss: 0.009066984057426453\n",
      "Epoch 4 Batch: 695 Train Loss: 0.009076414629817009\n",
      "Epoch 4 Batch: 696 Train Loss: 0.010118360631167889\n",
      "Epoch 4 Batch: 697 Train Loss: 0.008995932526886463\n",
      "Epoch 4 Batch: 698 Train Loss: 0.009779900312423706\n",
      "Epoch 4 Batch: 699 Train Loss: 0.009422482922673225\n",
      "Epoch 4 Batch: 700 Train Loss: 0.008648174814879894\n",
      "Epoch 4 Batch: 701 Train Loss: 0.008873351849615574\n",
      "Epoch 4 Batch: 702 Train Loss: 0.009121467359364033\n",
      "Epoch 4 Batch: 703 Train Loss: 0.006197488866746426\n",
      "Epoch 4 Batch: 704 Train Loss: 0.008526131510734558\n",
      "Epoch 4 Batch: 705 Train Loss: 0.008283021859824657\n",
      "Epoch 4 Batch: 706 Train Loss: 0.49829354882240295\n",
      "Epoch 4 Batch: 707 Train Loss: 0.00707461591809988\n",
      "Epoch 4 Batch: 708 Train Loss: 0.008203436620533466\n",
      "Epoch 4 Batch: 709 Train Loss: 0.007403012365102768\n",
      "Epoch 4 Batch: 710 Train Loss: 0.007286661304533482\n",
      "Epoch 4 Batch: 711 Train Loss: 0.00834181159734726\n",
      "Epoch 4 Batch: 712 Train Loss: 0.008504866622388363\n",
      "Epoch 4 Batch: 713 Train Loss: 0.00830916129052639\n",
      "Epoch 4 Batch: 714 Train Loss: 0.007368277758359909\n",
      "Epoch 4 Batch: 715 Train Loss: 0.006857939064502716\n",
      "Epoch 4 Batch: 716 Train Loss: 0.008001664653420448\n",
      "Epoch 4 Batch: 717 Train Loss: 0.005465355701744556\n",
      "Epoch 4 Batch: 718 Train Loss: 0.007693842984735966\n",
      "Epoch 4 Batch: 719 Train Loss: 0.0074781812727451324\n",
      "Epoch 4 Batch: 720 Train Loss: 0.007347488310188055\n",
      "Epoch 4 Batch: 721 Train Loss: 0.007147054187953472\n",
      "Epoch 4 Batch: 722 Train Loss: 0.00627049058675766\n",
      "Epoch 4 Batch: 723 Train Loss: 0.5056257247924805\n",
      "Epoch 4 Batch: 724 Train Loss: 0.006980834994465113\n",
      "Epoch 4 Batch: 725 Train Loss: 0.007136079482734203\n",
      "Epoch 4 Batch: 726 Train Loss: 0.007150930352509022\n",
      "Epoch 4 Batch: 727 Train Loss: 0.005749435164034367\n",
      "Epoch 4 Batch: 728 Train Loss: 0.005000146105885506\n",
      "Epoch 4 Batch: 729 Train Loss: 0.007144549395889044\n",
      "Epoch 4 Batch: 730 Train Loss: 0.005453153979033232\n",
      "Epoch 4 Batch: 731 Train Loss: 0.5069112777709961\n",
      "Epoch 4 Batch: 732 Train Loss: 0.006789489183574915\n",
      "Epoch 4 Batch: 733 Train Loss: 0.007043822202831507\n",
      "Epoch 4 Batch: 734 Train Loss: 0.0063273729756474495\n",
      "Epoch 4 Batch: 735 Train Loss: 0.007423757575452328\n",
      "Epoch 4 Batch: 736 Train Loss: 0.006739859469234943\n",
      "Epoch 4 Batch: 737 Train Loss: 0.0076963515020906925\n",
      "Epoch 4 Batch: 738 Train Loss: 0.008478077128529549\n",
      "Epoch 4 Batch: 739 Train Loss: 0.005996529944241047\n",
      "Epoch 4 Batch: 740 Train Loss: 0.007694539614021778\n",
      "Epoch 4 Batch: 741 Train Loss: 0.007716822437942028\n",
      "Epoch 4 Batch: 742 Train Loss: 0.006729714572429657\n",
      "Epoch 4 Batch: 743 Train Loss: 0.008325861766934395\n",
      "Epoch 4 Batch: 744 Train Loss: 0.4881843626499176\n",
      "Epoch 4 Batch: 745 Train Loss: 0.9591907262802124\n",
      "Epoch 4 Batch: 746 Train Loss: 0.008689992129802704\n",
      "Epoch 4 Batch: 747 Train Loss: 0.010793885216116905\n",
      "Epoch 4 Batch: 748 Train Loss: 0.010793306864798069\n",
      "Epoch 4 Batch: 749 Train Loss: 0.011578410863876343\n",
      "Epoch 4 Batch: 750 Train Loss: 0.44674620032310486\n",
      "Epoch 4 Batch: 751 Train Loss: 0.013370226137340069\n",
      "Epoch 4 Batch: 752 Train Loss: 0.016342679038643837\n",
      "Epoch 4 Batch: 753 Train Loss: 0.01397611666470766\n",
      "Epoch 4 Batch: 754 Train Loss: 0.018288346007466316\n",
      "Epoch 4 Batch: 755 Train Loss: 0.019618025049567223\n",
      "Epoch 4 Batch: 756 Train Loss: 0.015341319143772125\n",
      "Epoch 4 Batch: 757 Train Loss: 0.019957629963755608\n",
      "Epoch 4 Batch: 758 Train Loss: 0.41053128242492676\n",
      "Epoch 4 Batch: 759 Train Loss: 0.02076251432299614\n",
      "Epoch 4 Batch: 760 Train Loss: 0.01924922503530979\n",
      "Epoch 4 Batch: 761 Train Loss: 0.01950668916106224\n",
      "Epoch 4 Batch: 762 Train Loss: 0.0173727348446846\n",
      "Epoch 4 Batch: 763 Train Loss: 0.01965162344276905\n",
      "Epoch 4 Batch: 764 Train Loss: 0.021511610597372055\n",
      "Epoch 4 Batch: 765 Train Loss: 0.018703904002904892\n",
      "Epoch 4 Batch: 766 Train Loss: 0.02123439684510231\n",
      "Epoch 4 Batch: 767 Train Loss: 0.017412526533007622\n",
      "Epoch 4 Batch: 768 Train Loss: 0.015877481549978256\n",
      "Epoch 4 Batch: 769 Train Loss: 0.018047889694571495\n",
      "Epoch 4 Batch: 770 Train Loss: 0.425741046667099\n",
      "Epoch 4 Batch: 771 Train Loss: 0.01559582818299532\n",
      "Epoch 4 Batch: 772 Train Loss: 0.01616501621901989\n",
      "Epoch 4 Batch: 773 Train Loss: 0.0153431948274374\n",
      "Epoch 4 Batch: 774 Train Loss: 0.013382134027779102\n",
      "Epoch 4 Batch: 775 Train Loss: 0.01299323420971632\n",
      "Epoch 4 Batch: 776 Train Loss: 0.015798170119524002\n",
      "Epoch 4 Batch: 777 Train Loss: 0.015286075882613659\n",
      "Epoch 4 Batch: 778 Train Loss: 0.013347258791327477\n",
      "Epoch 4 Batch: 779 Train Loss: 0.435608446598053\n",
      "Epoch 4 Batch: 780 Train Loss: 0.014186571352183819\n",
      "Epoch 4 Batch: 781 Train Loss: 0.014120643027126789\n",
      "Epoch 4 Batch: 782 Train Loss: 0.012280433438718319\n",
      "Epoch 4 Batch: 783 Train Loss: 0.010972406715154648\n",
      "Epoch 4 Batch: 784 Train Loss: 0.012303007766604424\n",
      "Epoch 4 Batch: 785 Train Loss: 0.012295294553041458\n",
      "Epoch 4 Batch: 786 Train Loss: 0.43218231201171875\n",
      "Epoch 4 Batch: 787 Train Loss: 0.0119474520906806\n",
      "Epoch 4 Batch: 788 Train Loss: 0.009354986250400543\n",
      "Epoch 4 Batch: 789 Train Loss: 0.4595918655395508\n",
      "Epoch 4 Batch: 790 Train Loss: 0.01490647904574871\n",
      "Epoch 4 Batch: 791 Train Loss: 0.013904960826039314\n",
      "Epoch 4 Batch: 792 Train Loss: 0.01403624564409256\n",
      "Epoch 4 Batch: 793 Train Loss: 0.013718920759856701\n",
      "Epoch 4 Batch: 794 Train Loss: 0.015671540051698685\n",
      "Epoch 4 Batch: 795 Train Loss: 0.014386611990630627\n",
      "Epoch 4 Batch: 796 Train Loss: 0.0166800357401371\n",
      "Epoch 4 Batch: 797 Train Loss: 0.014123382978141308\n",
      "Epoch 4 Batch: 798 Train Loss: 0.014601578935980797\n",
      "Epoch 4 Batch: 799 Train Loss: 0.013056332245469093\n",
      "Epoch 4 Batch: 800 Train Loss: 0.00984040554612875\n",
      "Epoch 4 Batch: 801 Train Loss: 0.01285366527736187\n",
      "Epoch 4 Batch: 802 Train Loss: 0.015446382574737072\n",
      "Epoch 4 Batch: 803 Train Loss: 0.012914593331515789\n",
      "Epoch 4 Batch: 804 Train Loss: 0.011369557119905949\n",
      "Epoch 4 Batch: 805 Train Loss: 0.012031638994812965\n",
      "Epoch 4 Batch: 806 Train Loss: 0.011658375151455402\n",
      "Epoch 4 Batch: 807 Train Loss: 0.47894397377967834\n",
      "Epoch 4 Batch: 808 Train Loss: 0.009913232177495956\n",
      "Epoch 4 Batch: 809 Train Loss: 0.010196994990110397\n",
      "Epoch 4 Batch: 810 Train Loss: 0.011080702766776085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Batch: 811 Train Loss: 0.011179287917912006\n",
      "Epoch 4 Batch: 812 Train Loss: 0.010388670489192009\n",
      "Epoch 4 Batch: 813 Train Loss: 0.01040983758866787\n",
      "Epoch 4 Batch: 814 Train Loss: 0.009243441745638847\n",
      "Epoch 4 Batch: 815 Train Loss: 0.00889967568218708\n",
      "Epoch 4 Batch: 816 Train Loss: 0.007994761690497398\n",
      "Epoch 4 Batch: 817 Train Loss: 0.009584948420524597\n",
      "Epoch 4 Batch: 818 Train Loss: 0.009395058266818523\n",
      "Epoch 4 Batch: 819 Train Loss: 0.007938927039504051\n",
      "Epoch 4 Batch: 820 Train Loss: 0.007773169316351414\n",
      "Epoch 4 Batch: 821 Train Loss: 0.008462246507406235\n",
      "Epoch 4 Batch: 822 Train Loss: 0.008134469389915466\n",
      "Epoch 4 Batch: 823 Train Loss: 0.007946593686938286\n",
      "Epoch 4 Batch: 824 Train Loss: 0.007595784962177277\n",
      "Epoch 4 Batch: 825 Train Loss: 0.007333159446716309\n",
      "Epoch 4 Batch: 826 Train Loss: 0.007104068994522095\n",
      "Epoch 4 Batch: 827 Train Loss: 0.005481657572090626\n",
      "Epoch 4 Batch: 828 Train Loss: 0.5097951889038086\n",
      "Epoch 4 Batch: 829 Train Loss: 0.005376162938773632\n",
      "Epoch 4 Batch: 830 Train Loss: 0.006828161422163248\n",
      "Epoch 4 Batch: 831 Train Loss: 0.006135307718068361\n",
      "Epoch 4 Batch: 832 Train Loss: 0.006848384626209736\n",
      "Epoch 4 Batch: 833 Train Loss: 0.0069564334116876125\n",
      "Epoch 4 Batch: 834 Train Loss: 0.006873766891658306\n",
      "Epoch 4 Batch: 835 Train Loss: 0.006186533719301224\n",
      "Epoch 4 Batch: 836 Train Loss: 0.005235631950199604\n",
      "Epoch 4 Batch: 837 Train Loss: 0.006887572351843119\n",
      "Epoch 4 Batch: 838 Train Loss: 0.0064004757441580296\n",
      "Epoch 4 Batch: 839 Train Loss: 0.006122334394603968\n",
      "Epoch 4 Batch: 840 Train Loss: 0.005004211328923702\n",
      "Epoch 4 Batch: 841 Train Loss: 0.005707995034754276\n",
      "Epoch 4 Batch: 842 Train Loss: 0.006158670876175165\n",
      "Epoch 4 Batch: 843 Train Loss: 0.0053712413646280766\n",
      "Epoch 4 Batch: 844 Train Loss: 0.005785837769508362\n",
      "Epoch 4 Batch: 845 Train Loss: 0.005771287716925144\n",
      "Epoch 4 Batch: 846 Train Loss: 0.005249566398561001\n",
      "Epoch 4 Batch: 847 Train Loss: 0.0050109741277992725\n",
      "Epoch 4 Batch: 848 Train Loss: 0.5173295736312866\n",
      "Epoch 4 Batch: 849 Train Loss: 0.005270799621939659\n",
      "Epoch 4 Batch: 850 Train Loss: 0.0061389729380607605\n",
      "Epoch 4 Batch: 851 Train Loss: 0.005466232541948557\n",
      "Epoch 4 Batch: 852 Train Loss: 0.005381947383284569\n",
      "Epoch 4 Batch: 853 Train Loss: 0.005250588990747929\n",
      "Epoch 4 Batch: 854 Train Loss: 0.005482856184244156\n",
      "Epoch 4 Batch: 855 Train Loss: 0.0052906726486980915\n",
      "Epoch 4 Batch: 856 Train Loss: 0.005151708610355854\n",
      "Epoch 4 Batch: 857 Train Loss: 0.005268602166324854\n",
      "Epoch 4 Batch: 858 Train Loss: 0.006156194023787975\n",
      "Epoch 4 Batch: 859 Train Loss: 0.005435073748230934\n",
      "Epoch 4 Batch: 860 Train Loss: 0.005838985089212656\n",
      "Epoch 4 Batch: 861 Train Loss: 0.004838462918996811\n",
      "Epoch 4 Batch: 862 Train Loss: 0.00412176176905632\n",
      "Epoch 4 Batch: 863 Train Loss: 0.005838845856487751\n",
      "Epoch 4 Batch: 864 Train Loss: 0.005513261072337627\n",
      "Epoch 4 Batch: 865 Train Loss: 0.004792273975908756\n",
      "Epoch 4 Batch: 866 Train Loss: 0.00554101075977087\n",
      "Epoch 4 Batch: 867 Train Loss: 0.005001376383006573\n",
      "Epoch 4 Batch: 868 Train Loss: 0.004664971027523279\n",
      "Epoch 4 Batch: 869 Train Loss: 0.004999526310712099\n",
      "Epoch 4 Batch: 870 Train Loss: 0.02753441594541073\n",
      "Epoch 4 Batch: 871 Train Loss: 0.0041188267059624195\n",
      "Epoch 4 Batch: 872 Train Loss: 0.003521741135045886\n",
      "Epoch 4 Batch: 873 Train Loss: 0.004667781759053469\n",
      "Epoch 4 Batch: 874 Train Loss: 0.5102744102478027\n",
      "Epoch 4 Batch: 875 Train Loss: 0.005657538305968046\n",
      "Epoch 4 Batch: 876 Train Loss: 0.004916845820844173\n",
      "Epoch 4 Batch: 877 Train Loss: 0.005492061376571655\n",
      "Epoch 4 Batch: 878 Train Loss: 0.005311339162290096\n",
      "Epoch 4 Batch: 879 Train Loss: 0.007569233886897564\n",
      "Epoch 4 Batch: 880 Train Loss: 0.005586809944361448\n",
      "Epoch 6 Batch: 1 Train Loss: 0.010172760114073753\n",
      "Epoch 6 Batch: 2 Train Loss: 0.008337591774761677\n",
      "Epoch 6 Batch: 3 Train Loss: 0.010196482762694359\n",
      "Epoch 6 Batch: 4 Train Loss: 0.010523100383579731\n",
      "Epoch 6 Batch: 5 Train Loss: 0.4713892340660095\n",
      "Epoch 6 Batch: 6 Train Loss: 0.010619089007377625\n",
      "Epoch 6 Batch: 7 Train Loss: 0.01119528990238905\n",
      "Epoch 6 Batch: 8 Train Loss: 0.011564306914806366\n",
      "Epoch 6 Batch: 9 Train Loss: 0.011765691451728344\n",
      "Epoch 6 Batch: 10 Train Loss: 0.011971733532845974\n",
      "Epoch 6 Batch: 11 Train Loss: 0.00950487982481718\n",
      "Epoch 6 Batch: 12 Train Loss: 0.010722849518060684\n",
      "Epoch 6 Batch: 13 Train Loss: 0.010660405270755291\n",
      "Epoch 6 Batch: 14 Train Loss: 0.010526464320719242\n",
      "Epoch 6 Batch: 15 Train Loss: 0.01036553829908371\n",
      "Epoch 6 Batch: 16 Train Loss: 0.011337767355144024\n",
      "Epoch 6 Batch: 17 Train Loss: 0.009965246543288231\n",
      "Epoch 6 Batch: 18 Train Loss: 0.009747110307216644\n",
      "Epoch 6 Batch: 19 Train Loss: 0.01052702497690916\n",
      "Epoch 6 Batch: 20 Train Loss: 0.010135059244930744\n",
      "Epoch 6 Batch: 21 Train Loss: 0.009902602061629295\n",
      "Epoch 6 Batch: 22 Train Loss: 0.009511852636933327\n",
      "Epoch 6 Batch: 23 Train Loss: 0.009312327019870281\n",
      "Epoch 6 Batch: 24 Train Loss: 0.007976402528584003\n",
      "Epoch 6 Batch: 25 Train Loss: 0.007814590819180012\n",
      "Epoch 6 Batch: 26 Train Loss: 0.007400781847536564\n",
      "Epoch 6 Batch: 27 Train Loss: 0.007875347509980202\n",
      "Epoch 6 Batch: 28 Train Loss: 0.007663295604288578\n",
      "Epoch 6 Batch: 29 Train Loss: 0.0059029702097177505\n",
      "Epoch 6 Batch: 30 Train Loss: 0.006401422433555126\n",
      "Epoch 6 Batch: 31 Train Loss: 0.006757804658263922\n",
      "Epoch 6 Batch: 32 Train Loss: 0.006575630512088537\n",
      "Epoch 6 Batch: 33 Train Loss: 0.006264899857342243\n",
      "Epoch 6 Batch: 34 Train Loss: 0.005457627587020397\n",
      "Epoch 6 Batch: 35 Train Loss: 0.005853885319083929\n",
      "Epoch 6 Batch: 36 Train Loss: 0.0050691463984549046\n",
      "Epoch 6 Batch: 37 Train Loss: 0.004850143101066351\n",
      "Epoch 6 Batch: 38 Train Loss: 0.004680526442825794\n",
      "Epoch 6 Batch: 39 Train Loss: 0.0045616887509822845\n",
      "Epoch 6 Batch: 40 Train Loss: 0.0039303600788116455\n",
      "Epoch 6 Batch: 41 Train Loss: 0.004755668807774782\n",
      "Epoch 6 Batch: 42 Train Loss: 0.5442604422569275\n",
      "Epoch 6 Batch: 43 Train Loss: 0.00471093226224184\n",
      "Epoch 6 Batch: 44 Train Loss: 0.003387728240340948\n",
      "Epoch 6 Batch: 45 Train Loss: 0.004412317648530006\n",
      "Epoch 6 Batch: 46 Train Loss: 0.0050223208963871\n",
      "Epoch 6 Batch: 47 Train Loss: 0.003533434122800827\n",
      "Epoch 6 Batch: 48 Train Loss: 0.00452388683333993\n",
      "Epoch 6 Batch: 49 Train Loss: 0.528727650642395\n",
      "Epoch 6 Batch: 50 Train Loss: 0.004864255432039499\n",
      "Epoch 6 Batch: 51 Train Loss: 0.004005054477602243\n",
      "Epoch 6 Batch: 52 Train Loss: 0.005061112344264984\n",
      "Epoch 6 Batch: 53 Train Loss: 0.0056883227080106735\n",
      "Epoch 6 Batch: 54 Train Loss: 0.006260579917579889\n",
      "Epoch 6 Batch: 55 Train Loss: 0.004411148838698864\n",
      "Epoch 6 Batch: 56 Train Loss: 0.005406975280493498\n",
      "Epoch 6 Batch: 57 Train Loss: 0.005214087665081024\n",
      "Epoch 6 Batch: 58 Train Loss: 0.0068840994499623775\n",
      "Epoch 6 Batch: 59 Train Loss: 0.006139741279184818\n",
      "Epoch 6 Batch: 60 Train Loss: 0.005118296481668949\n",
      "Epoch 6 Batch: 61 Train Loss: 0.004731790162622929\n",
      "Epoch 6 Batch: 62 Train Loss: 0.006151565350592136\n",
      "Epoch 6 Batch: 63 Train Loss: 0.006080444436520338\n",
      "Epoch 6 Batch: 64 Train Loss: 0.005503904074430466\n",
      "Epoch 6 Batch: 65 Train Loss: 0.00622207298874855\n",
      "Epoch 6 Batch: 66 Train Loss: 0.004846220836043358\n",
      "Epoch 6 Batch: 67 Train Loss: 0.006323082838207483\n",
      "Epoch 6 Batch: 68 Train Loss: 0.006183816120028496\n",
      "Epoch 6 Batch: 69 Train Loss: 0.005590143613517284\n",
      "Epoch 6 Batch: 70 Train Loss: 0.004757421091198921\n",
      "Epoch 6 Batch: 71 Train Loss: 0.0053232884965837\n",
      "Epoch 6 Batch: 72 Train Loss: 0.005498290527611971\n",
      "Epoch 6 Batch: 73 Train Loss: 0.0048931678757071495\n",
      "Epoch 6 Batch: 74 Train Loss: 0.0052529736422002316\n",
      "Epoch 6 Batch: 75 Train Loss: 0.004435866605490446\n",
      "Epoch 6 Batch: 76 Train Loss: 0.005232005380094051\n",
      "Epoch 6 Batch: 77 Train Loss: 0.0052791377529501915\n",
      "Epoch 6 Batch: 78 Train Loss: 0.0051455432549119\n",
      "Epoch 6 Batch: 79 Train Loss: 0.004635567776858807\n",
      "Epoch 6 Batch: 80 Train Loss: 0.5215803384780884\n",
      "Epoch 6 Batch: 81 Train Loss: 0.0041845799423754215\n",
      "Epoch 6 Batch: 82 Train Loss: 0.004509711172431707\n",
      "Epoch 6 Batch: 83 Train Loss: 0.004349785856902599\n",
      "Epoch 6 Batch: 84 Train Loss: 0.005460967775434256\n",
      "Epoch 6 Batch: 85 Train Loss: 0.005432561039924622\n",
      "Epoch 6 Batch: 86 Train Loss: 0.006458953022956848\n",
      "Epoch 6 Batch: 87 Train Loss: 0.005041494034230709\n",
      "Epoch 6 Batch: 88 Train Loss: 0.005637746304273605\n",
      "Epoch 6 Batch: 89 Train Loss: 0.005025804508477449\n",
      "Epoch 6 Batch: 90 Train Loss: 0.0043157241307199\n",
      "Epoch 6 Batch: 91 Train Loss: 0.003982569091022015\n",
      "Epoch 6 Batch: 92 Train Loss: 0.0043057771399617195\n",
      "Epoch 6 Batch: 93 Train Loss: 0.005471637472510338\n",
      "Epoch 6 Batch: 94 Train Loss: 0.004801376722753048\n",
      "Epoch 6 Batch: 95 Train Loss: 1.0715632438659668\n",
      "Epoch 6 Batch: 96 Train Loss: 0.00450002821162343\n",
      "Epoch 6 Batch: 97 Train Loss: 0.0053227851167321205\n",
      "Epoch 6 Batch: 98 Train Loss: 0.0076730698347091675\n",
      "Epoch 6 Batch: 99 Train Loss: 0.5374840497970581\n",
      "Epoch 6 Batch: 100 Train Loss: 0.5273865461349487\n",
      "Epoch 6 Batch: 101 Train Loss: 0.00829504244029522\n",
      "Epoch 6 Batch: 102 Train Loss: 0.010773609392344952\n",
      "Epoch 6 Batch: 103 Train Loss: 0.00989578291773796\n",
      "Epoch 6 Batch: 104 Train Loss: 0.010843081399798393\n",
      "Epoch 6 Batch: 105 Train Loss: 0.008147375658154488\n",
      "Epoch 6 Batch: 106 Train Loss: 0.011376937851309776\n",
      "Epoch 6 Batch: 107 Train Loss: 0.011245164088904858\n",
      "Epoch 6 Batch: 108 Train Loss: 0.012502634897828102\n",
      "Epoch 6 Batch: 109 Train Loss: 0.0154227614402771\n",
      "Epoch 6 Batch: 110 Train Loss: 0.014894001185894012\n",
      "Epoch 6 Batch: 111 Train Loss: 0.4438680112361908\n",
      "Epoch 6 Batch: 112 Train Loss: 0.015202648937702179\n",
      "Epoch 6 Batch: 113 Train Loss: 0.013131496496498585\n",
      "Epoch 6 Batch: 114 Train Loss: 0.01674092933535576\n",
      "Epoch 6 Batch: 115 Train Loss: 0.01400583516806364\n",
      "Epoch 6 Batch: 116 Train Loss: 0.017510682344436646\n",
      "Epoch 6 Batch: 117 Train Loss: 0.014270205982029438\n",
      "Epoch 6 Batch: 118 Train Loss: 0.013985445722937584\n",
      "Epoch 6 Batch: 119 Train Loss: 0.013934751972556114\n",
      "Epoch 6 Batch: 120 Train Loss: 0.01716996170580387\n",
      "Epoch 6 Batch: 121 Train Loss: 0.015101844444870949\n",
      "Epoch 6 Batch: 122 Train Loss: 0.013095103204250336\n",
      "Epoch 6 Batch: 123 Train Loss: 0.014210762456059456\n",
      "Epoch 6 Batch: 124 Train Loss: 0.015377379953861237\n",
      "Epoch 6 Batch: 125 Train Loss: 0.014767144806683064\n",
      "Epoch 6 Batch: 126 Train Loss: 0.012798605486750603\n",
      "Epoch 6 Batch: 127 Train Loss: 0.01227633561939001\n",
      "Epoch 6 Batch: 128 Train Loss: 0.011558232828974724\n",
      "Epoch 6 Batch: 129 Train Loss: 0.012554395012557507\n",
      "Epoch 6 Batch: 130 Train Loss: 0.008125634863972664\n",
      "Epoch 6 Batch: 131 Train Loss: 0.011830245144665241\n",
      "Epoch 6 Batch: 132 Train Loss: 0.010039759799838066\n",
      "Epoch 6 Batch: 133 Train Loss: 0.009715674445033073\n",
      "Epoch 6 Batch: 134 Train Loss: 0.009850994683802128\n",
      "Epoch 6 Batch: 135 Train Loss: 0.009512454271316528\n",
      "Epoch 6 Batch: 136 Train Loss: 0.012979291379451752\n",
      "Epoch 6 Batch: 137 Train Loss: 0.008361643180251122\n",
      "Epoch 6 Batch: 138 Train Loss: 0.007360049523413181\n",
      "Epoch 6 Batch: 139 Train Loss: 0.007315012160688639\n",
      "Epoch 6 Batch: 140 Train Loss: 0.008000066503882408\n",
      "Epoch 6 Batch: 141 Train Loss: 0.4875767230987549\n",
      "Epoch 6 Batch: 142 Train Loss: 0.007546502165496349\n",
      "Epoch 6 Batch: 143 Train Loss: 0.007267275359481573\n",
      "Epoch 6 Batch: 144 Train Loss: 0.007450401782989502\n",
      "Epoch 6 Batch: 145 Train Loss: 0.007433257065713406\n",
      "Epoch 6 Batch: 146 Train Loss: 0.005645035300403833\n",
      "Epoch 6 Batch: 147 Train Loss: 0.007567030843347311\n",
      "Epoch 6 Batch: 148 Train Loss: 0.006704976316541433\n",
      "Epoch 6 Batch: 149 Train Loss: 0.5160002112388611\n",
      "Epoch 6 Batch: 150 Train Loss: 0.00768328458070755\n",
      "Epoch 6 Batch: 151 Train Loss: 0.007905088365077972\n",
      "Epoch 6 Batch: 152 Train Loss: 0.0071480474434792995\n",
      "Epoch 6 Batch: 153 Train Loss: 0.007027911953628063\n",
      "Epoch 6 Batch: 154 Train Loss: 0.4738771319389343\n",
      "Epoch 6 Batch: 155 Train Loss: 0.007512806449085474\n",
      "Epoch 6 Batch: 156 Train Loss: 0.008145530708134174\n",
      "Epoch 6 Batch: 157 Train Loss: 0.009263193234801292\n",
      "Epoch 6 Batch: 158 Train Loss: 0.009372862055897713\n",
      "Epoch 6 Batch: 159 Train Loss: 0.010089030489325523\n",
      "Epoch 6 Batch: 160 Train Loss: 0.009039937518537045\n",
      "Epoch 6 Batch: 161 Train Loss: 0.008643019944429398\n",
      "Epoch 6 Batch: 162 Train Loss: 0.4868243634700775\n",
      "Epoch 6 Batch: 163 Train Loss: 0.011244081892073154\n",
      "Epoch 6 Batch: 164 Train Loss: 0.010904052294790745\n",
      "Epoch 6 Batch: 165 Train Loss: 0.010242506861686707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Batch: 166 Train Loss: 0.00841977447271347\n",
      "Epoch 6 Batch: 167 Train Loss: 0.011305948719382286\n",
      "Epoch 6 Batch: 168 Train Loss: 0.012372943572700024\n",
      "Epoch 6 Batch: 169 Train Loss: 0.011007258668541908\n",
      "Epoch 6 Batch: 170 Train Loss: 0.012298031710088253\n",
      "Epoch 6 Batch: 171 Train Loss: 0.010075571946799755\n",
      "Epoch 6 Batch: 172 Train Loss: 0.009907960891723633\n",
      "Epoch 6 Batch: 173 Train Loss: 0.011273456737399101\n",
      "Epoch 6 Batch: 174 Train Loss: 0.01081786211580038\n",
      "Epoch 6 Batch: 175 Train Loss: 0.4518871307373047\n",
      "Epoch 6 Batch: 176 Train Loss: 0.011995695531368256\n",
      "Epoch 6 Batch: 177 Train Loss: 0.009622100740671158\n",
      "Epoch 6 Batch: 178 Train Loss: 0.00994171667844057\n",
      "Epoch 6 Batch: 179 Train Loss: 0.01154136098921299\n",
      "Epoch 6 Batch: 180 Train Loss: 0.011505039408802986\n",
      "Epoch 6 Batch: 181 Train Loss: 0.010740597732365131\n",
      "Epoch 6 Batch: 182 Train Loss: 0.009981108829379082\n",
      "Epoch 6 Batch: 183 Train Loss: 0.4464770257472992\n",
      "Epoch 6 Batch: 184 Train Loss: 0.012507528066635132\n",
      "Epoch 6 Batch: 185 Train Loss: 0.011257616803050041\n",
      "Epoch 6 Batch: 186 Train Loss: 0.01084387768059969\n",
      "Epoch 6 Batch: 187 Train Loss: 0.011382361873984337\n",
      "Epoch 6 Batch: 188 Train Loss: 0.009522421285510063\n",
      "Epoch 6 Batch: 189 Train Loss: 0.009459573775529861\n",
      "Epoch 6 Batch: 190 Train Loss: 0.010354417376220226\n",
      "Epoch 6 Batch: 191 Train Loss: 0.009304920211434364\n",
      "Epoch 6 Batch: 192 Train Loss: 0.01196584664285183\n",
      "Epoch 6 Batch: 193 Train Loss: 0.01072699949145317\n",
      "Epoch 6 Batch: 194 Train Loss: 0.00912684015929699\n",
      "Epoch 6 Batch: 195 Train Loss: 0.012226002290844917\n",
      "Epoch 6 Batch: 196 Train Loss: 0.009431025944650173\n",
      "Epoch 6 Batch: 197 Train Loss: 0.009056644514203072\n",
      "Epoch 6 Batch: 198 Train Loss: 0.008108866401016712\n",
      "Epoch 6 Batch: 199 Train Loss: 0.007844729349017143\n",
      "Epoch 6 Batch: 200 Train Loss: 0.006890601012855768\n",
      "Epoch 6 Batch: 201 Train Loss: 0.5092038512229919\n",
      "Epoch 6 Batch: 202 Train Loss: 0.008301688358187675\n",
      "Epoch 6 Batch: 203 Train Loss: 0.008356636390089989\n",
      "Epoch 6 Batch: 204 Train Loss: 0.006888561882078648\n",
      "Epoch 6 Batch: 205 Train Loss: 0.005804932676255703\n",
      "Epoch 6 Batch: 206 Train Loss: 0.006889549549669027\n",
      "Epoch 6 Batch: 207 Train Loss: 0.00680422643199563\n",
      "Epoch 6 Batch: 208 Train Loss: 0.008914611302316189\n",
      "Epoch 6 Batch: 209 Train Loss: 0.007768815848976374\n",
      "Epoch 6 Batch: 210 Train Loss: 0.007958952337503433\n",
      "Epoch 6 Batch: 211 Train Loss: 0.5019857287406921\n",
      "Epoch 6 Batch: 212 Train Loss: 0.007253290619701147\n",
      "Epoch 6 Batch: 213 Train Loss: 0.009434374049305916\n",
      "Epoch 6 Batch: 214 Train Loss: 0.007884122431278229\n",
      "Epoch 6 Batch: 215 Train Loss: 0.009018163196742535\n",
      "Epoch 6 Batch: 216 Train Loss: 0.007235872559249401\n",
      "Epoch 6 Batch: 217 Train Loss: 0.007263797335326672\n",
      "Epoch 6 Batch: 218 Train Loss: 0.008387429639697075\n",
      "Epoch 6 Batch: 219 Train Loss: 0.008093947544693947\n",
      "Epoch 6 Batch: 220 Train Loss: 0.0076926290057599545\n",
      "Epoch 6 Batch: 221 Train Loss: 0.008987410925328732\n",
      "Epoch 6 Batch: 222 Train Loss: 0.007925082929432392\n",
      "Epoch 6 Batch: 223 Train Loss: 0.4863972067832947\n",
      "Epoch 6 Batch: 224 Train Loss: 0.007162054069340229\n",
      "Epoch 6 Batch: 225 Train Loss: 0.008241378702223301\n",
      "Epoch 6 Batch: 226 Train Loss: 0.47246089577674866\n",
      "Epoch 6 Batch: 227 Train Loss: 0.009780054911971092\n",
      "Epoch 6 Batch: 228 Train Loss: 0.010446969419717789\n",
      "Epoch 6 Batch: 229 Train Loss: 0.009988926351070404\n",
      "Epoch 6 Batch: 230 Train Loss: 0.009809238836169243\n",
      "Epoch 6 Batch: 231 Train Loss: 0.011299158446490765\n",
      "Epoch 6 Batch: 232 Train Loss: 0.011168870143592358\n",
      "Epoch 6 Batch: 233 Train Loss: 0.011233260855078697\n",
      "Epoch 6 Batch: 234 Train Loss: 0.011974756605923176\n",
      "Epoch 6 Batch: 235 Train Loss: 0.011567901819944382\n",
      "Epoch 6 Batch: 236 Train Loss: 0.010076972655951977\n",
      "Epoch 6 Batch: 237 Train Loss: 0.010039741173386574\n",
      "Epoch 6 Batch: 238 Train Loss: 0.012507108971476555\n",
      "Epoch 6 Batch: 239 Train Loss: 0.011403501033782959\n",
      "Epoch 6 Batch: 240 Train Loss: 0.00904913805425167\n",
      "Epoch 6 Batch: 241 Train Loss: 0.010036522522568703\n",
      "Epoch 6 Batch: 242 Train Loss: 0.00932348519563675\n",
      "Epoch 6 Batch: 243 Train Loss: 0.009513401426374912\n",
      "Epoch 6 Batch: 244 Train Loss: 0.008788211271166801\n",
      "Epoch 6 Batch: 245 Train Loss: 0.009264232590794563\n",
      "Epoch 6 Batch: 246 Train Loss: 0.007081204559653997\n",
      "Epoch 6 Batch: 247 Train Loss: 0.007683396339416504\n",
      "Epoch 6 Batch: 248 Train Loss: 0.008511084131896496\n",
      "Epoch 6 Batch: 249 Train Loss: 0.008473800495266914\n",
      "Epoch 6 Batch: 250 Train Loss: 0.008940480649471283\n",
      "Epoch 6 Batch: 251 Train Loss: 0.007272058166563511\n",
      "Epoch 6 Batch: 252 Train Loss: 0.5214798450469971\n",
      "Epoch 6 Batch: 253 Train Loss: 0.007079721428453922\n",
      "Epoch 6 Batch: 254 Train Loss: 0.006473271641880274\n",
      "Epoch 6 Batch: 255 Train Loss: 0.006520189344882965\n",
      "Epoch 6 Batch: 256 Train Loss: 0.007130274083465338\n",
      "Epoch 6 Batch: 257 Train Loss: 0.00709687452763319\n",
      "Epoch 6 Batch: 258 Train Loss: 0.00640612468123436\n",
      "Epoch 6 Batch: 259 Train Loss: 0.00785367377102375\n",
      "Epoch 6 Batch: 260 Train Loss: 0.006399586796760559\n",
      "Epoch 6 Batch: 261 Train Loss: 0.006326763425022364\n",
      "Epoch 6 Batch: 262 Train Loss: 0.006850588135421276\n",
      "Epoch 6 Batch: 263 Train Loss: 0.00754693616181612\n",
      "Epoch 6 Batch: 264 Train Loss: 0.006615933030843735\n",
      "Epoch 6 Batch: 265 Train Loss: 0.005890130065381527\n",
      "Epoch 6 Batch: 266 Train Loss: 0.006520185619592667\n",
      "Epoch 6 Batch: 267 Train Loss: 0.006699440535157919\n",
      "Epoch 6 Batch: 268 Train Loss: 0.005813615396618843\n",
      "Epoch 6 Batch: 269 Train Loss: 0.005813192576169968\n",
      "Epoch 6 Batch: 270 Train Loss: 0.006190169602632523\n",
      "Epoch 6 Batch: 271 Train Loss: 0.006155696697533131\n",
      "Epoch 6 Batch: 272 Train Loss: 0.004451327957212925\n",
      "Epoch 6 Batch: 273 Train Loss: 0.005071630235761404\n",
      "Epoch 6 Batch: 274 Train Loss: 0.004931914620101452\n",
      "Epoch 6 Batch: 275 Train Loss: 0.004905263893306255\n",
      "Epoch 6 Batch: 276 Train Loss: 0.004662369843572378\n",
      "Epoch 6 Batch: 277 Train Loss: 0.003537200391292572\n",
      "Epoch 6 Batch: 278 Train Loss: 0.004163886420428753\n",
      "Epoch 6 Batch: 279 Train Loss: 0.00463095773011446\n",
      "Epoch 6 Batch: 280 Train Loss: 0.004093598574399948\n",
      "Epoch 6 Batch: 281 Train Loss: 0.0038421829231083393\n",
      "Epoch 6 Batch: 282 Train Loss: 0.0034100015182048082\n",
      "Epoch 6 Batch: 283 Train Loss: 0.002625569701194763\n",
      "Epoch 6 Batch: 284 Train Loss: 0.0036248154938220978\n",
      "Epoch 6 Batch: 285 Train Loss: 0.5636687874794006\n",
      "Epoch 6 Batch: 286 Train Loss: 0.003909700084477663\n",
      "Epoch 6 Batch: 287 Train Loss: 0.0035364399664103985\n",
      "Epoch 6 Batch: 288 Train Loss: 0.5428668856620789\n",
      "Epoch 6 Batch: 289 Train Loss: 0.004260838031768799\n",
      "Epoch 6 Batch: 290 Train Loss: 0.0052212378941476345\n",
      "Epoch 6 Batch: 291 Train Loss: 0.004819220397621393\n",
      "Epoch 6 Batch: 292 Train Loss: 0.00569961266592145\n",
      "Epoch 6 Batch: 293 Train Loss: 0.5240497589111328\n",
      "Epoch 6 Batch: 294 Train Loss: 0.0064474670216441154\n",
      "Epoch 6 Batch: 295 Train Loss: 0.006216691341251135\n",
      "Epoch 6 Batch: 296 Train Loss: 0.006554469466209412\n",
      "Epoch 6 Batch: 297 Train Loss: 0.006176823750138283\n",
      "Epoch 6 Batch: 298 Train Loss: 0.008081451058387756\n",
      "Epoch 6 Batch: 299 Train Loss: 0.008365467190742493\n",
      "Epoch 6 Batch: 300 Train Loss: 0.008628386072814465\n",
      "Epoch 6 Batch: 301 Train Loss: 0.00791948102414608\n",
      "Epoch 6 Batch: 302 Train Loss: 0.007219702936708927\n",
      "Epoch 6 Batch: 303 Train Loss: 0.008228328078985214\n",
      "Epoch 6 Batch: 304 Train Loss: 0.010223574005067348\n",
      "Epoch 6 Batch: 305 Train Loss: 0.007312025874853134\n",
      "Epoch 6 Batch: 306 Train Loss: 0.008181978948414326\n",
      "Epoch 6 Batch: 307 Train Loss: 0.00634721526876092\n",
      "Epoch 6 Batch: 308 Train Loss: 0.007035739719867706\n",
      "Epoch 6 Batch: 309 Train Loss: 0.009049983695149422\n",
      "Epoch 6 Batch: 310 Train Loss: 0.008742520585656166\n",
      "Epoch 6 Batch: 311 Train Loss: 0.008660143241286278\n",
      "Epoch 6 Batch: 312 Train Loss: 0.008494772017002106\n",
      "Epoch 6 Batch: 313 Train Loss: 0.007401063106954098\n",
      "Epoch 6 Batch: 314 Train Loss: 0.008065531961619854\n",
      "Epoch 6 Batch: 315 Train Loss: 0.0062392777763307095\n",
      "Epoch 6 Batch: 316 Train Loss: 0.006184392608702183\n",
      "Epoch 6 Batch: 317 Train Loss: 0.006542460527271032\n",
      "Epoch 6 Batch: 318 Train Loss: 0.0067168413661420345\n",
      "Epoch 6 Batch: 319 Train Loss: 0.007164711598306894\n",
      "Epoch 6 Batch: 320 Train Loss: 0.006106735672801733\n",
      "Epoch 6 Batch: 321 Train Loss: 0.005977890454232693\n",
      "Epoch 6 Batch: 322 Train Loss: 0.005803713575005531\n",
      "Epoch 6 Batch: 323 Train Loss: 0.006145135499536991\n",
      "Epoch 6 Batch: 324 Train Loss: 0.005558055825531483\n",
      "Epoch 6 Batch: 325 Train Loss: 0.005301542114466429\n",
      "Epoch 6 Batch: 326 Train Loss: 0.0051885731518268585\n",
      "Epoch 6 Batch: 327 Train Loss: 0.00552446348592639\n",
      "Epoch 6 Batch: 328 Train Loss: 0.0053587607108056545\n",
      "Epoch 6 Batch: 329 Train Loss: 0.005371692590415478\n",
      "Epoch 6 Batch: 330 Train Loss: 0.005041531752794981\n",
      "Epoch 6 Batch: 331 Train Loss: 0.0039040595293045044\n",
      "Epoch 6 Batch: 332 Train Loss: 0.0032497600186616182\n",
      "Epoch 6 Batch: 333 Train Loss: 0.00473292451351881\n",
      "Epoch 6 Batch: 334 Train Loss: 0.004049565177410841\n",
      "Epoch 6 Batch: 335 Train Loss: 0.0038982569240033627\n",
      "Epoch 6 Batch: 336 Train Loss: 0.004390113987028599\n",
      "Epoch 6 Batch: 337 Train Loss: 0.004122267477214336\n",
      "Epoch 6 Batch: 338 Train Loss: 0.5591138601303101\n",
      "Epoch 6 Batch: 339 Train Loss: 0.5483033657073975\n",
      "Epoch 6 Batch: 340 Train Loss: 0.004137496463954449\n",
      "Epoch 6 Batch: 341 Train Loss: 0.0047123669646680355\n",
      "Epoch 6 Batch: 342 Train Loss: 0.00468075554817915\n",
      "Epoch 6 Batch: 343 Train Loss: 0.0044075800105929375\n",
      "Epoch 6 Batch: 344 Train Loss: 0.004580401815474033\n",
      "Epoch 6 Batch: 345 Train Loss: 0.005380214657634497\n",
      "Epoch 6 Batch: 346 Train Loss: 0.006192881613969803\n",
      "Epoch 6 Batch: 347 Train Loss: 0.5079678893089294\n",
      "Epoch 6 Batch: 348 Train Loss: 0.005470613017678261\n",
      "Epoch 6 Batch: 349 Train Loss: 0.006515759974718094\n",
      "Epoch 6 Batch: 350 Train Loss: 0.007167869247496128\n",
      "Epoch 6 Batch: 351 Train Loss: 0.0072873481549322605\n",
      "Epoch 6 Batch: 352 Train Loss: 0.007560254540294409\n",
      "Epoch 6 Batch: 353 Train Loss: 0.0068523637019097805\n",
      "Epoch 6 Batch: 354 Train Loss: 0.00875752605497837\n",
      "Epoch 6 Batch: 355 Train Loss: 0.00871726218611002\n",
      "Epoch 6 Batch: 356 Train Loss: 0.00817982666194439\n",
      "Epoch 6 Batch: 357 Train Loss: 0.009223920293152332\n",
      "Epoch 6 Batch: 358 Train Loss: 0.008017577230930328\n",
      "Epoch 6 Batch: 359 Train Loss: 0.007604865822941065\n",
      "Epoch 6 Batch: 360 Train Loss: 0.009548629634082317\n",
      "Epoch 6 Batch: 361 Train Loss: 0.008032122626900673\n",
      "Epoch 6 Batch: 362 Train Loss: 0.006731112953275442\n",
      "Epoch 6 Batch: 363 Train Loss: 0.008022027090191841\n",
      "Epoch 6 Batch: 364 Train Loss: 0.008378887549042702\n",
      "Epoch 6 Batch: 365 Train Loss: 0.008207594975829124\n",
      "Epoch 6 Batch: 366 Train Loss: 0.006720514502376318\n",
      "Epoch 6 Batch: 367 Train Loss: 0.4805184304714203\n",
      "Epoch 6 Batch: 368 Train Loss: 0.4760270118713379\n",
      "Epoch 6 Batch: 369 Train Loss: 0.007919792085886002\n",
      "Epoch 6 Batch: 370 Train Loss: 0.007771492935717106\n",
      "Epoch 6 Batch: 371 Train Loss: 0.010834706015884876\n",
      "Epoch 6 Batch: 372 Train Loss: 0.009643997997045517\n",
      "Epoch 6 Batch: 373 Train Loss: 0.009929539635777473\n",
      "Epoch 6 Batch: 374 Train Loss: 0.49146661162376404\n",
      "Epoch 6 Batch: 375 Train Loss: 0.4855803847312927\n",
      "Epoch 6 Batch: 376 Train Loss: 0.009839377366006374\n",
      "Epoch 6 Batch: 377 Train Loss: 0.010754352435469627\n",
      "Epoch 6 Batch: 378 Train Loss: 0.016285637393593788\n",
      "Epoch 6 Batch: 379 Train Loss: 0.011611489579081535\n",
      "Epoch 6 Batch: 380 Train Loss: 0.015984460711479187\n",
      "Epoch 6 Batch: 381 Train Loss: 0.014557508751749992\n",
      "Epoch 6 Batch: 382 Train Loss: 0.01740904152393341\n",
      "Epoch 6 Batch: 383 Train Loss: 0.43565353751182556\n",
      "Epoch 6 Batch: 384 Train Loss: 0.017404211685061455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Batch: 385 Train Loss: 0.01854744181036949\n",
      "Epoch 6 Batch: 386 Train Loss: 0.016516055911779404\n",
      "Epoch 6 Batch: 387 Train Loss: 0.015437392517924309\n",
      "Epoch 6 Batch: 388 Train Loss: 0.01734290085732937\n",
      "Epoch 6 Batch: 389 Train Loss: 0.017608487978577614\n",
      "Epoch 6 Batch: 390 Train Loss: 0.01723356731235981\n",
      "Epoch 6 Batch: 391 Train Loss: 0.01594505086541176\n",
      "Epoch 6 Batch: 392 Train Loss: 0.016982141882181168\n",
      "Epoch 6 Batch: 393 Train Loss: 0.8122976422309875\n",
      "Epoch 6 Batch: 394 Train Loss: 0.019712267443537712\n",
      "Epoch 6 Batch: 395 Train Loss: 0.020471958443522453\n",
      "Epoch 6 Batch: 396 Train Loss: 0.01870940439403057\n",
      "Epoch 6 Batch: 397 Train Loss: 0.022170808166265488\n",
      "Epoch 6 Batch: 398 Train Loss: 0.019229546189308167\n",
      "Epoch 6 Batch: 399 Train Loss: 0.02085280418395996\n",
      "Epoch 6 Batch: 400 Train Loss: 0.017782727256417274\n",
      "Epoch 6 Batch: 401 Train Loss: 0.016852933913469315\n",
      "Epoch 6 Batch: 402 Train Loss: 0.020882299169898033\n",
      "Epoch 6 Batch: 403 Train Loss: 0.017196888104081154\n",
      "Epoch 6 Batch: 404 Train Loss: 0.01671171560883522\n",
      "Epoch 6 Batch: 405 Train Loss: 0.018356546759605408\n",
      "Epoch 6 Batch: 406 Train Loss: 0.01571371592581272\n",
      "Epoch 6 Batch: 407 Train Loss: 0.017778553068637848\n",
      "Epoch 6 Batch: 408 Train Loss: 0.015827756375074387\n",
      "Epoch 6 Batch: 409 Train Loss: 0.015608325600624084\n",
      "Epoch 6 Batch: 410 Train Loss: 0.013752755708992481\n",
      "Epoch 6 Batch: 411 Train Loss: 0.013335843570530415\n",
      "Epoch 6 Batch: 412 Train Loss: 0.4366474151611328\n",
      "Epoch 6 Batch: 413 Train Loss: 0.012744992971420288\n",
      "Epoch 6 Batch: 414 Train Loss: 0.012231339700520039\n",
      "Epoch 6 Batch: 415 Train Loss: 0.010926869697868824\n",
      "Epoch 6 Batch: 416 Train Loss: 0.011770031414926052\n",
      "Epoch 6 Batch: 417 Train Loss: 0.011336936615407467\n",
      "Epoch 6 Batch: 418 Train Loss: 0.01074843667447567\n",
      "Epoch 6 Batch: 419 Train Loss: 0.01111892145127058\n",
      "Epoch 6 Batch: 420 Train Loss: 0.4465932250022888\n",
      "Epoch 6 Batch: 421 Train Loss: 0.009946548379957676\n",
      "Epoch 6 Batch: 422 Train Loss: 0.010409452952444553\n",
      "Epoch 6 Batch: 423 Train Loss: 0.44214123487472534\n",
      "Epoch 6 Batch: 424 Train Loss: 0.012401438318192959\n",
      "Epoch 6 Batch: 425 Train Loss: 0.01344156265258789\n",
      "Epoch 6 Batch: 426 Train Loss: 0.013262301683425903\n",
      "Epoch 6 Batch: 427 Train Loss: 0.01256251148879528\n",
      "Epoch 6 Batch: 428 Train Loss: 0.009030593559145927\n",
      "Epoch 6 Batch: 429 Train Loss: 0.01378049235790968\n",
      "Epoch 6 Batch: 430 Train Loss: 0.011169636622071266\n",
      "Epoch 6 Batch: 431 Train Loss: 0.012200511991977692\n",
      "Epoch 6 Batch: 432 Train Loss: 0.011282520368695259\n",
      "Epoch 6 Batch: 433 Train Loss: 0.014023631811141968\n",
      "Epoch 6 Batch: 434 Train Loss: 0.011494101956486702\n",
      "Epoch 6 Batch: 435 Train Loss: 0.012056296691298485\n",
      "Epoch 6 Batch: 436 Train Loss: 0.012404244393110275\n",
      "Epoch 6 Batch: 437 Train Loss: 0.009468072094023228\n",
      "Epoch 6 Batch: 438 Train Loss: 0.009625019505620003\n",
      "Epoch 6 Batch: 439 Train Loss: 0.008536596782505512\n",
      "Epoch 6 Batch: 440 Train Loss: 0.010315688326954842\n",
      "Epoch 6 Batch: 441 Train Loss: 0.009954995475709438\n",
      "Epoch 6 Batch: 442 Train Loss: 0.4457079768180847\n",
      "Epoch 6 Batch: 443 Train Loss: 0.010363693349063396\n",
      "Epoch 6 Batch: 444 Train Loss: 0.009138347581028938\n",
      "Epoch 6 Batch: 445 Train Loss: 0.00715748593211174\n",
      "Epoch 6 Batch: 446 Train Loss: 0.009085958823561668\n",
      "Epoch 6 Batch: 447 Train Loss: 0.9480530619621277\n",
      "Epoch 6 Batch: 448 Train Loss: 0.010769136250019073\n",
      "Epoch 6 Batch: 449 Train Loss: 0.01229591853916645\n",
      "Epoch 6 Batch: 450 Train Loss: 0.012050819583237171\n",
      "Epoch 6 Batch: 451 Train Loss: 0.012728942558169365\n",
      "Epoch 6 Batch: 452 Train Loss: 0.010462614707648754\n",
      "Epoch 6 Batch: 453 Train Loss: 0.01247076503932476\n",
      "Epoch 6 Batch: 454 Train Loss: 0.012812724336981773\n",
      "Epoch 6 Batch: 455 Train Loss: 0.01097572036087513\n",
      "Epoch 6 Batch: 456 Train Loss: 0.011035842821002007\n",
      "Epoch 6 Batch: 457 Train Loss: 0.011126870289444923\n",
      "Epoch 6 Batch: 458 Train Loss: 0.4096558690071106\n",
      "Epoch 6 Batch: 459 Train Loss: 0.01239450927823782\n",
      "Epoch 6 Batch: 460 Train Loss: 0.013657602481544018\n",
      "Epoch 6 Batch: 461 Train Loss: 0.01408916525542736\n",
      "Epoch 6 Batch: 462 Train Loss: 0.016335900872945786\n",
      "Epoch 6 Batch: 463 Train Loss: 0.010769261047244072\n",
      "Epoch 6 Batch: 464 Train Loss: 0.014244365505874157\n",
      "Epoch 6 Batch: 465 Train Loss: 0.015038998797535896\n",
      "Epoch 6 Batch: 466 Train Loss: 0.011856096796691418\n",
      "Epoch 6 Batch: 467 Train Loss: 0.01569884456694126\n",
      "Epoch 6 Batch: 468 Train Loss: 0.48458418250083923\n",
      "Epoch 6 Batch: 469 Train Loss: 0.40172433853149414\n",
      "Epoch 6 Batch: 470 Train Loss: 0.014322723262012005\n",
      "Epoch 6 Batch: 471 Train Loss: 0.012514645233750343\n",
      "Epoch 6 Batch: 472 Train Loss: 0.01421312429010868\n",
      "Epoch 6 Batch: 473 Train Loss: 0.010949344374239445\n",
      "Epoch 6 Batch: 474 Train Loss: 0.014820275828242302\n",
      "Epoch 6 Batch: 475 Train Loss: 0.0175846666097641\n",
      "Epoch 6 Batch: 476 Train Loss: 0.01995563507080078\n",
      "Epoch 6 Batch: 477 Train Loss: 0.013530435040593147\n",
      "Epoch 6 Batch: 478 Train Loss: 0.39199763536453247\n",
      "Epoch 6 Batch: 479 Train Loss: 0.01259052287787199\n",
      "Epoch 6 Batch: 480 Train Loss: 0.014144623652100563\n",
      "Epoch 6 Batch: 481 Train Loss: 0.46564602851867676\n",
      "Epoch 6 Batch: 482 Train Loss: 0.01889118179678917\n",
      "Epoch 6 Batch: 483 Train Loss: 0.01650894805788994\n",
      "Epoch 6 Batch: 484 Train Loss: 0.0194146279245615\n",
      "Epoch 6 Batch: 485 Train Loss: 0.015494786202907562\n",
      "Epoch 6 Batch: 486 Train Loss: 0.015708204358816147\n",
      "Epoch 6 Batch: 487 Train Loss: 0.018083389848470688\n",
      "Epoch 6 Batch: 488 Train Loss: 0.019097577780485153\n",
      "Epoch 6 Batch: 489 Train Loss: 0.01871362142264843\n",
      "Epoch 6 Batch: 490 Train Loss: 0.020551802590489388\n",
      "Epoch 6 Batch: 491 Train Loss: 0.017655279487371445\n",
      "Epoch 6 Batch: 492 Train Loss: 0.015839118510484695\n",
      "Epoch 6 Batch: 493 Train Loss: 0.014239643700420856\n",
      "Epoch 6 Batch: 494 Train Loss: 0.014581811614334583\n",
      "Epoch 6 Batch: 495 Train Loss: 0.012014145962893963\n",
      "Epoch 6 Batch: 496 Train Loss: 0.012518917210400105\n",
      "Epoch 6 Batch: 497 Train Loss: 0.013708919286727905\n",
      "Epoch 6 Batch: 498 Train Loss: 0.013774223625659943\n",
      "Epoch 6 Batch: 499 Train Loss: 0.009950191713869572\n",
      "Epoch 6 Batch: 500 Train Loss: 0.01096470095217228\n",
      "Epoch 6 Batch: 501 Train Loss: 0.010434059426188469\n",
      "Epoch 6 Batch: 502 Train Loss: 0.009387366473674774\n",
      "Epoch 6 Batch: 503 Train Loss: 0.4457572400569916\n",
      "Epoch 6 Batch: 504 Train Loss: 0.009966704063117504\n",
      "Epoch 6 Batch: 505 Train Loss: 0.011205408722162247\n",
      "Epoch 6 Batch: 506 Train Loss: 0.008038274012506008\n",
      "Epoch 6 Batch: 507 Train Loss: 0.009191242977976799\n",
      "Epoch 6 Batch: 508 Train Loss: 0.44817811250686646\n",
      "Epoch 6 Batch: 509 Train Loss: 0.009328213520348072\n",
      "Epoch 6 Batch: 510 Train Loss: 0.00894563551992178\n",
      "Epoch 6 Batch: 511 Train Loss: 0.009806562215089798\n",
      "Epoch 6 Batch: 512 Train Loss: 0.007777100894600153\n",
      "Epoch 6 Batch: 513 Train Loss: 0.00924768764525652\n",
      "Epoch 6 Batch: 514 Train Loss: 0.00988322589546442\n",
      "Epoch 6 Batch: 515 Train Loss: 0.008444900624454021\n",
      "Epoch 6 Batch: 516 Train Loss: 0.011180768720805645\n",
      "Epoch 6 Batch: 517 Train Loss: 0.01035158522427082\n",
      "Epoch 6 Batch: 518 Train Loss: 0.009522365406155586\n",
      "Epoch 6 Batch: 519 Train Loss: 0.007960346527397633\n",
      "Epoch 6 Batch: 520 Train Loss: 0.011173161678016186\n",
      "Epoch 6 Batch: 521 Train Loss: 0.008274735882878304\n",
      "Epoch 6 Batch: 522 Train Loss: 0.008667114190757275\n",
      "Epoch 6 Batch: 523 Train Loss: 0.009140141308307648\n",
      "Epoch 6 Batch: 524 Train Loss: 0.8934085965156555\n",
      "Epoch 6 Batch: 525 Train Loss: 0.006139031611382961\n",
      "Epoch 6 Batch: 526 Train Loss: 0.009230691008269787\n",
      "Epoch 6 Batch: 527 Train Loss: 0.008676623925566673\n",
      "Epoch 6 Batch: 528 Train Loss: 0.010321912355720997\n",
      "Epoch 6 Batch: 529 Train Loss: 0.011332456022500992\n",
      "Epoch 6 Batch: 530 Train Loss: 0.012633448466658592\n",
      "Epoch 6 Batch: 531 Train Loss: 0.010102379135787487\n",
      "Epoch 6 Batch: 532 Train Loss: 0.006957925856113434\n",
      "Epoch 6 Batch: 533 Train Loss: 0.012005660682916641\n",
      "Epoch 6 Batch: 534 Train Loss: 0.013124401681125164\n",
      "Epoch 6 Batch: 535 Train Loss: 0.011935001239180565\n",
      "Epoch 6 Batch: 536 Train Loss: 0.008325079455971718\n",
      "Epoch 6 Batch: 537 Train Loss: 0.00658771488815546\n",
      "Epoch 6 Batch: 538 Train Loss: 0.012573644518852234\n",
      "Epoch 6 Batch: 539 Train Loss: 0.5269701480865479\n",
      "Epoch 6 Batch: 540 Train Loss: 0.011461501941084862\n",
      "Epoch 6 Batch: 541 Train Loss: 0.5172489881515503\n",
      "Epoch 6 Batch: 542 Train Loss: 0.015421221032738686\n",
      "Epoch 6 Batch: 543 Train Loss: 0.01262326817959547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Batch: 544 Train Loss: 0.40794286131858826\n",
      "Epoch 6 Batch: 545 Train Loss: 0.016095325350761414\n",
      "Epoch 6 Batch: 546 Train Loss: 0.013742099516093731\n",
      "Epoch 6 Batch: 547 Train Loss: 0.016424236819148064\n",
      "Epoch 6 Batch: 548 Train Loss: 0.017107123509049416\n",
      "Epoch 6 Batch: 549 Train Loss: 0.011476623825728893\n",
      "Epoch 6 Batch: 550 Train Loss: 0.014034727588295937\n",
      "Epoch 6 Batch: 551 Train Loss: 0.012963404878973961\n",
      "Epoch 6 Batch: 552 Train Loss: 0.4693244397640228\n",
      "Epoch 6 Batch: 553 Train Loss: 0.012379886582493782\n",
      "Epoch 6 Batch: 554 Train Loss: 0.0173120629042387\n",
      "Epoch 6 Batch: 555 Train Loss: 0.016358816996216774\n",
      "Epoch 6 Batch: 556 Train Loss: 0.016666229814291\n",
      "Epoch 6 Batch: 557 Train Loss: 0.017702965065836906\n",
      "Epoch 6 Batch: 558 Train Loss: 0.015340762212872505\n",
      "Epoch 6 Batch: 559 Train Loss: 0.015536339953541756\n",
      "Epoch 6 Batch: 560 Train Loss: 0.01618758961558342\n",
      "Epoch 6 Batch: 561 Train Loss: 0.014597170054912567\n",
      "Epoch 6 Batch: 562 Train Loss: 0.016244951635599136\n",
      "Epoch 6 Batch: 563 Train Loss: 0.01528738159686327\n",
      "Epoch 6 Batch: 564 Train Loss: 0.01544962264597416\n",
      "Epoch 6 Batch: 565 Train Loss: 0.01534266583621502\n",
      "Epoch 6 Batch: 566 Train Loss: 0.013160730712115765\n",
      "Epoch 6 Batch: 567 Train Loss: 0.01191776618361473\n",
      "Epoch 6 Batch: 568 Train Loss: 0.46738940477371216\n",
      "Epoch 6 Batch: 569 Train Loss: 0.01395891048014164\n",
      "Epoch 6 Batch: 570 Train Loss: 0.4322572648525238\n",
      "Epoch 6 Batch: 571 Train Loss: 0.014216957613825798\n",
      "Epoch 6 Batch: 572 Train Loss: 0.013371070846915245\n",
      "Epoch 6 Batch: 573 Train Loss: 0.012679609470069408\n",
      "Epoch 6 Batch: 574 Train Loss: 0.012003323063254356\n",
      "Epoch 6 Batch: 575 Train Loss: 0.4541897773742676\n",
      "Epoch 6 Batch: 576 Train Loss: 0.012906062416732311\n",
      "Epoch 6 Batch: 577 Train Loss: 0.010317969135940075\n",
      "Epoch 6 Batch: 578 Train Loss: 0.012253531254827976\n",
      "Epoch 6 Batch: 579 Train Loss: 0.011895840056240559\n",
      "Epoch 6 Batch: 580 Train Loss: 0.015202319249510765\n",
      "Epoch 6 Batch: 581 Train Loss: 0.014038192108273506\n",
      "Epoch 6 Batch: 582 Train Loss: 0.015074104070663452\n",
      "Epoch 6 Batch: 583 Train Loss: 0.013868249952793121\n",
      "Epoch 6 Batch: 584 Train Loss: 0.015126995742321014\n",
      "Epoch 6 Batch: 585 Train Loss: 0.009206249378621578\n",
      "Epoch 6 Batch: 586 Train Loss: 0.014555169269442558\n",
      "Epoch 6 Batch: 587 Train Loss: 0.44068557024002075\n",
      "Epoch 6 Batch: 588 Train Loss: 0.010133561678230762\n",
      "Epoch 6 Batch: 589 Train Loss: 0.010333715006709099\n",
      "Epoch 6 Batch: 590 Train Loss: 0.013246877118945122\n",
      "Epoch 6 Batch: 591 Train Loss: 0.011744541116058826\n",
      "Epoch 6 Batch: 592 Train Loss: 0.014649800956249237\n",
      "Epoch 6 Batch: 593 Train Loss: 0.01437362004071474\n",
      "Epoch 6 Batch: 594 Train Loss: 0.014284300617873669\n",
      "Epoch 6 Batch: 595 Train Loss: 0.012664953246712685\n",
      "Epoch 6 Batch: 596 Train Loss: 0.013795343227684498\n",
      "Epoch 6 Batch: 597 Train Loss: 0.4396427273750305\n",
      "Epoch 6 Batch: 598 Train Loss: 0.01369869988411665\n",
      "Epoch 6 Batch: 599 Train Loss: 0.013663046061992645\n",
      "Epoch 6 Batch: 600 Train Loss: 0.010998575016856194\n",
      "Epoch 6 Batch: 601 Train Loss: 0.010973910801112652\n",
      "Epoch 6 Batch: 602 Train Loss: 0.012255482375621796\n",
      "Epoch 6 Batch: 603 Train Loss: 0.012850344181060791\n",
      "Epoch 6 Batch: 604 Train Loss: 0.45937466621398926\n",
      "Epoch 6 Batch: 605 Train Loss: 0.012869151309132576\n",
      "Epoch 6 Batch: 606 Train Loss: 0.013635274954140186\n",
      "Epoch 6 Batch: 607 Train Loss: 0.012229026295244694\n",
      "Epoch 6 Batch: 608 Train Loss: 0.013191859237849712\n",
      "Epoch 6 Batch: 609 Train Loss: 0.012157561257481575\n",
      "Epoch 6 Batch: 610 Train Loss: 0.00961170345544815\n",
      "Epoch 6 Batch: 611 Train Loss: 0.013285087421536446\n",
      "Epoch 6 Batch: 612 Train Loss: 0.012845305725932121\n",
      "Epoch 6 Batch: 613 Train Loss: 0.011596296913921833\n",
      "Epoch 6 Batch: 614 Train Loss: 0.008547303266823292\n",
      "Epoch 6 Batch: 615 Train Loss: 0.011005520820617676\n",
      "Epoch 6 Batch: 616 Train Loss: 0.010835027322173119\n",
      "Epoch 6 Batch: 617 Train Loss: 0.46472692489624023\n",
      "Epoch 6 Batch: 618 Train Loss: 0.010636989958584309\n",
      "Epoch 6 Batch: 619 Train Loss: 0.010380791500210762\n",
      "Epoch 6 Batch: 620 Train Loss: 0.010418761521577835\n",
      "Epoch 6 Batch: 621 Train Loss: 0.010341460816562176\n",
      "Epoch 6 Batch: 622 Train Loss: 0.011397406458854675\n",
      "Epoch 6 Batch: 623 Train Loss: 0.007885945029556751\n",
      "Epoch 6 Batch: 624 Train Loss: 0.011135146021842957\n",
      "Epoch 6 Batch: 625 Train Loss: 0.009855978190898895\n",
      "Epoch 6 Batch: 626 Train Loss: 0.010520601645112038\n",
      "Epoch 6 Batch: 627 Train Loss: 0.01025520171970129\n",
      "Epoch 6 Batch: 628 Train Loss: 0.009005825966596603\n",
      "Epoch 6 Batch: 629 Train Loss: 0.010010374709963799\n",
      "Epoch 6 Batch: 630 Train Loss: 0.007580176927149296\n",
      "Epoch 6 Batch: 631 Train Loss: 0.0072283619083464146\n",
      "Epoch 6 Batch: 632 Train Loss: 0.008475318551063538\n",
      "Epoch 6 Batch: 633 Train Loss: 0.0075818696059286594\n",
      "Epoch 6 Batch: 634 Train Loss: 0.008101321756839752\n",
      "Epoch 6 Batch: 635 Train Loss: 0.008023416623473167\n",
      "Epoch 6 Batch: 636 Train Loss: 0.007975295186042786\n",
      "Epoch 6 Batch: 637 Train Loss: 0.0061691501177847385\n",
      "Epoch 6 Batch: 638 Train Loss: 0.0055895294062793255\n",
      "Epoch 6 Batch: 639 Train Loss: 0.006031079683452845\n",
      "Epoch 6 Batch: 640 Train Loss: 0.006984143052250147\n",
      "Epoch 6 Batch: 641 Train Loss: 0.006143623031675816\n",
      "Epoch 6 Batch: 642 Train Loss: 0.006171549670398235\n",
      "Epoch 6 Batch: 643 Train Loss: 0.00620611384510994\n",
      "Epoch 6 Batch: 644 Train Loss: 0.005686273332685232\n",
      "Epoch 6 Batch: 645 Train Loss: 0.005274337716400623\n",
      "Epoch 6 Batch: 646 Train Loss: 0.005249904468655586\n",
      "Epoch 6 Batch: 647 Train Loss: 0.00566874910145998\n",
      "Epoch 6 Batch: 648 Train Loss: 0.004684752784669399\n",
      "Epoch 6 Batch: 649 Train Loss: 0.005432748235762119\n",
      "Epoch 6 Batch: 650 Train Loss: 0.004716456867754459\n",
      "Epoch 6 Batch: 651 Train Loss: 0.003939294256269932\n",
      "Epoch 6 Batch: 652 Train Loss: 0.00467012170702219\n",
      "Epoch 6 Batch: 653 Train Loss: 0.00402714591473341\n",
      "Epoch 6 Batch: 654 Train Loss: 0.0033819638192653656\n",
      "Epoch 6 Batch: 655 Train Loss: 0.0045567462220788\n",
      "Epoch 6 Batch: 656 Train Loss: 0.0036995508708059788\n",
      "Epoch 6 Batch: 657 Train Loss: 0.004228966776281595\n",
      "Epoch 6 Batch: 658 Train Loss: 0.0037848681677132845\n",
      "Epoch 6 Batch: 659 Train Loss: 0.004091882146894932\n",
      "Epoch 6 Batch: 660 Train Loss: 0.0034091684501618147\n",
      "Epoch 6 Batch: 661 Train Loss: 0.003655135165899992\n",
      "Epoch 6 Batch: 662 Train Loss: 0.003388482378795743\n",
      "Epoch 6 Batch: 663 Train Loss: 0.5775583982467651\n",
      "Epoch 6 Batch: 664 Train Loss: 0.003543555736541748\n",
      "Epoch 6 Batch: 665 Train Loss: 0.00383770652115345\n",
      "Epoch 6 Batch: 666 Train Loss: 0.0037719167303293943\n",
      "Epoch 6 Batch: 667 Train Loss: 0.003157703671604395\n",
      "Epoch 6 Batch: 668 Train Loss: 0.004078867379575968\n",
      "Epoch 6 Batch: 669 Train Loss: 0.0019574512261897326\n",
      "Epoch 6 Batch: 670 Train Loss: 0.004163593985140324\n",
      "Epoch 6 Batch: 671 Train Loss: 0.004137333948165178\n",
      "Epoch 6 Batch: 672 Train Loss: 0.0037141453940421343\n",
      "Epoch 6 Batch: 673 Train Loss: 0.0041529713198542595\n",
      "Epoch 6 Batch: 674 Train Loss: 0.552445113658905\n",
      "Epoch 6 Batch: 675 Train Loss: 0.004352380987256765\n",
      "Epoch 6 Batch: 676 Train Loss: 0.004583172034472227\n",
      "Epoch 6 Batch: 677 Train Loss: 0.004260993096977472\n",
      "Epoch 6 Batch: 678 Train Loss: 0.004899604711681604\n",
      "Epoch 6 Batch: 679 Train Loss: 0.004621496424078941\n",
      "Epoch 6 Batch: 680 Train Loss: 0.0050980024971067905\n",
      "Epoch 6 Batch: 681 Train Loss: 0.005266118329018354\n",
      "Epoch 6 Batch: 682 Train Loss: 0.004689327906817198\n",
      "Epoch 6 Batch: 683 Train Loss: 0.005327828694134951\n",
      "Epoch 6 Batch: 684 Train Loss: 0.00474149826914072\n",
      "Epoch 6 Batch: 685 Train Loss: 0.004130885936319828\n",
      "Epoch 6 Batch: 686 Train Loss: 0.005239001475274563\n",
      "Epoch 6 Batch: 687 Train Loss: 0.004840184934437275\n",
      "Epoch 6 Batch: 688 Train Loss: 0.00468928599730134\n",
      "Epoch 6 Batch: 689 Train Loss: 0.0052665532566607\n",
      "Epoch 6 Batch: 690 Train Loss: 0.00392816262319684\n",
      "Epoch 6 Batch: 691 Train Loss: 0.004617223050445318\n",
      "Epoch 6 Batch: 692 Train Loss: 0.003084725234657526\n",
      "Epoch 6 Batch: 693 Train Loss: 0.005098011344671249\n",
      "Epoch 6 Batch: 694 Train Loss: 0.00430560065433383\n",
      "Epoch 6 Batch: 695 Train Loss: 0.004555530846118927\n",
      "Epoch 6 Batch: 696 Train Loss: 0.5459501147270203\n",
      "Epoch 6 Batch: 697 Train Loss: 0.004291479475796223\n",
      "Epoch 6 Batch: 698 Train Loss: 0.003926629666239023\n",
      "Epoch 6 Batch: 699 Train Loss: 0.005219564773142338\n",
      "Epoch 6 Batch: 700 Train Loss: 0.004264617804437876\n",
      "Epoch 6 Batch: 701 Train Loss: 0.003783599706366658\n",
      "Epoch 6 Batch: 702 Train Loss: 0.004949972499161959\n",
      "Epoch 6 Batch: 703 Train Loss: 0.005566050298511982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Batch: 704 Train Loss: 0.00501555809751153\n",
      "Epoch 6 Batch: 705 Train Loss: 0.0055795167572796345\n",
      "Epoch 6 Batch: 706 Train Loss: 0.522156834602356\n",
      "Epoch 6 Batch: 707 Train Loss: 0.005264010746032\n",
      "Epoch 6 Batch: 708 Train Loss: 0.006326386239379644\n",
      "Epoch 6 Batch: 709 Train Loss: 0.006268353201448917\n",
      "Epoch 6 Batch: 710 Train Loss: 0.006870018783956766\n",
      "Epoch 6 Batch: 711 Train Loss: 0.00584469735622406\n",
      "Epoch 6 Batch: 712 Train Loss: 0.005808666348457336\n",
      "Epoch 6 Batch: 713 Train Loss: 0.005721792578697205\n",
      "Epoch 6 Batch: 714 Train Loss: 0.514931321144104\n",
      "Epoch 6 Batch: 715 Train Loss: 0.006637109909206629\n",
      "Epoch 6 Batch: 716 Train Loss: 0.006650020368397236\n",
      "Epoch 6 Batch: 717 Train Loss: 0.9936162233352661\n",
      "Epoch 6 Batch: 718 Train Loss: 0.007365291006863117\n",
      "Epoch 6 Batch: 719 Train Loss: 0.010146822780370712\n",
      "Epoch 6 Batch: 720 Train Loss: 0.010917523875832558\n",
      "Epoch 6 Batch: 721 Train Loss: 0.01054761279374361\n",
      "Epoch 6 Batch: 722 Train Loss: 0.012742610648274422\n",
      "Epoch 6 Batch: 723 Train Loss: 0.011718783527612686\n",
      "Epoch 6 Batch: 724 Train Loss: 0.010899965651333332\n",
      "Epoch 6 Batch: 725 Train Loss: 0.01430357713252306\n",
      "Epoch 6 Batch: 726 Train Loss: 0.42950671911239624\n",
      "Epoch 6 Batch: 727 Train Loss: 0.016430653631687164\n",
      "Epoch 6 Batch: 728 Train Loss: 0.014531736262142658\n",
      "Epoch 6 Batch: 729 Train Loss: 0.017748190090060234\n",
      "Epoch 6 Batch: 730 Train Loss: 0.01586727239191532\n",
      "Epoch 6 Batch: 731 Train Loss: 0.014652147889137268\n",
      "Epoch 6 Batch: 732 Train Loss: 0.015240421518683434\n",
      "Epoch 6 Batch: 733 Train Loss: 0.015227203257381916\n",
      "Epoch 6 Batch: 734 Train Loss: 0.019027214497327805\n",
      "Epoch 6 Batch: 735 Train Loss: 0.0161731019616127\n",
      "Epoch 6 Batch: 736 Train Loss: 0.01451071910560131\n",
      "Epoch 6 Batch: 737 Train Loss: 0.014970101416110992\n",
      "Epoch 6 Batch: 738 Train Loss: 0.015294772572815418\n",
      "Epoch 6 Batch: 739 Train Loss: 0.017404640093445778\n",
      "Epoch 6 Batch: 740 Train Loss: 0.015605579130351543\n",
      "Epoch 6 Batch: 741 Train Loss: 0.016296379268169403\n",
      "Epoch 6 Batch: 742 Train Loss: 0.014477549120783806\n",
      "Epoch 6 Batch: 743 Train Loss: 0.011624597944319248\n",
      "Epoch 6 Batch: 744 Train Loss: 0.015004502609372139\n",
      "Epoch 6 Batch: 745 Train Loss: 0.43068113923072815\n",
      "Epoch 6 Batch: 746 Train Loss: 0.011705500073730946\n",
      "Epoch 6 Batch: 747 Train Loss: 0.47412604093551636\n",
      "Epoch 6 Batch: 748 Train Loss: 0.011824135668575764\n",
      "Epoch 6 Batch: 749 Train Loss: 0.01329253613948822\n",
      "Epoch 6 Batch: 750 Train Loss: 0.42474818229675293\n",
      "Epoch 6 Batch: 751 Train Loss: 0.011818485334515572\n",
      "Epoch 6 Batch: 752 Train Loss: 0.014091499149799347\n",
      "Epoch 6 Batch: 753 Train Loss: 0.010807101614773273\n",
      "Epoch 6 Batch: 754 Train Loss: 0.01290420163422823\n",
      "Epoch 6 Batch: 755 Train Loss: 0.013081066310405731\n",
      "Epoch 6 Batch: 756 Train Loss: 0.013950014486908913\n",
      "Epoch 6 Batch: 757 Train Loss: 0.013970175758004189\n",
      "Epoch 6 Batch: 758 Train Loss: 0.011919018812477589\n",
      "Epoch 6 Batch: 759 Train Loss: 0.017374631017446518\n",
      "Epoch 6 Batch: 760 Train Loss: 0.013582250103354454\n",
      "Epoch 6 Batch: 761 Train Loss: 0.014454508200287819\n",
      "Epoch 6 Batch: 762 Train Loss: 0.01301993615925312\n",
      "Epoch 6 Batch: 763 Train Loss: 0.014207249507308006\n",
      "Epoch 6 Batch: 764 Train Loss: 0.013052977621555328\n",
      "Epoch 6 Batch: 765 Train Loss: 0.009469671174883842\n",
      "Epoch 6 Batch: 766 Train Loss: 0.012227656319737434\n",
      "Epoch 6 Batch: 767 Train Loss: 0.012739419937133789\n",
      "Epoch 6 Batch: 768 Train Loss: 0.4278941750526428\n",
      "Epoch 6 Batch: 769 Train Loss: 0.012342301197350025\n",
      "Epoch 6 Batch: 770 Train Loss: 0.010756474919617176\n",
      "Epoch 6 Batch: 771 Train Loss: 0.011567848734557629\n",
      "Epoch 6 Batch: 772 Train Loss: 0.010721639730036259\n",
      "Epoch 6 Batch: 773 Train Loss: 0.010446910746395588\n",
      "Epoch 6 Batch: 774 Train Loss: 0.013280917890369892\n",
      "Epoch 6 Batch: 775 Train Loss: 0.01303091086447239\n",
      "Epoch 6 Batch: 776 Train Loss: 0.007664320059120655\n",
      "Epoch 6 Batch: 777 Train Loss: 0.008856037631630898\n",
      "Epoch 6 Batch: 778 Train Loss: 0.00861038826406002\n",
      "Epoch 6 Batch: 779 Train Loss: 0.009745216928422451\n",
      "Epoch 6 Batch: 780 Train Loss: 0.00878415908664465\n",
      "Epoch 6 Batch: 781 Train Loss: 0.009812945500016212\n",
      "Epoch 6 Batch: 782 Train Loss: 0.00886191800236702\n",
      "Epoch 6 Batch: 783 Train Loss: 0.5190799236297607\n",
      "Epoch 6 Batch: 784 Train Loss: 0.00863709021359682\n",
      "Epoch 6 Batch: 785 Train Loss: 0.0075620440766215324\n",
      "Epoch 6 Batch: 786 Train Loss: 0.008122432045638561\n",
      "Epoch 6 Batch: 787 Train Loss: 0.009779592044651508\n",
      "Epoch 6 Batch: 788 Train Loss: 0.00967644713819027\n",
      "Epoch 6 Batch: 789 Train Loss: 0.007328520063310862\n",
      "Epoch 6 Batch: 790 Train Loss: 0.00854538381099701\n",
      "Epoch 6 Batch: 791 Train Loss: 0.007295265793800354\n",
      "Epoch 6 Batch: 792 Train Loss: 0.008104863576591015\n",
      "Epoch 6 Batch: 793 Train Loss: 0.00882816594094038\n",
      "Epoch 6 Batch: 794 Train Loss: 0.008395323529839516\n",
      "Epoch 6 Batch: 795 Train Loss: 0.008604438975453377\n",
      "Epoch 6 Batch: 796 Train Loss: 0.007774719502776861\n",
      "Epoch 6 Batch: 797 Train Loss: 0.008151251822710037\n",
      "Epoch 6 Batch: 798 Train Loss: 0.007211766205728054\n",
      "Epoch 6 Batch: 799 Train Loss: 0.0067828865721821785\n",
      "Epoch 6 Batch: 800 Train Loss: 0.006585787981748581\n",
      "Epoch 6 Batch: 801 Train Loss: 0.006079220678657293\n",
      "Epoch 6 Batch: 802 Train Loss: 0.005607230123132467\n",
      "Epoch 6 Batch: 803 Train Loss: 0.005729286000132561\n",
      "Epoch 6 Batch: 804 Train Loss: 0.00583265395835042\n",
      "Epoch 6 Batch: 805 Train Loss: 0.0061430661007761955\n",
      "Epoch 6 Batch: 806 Train Loss: 0.005488644354045391\n",
      "Epoch 6 Batch: 807 Train Loss: 0.5433326959609985\n",
      "Epoch 6 Batch: 808 Train Loss: 0.5014030933380127\n",
      "Epoch 6 Batch: 809 Train Loss: 0.004616346210241318\n",
      "Epoch 6 Batch: 810 Train Loss: 0.0059067253023386\n",
      "Epoch 6 Batch: 811 Train Loss: 0.007300128694623709\n",
      "Epoch 6 Batch: 812 Train Loss: 0.006999841425567865\n",
      "Epoch 6 Batch: 813 Train Loss: 0.5141222476959229\n",
      "Epoch 6 Batch: 814 Train Loss: 0.008696912787854671\n",
      "Epoch 6 Batch: 815 Train Loss: 0.008205358870327473\n",
      "Epoch 6 Batch: 816 Train Loss: 0.00913974829018116\n",
      "Epoch 6 Batch: 817 Train Loss: 0.4869420528411865\n",
      "Epoch 6 Batch: 818 Train Loss: 0.0093945087864995\n",
      "Epoch 6 Batch: 819 Train Loss: 0.00882459245622158\n",
      "Epoch 6 Batch: 820 Train Loss: 0.011667280457913876\n",
      "Epoch 6 Batch: 821 Train Loss: 0.011174505576491356\n",
      "Epoch 6 Batch: 822 Train Loss: 0.011345388367772102\n",
      "Epoch 6 Batch: 823 Train Loss: 0.010288381949067116\n",
      "Epoch 6 Batch: 824 Train Loss: 0.010528415441513062\n",
      "Epoch 6 Batch: 825 Train Loss: 0.012119577266275883\n",
      "Epoch 6 Batch: 826 Train Loss: 0.014562668278813362\n",
      "Epoch 6 Batch: 827 Train Loss: 0.009613108821213245\n",
      "Epoch 6 Batch: 828 Train Loss: 0.013751372694969177\n",
      "Epoch 6 Batch: 829 Train Loss: 0.013714984059333801\n",
      "Epoch 6 Batch: 830 Train Loss: 0.013602393679320812\n",
      "Epoch 6 Batch: 831 Train Loss: 0.013367220759391785\n",
      "Epoch 6 Batch: 832 Train Loss: 0.013187830336391926\n",
      "Epoch 6 Batch: 833 Train Loss: 0.012782640755176544\n",
      "Epoch 6 Batch: 834 Train Loss: 0.011301704682409763\n",
      "Epoch 6 Batch: 835 Train Loss: 0.009680835530161858\n",
      "Epoch 6 Batch: 836 Train Loss: 0.009273959323763847\n",
      "Epoch 6 Batch: 837 Train Loss: 0.010360894724726677\n",
      "Epoch 6 Batch: 838 Train Loss: 0.009835830889642239\n",
      "Epoch 6 Batch: 839 Train Loss: 0.00970892608165741\n",
      "Epoch 6 Batch: 840 Train Loss: 0.008239868097007275\n",
      "Epoch 6 Batch: 841 Train Loss: 0.009062269702553749\n",
      "Epoch 6 Batch: 842 Train Loss: 0.007604567799717188\n",
      "Epoch 6 Batch: 843 Train Loss: 0.008510716259479523\n",
      "Epoch 6 Batch: 844 Train Loss: 0.48338109254837036\n",
      "Epoch 6 Batch: 845 Train Loss: 0.007172601763159037\n",
      "Epoch 6 Batch: 846 Train Loss: 0.00734694441780448\n",
      "Epoch 6 Batch: 847 Train Loss: 0.009191988036036491\n",
      "Epoch 6 Batch: 848 Train Loss: 0.009129989892244339\n",
      "Epoch 6 Batch: 849 Train Loss: 0.009061639197170734\n",
      "Epoch 6 Batch: 850 Train Loss: 0.4792153835296631\n",
      "Epoch 6 Batch: 851 Train Loss: 0.006481294520199299\n",
      "Epoch 6 Batch: 852 Train Loss: 0.009545966982841492\n",
      "Epoch 6 Batch: 853 Train Loss: 0.009741151705384254\n",
      "Epoch 6 Batch: 854 Train Loss: 0.006893234793096781\n",
      "Epoch 6 Batch: 855 Train Loss: 0.00698205828666687\n",
      "Epoch 6 Batch: 856 Train Loss: 0.008023511618375778\n",
      "Epoch 6 Batch: 857 Train Loss: 0.010005304589867592\n",
      "Epoch 6 Batch: 858 Train Loss: 0.006984400562942028\n",
      "Epoch 6 Batch: 859 Train Loss: 0.008927739225327969\n",
      "Epoch 6 Batch: 860 Train Loss: 0.008998040109872818\n",
      "Epoch 6 Batch: 861 Train Loss: 0.4687619209289551\n",
      "Epoch 6 Batch: 862 Train Loss: 0.009152176789939404\n",
      "Epoch 6 Batch: 863 Train Loss: 0.00821636337786913\n",
      "Epoch 6 Batch: 864 Train Loss: 0.010453314520418644\n",
      "Epoch 6 Batch: 865 Train Loss: 0.008608443662524223\n",
      "Epoch 6 Batch: 866 Train Loss: 0.010638309642672539\n",
      "Epoch 6 Batch: 867 Train Loss: 0.00912916474044323\n",
      "Epoch 6 Batch: 868 Train Loss: 0.00945486780256033\n",
      "Epoch 6 Batch: 869 Train Loss: 0.010540047660470009\n",
      "Epoch 6 Batch: 870 Train Loss: 0.00855574756860733\n",
      "Epoch 6 Batch: 871 Train Loss: 0.4769935607910156\n",
      "Epoch 6 Batch: 872 Train Loss: 0.45392975211143494\n",
      "Epoch 6 Batch: 873 Train Loss: 0.00880423653870821\n",
      "Epoch 6 Batch: 874 Train Loss: 0.009825709275901318\n",
      "Epoch 6 Batch: 875 Train Loss: 0.012512455694377422\n",
      "Epoch 6 Batch: 876 Train Loss: 0.01296599954366684\n",
      "Epoch 6 Batch: 877 Train Loss: 0.011823669075965881\n",
      "Epoch 6 Batch: 878 Train Loss: 0.4296550154685974\n",
      "Epoch 6 Batch: 879 Train Loss: 0.01350521482527256\n",
      "Epoch 6 Batch: 880 Train Loss: 0.449049174785614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Batch: 1 Train Loss: 0.013830861076712608\n",
      "Epoch 8 Batch: 2 Train Loss: 0.010681130923330784\n",
      "Epoch 8 Batch: 3 Train Loss: 0.012439492158591747\n",
      "Epoch 8 Batch: 4 Train Loss: 0.4345153868198395\n",
      "Epoch 8 Batch: 5 Train Loss: 0.011400263756513596\n",
      "Epoch 8 Batch: 6 Train Loss: 0.01194423995912075\n",
      "Epoch 8 Batch: 7 Train Loss: 0.010635705664753914\n",
      "Epoch 8 Batch: 8 Train Loss: 0.012215976603329182\n",
      "Epoch 8 Batch: 9 Train Loss: 0.010934924706816673\n",
      "Epoch 8 Batch: 10 Train Loss: 0.010829092003405094\n",
      "Epoch 8 Batch: 11 Train Loss: 0.010426322929561138\n",
      "Epoch 8 Batch: 12 Train Loss: 0.4824633002281189\n",
      "Epoch 8 Batch: 13 Train Loss: 0.01295925211161375\n",
      "Epoch 8 Batch: 14 Train Loss: 0.009671157225966454\n",
      "Epoch 8 Batch: 15 Train Loss: 0.012626992538571358\n",
      "Epoch 8 Batch: 16 Train Loss: 0.013287356123328209\n",
      "Epoch 8 Batch: 17 Train Loss: 0.012043843977153301\n",
      "Epoch 8 Batch: 18 Train Loss: 0.013145886361598969\n",
      "Epoch 8 Batch: 19 Train Loss: 0.010319476947188377\n",
      "Epoch 8 Batch: 20 Train Loss: 0.011129656806588173\n",
      "Epoch 8 Batch: 21 Train Loss: 0.01055096834897995\n",
      "Epoch 8 Batch: 22 Train Loss: 0.01178041659295559\n",
      "Epoch 8 Batch: 23 Train Loss: 0.012519462034106255\n",
      "Epoch 8 Batch: 24 Train Loss: 0.4394887387752533\n",
      "Epoch 8 Batch: 25 Train Loss: 0.008723320439457893\n",
      "Epoch 8 Batch: 26 Train Loss: 0.010593675076961517\n",
      "Epoch 8 Batch: 27 Train Loss: 0.009565752930939198\n",
      "Epoch 8 Batch: 28 Train Loss: 0.009215355850756168\n",
      "Epoch 8 Batch: 29 Train Loss: 0.012025089003145695\n",
      "Epoch 8 Batch: 30 Train Loss: 0.009397538378834724\n",
      "Epoch 8 Batch: 31 Train Loss: 0.011794297024607658\n",
      "Epoch 8 Batch: 32 Train Loss: 0.010233464650809765\n",
      "Epoch 8 Batch: 33 Train Loss: 0.4404146075248718\n",
      "Epoch 8 Batch: 34 Train Loss: 0.009109855629503727\n",
      "Epoch 8 Batch: 35 Train Loss: 0.01228947751224041\n",
      "Epoch 8 Batch: 36 Train Loss: 0.009730921126902103\n",
      "Epoch 8 Batch: 37 Train Loss: 0.011251253075897694\n",
      "Epoch 8 Batch: 38 Train Loss: 0.009928828105330467\n",
      "Epoch 8 Batch: 39 Train Loss: 0.01121453382074833\n",
      "Epoch 8 Batch: 40 Train Loss: 0.49534082412719727\n",
      "Epoch 8 Batch: 41 Train Loss: 0.008719174191355705\n",
      "Epoch 8 Batch: 42 Train Loss: 0.01247808150947094\n",
      "Epoch 8 Batch: 43 Train Loss: 0.012641075067222118\n",
      "Epoch 8 Batch: 44 Train Loss: 0.01184120588004589\n",
      "Epoch 8 Batch: 45 Train Loss: 0.010567673482000828\n",
      "Epoch 8 Batch: 46 Train Loss: 0.01268858928233385\n",
      "Epoch 8 Batch: 47 Train Loss: 0.01108191255480051\n",
      "Epoch 8 Batch: 48 Train Loss: 0.0076919784769415855\n",
      "Epoch 8 Batch: 49 Train Loss: 0.010234842076897621\n",
      "Epoch 8 Batch: 50 Train Loss: 0.013142580166459084\n",
      "Epoch 8 Batch: 51 Train Loss: 0.01021615695208311\n",
      "Epoch 8 Batch: 52 Train Loss: 0.008650672622025013\n",
      "Epoch 8 Batch: 53 Train Loss: 0.011736268177628517\n",
      "Epoch 8 Batch: 54 Train Loss: 0.01039902400225401\n",
      "Epoch 8 Batch: 55 Train Loss: 0.009403804317116737\n",
      "Epoch 8 Batch: 56 Train Loss: 0.010779818519949913\n",
      "Epoch 8 Batch: 57 Train Loss: 0.009614670649170876\n",
      "Epoch 8 Batch: 58 Train Loss: 0.0070533170364797115\n",
      "Epoch 8 Batch: 59 Train Loss: 0.009405747056007385\n",
      "Epoch 8 Batch: 60 Train Loss: 0.009097618982195854\n",
      "Epoch 8 Batch: 61 Train Loss: 0.007771805860102177\n",
      "Epoch 8 Batch: 62 Train Loss: 0.5104480981826782\n",
      "Epoch 8 Batch: 63 Train Loss: 0.007269725203514099\n",
      "Epoch 8 Batch: 64 Train Loss: 0.007721447851508856\n",
      "Epoch 8 Batch: 65 Train Loss: 0.008063140325248241\n",
      "Epoch 8 Batch: 66 Train Loss: 0.00807531364262104\n",
      "Epoch 8 Batch: 67 Train Loss: 0.006886504590511322\n",
      "Epoch 8 Batch: 68 Train Loss: 0.008475483395159245\n",
      "Epoch 8 Batch: 69 Train Loss: 0.007026104722172022\n",
      "Epoch 8 Batch: 70 Train Loss: 0.0071592084132134914\n",
      "Epoch 8 Batch: 71 Train Loss: 0.007764061447232962\n",
      "Epoch 8 Batch: 72 Train Loss: 0.007646526210010052\n",
      "Epoch 8 Batch: 73 Train Loss: 0.005318672861903906\n",
      "Epoch 8 Batch: 74 Train Loss: 0.006547071039676666\n",
      "Epoch 8 Batch: 75 Train Loss: 0.0065752603113651276\n",
      "Epoch 8 Batch: 76 Train Loss: 0.006151530891656876\n",
      "Epoch 8 Batch: 77 Train Loss: 0.0057992273941636086\n",
      "Epoch 8 Batch: 78 Train Loss: 0.006292333360761404\n",
      "Epoch 8 Batch: 79 Train Loss: 0.00627133110538125\n",
      "Epoch 8 Batch: 80 Train Loss: 0.006717021111398935\n",
      "Epoch 8 Batch: 81 Train Loss: 0.005616849288344383\n",
      "Epoch 8 Batch: 82 Train Loss: 0.00490682665258646\n",
      "Epoch 8 Batch: 83 Train Loss: 0.006221077870577574\n",
      "Epoch 8 Batch: 84 Train Loss: 0.005311083979904652\n",
      "Epoch 8 Batch: 85 Train Loss: 0.004742536693811417\n",
      "Epoch 8 Batch: 86 Train Loss: 0.005656195804476738\n",
      "Epoch 8 Batch: 87 Train Loss: 0.005417943932116032\n",
      "Epoch 8 Batch: 88 Train Loss: 0.005365693476051092\n",
      "Epoch 8 Batch: 89 Train Loss: 0.0048308284021914005\n",
      "Epoch 8 Batch: 90 Train Loss: 0.0050051514990627766\n",
      "Epoch 8 Batch: 91 Train Loss: 0.004579526372253895\n",
      "Epoch 8 Batch: 92 Train Loss: 0.0037054207641631365\n",
      "Epoch 8 Batch: 93 Train Loss: 0.004270558245480061\n",
      "Epoch 8 Batch: 94 Train Loss: 0.004162861034274101\n",
      "Epoch 8 Batch: 95 Train Loss: 0.00405904371291399\n",
      "Epoch 8 Batch: 96 Train Loss: 0.003751137061044574\n",
      "Epoch 8 Batch: 97 Train Loss: 0.004457359202206135\n",
      "Epoch 8 Batch: 98 Train Loss: 0.00325528671965003\n",
      "Epoch 8 Batch: 99 Train Loss: 0.003679755376651883\n",
      "Epoch 8 Batch: 100 Train Loss: 0.003169754520058632\n",
      "Epoch 8 Batch: 101 Train Loss: 0.0034507140517234802\n",
      "Epoch 8 Batch: 102 Train Loss: 0.0038405773229897022\n",
      "Epoch 8 Batch: 103 Train Loss: 0.5708562135696411\n",
      "Epoch 8 Batch: 104 Train Loss: 0.003060237504541874\n",
      "Epoch 8 Batch: 105 Train Loss: 0.0047210343182086945\n",
      "Epoch 8 Batch: 106 Train Loss: 0.00401154812425375\n",
      "Epoch 8 Batch: 107 Train Loss: 0.0041330670937895775\n",
      "Epoch 8 Batch: 108 Train Loss: 0.5485178232192993\n",
      "Epoch 8 Batch: 109 Train Loss: 0.0040562693029642105\n",
      "Epoch 8 Batch: 110 Train Loss: 0.004242350347340107\n",
      "Epoch 8 Batch: 111 Train Loss: 0.004481025971472263\n",
      "Epoch 8 Batch: 112 Train Loss: 0.0046881865710020065\n",
      "Epoch 8 Batch: 113 Train Loss: 0.004273800179362297\n",
      "Epoch 8 Batch: 114 Train Loss: 0.5281345844268799\n",
      "Epoch 8 Batch: 115 Train Loss: 0.005403498187661171\n",
      "Epoch 8 Batch: 116 Train Loss: 0.006353291217237711\n",
      "Epoch 8 Batch: 117 Train Loss: 0.006735475268214941\n",
      "Epoch 8 Batch: 118 Train Loss: 0.007048088125884533\n",
      "Epoch 8 Batch: 119 Train Loss: 0.006585537455976009\n",
      "Epoch 8 Batch: 120 Train Loss: 0.007520801387727261\n",
      "Epoch 8 Batch: 121 Train Loss: 0.008426148444414139\n",
      "Epoch 8 Batch: 122 Train Loss: 0.007107920944690704\n",
      "Epoch 8 Batch: 123 Train Loss: 0.4907906651496887\n",
      "Epoch 8 Batch: 124 Train Loss: 0.007648675702512264\n",
      "Epoch 8 Batch: 125 Train Loss: 0.00887534860521555\n",
      "Epoch 8 Batch: 126 Train Loss: 0.009205587208271027\n",
      "Epoch 8 Batch: 127 Train Loss: 0.008560501039028168\n",
      "Epoch 8 Batch: 128 Train Loss: 0.008741023950278759\n",
      "Epoch 8 Batch: 129 Train Loss: 0.009865091182291508\n",
      "Epoch 8 Batch: 130 Train Loss: 0.00997125543653965\n",
      "Epoch 8 Batch: 131 Train Loss: 0.009982049465179443\n",
      "Epoch 8 Batch: 132 Train Loss: 0.010027185082435608\n",
      "Epoch 8 Batch: 133 Train Loss: 0.4718858599662781\n",
      "Epoch 8 Batch: 134 Train Loss: 0.009347103536128998\n",
      "Epoch 8 Batch: 135 Train Loss: 0.009617243893444538\n",
      "Epoch 8 Batch: 136 Train Loss: 0.01092004869133234\n",
      "Epoch 8 Batch: 137 Train Loss: 0.009961147792637348\n",
      "Epoch 8 Batch: 138 Train Loss: 0.010133480653166771\n",
      "Epoch 8 Batch: 139 Train Loss: 0.45996513962745667\n",
      "Epoch 8 Batch: 140 Train Loss: 0.010505697689950466\n",
      "Epoch 8 Batch: 141 Train Loss: 0.010928255505859852\n",
      "Epoch 8 Batch: 142 Train Loss: 0.012398229911923409\n",
      "Epoch 8 Batch: 143 Train Loss: 0.01008688285946846\n",
      "Epoch 8 Batch: 144 Train Loss: 0.010223442688584328\n",
      "Epoch 8 Batch: 145 Train Loss: 0.011562056839466095\n",
      "Epoch 8 Batch: 146 Train Loss: 0.44775962829589844\n",
      "Epoch 8 Batch: 147 Train Loss: 0.01068919524550438\n",
      "Epoch 8 Batch: 148 Train Loss: 0.01234625093638897\n",
      "Epoch 8 Batch: 149 Train Loss: 0.014111509546637535\n",
      "Epoch 8 Batch: 150 Train Loss: 0.4369206428527832\n",
      "Epoch 8 Batch: 151 Train Loss: 0.014851254411041737\n",
      "Epoch 8 Batch: 152 Train Loss: 0.014223067089915276\n",
      "Epoch 8 Batch: 153 Train Loss: 0.43574851751327515\n",
      "Epoch 8 Batch: 154 Train Loss: 0.015356013551354408\n",
      "Epoch 8 Batch: 155 Train Loss: 0.016075806692242622\n",
      "Epoch 8 Batch: 156 Train Loss: 0.012491827830672264\n",
      "Epoch 8 Batch: 157 Train Loss: 0.016345016658306122\n",
      "Epoch 8 Batch: 158 Train Loss: 0.4206830561161041\n",
      "Epoch 8 Batch: 159 Train Loss: 0.020584234967827797\n",
      "Epoch 8 Batch: 160 Train Loss: 0.021154005080461502\n",
      "Epoch 8 Batch: 161 Train Loss: 0.02176058664917946\n",
      "Epoch 8 Batch: 162 Train Loss: 0.02109391614794731\n",
      "Epoch 8 Batch: 163 Train Loss: 0.02117048017680645\n",
      "Epoch 8 Batch: 164 Train Loss: 0.01675967499613762\n",
      "Epoch 8 Batch: 165 Train Loss: 0.020691195502877235\n",
      "Epoch 8 Batch: 166 Train Loss: 0.0182303749024868\n",
      "Epoch 8 Batch: 167 Train Loss: 0.015966741368174553\n",
      "Epoch 8 Batch: 168 Train Loss: 0.01913655921816826\n",
      "Epoch 8 Batch: 169 Train Loss: 0.018721088767051697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Batch: 170 Train Loss: 0.016081562265753746\n",
      "Epoch 8 Batch: 171 Train Loss: 0.013523587957024574\n",
      "Epoch 8 Batch: 172 Train Loss: 0.013255471363663673\n",
      "Epoch 8 Batch: 173 Train Loss: 0.014338779263198376\n",
      "Epoch 8 Batch: 174 Train Loss: 0.010573921725153923\n",
      "Epoch 8 Batch: 175 Train Loss: 0.014411039650440216\n",
      "Epoch 8 Batch: 176 Train Loss: 0.01359475590288639\n",
      "Epoch 8 Batch: 177 Train Loss: 0.009411109611392021\n",
      "Epoch 8 Batch: 178 Train Loss: 0.013087858445942402\n",
      "Epoch 8 Batch: 179 Train Loss: 0.008416073396801949\n",
      "Epoch 8 Batch: 180 Train Loss: 0.009123256430029869\n",
      "Epoch 8 Batch: 181 Train Loss: 0.008703813888132572\n",
      "Epoch 8 Batch: 182 Train Loss: 0.010267822071909904\n",
      "Epoch 8 Batch: 183 Train Loss: 0.009981540031731129\n",
      "Epoch 8 Batch: 184 Train Loss: 0.008827170357108116\n",
      "Epoch 8 Batch: 185 Train Loss: 0.008617207407951355\n",
      "Epoch 8 Batch: 186 Train Loss: 0.008557343855500221\n",
      "Epoch 8 Batch: 187 Train Loss: 0.008675489574670792\n",
      "Epoch 8 Batch: 188 Train Loss: 0.006199497263878584\n",
      "Epoch 8 Batch: 189 Train Loss: 0.00679663335904479\n",
      "Epoch 8 Batch: 190 Train Loss: 0.007497446145862341\n",
      "Epoch 8 Batch: 191 Train Loss: 0.005909348838031292\n",
      "Epoch 8 Batch: 192 Train Loss: 0.5149479508399963\n",
      "Epoch 8 Batch: 193 Train Loss: 0.005716415122151375\n",
      "Epoch 8 Batch: 194 Train Loss: 0.007017250172793865\n",
      "Epoch 8 Batch: 195 Train Loss: 0.006814724765717983\n",
      "Epoch 8 Batch: 196 Train Loss: 0.006921370979398489\n",
      "Epoch 8 Batch: 197 Train Loss: 0.00681238854303956\n",
      "Epoch 8 Batch: 198 Train Loss: 0.4981599450111389\n",
      "Epoch 8 Batch: 199 Train Loss: 0.004904435481876135\n",
      "Epoch 8 Batch: 200 Train Loss: 0.005244302563369274\n",
      "Epoch 8 Batch: 201 Train Loss: 0.007013390306383371\n",
      "Epoch 8 Batch: 202 Train Loss: 0.00835468340665102\n",
      "Epoch 8 Batch: 203 Train Loss: 0.007922765798866749\n",
      "Epoch 8 Batch: 204 Train Loss: 0.00791397225111723\n",
      "Epoch 8 Batch: 205 Train Loss: 0.0062344809994101524\n",
      "Epoch 8 Batch: 206 Train Loss: 0.008162961341440678\n",
      "Epoch 8 Batch: 207 Train Loss: 0.007153122685849667\n",
      "Epoch 8 Batch: 208 Train Loss: 0.4976769983768463\n",
      "Epoch 8 Batch: 209 Train Loss: 0.4752406179904938\n",
      "Epoch 8 Batch: 210 Train Loss: 0.008199912495911121\n",
      "Epoch 8 Batch: 211 Train Loss: 0.0077064805664122105\n",
      "Epoch 8 Batch: 212 Train Loss: 0.0070327529683709145\n",
      "Epoch 8 Batch: 213 Train Loss: 0.010549211874604225\n",
      "Epoch 8 Batch: 214 Train Loss: 0.010947471484541893\n",
      "Epoch 8 Batch: 215 Train Loss: 0.010552341118454933\n",
      "Epoch 8 Batch: 216 Train Loss: 0.01078468281775713\n",
      "Epoch 8 Batch: 217 Train Loss: 0.012057873420417309\n",
      "Epoch 8 Batch: 218 Train Loss: 0.012160377576947212\n",
      "Epoch 8 Batch: 219 Train Loss: 0.008434594608843327\n",
      "Epoch 8 Batch: 220 Train Loss: 0.011976132169365883\n",
      "Epoch 8 Batch: 221 Train Loss: 0.4436900019645691\n",
      "Epoch 8 Batch: 222 Train Loss: 0.012020031921565533\n",
      "Epoch 8 Batch: 223 Train Loss: 0.012168698012828827\n",
      "Epoch 8 Batch: 224 Train Loss: 0.013861164450645447\n",
      "Epoch 8 Batch: 225 Train Loss: 0.012259546667337418\n",
      "Epoch 8 Batch: 226 Train Loss: 0.012317751534283161\n",
      "Epoch 8 Batch: 227 Train Loss: 0.013115805573761463\n",
      "Epoch 8 Batch: 228 Train Loss: 0.013707267120480537\n",
      "Epoch 8 Batch: 229 Train Loss: 0.012515068054199219\n",
      "Epoch 8 Batch: 230 Train Loss: 0.013007072731852531\n",
      "Epoch 8 Batch: 231 Train Loss: 0.011710606515407562\n",
      "Epoch 8 Batch: 232 Train Loss: 0.00831686146557331\n",
      "Epoch 8 Batch: 233 Train Loss: 0.0111770648509264\n",
      "Epoch 8 Batch: 234 Train Loss: 0.010210348293185234\n",
      "Epoch 8 Batch: 235 Train Loss: 0.010542144998908043\n",
      "Epoch 8 Batch: 236 Train Loss: 0.008708244189620018\n",
      "Epoch 8 Batch: 237 Train Loss: 0.009988807141780853\n",
      "Epoch 8 Batch: 238 Train Loss: 0.009679357521235943\n",
      "Epoch 8 Batch: 239 Train Loss: 0.009889100678265095\n",
      "Epoch 8 Batch: 240 Train Loss: 0.007846814580261707\n",
      "Epoch 8 Batch: 241 Train Loss: 0.008644016459584236\n",
      "Epoch 8 Batch: 242 Train Loss: 0.0084496159106493\n",
      "Epoch 8 Batch: 243 Train Loss: 0.008600793778896332\n",
      "Epoch 8 Batch: 244 Train Loss: 0.00814044289290905\n",
      "Epoch 8 Batch: 245 Train Loss: 0.007739357650279999\n",
      "Epoch 8 Batch: 246 Train Loss: 0.47682133316993713\n",
      "Epoch 8 Batch: 247 Train Loss: 0.007801687810570002\n",
      "Epoch 8 Batch: 248 Train Loss: 0.00635164137929678\n",
      "Epoch 8 Batch: 249 Train Loss: 0.008481877855956554\n",
      "Epoch 8 Batch: 250 Train Loss: 0.008183743804693222\n",
      "Epoch 8 Batch: 251 Train Loss: 0.006267499178647995\n",
      "Epoch 8 Batch: 252 Train Loss: 0.006223171949386597\n",
      "Epoch 8 Batch: 253 Train Loss: 0.006437571253627539\n",
      "Epoch 8 Batch: 254 Train Loss: 0.007620201911777258\n",
      "Epoch 8 Batch: 255 Train Loss: 0.00785384327173233\n",
      "Epoch 8 Batch: 256 Train Loss: 0.0071630580350756645\n",
      "Epoch 8 Batch: 257 Train Loss: 0.006591050885617733\n",
      "Epoch 8 Batch: 258 Train Loss: 0.5290039777755737\n",
      "Epoch 8 Batch: 259 Train Loss: 0.47819510102272034\n",
      "Epoch 8 Batch: 260 Train Loss: 0.005557012744247913\n",
      "Epoch 8 Batch: 261 Train Loss: 0.005886806640774012\n",
      "Epoch 8 Batch: 262 Train Loss: 0.007531818002462387\n",
      "Epoch 8 Batch: 263 Train Loss: 0.007648830767720938\n",
      "Epoch 8 Batch: 264 Train Loss: 0.00748444814234972\n",
      "Epoch 8 Batch: 265 Train Loss: 0.0072510079480707645\n",
      "Epoch 8 Batch: 266 Train Loss: 0.008279082365334034\n",
      "Epoch 8 Batch: 267 Train Loss: 0.011282695457339287\n",
      "Epoch 8 Batch: 268 Train Loss: 0.01047147624194622\n",
      "Epoch 8 Batch: 269 Train Loss: 0.49376440048217773\n",
      "Epoch 8 Batch: 270 Train Loss: 0.010474450886249542\n",
      "Epoch 8 Batch: 271 Train Loss: 0.012117366306483746\n",
      "Epoch 8 Batch: 272 Train Loss: 0.009773248806595802\n",
      "Epoch 8 Batch: 273 Train Loss: 0.009032501839101315\n",
      "Epoch 8 Batch: 274 Train Loss: 0.01164089422672987\n",
      "Epoch 8 Batch: 275 Train Loss: 0.01053584087640047\n",
      "Epoch 8 Batch: 276 Train Loss: 0.009516778402030468\n",
      "Epoch 8 Batch: 277 Train Loss: 0.009187071584165096\n",
      "Epoch 8 Batch: 278 Train Loss: 0.011411255225539207\n",
      "Epoch 8 Batch: 279 Train Loss: 0.010917291976511478\n",
      "Epoch 8 Batch: 280 Train Loss: 0.00947810523211956\n",
      "Epoch 8 Batch: 281 Train Loss: 0.010950278490781784\n",
      "Epoch 8 Batch: 282 Train Loss: 0.010736586526036263\n",
      "Epoch 8 Batch: 283 Train Loss: 0.008399231359362602\n",
      "Epoch 8 Batch: 284 Train Loss: 0.009338329546153545\n",
      "Epoch 8 Batch: 285 Train Loss: 0.009993083775043488\n",
      "Epoch 8 Batch: 286 Train Loss: 0.008057540282607079\n",
      "Epoch 8 Batch: 287 Train Loss: 0.009262638166546822\n",
      "Epoch 8 Batch: 288 Train Loss: 0.009531390853226185\n",
      "Epoch 8 Batch: 289 Train Loss: 0.008722618222236633\n",
      "Epoch 8 Batch: 290 Train Loss: 0.0075081647373735905\n",
      "Epoch 8 Batch: 291 Train Loss: 0.008490652777254581\n",
      "Epoch 8 Batch: 292 Train Loss: 0.007917094975709915\n",
      "Epoch 8 Batch: 293 Train Loss: 0.006237484980374575\n",
      "Epoch 8 Batch: 294 Train Loss: 0.007675508502870798\n",
      "Epoch 8 Batch: 295 Train Loss: 0.006367847323417664\n",
      "Epoch 8 Batch: 296 Train Loss: 0.005515684839338064\n",
      "Epoch 8 Batch: 297 Train Loss: 0.0064362711273133755\n",
      "Epoch 8 Batch: 298 Train Loss: 0.00576762342825532\n",
      "Epoch 8 Batch: 299 Train Loss: 0.006497609429061413\n",
      "Epoch 8 Batch: 300 Train Loss: 0.5063326358795166\n",
      "Epoch 8 Batch: 301 Train Loss: 0.005477496422827244\n",
      "Epoch 8 Batch: 302 Train Loss: 0.006632980890572071\n",
      "Epoch 8 Batch: 303 Train Loss: 0.005992839578539133\n",
      "Epoch 8 Batch: 304 Train Loss: 0.006599722895771265\n",
      "Epoch 8 Batch: 305 Train Loss: 0.004433864261955023\n",
      "Epoch 8 Batch: 306 Train Loss: 0.006389097310602665\n",
      "Epoch 8 Batch: 307 Train Loss: 0.006586428731679916\n",
      "Epoch 8 Batch: 308 Train Loss: 0.005996392574161291\n",
      "Epoch 8 Batch: 309 Train Loss: 0.006667049136012793\n",
      "Epoch 8 Batch: 310 Train Loss: 0.004358581732958555\n",
      "Epoch 8 Batch: 311 Train Loss: 0.5307973623275757\n",
      "Epoch 8 Batch: 312 Train Loss: 0.005124726798385382\n",
      "Epoch 8 Batch: 313 Train Loss: 0.0058216070756316185\n",
      "Epoch 8 Batch: 314 Train Loss: 0.005964343436062336\n",
      "Epoch 8 Batch: 315 Train Loss: 0.5155590176582336\n",
      "Epoch 8 Batch: 316 Train Loss: 0.00713878870010376\n",
      "Epoch 8 Batch: 317 Train Loss: 0.00743064284324646\n",
      "Epoch 8 Batch: 318 Train Loss: 0.007152707315981388\n",
      "Epoch 8 Batch: 319 Train Loss: 0.00829203799366951\n",
      "Epoch 8 Batch: 320 Train Loss: 0.007587012834846973\n",
      "Epoch 8 Batch: 321 Train Loss: 0.006933663971722126\n",
      "Epoch 8 Batch: 322 Train Loss: 0.007159287575632334\n",
      "Epoch 8 Batch: 323 Train Loss: 0.008085583336651325\n",
      "Epoch 8 Batch: 324 Train Loss: 0.009039998054504395\n",
      "Epoch 8 Batch: 325 Train Loss: 0.009053606539964676\n",
      "Epoch 8 Batch: 326 Train Loss: 0.008152930997312069\n",
      "Epoch 8 Batch: 327 Train Loss: 0.008970332331955433\n",
      "Epoch 8 Batch: 328 Train Loss: 0.009034055285155773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Batch: 329 Train Loss: 0.00877129752188921\n",
      "Epoch 8 Batch: 330 Train Loss: 0.008733617141842842\n",
      "Epoch 8 Batch: 331 Train Loss: 0.00785329844802618\n",
      "Epoch 8 Batch: 332 Train Loss: 0.006698682904243469\n",
      "Epoch 8 Batch: 333 Train Loss: 0.008203993551433086\n",
      "Epoch 8 Batch: 334 Train Loss: 0.005645120050758123\n",
      "Epoch 8 Batch: 335 Train Loss: 0.007959066890180111\n",
      "Epoch 8 Batch: 336 Train Loss: 0.006066571921110153\n",
      "Epoch 8 Batch: 337 Train Loss: 0.006749673280864954\n",
      "Epoch 8 Batch: 338 Train Loss: 0.4954395294189453\n",
      "Epoch 8 Batch: 339 Train Loss: 0.007366567850112915\n",
      "Epoch 8 Batch: 340 Train Loss: 0.48894110321998596\n",
      "Epoch 8 Batch: 341 Train Loss: 0.005556266289204359\n",
      "Epoch 8 Batch: 342 Train Loss: 0.007695489563047886\n",
      "Epoch 8 Batch: 343 Train Loss: 0.009240671992301941\n",
      "Epoch 8 Batch: 344 Train Loss: 0.008338140323758125\n",
      "Epoch 8 Batch: 345 Train Loss: 0.006158673204481602\n",
      "Epoch 8 Batch: 346 Train Loss: 0.009885862469673157\n",
      "Epoch 8 Batch: 347 Train Loss: 0.007463610265403986\n",
      "Epoch 8 Batch: 348 Train Loss: 0.010522192344069481\n",
      "Epoch 8 Batch: 349 Train Loss: 0.009996747598052025\n",
      "Epoch 8 Batch: 350 Train Loss: 0.492779016494751\n",
      "Epoch 8 Batch: 351 Train Loss: 0.010260422714054585\n",
      "Epoch 8 Batch: 352 Train Loss: 0.4486033022403717\n",
      "Epoch 8 Batch: 353 Train Loss: 0.01088703889399767\n",
      "Epoch 8 Batch: 354 Train Loss: 0.011493212543427944\n",
      "Epoch 8 Batch: 355 Train Loss: 0.012475733645260334\n",
      "Epoch 8 Batch: 356 Train Loss: 0.011377774178981781\n",
      "Epoch 8 Batch: 357 Train Loss: 0.012233645655214787\n",
      "Epoch 8 Batch: 358 Train Loss: 0.011386595666408539\n",
      "Epoch 8 Batch: 359 Train Loss: 0.01532500796020031\n",
      "Epoch 8 Batch: 360 Train Loss: 0.013189400546252728\n",
      "Epoch 8 Batch: 361 Train Loss: 0.01596001163125038\n",
      "Epoch 8 Batch: 362 Train Loss: 0.013084016740322113\n",
      "Epoch 8 Batch: 363 Train Loss: 0.013439586386084557\n",
      "Epoch 8 Batch: 364 Train Loss: 0.011621354147791862\n",
      "Epoch 8 Batch: 365 Train Loss: 0.010873089544475079\n",
      "Epoch 8 Batch: 366 Train Loss: 0.01264740526676178\n",
      "Epoch 8 Batch: 367 Train Loss: 0.012322434224188328\n",
      "Epoch 8 Batch: 368 Train Loss: 0.4740463197231293\n",
      "Epoch 8 Batch: 369 Train Loss: 0.012546084821224213\n",
      "Epoch 8 Batch: 370 Train Loss: 0.011566167697310448\n",
      "Epoch 8 Batch: 371 Train Loss: 0.01134353969246149\n",
      "Epoch 8 Batch: 372 Train Loss: 0.010674295015633106\n",
      "Epoch 8 Batch: 373 Train Loss: 0.006141873076558113\n",
      "Epoch 8 Batch: 374 Train Loss: 0.012147142551839352\n",
      "Epoch 8 Batch: 375 Train Loss: 0.009925419464707375\n",
      "Epoch 8 Batch: 376 Train Loss: 0.012007496319711208\n",
      "Epoch 8 Batch: 377 Train Loss: 0.010211681947112083\n",
      "Epoch 8 Batch: 378 Train Loss: 0.009725343436002731\n",
      "Epoch 8 Batch: 379 Train Loss: 0.00853340607136488\n",
      "Epoch 8 Batch: 380 Train Loss: 0.010180196724832058\n",
      "Epoch 8 Batch: 381 Train Loss: 0.008990949019789696\n",
      "Epoch 8 Batch: 382 Train Loss: 0.009624458849430084\n",
      "Epoch 8 Batch: 383 Train Loss: 0.008470358327031136\n",
      "Epoch 8 Batch: 384 Train Loss: 0.4694767892360687\n",
      "Epoch 8 Batch: 385 Train Loss: 0.007970824837684631\n",
      "Epoch 8 Batch: 386 Train Loss: 0.008562619797885418\n",
      "Epoch 8 Batch: 387 Train Loss: 0.008602485992014408\n",
      "Epoch 8 Batch: 388 Train Loss: 0.009234965778887272\n",
      "Epoch 8 Batch: 389 Train Loss: 0.007742167916148901\n",
      "Epoch 8 Batch: 390 Train Loss: 0.008288735523819923\n",
      "Epoch 8 Batch: 391 Train Loss: 0.009469426237046719\n",
      "Epoch 8 Batch: 392 Train Loss: 0.008552624844014645\n",
      "Epoch 8 Batch: 393 Train Loss: 0.9785162210464478\n",
      "Epoch 8 Batch: 394 Train Loss: 0.007681342773139477\n",
      "Epoch 8 Batch: 395 Train Loss: 0.008675642311573029\n",
      "Epoch 8 Batch: 396 Train Loss: 0.010066512040793896\n",
      "Epoch 8 Batch: 397 Train Loss: 0.011250181123614311\n",
      "Epoch 8 Batch: 398 Train Loss: 0.011447333730757236\n",
      "Epoch 8 Batch: 399 Train Loss: 0.008051090873777866\n",
      "Epoch 8 Batch: 400 Train Loss: 0.008309668861329556\n",
      "Epoch 8 Batch: 401 Train Loss: 0.009542571380734444\n",
      "Epoch 8 Batch: 402 Train Loss: 0.449019193649292\n",
      "Epoch 8 Batch: 403 Train Loss: 0.01283054519444704\n",
      "Epoch 8 Batch: 404 Train Loss: 0.013087483122944832\n",
      "Epoch 8 Batch: 405 Train Loss: 0.013601777143776417\n",
      "Epoch 8 Batch: 406 Train Loss: 0.013659410178661346\n",
      "Epoch 8 Batch: 407 Train Loss: 0.011229453608393669\n",
      "Epoch 8 Batch: 408 Train Loss: 0.013935288414359093\n",
      "Epoch 8 Batch: 409 Train Loss: 0.012632876634597778\n",
      "Epoch 8 Batch: 410 Train Loss: 0.4436769485473633\n",
      "Epoch 8 Batch: 411 Train Loss: 0.014407748356461525\n",
      "Epoch 8 Batch: 412 Train Loss: 0.43613964319229126\n",
      "Epoch 8 Batch: 413 Train Loss: 0.015315154567360878\n",
      "Epoch 8 Batch: 414 Train Loss: 0.014183634892106056\n",
      "Epoch 8 Batch: 415 Train Loss: 0.013262225314974785\n",
      "Epoch 8 Batch: 416 Train Loss: 0.013244064524769783\n",
      "Epoch 8 Batch: 417 Train Loss: 0.013640058226883411\n",
      "Epoch 8 Batch: 418 Train Loss: 0.015352043323218822\n",
      "Epoch 8 Batch: 419 Train Loss: 0.013199846260249615\n",
      "Epoch 8 Batch: 420 Train Loss: 0.01687149703502655\n",
      "Epoch 8 Batch: 421 Train Loss: 0.015169506892561913\n",
      "Epoch 8 Batch: 422 Train Loss: 0.01495315134525299\n",
      "Epoch 8 Batch: 423 Train Loss: 0.015476657077670097\n",
      "Epoch 8 Batch: 424 Train Loss: 0.016987798735499382\n",
      "Epoch 8 Batch: 425 Train Loss: 0.013971425592899323\n",
      "Epoch 8 Batch: 426 Train Loss: 0.016549060121178627\n",
      "Epoch 8 Batch: 427 Train Loss: 0.014772402122616768\n",
      "Epoch 8 Batch: 428 Train Loss: 0.013896048069000244\n",
      "Epoch 8 Batch: 429 Train Loss: 0.014169061556458473\n",
      "Epoch 8 Batch: 430 Train Loss: 0.013242321088910103\n",
      "Epoch 8 Batch: 431 Train Loss: 0.011997770518064499\n",
      "Epoch 8 Batch: 432 Train Loss: 0.011829128488898277\n",
      "Epoch 8 Batch: 433 Train Loss: 0.44189924001693726\n",
      "Epoch 8 Batch: 434 Train Loss: 0.011658469215035439\n",
      "Epoch 8 Batch: 435 Train Loss: 0.011588241904973984\n",
      "Epoch 8 Batch: 436 Train Loss: 0.43973255157470703\n",
      "Epoch 8 Batch: 437 Train Loss: 0.012258296832442284\n",
      "Epoch 8 Batch: 438 Train Loss: 0.012059462256729603\n",
      "Epoch 8 Batch: 439 Train Loss: 0.010635193437337875\n",
      "Epoch 8 Batch: 440 Train Loss: 0.012903260998427868\n",
      "Epoch 8 Batch: 441 Train Loss: 0.011428363621234894\n",
      "Epoch 8 Batch: 442 Train Loss: 0.01141438353806734\n",
      "Epoch 8 Batch: 443 Train Loss: 0.010999650694429874\n",
      "Epoch 8 Batch: 444 Train Loss: 0.009704606607556343\n",
      "Epoch 8 Batch: 445 Train Loss: 0.010672791860997677\n",
      "Epoch 8 Batch: 446 Train Loss: 0.011576446704566479\n",
      "Epoch 8 Batch: 447 Train Loss: 0.4909477233886719\n",
      "Epoch 8 Batch: 448 Train Loss: 0.010256359353661537\n",
      "Epoch 8 Batch: 449 Train Loss: 0.010619579814374447\n",
      "Epoch 8 Batch: 450 Train Loss: 0.009832401759922504\n",
      "Epoch 8 Batch: 451 Train Loss: 0.010757982730865479\n",
      "Epoch 8 Batch: 452 Train Loss: 0.012567222118377686\n",
      "Epoch 8 Batch: 453 Train Loss: 0.008310404606163502\n",
      "Epoch 8 Batch: 454 Train Loss: 0.010589616373181343\n",
      "Epoch 8 Batch: 455 Train Loss: 0.012152500450611115\n",
      "Epoch 8 Batch: 456 Train Loss: 0.009425101801753044\n",
      "Epoch 8 Batch: 457 Train Loss: 0.0106326499953866\n",
      "Epoch 8 Batch: 458 Train Loss: 0.007904162630438805\n",
      "Epoch 8 Batch: 459 Train Loss: 0.009161895141005516\n",
      "Epoch 8 Batch: 460 Train Loss: 0.010893847793340683\n",
      "Epoch 8 Batch: 461 Train Loss: 0.008100971579551697\n",
      "Epoch 8 Batch: 462 Train Loss: 0.010002806782722473\n",
      "Epoch 8 Batch: 463 Train Loss: 0.00955614261329174\n",
      "Epoch 8 Batch: 464 Train Loss: 0.008839139714837074\n",
      "Epoch 8 Batch: 465 Train Loss: 0.007052981294691563\n",
      "Epoch 8 Batch: 466 Train Loss: 0.009081536903977394\n",
      "Epoch 8 Batch: 467 Train Loss: 0.5127238035202026\n",
      "Epoch 8 Batch: 468 Train Loss: 0.007822936400771141\n",
      "Epoch 8 Batch: 469 Train Loss: 0.007518275640904903\n",
      "Epoch 8 Batch: 470 Train Loss: 0.008560454472899437\n",
      "Epoch 8 Batch: 471 Train Loss: 0.009454072453081608\n",
      "Epoch 8 Batch: 472 Train Loss: 0.008355618454515934\n",
      "Epoch 8 Batch: 473 Train Loss: 0.007888136431574821\n",
      "Epoch 8 Batch: 474 Train Loss: 0.008818646892905235\n",
      "Epoch 8 Batch: 475 Train Loss: 0.006794586777687073\n",
      "Epoch 8 Batch: 476 Train Loss: 0.0071576619520783424\n",
      "Epoch 8 Batch: 477 Train Loss: 0.00853666104376316\n",
      "Epoch 8 Batch: 478 Train Loss: 0.00603301590308547\n",
      "Epoch 8 Batch: 479 Train Loss: 0.006767588201910257\n",
      "Epoch 8 Batch: 480 Train Loss: 0.0067892358638346195\n",
      "Epoch 8 Batch: 481 Train Loss: 0.006479746662080288\n",
      "Epoch 8 Batch: 482 Train Loss: 0.006827960256487131\n",
      "Epoch 8 Batch: 483 Train Loss: 0.006610698997974396\n",
      "Epoch 8 Batch: 484 Train Loss: 0.006768965162336826\n",
      "Epoch 8 Batch: 485 Train Loss: 0.006714230868965387\n",
      "Epoch 8 Batch: 486 Train Loss: 0.005834819283336401\n",
      "Epoch 8 Batch: 487 Train Loss: 0.005360577255487442\n",
      "Epoch 8 Batch: 488 Train Loss: 0.00598291726782918\n",
      "Epoch 8 Batch: 489 Train Loss: 0.005374629516154528\n",
      "Epoch 8 Batch: 490 Train Loss: 0.00461467495188117\n",
      "Epoch 8 Batch: 491 Train Loss: 0.0056054797023534775\n",
      "Epoch 8 Batch: 492 Train Loss: 0.005289665423333645\n",
      "Epoch 8 Batch: 493 Train Loss: 0.005149464122951031\n",
      "Epoch 8 Batch: 494 Train Loss: 0.005394545383751392\n",
      "Epoch 8 Batch: 495 Train Loss: 0.004093761555850506\n",
      "Epoch 8 Batch: 496 Train Loss: 0.004297124221920967\n",
      "Epoch 8 Batch: 497 Train Loss: 0.004900064319372177\n",
      "Epoch 8 Batch: 498 Train Loss: 0.004273568280041218\n",
      "Epoch 8 Batch: 499 Train Loss: 0.5365908145904541\n",
      "Epoch 8 Batch: 500 Train Loss: 0.003994924481958151\n",
      "Epoch 8 Batch: 501 Train Loss: 0.004750838503241539\n",
      "Epoch 8 Batch: 502 Train Loss: 0.5477202534675598\n",
      "Epoch 8 Batch: 503 Train Loss: 0.004501536954194307\n",
      "Epoch 8 Batch: 504 Train Loss: 0.004367614630609751\n",
      "Epoch 8 Batch: 505 Train Loss: 0.005301327910274267\n",
      "Epoch 8 Batch: 506 Train Loss: 0.004867894574999809\n",
      "Epoch 8 Batch: 507 Train Loss: 0.0059743961319327354\n",
      "Epoch 8 Batch: 508 Train Loss: 0.005752775352448225\n",
      "Epoch 8 Batch: 509 Train Loss: 0.0065728770568966866\n",
      "Epoch 8 Batch: 510 Train Loss: 0.006211685482412577\n",
      "Epoch 8 Batch: 511 Train Loss: 0.006134841591119766\n",
      "Epoch 8 Batch: 512 Train Loss: 0.006178318988531828\n",
      "Epoch 8 Batch: 513 Train Loss: 0.006560892798006535\n",
      "Epoch 8 Batch: 514 Train Loss: 0.006478508003056049\n",
      "Epoch 8 Batch: 515 Train Loss: 0.0060933674685657024\n",
      "Epoch 8 Batch: 516 Train Loss: 0.006145048420876265\n",
      "Epoch 8 Batch: 517 Train Loss: 0.005222788546234369\n",
      "Epoch 8 Batch: 518 Train Loss: 0.005871315486729145\n",
      "Epoch 8 Batch: 519 Train Loss: 0.006413252092897892\n",
      "Epoch 8 Batch: 520 Train Loss: 0.0057282173074781895\n",
      "Epoch 8 Batch: 521 Train Loss: 0.0062332479283213615\n",
      "Epoch 8 Batch: 522 Train Loss: 0.0055483439937233925\n",
      "Epoch 8 Batch: 523 Train Loss: 0.006326459348201752\n",
      "Epoch 8 Batch: 524 Train Loss: 0.0054222336038947105\n",
      "Epoch 8 Batch: 525 Train Loss: 0.005937897600233555\n",
      "Epoch 8 Batch: 526 Train Loss: 0.005134587176144123\n",
      "Epoch 8 Batch: 527 Train Loss: 0.005094229243695736\n",
      "Epoch 8 Batch: 528 Train Loss: 0.518979549407959\n",
      "Epoch 8 Batch: 529 Train Loss: 0.5303472876548767\n",
      "Epoch 8 Batch: 530 Train Loss: 0.5232200622558594\n",
      "Epoch 8 Batch: 531 Train Loss: 0.006787333637475967\n",
      "Epoch 8 Batch: 532 Train Loss: 0.00770120183005929\n",
      "Epoch 8 Batch: 533 Train Loss: 0.007257281802594662\n",
      "Epoch 8 Batch: 534 Train Loss: 0.008665683679282665\n",
      "Epoch 8 Batch: 535 Train Loss: 0.47705307602882385\n",
      "Epoch 8 Batch: 536 Train Loss: 0.010072280652821064\n",
      "Epoch 8 Batch: 537 Train Loss: 0.008706013672053814\n",
      "Epoch 8 Batch: 538 Train Loss: 0.009315351024270058\n",
      "Epoch 8 Batch: 539 Train Loss: 0.010985849425196648\n",
      "Epoch 8 Batch: 540 Train Loss: 0.011506645008921623\n",
      "Epoch 8 Batch: 541 Train Loss: 0.013249201700091362\n",
      "Epoch 8 Batch: 542 Train Loss: 0.013593850657343864\n",
      "Epoch 8 Batch: 543 Train Loss: 0.013903260231018066\n",
      "Epoch 8 Batch: 544 Train Loss: 0.013977388851344585\n",
      "Epoch 8 Batch: 545 Train Loss: 0.012554909102618694\n",
      "Epoch 8 Batch: 546 Train Loss: 0.013977366499602795\n",
      "Epoch 8 Batch: 547 Train Loss: 0.013826807029545307\n",
      "Epoch 8 Batch: 548 Train Loss: 0.01369150448590517\n",
      "Epoch 8 Batch: 549 Train Loss: 0.012194685637950897\n",
      "Epoch 8 Batch: 550 Train Loss: 0.011991418898105621\n",
      "Epoch 8 Batch: 551 Train Loss: 0.01030685193836689\n",
      "Epoch 8 Batch: 552 Train Loss: 0.01264129113405943\n",
      "Epoch 8 Batch: 553 Train Loss: 0.012330977246165276\n",
      "Epoch 8 Batch: 554 Train Loss: 0.010764757171273232\n",
      "Epoch 8 Batch: 555 Train Loss: 0.011477863416075706\n",
      "Epoch 8 Batch: 556 Train Loss: 0.011177816428244114\n",
      "Epoch 8 Batch: 557 Train Loss: 0.010881599970161915\n",
      "Epoch 8 Batch: 558 Train Loss: 0.009299203753471375\n",
      "Epoch 8 Batch: 559 Train Loss: 0.009049070999026299\n",
      "Epoch 8 Batch: 560 Train Loss: 0.009739345870912075\n",
      "Epoch 8 Batch: 561 Train Loss: 0.00792666058987379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Batch: 562 Train Loss: 0.008199566043913364\n",
      "Epoch 8 Batch: 563 Train Loss: 0.007770112250000238\n",
      "Epoch 8 Batch: 564 Train Loss: 0.0059187207370996475\n",
      "Epoch 8 Batch: 565 Train Loss: 0.006538978312164545\n",
      "Epoch 8 Batch: 566 Train Loss: 0.00544483819976449\n",
      "Epoch 8 Batch: 567 Train Loss: 0.006767584476619959\n",
      "Epoch 8 Batch: 568 Train Loss: 0.006538530346006155\n",
      "Epoch 8 Batch: 569 Train Loss: 0.006421045400202274\n",
      "Epoch 8 Batch: 570 Train Loss: 0.005510421935468912\n",
      "Epoch 8 Batch: 571 Train Loss: 0.005940139293670654\n",
      "Epoch 8 Batch: 572 Train Loss: 0.00574751291424036\n",
      "Epoch 8 Batch: 573 Train Loss: 0.005533994175493717\n",
      "Epoch 8 Batch: 574 Train Loss: 0.005302720703184605\n",
      "Epoch 8 Batch: 575 Train Loss: 0.005191609729081392\n",
      "Epoch 8 Batch: 576 Train Loss: 0.005605801474303007\n",
      "Epoch 8 Batch: 577 Train Loss: 0.00543431518599391\n",
      "Epoch 8 Batch: 578 Train Loss: 0.005346968770027161\n",
      "Epoch 8 Batch: 579 Train Loss: 0.004612045828253031\n",
      "Epoch 8 Batch: 580 Train Loss: 0.0044515663757920265\n",
      "Epoch 8 Batch: 581 Train Loss: 0.004322449676692486\n",
      "Epoch 8 Batch: 582 Train Loss: 0.00470124464482069\n",
      "Epoch 8 Batch: 583 Train Loss: 0.004525177646428347\n",
      "Epoch 8 Batch: 584 Train Loss: 0.004441818688064814\n",
      "Epoch 8 Batch: 585 Train Loss: 0.0034600861836224794\n",
      "Epoch 8 Batch: 586 Train Loss: 0.003776324912905693\n",
      "Epoch 8 Batch: 587 Train Loss: 0.0036585568450391293\n",
      "Epoch 8 Batch: 588 Train Loss: 0.0035276778507977724\n",
      "Epoch 8 Batch: 589 Train Loss: 0.003098104614764452\n",
      "Epoch 8 Batch: 590 Train Loss: 0.003446323098614812\n",
      "Epoch 8 Batch: 591 Train Loss: 0.0032937233336269855\n",
      "Epoch 8 Batch: 592 Train Loss: 0.0032516170758754015\n",
      "Epoch 8 Batch: 593 Train Loss: 0.0035210144706070423\n",
      "Epoch 8 Batch: 594 Train Loss: 0.00271989731118083\n",
      "Epoch 8 Batch: 595 Train Loss: 0.003054243978112936\n",
      "Epoch 8 Batch: 596 Train Loss: 0.002313682110980153\n",
      "Epoch 8 Batch: 597 Train Loss: 0.0028790482319891453\n",
      "Epoch 8 Batch: 598 Train Loss: 0.0031597677152603865\n",
      "Epoch 8 Batch: 599 Train Loss: 0.0027630762197077274\n",
      "Epoch 8 Batch: 600 Train Loss: 0.0030960377771407366\n",
      "Epoch 8 Batch: 601 Train Loss: 0.002090166322886944\n",
      "Epoch 8 Batch: 602 Train Loss: 0.0030549720395356417\n",
      "Epoch 8 Batch: 603 Train Loss: 0.5902000665664673\n",
      "Epoch 8 Batch: 604 Train Loss: 0.5830867886543274\n",
      "Epoch 8 Batch: 605 Train Loss: 0.0032709683291614056\n",
      "Epoch 8 Batch: 606 Train Loss: 0.0031872864346951246\n",
      "Epoch 8 Batch: 607 Train Loss: 0.003034265711903572\n",
      "Epoch 8 Batch: 608 Train Loss: 0.004053886979818344\n",
      "Epoch 8 Batch: 609 Train Loss: 0.00427255779504776\n",
      "Epoch 8 Batch: 610 Train Loss: 0.0035744556225836277\n",
      "Epoch 8 Batch: 611 Train Loss: 0.004161484073847532\n",
      "Epoch 8 Batch: 612 Train Loss: 0.004308087285608053\n",
      "Epoch 8 Batch: 613 Train Loss: 0.003900516079738736\n",
      "Epoch 8 Batch: 614 Train Loss: 0.5326687097549438\n",
      "Epoch 8 Batch: 615 Train Loss: 0.00422495836392045\n",
      "Epoch 8 Batch: 616 Train Loss: 0.004400325007736683\n",
      "Epoch 8 Batch: 617 Train Loss: 0.004859813489019871\n",
      "Epoch 8 Batch: 618 Train Loss: 1.020464539527893\n",
      "Epoch 8 Batch: 619 Train Loss: 0.007406591437757015\n",
      "Epoch 8 Batch: 620 Train Loss: 0.007982761599123478\n",
      "Epoch 8 Batch: 621 Train Loss: 0.008914804086089134\n",
      "Epoch 8 Batch: 622 Train Loss: 0.009532497264444828\n",
      "Epoch 8 Batch: 623 Train Loss: 0.009269990026950836\n",
      "Epoch 8 Batch: 624 Train Loss: 0.00747718196362257\n",
      "Epoch 8 Batch: 625 Train Loss: 0.010501575656235218\n",
      "Epoch 8 Batch: 626 Train Loss: 0.010715330019593239\n",
      "Epoch 8 Batch: 627 Train Loss: 0.011715425178408623\n",
      "Epoch 8 Batch: 628 Train Loss: 0.4403640329837799\n",
      "Epoch 8 Batch: 629 Train Loss: 0.013685700483620167\n",
      "Epoch 8 Batch: 630 Train Loss: 0.013745637610554695\n",
      "Epoch 8 Batch: 631 Train Loss: 0.013546401634812355\n",
      "Epoch 8 Batch: 632 Train Loss: 0.4254724979400635\n",
      "Epoch 8 Batch: 633 Train Loss: 0.01191666442900896\n",
      "Epoch 8 Batch: 634 Train Loss: 0.01717858947813511\n",
      "Epoch 8 Batch: 635 Train Loss: 0.014227296225726604\n",
      "Epoch 8 Batch: 636 Train Loss: 0.014742972329258919\n",
      "Epoch 8 Batch: 637 Train Loss: 0.4003627896308899\n",
      "Epoch 8 Batch: 638 Train Loss: 0.018358230590820312\n",
      "Epoch 8 Batch: 639 Train Loss: 0.016075927764177322\n",
      "Epoch 8 Batch: 640 Train Loss: 0.014754894189536572\n",
      "Epoch 8 Batch: 641 Train Loss: 0.3892061114311218\n",
      "Epoch 8 Batch: 642 Train Loss: 0.014553715474903584\n",
      "Epoch 8 Batch: 643 Train Loss: 0.02413354441523552\n",
      "Epoch 8 Batch: 644 Train Loss: 0.019114326685667038\n",
      "Epoch 8 Batch: 645 Train Loss: 0.023999953642487526\n",
      "Epoch 8 Batch: 646 Train Loss: 0.022624585777521133\n",
      "Epoch 8 Batch: 647 Train Loss: 0.021173905581235886\n",
      "Epoch 8 Batch: 648 Train Loss: 0.37516459822654724\n",
      "Epoch 8 Batch: 649 Train Loss: 0.012218741700053215\n",
      "Epoch 8 Batch: 650 Train Loss: 0.015467542223632336\n",
      "Epoch 8 Batch: 651 Train Loss: 0.01855517365038395\n",
      "Epoch 8 Batch: 652 Train Loss: 0.01848786324262619\n",
      "Epoch 8 Batch: 653 Train Loss: 0.024990804493427277\n",
      "Epoch 8 Batch: 654 Train Loss: 0.019411247223615646\n",
      "Epoch 8 Batch: 655 Train Loss: 0.017956990748643875\n",
      "Epoch 8 Batch: 656 Train Loss: 0.015808243304491043\n",
      "Epoch 8 Batch: 657 Train Loss: 0.014937316998839378\n",
      "Epoch 8 Batch: 658 Train Loss: 0.02563958242535591\n",
      "Epoch 8 Batch: 659 Train Loss: 0.012701233848929405\n",
      "Epoch 8 Batch: 660 Train Loss: 0.019596021622419357\n",
      "Epoch 8 Batch: 661 Train Loss: 0.4687306880950928\n",
      "Epoch 8 Batch: 662 Train Loss: 0.021156582981348038\n",
      "Epoch 8 Batch: 663 Train Loss: 0.013263849541544914\n",
      "Epoch 8 Batch: 664 Train Loss: 0.01759812980890274\n",
      "Epoch 8 Batch: 665 Train Loss: 0.01351259183138609\n",
      "Epoch 8 Batch: 666 Train Loss: 0.012142742983996868\n",
      "Epoch 8 Batch: 667 Train Loss: 0.014047528617084026\n",
      "Epoch 8 Batch: 668 Train Loss: 0.011617016978561878\n",
      "Epoch 8 Batch: 669 Train Loss: 0.018550308421254158\n",
      "Epoch 8 Batch: 670 Train Loss: 0.015850592404603958\n",
      "Epoch 8 Batch: 671 Train Loss: 0.014276593923568726\n",
      "Epoch 8 Batch: 672 Train Loss: 0.012680133804678917\n",
      "Epoch 8 Batch: 673 Train Loss: 0.015659987926483154\n",
      "Epoch 8 Batch: 674 Train Loss: 0.011658997274935246\n",
      "Epoch 8 Batch: 675 Train Loss: 0.012703465297818184\n",
      "Epoch 8 Batch: 676 Train Loss: 0.010569429025053978\n",
      "Epoch 8 Batch: 677 Train Loss: 0.006601558066904545\n",
      "Epoch 8 Batch: 678 Train Loss: 0.010415739379823208\n",
      "Epoch 8 Batch: 679 Train Loss: 0.008056131191551685\n",
      "Epoch 8 Batch: 680 Train Loss: 0.008203857578337193\n",
      "Epoch 8 Batch: 681 Train Loss: 0.008047839626669884\n",
      "Epoch 8 Batch: 682 Train Loss: 0.008652565069496632\n",
      "Epoch 8 Batch: 683 Train Loss: 0.012912837788462639\n",
      "Epoch 8 Batch: 684 Train Loss: 0.4631802439689636\n",
      "Epoch 8 Batch: 685 Train Loss: 0.46193090081214905\n",
      "Epoch 8 Batch: 686 Train Loss: 0.45782867074012756\n",
      "Epoch 8 Batch: 687 Train Loss: 0.4479925036430359\n",
      "Epoch 8 Batch: 688 Train Loss: 0.010664311237633228\n",
      "Epoch 8 Batch: 689 Train Loss: 0.00929905753582716\n",
      "Epoch 8 Batch: 690 Train Loss: 0.4930427074432373\n",
      "Epoch 8 Batch: 691 Train Loss: 0.01041830237954855\n",
      "Epoch 8 Batch: 692 Train Loss: 0.014587664976716042\n",
      "Epoch 8 Batch: 693 Train Loss: 0.01727047935128212\n",
      "Epoch 8 Batch: 694 Train Loss: 0.012022646144032478\n",
      "Epoch 8 Batch: 695 Train Loss: 0.015303236432373524\n",
      "Epoch 8 Batch: 696 Train Loss: 0.01442700158804655\n",
      "Epoch 8 Batch: 697 Train Loss: 0.4580243229866028\n",
      "Epoch 8 Batch: 698 Train Loss: 0.014402246102690697\n",
      "Epoch 8 Batch: 699 Train Loss: 0.021269824355840683\n",
      "Epoch 8 Batch: 700 Train Loss: 0.021930072456598282\n",
      "Epoch 8 Batch: 701 Train Loss: 0.022363485768437386\n",
      "Epoch 8 Batch: 702 Train Loss: 0.02097991108894348\n",
      "Epoch 8 Batch: 703 Train Loss: 0.020986799150705338\n",
      "Epoch 8 Batch: 704 Train Loss: 0.01926441863179207\n",
      "Epoch 8 Batch: 705 Train Loss: 0.022284196689724922\n",
      "Epoch 8 Batch: 706 Train Loss: 0.029280927032232285\n",
      "Epoch 8 Batch: 707 Train Loss: 0.02125345543026924\n",
      "Epoch 8 Batch: 708 Train Loss: 0.024797853082418442\n",
      "Epoch 8 Batch: 709 Train Loss: 0.02134159952402115\n",
      "Epoch 8 Batch: 710 Train Loss: 0.019056793302297592\n",
      "Epoch 8 Batch: 711 Train Loss: 0.01796950027346611\n",
      "Epoch 8 Batch: 712 Train Loss: 0.019846227020025253\n",
      "Epoch 8 Batch: 713 Train Loss: 0.017600776627659798\n",
      "Epoch 8 Batch: 714 Train Loss: 0.016720620915293694\n",
      "Epoch 8 Batch: 715 Train Loss: 0.014666125178337097\n",
      "Epoch 8 Batch: 716 Train Loss: 0.014244144782423973\n",
      "Epoch 8 Batch: 717 Train Loss: 0.01387045718729496\n",
      "Epoch 8 Batch: 718 Train Loss: 0.013857094570994377\n",
      "Epoch 8 Batch: 719 Train Loss: 0.012505700811743736\n",
      "Epoch 8 Batch: 720 Train Loss: 0.010921304114162922\n",
      "Epoch 8 Batch: 721 Train Loss: 0.009260758757591248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Batch: 722 Train Loss: 0.010376440361142159\n",
      "Epoch 8 Batch: 723 Train Loss: 0.010685616172850132\n",
      "Epoch 8 Batch: 724 Train Loss: 0.011083207093179226\n",
      "Epoch 8 Batch: 725 Train Loss: 0.008481191471219063\n",
      "Epoch 8 Batch: 726 Train Loss: 0.00848471000790596\n",
      "Epoch 8 Batch: 727 Train Loss: 0.009152904152870178\n",
      "Epoch 8 Batch: 728 Train Loss: 0.007051208056509495\n",
      "Epoch 8 Batch: 729 Train Loss: 0.0076396772637963295\n",
      "Epoch 8 Batch: 730 Train Loss: 0.007352763321250677\n",
      "Epoch 8 Batch: 731 Train Loss: 0.0067216320894658566\n",
      "Epoch 8 Batch: 732 Train Loss: 0.006174447480589151\n",
      "Epoch 8 Batch: 733 Train Loss: 0.005615527741611004\n",
      "Epoch 8 Batch: 734 Train Loss: 0.00669772457331419\n",
      "Epoch 8 Batch: 735 Train Loss: 0.006433135364204645\n",
      "Epoch 8 Batch: 736 Train Loss: 0.005708866752684116\n",
      "Epoch 8 Batch: 737 Train Loss: 0.004806048236787319\n",
      "Epoch 8 Batch: 738 Train Loss: 0.005513220094144344\n",
      "Epoch 8 Batch: 739 Train Loss: 0.549644947052002\n",
      "Epoch 8 Batch: 740 Train Loss: 0.004166022874414921\n",
      "Epoch 8 Batch: 741 Train Loss: 0.00417956430464983\n",
      "Epoch 8 Batch: 742 Train Loss: 0.005214150995016098\n",
      "Epoch 8 Batch: 743 Train Loss: 0.5103018879890442\n",
      "Epoch 8 Batch: 744 Train Loss: 0.5052258968353271\n",
      "Epoch 8 Batch: 745 Train Loss: 0.005842854268848896\n",
      "Epoch 8 Batch: 746 Train Loss: 0.007355152163654566\n",
      "Epoch 8 Batch: 747 Train Loss: 0.00650280574336648\n",
      "Epoch 8 Batch: 748 Train Loss: 0.0083627849817276\n",
      "Epoch 8 Batch: 749 Train Loss: 0.47094863653182983\n",
      "Epoch 8 Batch: 750 Train Loss: 0.007808415684849024\n",
      "Epoch 8 Batch: 751 Train Loss: 0.009115722961723804\n",
      "Epoch 8 Batch: 752 Train Loss: 0.00964873842895031\n",
      "Epoch 8 Batch: 753 Train Loss: 0.008483453653752804\n",
      "Epoch 8 Batch: 754 Train Loss: 0.01050927396863699\n",
      "Epoch 8 Batch: 755 Train Loss: 0.010251456871628761\n",
      "Epoch 8 Batch: 756 Train Loss: 0.010811837390065193\n",
      "Epoch 8 Batch: 757 Train Loss: 0.012249507941305637\n",
      "Epoch 8 Batch: 758 Train Loss: 0.010503197088837624\n",
      "Epoch 8 Batch: 759 Train Loss: 0.011859226040542126\n",
      "Epoch 8 Batch: 760 Train Loss: 0.4285219609737396\n",
      "Epoch 8 Batch: 761 Train Loss: 0.48229560256004333\n",
      "Epoch 8 Batch: 762 Train Loss: 0.014144535176455975\n",
      "Epoch 8 Batch: 763 Train Loss: 0.4152648448944092\n",
      "Epoch 8 Batch: 764 Train Loss: 0.014448950998485088\n",
      "Epoch 8 Batch: 765 Train Loss: 0.0182209312915802\n",
      "Epoch 8 Batch: 766 Train Loss: 0.01624538190662861\n",
      "Epoch 8 Batch: 767 Train Loss: 0.013557635247707367\n",
      "Epoch 8 Batch: 768 Train Loss: 0.02072283998131752\n",
      "Epoch 8 Batch: 769 Train Loss: 0.011804139241576195\n",
      "Epoch 8 Batch: 770 Train Loss: 0.015201637521386147\n",
      "Epoch 8 Batch: 771 Train Loss: 0.018047213554382324\n",
      "Epoch 8 Batch: 772 Train Loss: 0.019433382898569107\n",
      "Epoch 8 Batch: 773 Train Loss: 0.021661223843693733\n",
      "Epoch 8 Batch: 774 Train Loss: 0.0190876554697752\n",
      "Epoch 8 Batch: 775 Train Loss: 0.019882932305336\n",
      "Epoch 8 Batch: 776 Train Loss: 0.017042037099599838\n",
      "Epoch 8 Batch: 777 Train Loss: 0.018910031765699387\n",
      "Epoch 8 Batch: 778 Train Loss: 0.013999944552779198\n",
      "Epoch 8 Batch: 779 Train Loss: 0.3982886075973511\n",
      "Epoch 8 Batch: 780 Train Loss: 0.01455796230584383\n",
      "Epoch 8 Batch: 781 Train Loss: 0.017711909487843513\n",
      "Epoch 8 Batch: 782 Train Loss: 0.01650518923997879\n",
      "Epoch 8 Batch: 783 Train Loss: 0.013005333952605724\n",
      "Epoch 8 Batch: 784 Train Loss: 0.01904657855629921\n",
      "Epoch 8 Batch: 785 Train Loss: 0.01558652799576521\n",
      "Epoch 8 Batch: 786 Train Loss: 0.01813799887895584\n",
      "Epoch 8 Batch: 787 Train Loss: 0.012722042389214039\n",
      "Epoch 8 Batch: 788 Train Loss: 0.013264523819088936\n",
      "Epoch 8 Batch: 789 Train Loss: 0.4790651202201843\n",
      "Epoch 8 Batch: 790 Train Loss: 0.012006767094135284\n",
      "Epoch 8 Batch: 791 Train Loss: 0.010123045183718204\n",
      "Epoch 8 Batch: 792 Train Loss: 0.013490338809788227\n",
      "Epoch 8 Batch: 793 Train Loss: 0.013474596664309502\n",
      "Epoch 8 Batch: 794 Train Loss: 0.011753136292099953\n",
      "Epoch 8 Batch: 795 Train Loss: 0.010636178776621819\n",
      "Epoch 8 Batch: 796 Train Loss: 0.009516141377389431\n",
      "Epoch 8 Batch: 797 Train Loss: 0.4818841814994812\n",
      "Epoch 8 Batch: 798 Train Loss: 0.012022143229842186\n",
      "Epoch 8 Batch: 799 Train Loss: 0.011805037036538124\n",
      "Epoch 8 Batch: 800 Train Loss: 0.01221757847815752\n",
      "Epoch 8 Batch: 801 Train Loss: 0.011782820336520672\n",
      "Epoch 8 Batch: 802 Train Loss: 0.01376254577189684\n",
      "Epoch 8 Batch: 803 Train Loss: 0.010549872182309628\n",
      "Epoch 8 Batch: 804 Train Loss: 0.009908691979944706\n",
      "Epoch 8 Batch: 805 Train Loss: 0.010743256658315659\n",
      "Epoch 8 Batch: 806 Train Loss: 0.010684715583920479\n",
      "Epoch 8 Batch: 807 Train Loss: 0.01035638339817524\n",
      "Epoch 8 Batch: 808 Train Loss: 0.010896418243646622\n",
      "Epoch 8 Batch: 809 Train Loss: 0.009680981747806072\n",
      "Epoch 8 Batch: 810 Train Loss: 0.007483439054340124\n",
      "Epoch 8 Batch: 811 Train Loss: 0.008522269316017628\n",
      "Epoch 8 Batch: 812 Train Loss: 0.009483027271926403\n",
      "Epoch 8 Batch: 813 Train Loss: 0.00893104262650013\n",
      "Epoch 8 Batch: 814 Train Loss: 0.008677824400365353\n",
      "Epoch 8 Batch: 815 Train Loss: 0.007602317724376917\n",
      "Epoch 8 Batch: 816 Train Loss: 0.007619066629558802\n",
      "Epoch 8 Batch: 817 Train Loss: 0.007467788644134998\n",
      "Epoch 8 Batch: 818 Train Loss: 0.007691408507525921\n",
      "Epoch 8 Batch: 819 Train Loss: 0.008401971310377121\n",
      "Epoch 8 Batch: 820 Train Loss: 0.007020159158855677\n",
      "Epoch 8 Batch: 821 Train Loss: 0.007384961936622858\n",
      "Epoch 8 Batch: 822 Train Loss: 0.482858806848526\n",
      "Epoch 8 Batch: 823 Train Loss: 0.007728226482868195\n",
      "Epoch 8 Batch: 824 Train Loss: 0.007789785508066416\n",
      "Epoch 8 Batch: 825 Train Loss: 0.006923963315784931\n",
      "Epoch 8 Batch: 826 Train Loss: 0.007156773004680872\n",
      "Epoch 8 Batch: 827 Train Loss: 0.008284945972263813\n",
      "Epoch 8 Batch: 828 Train Loss: 0.005702837835997343\n",
      "Epoch 8 Batch: 829 Train Loss: 0.006538303103297949\n",
      "Epoch 8 Batch: 830 Train Loss: 0.007608366198837757\n",
      "Epoch 8 Batch: 831 Train Loss: 0.5152741074562073\n",
      "Epoch 8 Batch: 832 Train Loss: 0.006840351037681103\n",
      "Epoch 8 Batch: 833 Train Loss: 0.008425373584032059\n",
      "Epoch 8 Batch: 834 Train Loss: 0.00746974628418684\n",
      "Epoch 8 Batch: 835 Train Loss: 0.006541470531374216\n",
      "Epoch 8 Batch: 836 Train Loss: 0.007311180233955383\n",
      "Epoch 8 Batch: 837 Train Loss: 0.008506323210895061\n",
      "Epoch 8 Batch: 838 Train Loss: 0.006213481072336435\n",
      "Epoch 8 Batch: 839 Train Loss: 0.006619470659643412\n",
      "Epoch 8 Batch: 840 Train Loss: 0.007704888936132193\n",
      "Epoch 8 Batch: 841 Train Loss: 0.007069783750921488\n",
      "Epoch 8 Batch: 842 Train Loss: 0.00718803983181715\n",
      "Epoch 8 Batch: 843 Train Loss: 0.005863364320248365\n",
      "Epoch 8 Batch: 844 Train Loss: 0.008221983909606934\n",
      "Epoch 8 Batch: 845 Train Loss: 0.006039171479642391\n",
      "Epoch 8 Batch: 846 Train Loss: 0.0071030668914318085\n",
      "Epoch 8 Batch: 847 Train Loss: 0.006811297032982111\n",
      "Epoch 8 Batch: 848 Train Loss: 0.006675674580037594\n",
      "Epoch 8 Batch: 849 Train Loss: 0.006966941058635712\n",
      "Epoch 8 Batch: 850 Train Loss: 0.4926093518733978\n",
      "Epoch 8 Batch: 851 Train Loss: 0.00608854740858078\n",
      "Epoch 8 Batch: 852 Train Loss: 0.007647117134183645\n",
      "Epoch 8 Batch: 853 Train Loss: 0.006756828166544437\n",
      "Epoch 8 Batch: 854 Train Loss: 0.007843248546123505\n",
      "Epoch 8 Batch: 855 Train Loss: 0.007694605737924576\n",
      "Epoch 8 Batch: 856 Train Loss: 0.007050901651382446\n",
      "Epoch 8 Batch: 857 Train Loss: 0.0072751580737531185\n",
      "Epoch 8 Batch: 858 Train Loss: 0.512296199798584\n",
      "Epoch 8 Batch: 859 Train Loss: 0.5061256289482117\n",
      "Epoch 8 Batch: 860 Train Loss: 0.008120090700685978\n",
      "Epoch 8 Batch: 861 Train Loss: 0.009176088497042656\n",
      "Epoch 8 Batch: 862 Train Loss: 0.007882049307227135\n",
      "Epoch 8 Batch: 863 Train Loss: 0.009742015972733498\n",
      "Epoch 8 Batch: 864 Train Loss: 0.009030640125274658\n",
      "Epoch 8 Batch: 865 Train Loss: 0.4698924422264099\n",
      "Epoch 8 Batch: 866 Train Loss: 0.009923947975039482\n",
      "Epoch 8 Batch: 867 Train Loss: 0.010501991957426071\n",
      "Epoch 8 Batch: 868 Train Loss: 0.010958044789731503\n",
      "Epoch 8 Batch: 869 Train Loss: 0.011421306058764458\n",
      "Epoch 8 Batch: 870 Train Loss: 0.011775679886341095\n",
      "Epoch 8 Batch: 871 Train Loss: 0.011672680266201496\n",
      "Epoch 8 Batch: 872 Train Loss: 0.012393633835017681\n",
      "Epoch 8 Batch: 873 Train Loss: 0.011662552133202553\n",
      "Epoch 8 Batch: 874 Train Loss: 0.45071130990982056\n",
      "Epoch 8 Batch: 875 Train Loss: 0.012834936380386353\n",
      "Epoch 8 Batch: 876 Train Loss: 0.014298247173428535\n",
      "Epoch 8 Batch: 877 Train Loss: 0.013002562336623669\n",
      "Epoch 8 Batch: 878 Train Loss: 0.013149557635188103\n",
      "Epoch 8 Batch: 879 Train Loss: 0.013217826373875141\n",
      "Epoch 8 Batch: 880 Train Loss: 0.011645935475826263\n"
     ]
    }
   ],
   "source": [
    "#from ignite.metrics import Precision, Recall\n",
    "#from ignite.metrics import Precision ### L√ÑGG TILL IGNITE\n",
    "#train_precision = Precision()\n",
    "#train_recall = Recall()\n",
    "\n",
    "#test_precision = Precision()\n",
    "#test_recall = Recall()\n",
    "# https://pytorch.org/ignite/metrics.html\n",
    "\n",
    "\n",
    "epochs = 10 # Ju st√∂rra dataset ju f√§rre epochs beh√∂vs\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_correct = []\n",
    "test_correct = []\n",
    "\n",
    "# For loop epochs \n",
    "\n",
    "for i in range(epochs):\n",
    "    trn_correct = 0\n",
    "    tst_correct = 0 \n",
    "\n",
    "    # Train\n",
    "\n",
    "    for b, (X_train, y_train) in enumerate(train_loader):\n",
    "        \n",
    "        # Skip iteration if batch size not equal to stated dim\n",
    "        \n",
    "            \n",
    "        #print(X_train.shape, y_train.shape) \n",
    "        \n",
    "        b += 1\n",
    "        \n",
    "        y_pred = model(X_train.view(batch_size, -1))  # Flatten input\n",
    "        lossTrain = criterion(y_pred, y_train)\n",
    "\n",
    "        predicted = torch.max(y_pred.data,1)[1]\n",
    "\n",
    "        #calculate precision and recall\n",
    "        #train_precision.update((y_pred, y_train))\n",
    "        #train_recall.update((y_pred, y_train))\n",
    "      \n",
    "\n",
    "        batch_corr = (predicted == y_train).sum()\n",
    "        trn_correct += batch_corr\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        lossTrain.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if b%2 == 0: \n",
    "            print(f\"Epoch {i} Batch: {b} Train Loss: {lossTrain.item()}\")\n",
    "\n",
    "    train_losses.append(lossTrain.data.item())\n",
    "    train_correct.append(trn_correct)\n",
    "\n",
    "    # Test\n",
    "    with torch.no_grad():\n",
    "        for b, (X_test,y_test) in enumerate(test_loader):\n",
    "            y_val = model(X_train.view(batch_size, -1))\n",
    "\n",
    "            predicted = torch.max(y_val.data,1)[1]\n",
    "\n",
    "            \n",
    "        loss = criterion(y_val, y_test)\n",
    "        test_losses.append(loss)\n",
    "        test_correct.append(trn_correct)\n",
    "\n",
    "        #if b%2 == 0:\n",
    "            #print(f\"Epoch {i} Batch: {b} Train Loss: {lossTrain.item()} Validation Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "922d958b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fa7db715760>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label=\"Train losses\")\n",
    "plt.plot(test_losses, label= \"Test losses\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e4edd36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sample 1465 selected with state 0. Model predict state is 0\n",
      "Random sample 514 selected with state 0. Model predict state is 0\n",
      "Random sample 1380 selected with state 0. Model predict state is 0\n",
      "Random sample 1874 selected with state 0. Model predict state is 0\n",
      "Random sample 1817 selected with state 0. Model predict state is 0\n",
      "Random sample 2015 selected with state 0. Model predict state is 0\n",
      "Random sample 359 selected with state 1. Model predict state is 1\n",
      "Random sample 349 selected with state 0. Model predict state is 0\n",
      "Random sample 1158 selected with state 0. Model predict state is 0\n",
      "Random sample 545 selected with state 0. Model predict state is 0\n",
      "Random sample 1919 selected with state 1. Model predict state is 1\n",
      "Random sample 762 selected with state 0. Model predict state is 0\n",
      "Random sample 1286 selected with state 1. Model predict state is 1\n",
      "Random sample 2160 selected with state 0. Model predict state is 0\n",
      "Random sample 484 selected with state 0. Model predict state is 0\n",
      "Random sample 520 selected with state 0. Model predict state is 0\n",
      "Random sample 375 selected with state 0. Model predict state is 0\n",
      "Random sample 564 selected with state 0. Model predict state is 0\n",
      "Random sample 1382 selected with state 0. Model predict state is 0\n",
      "Random sample 1423 selected with state 0. Model predict state is 0\n",
      "Random sample 1156 selected with state 0. Model predict state is 0\n",
      "Random sample 764 selected with state 0. Model predict state is 0\n",
      "Random sample 1149 selected with state 0. Model predict state is 0\n",
      "Random sample 1452 selected with state 0. Model predict state is 0\n",
      "Random sample 399 selected with state 0. Model predict state is 0\n",
      "Random sample 416 selected with state 0. Model predict state is 0\n",
      "Random sample 2157 selected with state 0. Model predict state is 0\n",
      "Random sample 1030 selected with state 0. Model predict state is 0\n",
      "Random sample 2049 selected with state 0. Model predict state is 0\n",
      "Random sample 165 selected with state 0. Model predict state is 0\n",
      "Random sample 189 selected with state 0. Model predict state is 0\n",
      "Random sample 731 selected with state 0. Model predict state is 0\n",
      "Random sample 230 selected with state 0. Model predict state is 0\n",
      "Random sample 756 selected with state 0. Model predict state is 0\n",
      "Random sample 1037 selected with state 0. Model predict state is 0\n",
      "Random sample 1926 selected with state 0. Model predict state is 0\n",
      "Random sample 892 selected with state 0. Model predict state is 0\n",
      "Random sample 957 selected with state 0. Model predict state is 0\n",
      "Random sample 1251 selected with state 0. Model predict state is 0\n",
      "Random sample 2130 selected with state 0. Model predict state is 0\n",
      "Random sample 1630 selected with state 0. Model predict state is 0\n",
      "Random sample 496 selected with state 0. Model predict state is 0\n",
      "Random sample 1145 selected with state 1. Model predict state is 1\n",
      "Random sample 2120 selected with state 0. Model predict state is 0\n",
      "Random sample 1556 selected with state 0. Model predict state is 0\n",
      "Random sample 1193 selected with state 0. Model predict state is 0\n",
      "Random sample 750 selected with state 0. Model predict state is 0\n",
      "Random sample 1163 selected with state 0. Model predict state is 0\n",
      "Random sample 686 selected with state 0. Model predict state is 0\n",
      "Random sample 22 selected with state 0. Model predict state is 0\n",
      "Random sample 1066 selected with state 0. Model predict state is 0\n",
      "Random sample 1199 selected with state 0. Model predict state is 0\n",
      "Random sample 1955 selected with state 0. Model predict state is 0\n",
      "Random sample 1135 selected with state 0. Model predict state is 0\n",
      "Random sample 1553 selected with state 0. Model predict state is 0\n",
      "Random sample 2046 selected with state 0. Model predict state is 0\n",
      "Random sample 162 selected with state 0. Model predict state is 0\n",
      "Random sample 1500 selected with state 0. Model predict state is 0\n",
      "Random sample 5 selected with state 0. Model predict state is 0\n",
      "Random sample 1830 selected with state 1. Model predict state is 1\n",
      "Random sample 660 selected with state 0. Model predict state is 0\n",
      "Random sample 1178 selected with state 0. Model predict state is 0\n",
      "Random sample 1954 selected with state 1. Model predict state is 1\n",
      "Random sample 898 selected with state 0. Model predict state is 0\n",
      "Random sample 1737 selected with state 0. Model predict state is 0\n",
      "Random sample 580 selected with state 1. Model predict state is 1\n",
      "Random sample 1152 selected with state 0. Model predict state is 0\n",
      "Random sample 1647 selected with state 0. Model predict state is 0\n",
      "Random sample 1368 selected with state 0. Model predict state is 0\n",
      "Random sample 2182 selected with state 0. Model predict state is 0\n",
      "Random sample 1895 selected with state 0. Model predict state is 0\n",
      "Random sample 1930 selected with state 0. Model predict state is 0\n",
      "Random sample 383 selected with state 0. Model predict state is 0\n",
      "Random sample 195 selected with state 0. Model predict state is 0\n",
      "Random sample 65 selected with state 0. Model predict state is 0\n",
      "Random sample 1734 selected with state 1. Model predict state is 1\n",
      "Random sample 232 selected with state 0. Model predict state is 0\n",
      "Random sample 1920 selected with state 0. Model predict state is 0\n",
      "Random sample 2043 selected with state 0. Model predict state is 0\n",
      "Random sample 1453 selected with state 0. Model predict state is 0\n",
      "Random sample 1792 selected with state 0. Model predict state is 0\n",
      "Random sample 1734 selected with state 1. Model predict state is 1\n",
      "Random sample 1940 selected with state 0. Model predict state is 0\n",
      "Random sample 412 selected with state 0. Model predict state is 0\n",
      "Random sample 905 selected with state 0. Model predict state is 0\n",
      "Random sample 709 selected with state 0. Model predict state is 0\n",
      "Random sample 1462 selected with state 0. Model predict state is 0\n",
      "Random sample 123 selected with state 0. Model predict state is 0\n",
      "Random sample 310 selected with state 0. Model predict state is 0\n",
      "Random sample 1247 selected with state 0. Model predict state is 0\n",
      "Random sample 917 selected with state 0. Model predict state is 0\n",
      "Random sample 269 selected with state 0. Model predict state is 0\n",
      "Random sample 1603 selected with state 0. Model predict state is 0\n",
      "Random sample 1738 selected with state 0. Model predict state is 0\n",
      "Random sample 918 selected with state 1. Model predict state is 1\n",
      "Random sample 1308 selected with state 0. Model predict state is 0\n",
      "Random sample 1708 selected with state 0. Model predict state is 0\n",
      "Random sample 1596 selected with state 0. Model predict state is 0\n",
      "Random sample 410 selected with state 1. Model predict state is 0\n",
      "Random sample 1489 selected with state 0. Model predict state is 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for _ in range(100):\n",
    "    # Select random sample\n",
    "    i = np.random.randint(1,len(test_set))\n",
    "    x = test_set[i][0]\n",
    "    y = test_set[i][1]\n",
    "\n",
    "\n",
    "    # Evaluate on sample\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        new_pred =model(x.view(1,-1))\n",
    "        pred_int = int(torch.max(new_pred.data,1)[1])\n",
    "    print(f\"Random sample {i} selected with state {y}. Model predict state is {pred_int}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "4915d705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sample 2535 selected with state 0. Model predict state is 0\n",
      "Random sample 4295 selected with state 0. Model predict state is 0\n",
      "Random sample 2880 selected with state 0. Model predict state is 0\n",
      "Random sample 1189 selected with state 0. Model predict state is 0\n",
      "Random sample 4404 selected with state 0. Model predict state is 0\n",
      "Random sample 6846 selected with state 0. Model predict state is 0\n",
      "Random sample 1092 selected with state 0. Model predict state is 0\n",
      "Random sample 7774 selected with state 0. Model predict state is 0\n",
      "Random sample 3671 selected with state 0. Model predict state is 0\n",
      "Random sample 4150 selected with state 0. Model predict state is 0\n",
      "Random sample 7037 selected with state 0. Model predict state is 0\n",
      "Random sample 2363 selected with state 0. Model predict state is 0\n",
      "Random sample 4698 selected with state 0. Model predict state is 0\n",
      "Random sample 6368 selected with state 0. Model predict state is 0\n",
      "Random sample 5386 selected with state 0. Model predict state is 0\n",
      "Random sample 6935 selected with state 0. Model predict state is 0\n",
      "Random sample 462 selected with state 0. Model predict state is 0\n",
      "Random sample 6838 selected with state 0. Model predict state is 0\n",
      "Random sample 1175 selected with state 0. Model predict state is 0\n",
      "Random sample 5613 selected with state 0. Model predict state is 0\n",
      "Random sample 3081 selected with state 0. Model predict state is 0\n",
      "Random sample 3687 selected with state 0. Model predict state is 0\n",
      "Random sample 5587 selected with state 0. Model predict state is 0\n",
      "Random sample 1196 selected with state 0. Model predict state is 0\n",
      "Random sample 4282 selected with state 0. Model predict state is 0\n",
      "Random sample 635 selected with state 0. Model predict state is 0\n",
      "Random sample 7321 selected with state 0. Model predict state is 0\n",
      "Random sample 6734 selected with state 0. Model predict state is 0\n",
      "Random sample 7633 selected with state 0. Model predict state is 0\n",
      "Random sample 5912 selected with state 0. Model predict state is 0\n",
      "Random sample 4687 selected with state 0. Model predict state is 0\n",
      "Random sample 7899 selected with state 0. Model predict state is 0\n",
      "Random sample 6382 selected with state 0. Model predict state is 0\n",
      "Random sample 3885 selected with state 0. Model predict state is 0\n",
      "Random sample 7352 selected with state 0. Model predict state is 0\n",
      "Random sample 5203 selected with state 0. Model predict state is 0\n",
      "Random sample 6257 selected with state 0. Model predict state is 0\n",
      "Random sample 1162 selected with state 0. Model predict state is 0\n",
      "Random sample 7154 selected with state 0. Model predict state is 0\n",
      "Random sample 4081 selected with state 0. Model predict state is 0\n",
      "Random sample 6078 selected with state 0. Model predict state is 0\n",
      "Random sample 1279 selected with state 0. Model predict state is 0\n",
      "Random sample 2870 selected with state 1. Model predict state is 1\n",
      "Random sample 6654 selected with state 0. Model predict state is 0\n",
      "Random sample 2392 selected with state 0. Model predict state is 0\n",
      "Random sample 3352 selected with state 0. Model predict state is 0\n",
      "Random sample 791 selected with state 0. Model predict state is 0\n",
      "Random sample 2363 selected with state 0. Model predict state is 0\n",
      "Random sample 3786 selected with state 0. Model predict state is 0\n",
      "Random sample 5473 selected with state 0. Model predict state is 0\n",
      "Random sample 7791 selected with state 0. Model predict state is 0\n",
      "Random sample 6327 selected with state 0. Model predict state is 0\n",
      "Random sample 7781 selected with state 0. Model predict state is 0\n",
      "Random sample 294 selected with state 0. Model predict state is 0\n",
      "Random sample 6051 selected with state 0. Model predict state is 0\n",
      "Random sample 3502 selected with state 0. Model predict state is 0\n",
      "Random sample 519 selected with state 0. Model predict state is 0\n",
      "Random sample 8023 selected with state 0. Model predict state is 0\n",
      "Random sample 6961 selected with state 0. Model predict state is 0\n",
      "Random sample 7542 selected with state 0. Model predict state is 0\n",
      "Random sample 828 selected with state 0. Model predict state is 0\n",
      "Random sample 3398 selected with state 0. Model predict state is 0\n",
      "Random sample 1980 selected with state 0. Model predict state is 0\n",
      "Random sample 3844 selected with state 0. Model predict state is 0\n",
      "Random sample 1815 selected with state 0. Model predict state is 0\n",
      "Random sample 2151 selected with state 0. Model predict state is 0\n",
      "Random sample 3563 selected with state 0. Model predict state is 0\n",
      "Random sample 1761 selected with state 0. Model predict state is 0\n",
      "Random sample 1412 selected with state 0. Model predict state is 0\n",
      "Random sample 1771 selected with state 0. Model predict state is 0\n",
      "Random sample 4057 selected with state 0. Model predict state is 0\n",
      "Random sample 368 selected with state 0. Model predict state is 0\n",
      "Random sample 4321 selected with state 0. Model predict state is 0\n",
      "Random sample 7917 selected with state 0. Model predict state is 0\n",
      "Random sample 7851 selected with state 0. Model predict state is 0\n",
      "Random sample 1803 selected with state 0. Model predict state is 0\n",
      "Random sample 4195 selected with state 0. Model predict state is 0\n",
      "Random sample 2804 selected with state 0. Model predict state is 0\n",
      "Random sample 5610 selected with state 0. Model predict state is 0\n",
      "Random sample 1871 selected with state 0. Model predict state is 0\n",
      "Random sample 995 selected with state 0. Model predict state is 0\n",
      "Random sample 6769 selected with state 0. Model predict state is 0\n",
      "Random sample 2507 selected with state 0. Model predict state is 0\n",
      "Random sample 1575 selected with state 0. Model predict state is 0\n",
      "Random sample 5137 selected with state 0. Model predict state is 0\n",
      "Random sample 5922 selected with state 0. Model predict state is 0\n",
      "Random sample 6521 selected with state 0. Model predict state is 0\n",
      "Random sample 3254 selected with state 0. Model predict state is 0\n",
      "Random sample 4503 selected with state 0. Model predict state is 0\n",
      "Random sample 4405 selected with state 0. Model predict state is 0\n",
      "Random sample 495 selected with state 0. Model predict state is 0\n",
      "Random sample 3435 selected with state 0. Model predict state is 0\n",
      "Random sample 7414 selected with state 0. Model predict state is 0\n",
      "Random sample 952 selected with state 0. Model predict state is 0\n",
      "Random sample 4612 selected with state 0. Model predict state is 0\n",
      "Random sample 805 selected with state 0. Model predict state is 0\n",
      "Random sample 3597 selected with state 0. Model predict state is 0\n",
      "Random sample 952 selected with state 0. Model predict state is 0\n",
      "Random sample 1285 selected with state 0. Model predict state is 0\n",
      "Random sample 4077 selected with state 0. Model predict state is 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for _ in range(100):\n",
    "    # Select random sample\n",
    "    i = np.random.randint(1,len(train_set))\n",
    "    x = train_set[i][0]\n",
    "    y = train_set[i][1]\n",
    "\n",
    "\n",
    "    # Evaluate on sample\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        new_pred =model(x.view(1,-1))\n",
    "        pred_int = int(torch.max(new_pred.data,1)[1])\n",
    "    print(f\"Random sample {i} selected with state {y}. Model predict state is {pred_int}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2dcbe3",
   "metadata": {},
   "source": [
    "## Create confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5decb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ba504d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2200"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "25a22078",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader_all = DataLoader(dataset=test_set, batch_size=len(test_set), shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f4fc96b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    for X_test, y_test in test_loader_all:\n",
    "        y_val = model(X_test.view(len(test_set),-1))\n",
    "        predicted = torch.max(y_val,1)[1]\n",
    "        correct += (predicted == y_test).sum()\n",
    "        \n",
    "# Create confusion matrix        \n",
    "arr = confusion_matrix(y_test.view(-1), predicted.view(-1))\n",
    "\n",
    "arr2 = np.zeros([2,2])\n",
    "arr2[0] = (arr[0]/arr[0].sum()).astype(int)\n",
    "arr2[1] = (arr[1]/arr[1].sum()).astype(int)\n",
    "\n",
    "df_cm = pd.DataFrame(arr2)\n",
    "#sns.heatmap(df_cm, annot=True, cmap=\"BuGn\")\n",
    "sns.heatmap(df_cm, annot=True, cmap=\"BuGn\",fmt='.2%')\n",
    "plt.xlabel(\"prediction\")\n",
    "plt.ylabel(\"label (ground truth)\")\n",
    "plt.title(\"Confusion matrix (in percent)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d91e5d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4a41f2a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0,  ..., 0, 0, 0])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "63f3edbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 0, 0, 0])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a79bdda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a7a394",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
