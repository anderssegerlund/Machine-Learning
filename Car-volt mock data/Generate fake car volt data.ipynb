{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22bef119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a60668e",
   "metadata": {},
   "source": [
    "## Generate fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22b892c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bc3e88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    x_min = x.min()\n",
    "    x_max = x.max()\n",
    "    \n",
    "    x = (x-x_min)/(x_max -x_min)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3144c776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "time_steps = 2\n",
    "n_cells = 4 # Amount of cells in pack\n",
    "x = np.zeros([time_steps,n_cells])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,2)\n",
    "\n",
    "failed_cell = 1 # Which cell is the failing cell\n",
    "is_fail = True# Does the pack include a failed cell\n",
    "volt_stochastic = 0.9\n",
    "for i in range(time_steps):\n",
    "    sigma = 0.0 # How much is the fluctuation\n",
    "    median = random.uniform(2, 4)\n",
    "    volt = np.random.normal(loc=median, scale=sigma, size=n_cells)\n",
    "    if is_fail:\n",
    "        r = random.random()\n",
    "        if r < volt_stochastic:\n",
    "            stochstic_amplifier = random.randint(1,10)\n",
    "            \n",
    "            y = (1-i/stochstic_amplifier)\n",
    "            #print(y)\n",
    "            volt[failed_cell]*=y\n",
    "    ax[1].scatter(i,y)\n",
    "    x[i,:] = volt\n",
    "            \n",
    "    ax[0].plot(volt, label=i)\n",
    "#plt.ylim(0,16)\n",
    "#plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22bbba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequence(time_steps=2, n_cells=4, failed_cell=1, is_fail=True, sigma=0.0, volt_stochastic = 0.9, normalize_x=False):\n",
    "    x = np.zeros([time_steps,n_cells])\n",
    "    for i in range(time_steps):\n",
    "        median = random.uniform(2, 4)\n",
    "        volt = np.random.normal(loc=median, scale=sigma, size=n_cells)\n",
    "        if is_fail:\n",
    "            r = random.random()\n",
    "            if r < volt_stochastic:\n",
    "                stochstic_amplifier = random.randint(1,10) # 100, 1000\n",
    "\n",
    "                y = (1-i/stochstic_amplifier)\n",
    "                #print(y)\n",
    "                volt[failed_cell]*=y\n",
    "        x[i,:] = volt\n",
    "    if normalize_x:\n",
    "        x = normalize(x)\n",
    "    if is_fail:\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 0\n",
    "    return x, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f7fba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps=2 \n",
    "n_cells=4\n",
    "failed_cell=1 \n",
    "is_fail=True \n",
    "sigma=0.0 \n",
    "volt_stochastic = 0.9 \n",
    "normalize_x=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e93b1ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros([time_steps,n_cells])\n",
    "for i in range(time_steps):\n",
    "    median = random.uniform(2, 4)\n",
    "    volt = np.random.normal(loc=median, scale=sigma, size=n_cells)\n",
    "    if is_fail:\n",
    "        r = random.random()\n",
    "        if r < volt_stochastic:\n",
    "            stochstic_amplifier = random.randint(1,10) # 100, 1000\n",
    "\n",
    "            y = (1-i/stochstic_amplifier)\n",
    "            #print(y)\n",
    "            volt[failed_cell]*=y\n",
    "    x[i,:] = volt\n",
    "if normalize_x:\n",
    "    x = normalize(x)\n",
    "if is_fail:\n",
    "    label = 1\n",
    "else:\n",
    "    label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57e14190",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 2\n",
    "n_cells =4\n",
    "x,y = create_sequence(time_steps=time_steps, n_cells=n_cells, failed_cell=2, is_fail=True, sigma=0.0, volt_stochastic = 0.9, normalize_x=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "8aa70bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = create_sequence(time_steps=time_steps, n_cells=n_cells, failed_cell=2, is_fail=True, sigma=0.0, volt_stochastic = 0.9, normalize_x=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "66a6bfd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.23980265, 0.23980265, 0.23980265, 0.23980265],\n",
       "       [1.        , 1.        , 0.        , 1.        ]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "9e7fba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(n_healthy=2, n_fails=2,time_steps=time_steps):\n",
    "    data = []\n",
    "    \n",
    "    data_info ={\"index_1\":[],\n",
    "           \"index_0\": []} \n",
    "    \n",
    "    for i in range(n_healthy+n_fails):\n",
    "        if i >= n_healthy:\n",
    "            fail=True\n",
    "        else:\n",
    "            fail=False\n",
    "        x,y = create_sequence(time_steps=time_steps, n_cells=n_cells, failed_cell=2, is_fail=fail, sigma=0.0, volt_stochastic = 0.9, normalize_x=False)\n",
    "        data.append((torch.FloatTensor(x),y))\n",
    "        \n",
    "        #Finn data_info\n",
    "        if y == 1:\n",
    "            data_info[\"index_1\"].append(i)\n",
    "            \n",
    "        if y == 0:\n",
    "            data_info[\"index_0\"].append(i)\n",
    "            \n",
    "    return (data, data_info)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "4b9bbe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, data_info = create_data(n_healthy=1000, n_fails=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf31d30",
   "metadata": {},
   "source": [
    "## plot n random samples from failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "a0a67a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x864 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "select_samples = 5\n",
    "sample_from = random.randint(min(data_info[\"index_1\"]), max(data_info[\"index_1\"])-select_samples)\n",
    "fig, ax = plt.subplots(select_samples,1,figsize=(12,12))\n",
    "for i,j in enumerate(range(sample_from, sample_from+select_samples)):\n",
    "\n",
    "    ax[i].plot(data[j][0].T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a02d363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69109e99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae3ce23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e88b47b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbdc160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1a6db1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3257a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc2ef8a6",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "27b2454d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0c4cb451",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, data_info = create_data(n_healthy=100, n_fails=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "a2bf0168",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(data)*0.8)\n",
    "test_size = len(data) - train_size\n",
    "train_set, test_set = torch.utils.data.random_split(data,[train_size, test_size])\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e61c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c395ceb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e0ab2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "a2f4bee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNMultilayerperceptron(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size=(time_steps*n_cells),output_size=2, layers=[220,84]):  # 120, 84\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, layers[0])\n",
    "        self.fc2 = nn.Linear(layers[0], layers[1])\n",
    "        self.fc2b = nn.Linear(layers[1], 500)\n",
    "        self.fc2c = nn.Linear(500, layers[1])\n",
    "        self.fc2d = nn.Linear(layers[1], layers[1])\n",
    "        self.fc3 = nn.Linear(layers[1], output_size)\n",
    "\n",
    "    def forward(self,X):\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = F.relu(self.fc2(X))\n",
    "        X = F.relu(self.fc2b(X))\n",
    "        X = F.relu(self.fc2c(X))\n",
    "        X = F.relu(self.fc2d(X))\n",
    "        X = self.fc3(X)\n",
    "\n",
    "        return F.log_softmax(X,dim=1) # PGA multiclass classification\n",
    "        #return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "3ddbf0dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ANNMultilayerperceptron(\n",
       "  (fc1): Linear(in_features=8, out_features=220, bias=True)\n",
       "  (fc2): Linear(in_features=220, out_features=84, bias=True)\n",
       "  (fc2b): Linear(in_features=84, out_features=500, bias=True)\n",
       "  (fc2c): Linear(in_features=500, out_features=84, bias=True)\n",
       "  (fc2d): Linear(in_features=84, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ANNMultilayerperceptron()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715f24d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc52a08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8497c260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "e65d3518",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "#optimizer = torch.optim.Adadelta(model.parameters(), lr=0.001)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "cd05a0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch: 2 Train Loss: 0.693619966506958\n",
      "Epoch 0 Batch: 4 Train Loss: 0.699508547782898\n",
      "Epoch 0 Batch: 6 Train Loss: 0.6865253448486328\n",
      "Epoch 0 Batch: 8 Train Loss: 0.665403425693512\n",
      "Epoch 0 Batch: 10 Train Loss: 0.7262176871299744\n",
      "Epoch 0 Batch: 12 Train Loss: 0.7205796241760254\n",
      "Epoch 0 Batch: 14 Train Loss: 0.7056286931037903\n",
      "Epoch 0 Batch: 16 Train Loss: 0.6938522458076477\n",
      "Epoch 0 Batch: 18 Train Loss: 0.6809064149856567\n",
      "Epoch 0 Batch: 20 Train Loss: 0.6940392255783081\n",
      "Epoch 0 Batch: 22 Train Loss: 0.6747692823410034\n",
      "Epoch 0 Batch: 24 Train Loss: 0.7043606638908386\n",
      "Epoch 0 Batch: 26 Train Loss: 0.6908827424049377\n",
      "Epoch 0 Batch: 28 Train Loss: 0.6946858763694763\n",
      "Epoch 0 Batch: 30 Train Loss: 0.6524922847747803\n",
      "Epoch 0 Batch: 32 Train Loss: 0.6570456624031067\n",
      "Epoch 0 Batch: 34 Train Loss: 0.7095076441764832\n",
      "Epoch 0 Batch: 36 Train Loss: 0.6846564412117004\n",
      "Epoch 0 Batch: 38 Train Loss: 0.7153660655021667\n",
      "Epoch 0 Batch: 40 Train Loss: 0.6731647253036499\n",
      "Epoch 0 Batch: 42 Train Loss: 0.7238582372665405\n",
      "Epoch 0 Batch: 44 Train Loss: 0.6709386110305786\n",
      "Epoch 0 Batch: 46 Train Loss: 0.6881898045539856\n",
      "Epoch 0 Batch: 48 Train Loss: 0.6803551912307739\n",
      "Epoch 0 Batch: 50 Train Loss: 0.687871515750885\n",
      "Epoch 0 Batch: 52 Train Loss: 0.6754872798919678\n",
      "Epoch 0 Batch: 54 Train Loss: 0.6811074018478394\n",
      "Epoch 0 Batch: 56 Train Loss: 0.6887472867965698\n",
      "Epoch 0 Batch: 58 Train Loss: 0.6849513649940491\n",
      "Epoch 0 Batch: 60 Train Loss: 0.6911337971687317\n",
      "Epoch 0 Batch: 62 Train Loss: 0.6908188462257385\n",
      "Epoch 0 Batch: 64 Train Loss: 0.6842806935310364\n",
      "Epoch 0 Batch: 66 Train Loss: 0.6747382879257202\n",
      "Epoch 0 Batch: 68 Train Loss: 0.6616839170455933\n",
      "Epoch 0 Batch: 70 Train Loss: 0.6763951182365417\n",
      "Epoch 0 Batch: 72 Train Loss: 0.6622881889343262\n",
      "Epoch 0 Batch: 74 Train Loss: 0.6925272941589355\n",
      "Epoch 0 Batch: 76 Train Loss: 0.7009465098381042\n",
      "Epoch 0 Batch: 78 Train Loss: 0.6780478954315186\n",
      "Epoch 0 Batch: 80 Train Loss: 0.6540047526359558\n",
      "Epoch 0 Batch: 82 Train Loss: 0.6641167998313904\n",
      "Epoch 0 Batch: 84 Train Loss: 0.6821665167808533\n",
      "Epoch 0 Batch: 86 Train Loss: 0.6807005405426025\n",
      "Epoch 0 Batch: 88 Train Loss: 0.6734873056411743\n",
      "Epoch 0 Batch: 90 Train Loss: 0.6759670376777649\n",
      "Epoch 0 Batch: 92 Train Loss: 0.6745610237121582\n",
      "Epoch 0 Batch: 94 Train Loss: 0.6669069528579712\n",
      "Epoch 0 Batch: 96 Train Loss: 0.6879817247390747\n",
      "Epoch 0 Batch: 98 Train Loss: 0.6475182771682739\n",
      "Epoch 0 Batch: 100 Train Loss: 0.6619738340377808\n",
      "Epoch 0 Batch: 102 Train Loss: 0.6627811193466187\n",
      "Epoch 0 Batch: 104 Train Loss: 0.6399081945419312\n",
      "Epoch 0 Batch: 106 Train Loss: 0.6699644327163696\n",
      "Epoch 0 Batch: 108 Train Loss: 0.6584516167640686\n",
      "Epoch 0 Batch: 110 Train Loss: 0.6444387435913086\n",
      "Epoch 0 Batch: 112 Train Loss: 0.6742981672286987\n",
      "Epoch 0 Batch: 114 Train Loss: 0.662889838218689\n",
      "Epoch 0 Batch: 116 Train Loss: 0.6577185392379761\n",
      "Epoch 0 Batch: 118 Train Loss: 0.6572023630142212\n",
      "Epoch 0 Batch: 120 Train Loss: 0.6182034015655518\n",
      "Epoch 0 Batch: 122 Train Loss: 0.6391934156417847\n",
      "Epoch 0 Batch: 124 Train Loss: 0.7779368758201599\n",
      "Epoch 0 Batch: 126 Train Loss: 0.6694989204406738\n",
      "Epoch 0 Batch: 128 Train Loss: 0.6627956032752991\n",
      "Epoch 0 Batch: 130 Train Loss: 0.6852998733520508\n",
      "Epoch 0 Batch: 132 Train Loss: 0.7011043429374695\n",
      "Epoch 0 Batch: 134 Train Loss: 0.6679056882858276\n",
      "Epoch 0 Batch: 136 Train Loss: 0.6576800346374512\n",
      "Epoch 0 Batch: 138 Train Loss: 0.6441536545753479\n",
      "Epoch 0 Batch: 140 Train Loss: 0.6519608497619629\n",
      "Epoch 0 Batch: 142 Train Loss: 0.5934718251228333\n",
      "Epoch 0 Batch: 144 Train Loss: 0.6352173089981079\n",
      "Epoch 0 Batch: 146 Train Loss: 0.6132351160049438\n",
      "Epoch 0 Batch: 148 Train Loss: 0.6713473200798035\n",
      "Epoch 0 Batch: 150 Train Loss: 0.6794029474258423\n",
      "Epoch 0 Batch: 152 Train Loss: 0.6647626757621765\n",
      "Epoch 0 Batch: 154 Train Loss: 0.615428626537323\n",
      "Epoch 0 Batch: 156 Train Loss: 0.6228877305984497\n",
      "Epoch 0 Batch: 158 Train Loss: 0.6803406476974487\n",
      "Epoch 0 Batch: 160 Train Loss: 0.7203674912452698\n",
      "Epoch 1 Batch: 2 Train Loss: 0.6751906871795654\n",
      "Epoch 1 Batch: 4 Train Loss: 0.5530700087547302\n",
      "Epoch 1 Batch: 6 Train Loss: 0.6760340929031372\n",
      "Epoch 1 Batch: 8 Train Loss: 0.605539083480835\n",
      "Epoch 1 Batch: 10 Train Loss: 0.6041800379753113\n",
      "Epoch 1 Batch: 12 Train Loss: 0.6071792244911194\n",
      "Epoch 1 Batch: 14 Train Loss: 0.6833787560462952\n",
      "Epoch 1 Batch: 16 Train Loss: 0.6407966017723083\n",
      "Epoch 1 Batch: 18 Train Loss: 0.6642554998397827\n",
      "Epoch 1 Batch: 20 Train Loss: 0.6544797420501709\n",
      "Epoch 1 Batch: 22 Train Loss: 0.663904070854187\n",
      "Epoch 1 Batch: 24 Train Loss: 0.6883736252784729\n",
      "Epoch 1 Batch: 26 Train Loss: 0.5729882121086121\n",
      "Epoch 1 Batch: 28 Train Loss: 0.5987496376037598\n",
      "Epoch 1 Batch: 30 Train Loss: 0.5210729837417603\n",
      "Epoch 1 Batch: 32 Train Loss: 0.5395215749740601\n",
      "Epoch 1 Batch: 34 Train Loss: 0.5779088735580444\n",
      "Epoch 1 Batch: 36 Train Loss: 0.5815613865852356\n",
      "Epoch 1 Batch: 38 Train Loss: 0.5082192420959473\n",
      "Epoch 1 Batch: 40 Train Loss: 0.6254575848579407\n",
      "Epoch 1 Batch: 42 Train Loss: 0.6027183532714844\n",
      "Epoch 1 Batch: 44 Train Loss: 0.583880603313446\n",
      "Epoch 1 Batch: 46 Train Loss: 0.5371817350387573\n",
      "Epoch 1 Batch: 48 Train Loss: 0.503764808177948\n",
      "Epoch 1 Batch: 50 Train Loss: 0.6433480978012085\n",
      "Epoch 1 Batch: 52 Train Loss: 0.6894634962081909\n",
      "Epoch 1 Batch: 54 Train Loss: 0.5437155961990356\n",
      "Epoch 1 Batch: 56 Train Loss: 0.4271549582481384\n",
      "Epoch 1 Batch: 58 Train Loss: 0.5771876573562622\n",
      "Epoch 1 Batch: 60 Train Loss: 0.47659534215927124\n",
      "Epoch 1 Batch: 62 Train Loss: 0.4861243665218353\n",
      "Epoch 1 Batch: 64 Train Loss: 0.5430810451507568\n",
      "Epoch 1 Batch: 66 Train Loss: 0.46553415060043335\n",
      "Epoch 1 Batch: 68 Train Loss: 0.43683353066444397\n",
      "Epoch 1 Batch: 70 Train Loss: 0.5133139491081238\n",
      "Epoch 1 Batch: 72 Train Loss: 0.4537910521030426\n",
      "Epoch 1 Batch: 74 Train Loss: 0.5234242677688599\n",
      "Epoch 1 Batch: 76 Train Loss: 0.4513050615787506\n",
      "Epoch 1 Batch: 78 Train Loss: 0.5382689833641052\n",
      "Epoch 1 Batch: 80 Train Loss: 0.4480195939540863\n",
      "Epoch 1 Batch: 82 Train Loss: 0.38143330812454224\n",
      "Epoch 1 Batch: 84 Train Loss: 0.4239841401576996\n",
      "Epoch 1 Batch: 86 Train Loss: 0.3560425341129303\n",
      "Epoch 1 Batch: 88 Train Loss: 0.48005399107933044\n",
      "Epoch 1 Batch: 90 Train Loss: 0.3634374439716339\n",
      "Epoch 1 Batch: 92 Train Loss: 0.5766416788101196\n",
      "Epoch 1 Batch: 94 Train Loss: 0.4425007700920105\n",
      "Epoch 1 Batch: 96 Train Loss: 0.4033464789390564\n",
      "Epoch 1 Batch: 98 Train Loss: 0.471534788608551\n",
      "Epoch 1 Batch: 100 Train Loss: 0.41743698716163635\n",
      "Epoch 1 Batch: 102 Train Loss: 0.5623162388801575\n",
      "Epoch 1 Batch: 104 Train Loss: 0.2539263069629669\n",
      "Epoch 1 Batch: 106 Train Loss: 0.5478583574295044\n",
      "Epoch 1 Batch: 108 Train Loss: 0.3405095934867859\n",
      "Epoch 1 Batch: 110 Train Loss: 0.3480880856513977\n",
      "Epoch 1 Batch: 112 Train Loss: 0.3608836233615875\n",
      "Epoch 1 Batch: 114 Train Loss: 0.3934905529022217\n",
      "Epoch 1 Batch: 116 Train Loss: 0.3822872042655945\n",
      "Epoch 1 Batch: 118 Train Loss: 0.5041744112968445\n",
      "Epoch 1 Batch: 120 Train Loss: 0.3685435652732849\n",
      "Epoch 1 Batch: 122 Train Loss: 0.2969128489494324\n",
      "Epoch 1 Batch: 124 Train Loss: 0.2768523395061493\n",
      "Epoch 1 Batch: 126 Train Loss: 0.3979671597480774\n",
      "Epoch 1 Batch: 128 Train Loss: 0.42323532700538635\n",
      "Epoch 1 Batch: 130 Train Loss: 0.37252360582351685\n",
      "Epoch 1 Batch: 132 Train Loss: 0.2619263529777527\n",
      "Epoch 1 Batch: 134 Train Loss: 0.3117865025997162\n",
      "Epoch 1 Batch: 136 Train Loss: 0.24514372646808624\n",
      "Epoch 1 Batch: 138 Train Loss: 0.4758435785770416\n",
      "Epoch 1 Batch: 140 Train Loss: 0.24325518310070038\n",
      "Epoch 1 Batch: 142 Train Loss: 0.2742612957954407\n",
      "Epoch 1 Batch: 144 Train Loss: 0.30555129051208496\n",
      "Epoch 1 Batch: 146 Train Loss: 0.2807878851890564\n",
      "Epoch 1 Batch: 148 Train Loss: 0.21647731959819794\n",
      "Epoch 1 Batch: 150 Train Loss: 0.22550180554389954\n",
      "Epoch 1 Batch: 152 Train Loss: 0.23201031982898712\n",
      "Epoch 1 Batch: 154 Train Loss: 0.4110453724861145\n",
      "Epoch 1 Batch: 156 Train Loss: 0.4140334725379944\n",
      "Epoch 1 Batch: 158 Train Loss: 0.3140835165977478\n",
      "Epoch 1 Batch: 160 Train Loss: 0.41492438316345215\n",
      "Epoch 2 Batch: 2 Train Loss: 0.37245994806289673\n",
      "Epoch 2 Batch: 4 Train Loss: 0.4297350347042084\n",
      "Epoch 2 Batch: 6 Train Loss: 0.15373802185058594\n",
      "Epoch 2 Batch: 8 Train Loss: 0.356967031955719\n",
      "Epoch 2 Batch: 10 Train Loss: 0.3219716250896454\n",
      "Epoch 2 Batch: 12 Train Loss: 0.3711014688014984\n",
      "Epoch 2 Batch: 14 Train Loss: 0.26921650767326355\n",
      "Epoch 2 Batch: 16 Train Loss: 0.2237997055053711\n",
      "Epoch 2 Batch: 18 Train Loss: 0.2101997584104538\n",
      "Epoch 2 Batch: 20 Train Loss: 0.10076288878917694\n",
      "Epoch 2 Batch: 22 Train Loss: 0.1752830296754837\n",
      "Epoch 2 Batch: 24 Train Loss: 0.4390670359134674\n",
      "Epoch 2 Batch: 26 Train Loss: 0.28098833560943604\n",
      "Epoch 2 Batch: 28 Train Loss: 0.10997029393911362\n",
      "Epoch 2 Batch: 30 Train Loss: 0.7864699363708496\n",
      "Epoch 2 Batch: 32 Train Loss: 0.21085838973522186\n",
      "Epoch 2 Batch: 34 Train Loss: 0.3345378041267395\n",
      "Epoch 2 Batch: 36 Train Loss: 0.2947262227535248\n",
      "Epoch 2 Batch: 38 Train Loss: 0.15641562640666962\n",
      "Epoch 2 Batch: 40 Train Loss: 0.5126325488090515\n",
      "Epoch 2 Batch: 42 Train Loss: 0.21583421528339386\n",
      "Epoch 2 Batch: 44 Train Loss: 0.15745961666107178\n",
      "Epoch 2 Batch: 46 Train Loss: 0.1946624219417572\n",
      "Epoch 2 Batch: 48 Train Loss: 0.18975311517715454\n",
      "Epoch 2 Batch: 50 Train Loss: 0.12657530605793\n",
      "Epoch 2 Batch: 52 Train Loss: 0.21588322520256042\n",
      "Epoch 2 Batch: 54 Train Loss: 0.14039243757724762\n",
      "Epoch 2 Batch: 56 Train Loss: 0.07472482323646545\n",
      "Epoch 2 Batch: 58 Train Loss: 0.12547902762889862\n",
      "Epoch 2 Batch: 60 Train Loss: 0.1826295256614685\n",
      "Epoch 2 Batch: 62 Train Loss: 0.11855938285589218\n",
      "Epoch 2 Batch: 64 Train Loss: 0.16739538311958313\n",
      "Epoch 2 Batch: 66 Train Loss: 0.25932782888412476\n",
      "Epoch 2 Batch: 68 Train Loss: 0.10757418721914291\n",
      "Epoch 2 Batch: 70 Train Loss: 0.11173257976770401\n",
      "Epoch 2 Batch: 72 Train Loss: 0.12278611958026886\n",
      "Epoch 2 Batch: 74 Train Loss: 0.3151223063468933\n",
      "Epoch 2 Batch: 76 Train Loss: 0.16058765351772308\n",
      "Epoch 2 Batch: 78 Train Loss: 0.33549144864082336\n",
      "Epoch 2 Batch: 80 Train Loss: 0.37313371896743774\n",
      "Epoch 2 Batch: 82 Train Loss: 0.4428393840789795\n",
      "Epoch 2 Batch: 84 Train Loss: 0.27767351269721985\n",
      "Epoch 2 Batch: 86 Train Loss: 0.6330520510673523\n",
      "Epoch 2 Batch: 88 Train Loss: 0.1588563621044159\n",
      "Epoch 2 Batch: 90 Train Loss: 0.44307175278663635\n",
      "Epoch 2 Batch: 92 Train Loss: 0.397448867559433\n",
      "Epoch 2 Batch: 94 Train Loss: 0.3247908055782318\n",
      "Epoch 2 Batch: 96 Train Loss: 0.11799224466085434\n",
      "Epoch 2 Batch: 98 Train Loss: 0.28755271434783936\n",
      "Epoch 2 Batch: 100 Train Loss: 0.1572764813899994\n",
      "Epoch 2 Batch: 102 Train Loss: 0.10444966703653336\n",
      "Epoch 2 Batch: 104 Train Loss: 0.09287805110216141\n",
      "Epoch 2 Batch: 106 Train Loss: 0.5581364631652832\n",
      "Epoch 2 Batch: 108 Train Loss: 0.10620709508657455\n",
      "Epoch 2 Batch: 110 Train Loss: 0.3350829780101776\n",
      "Epoch 2 Batch: 112 Train Loss: 0.09195467829704285\n",
      "Epoch 2 Batch: 114 Train Loss: 0.19768814742565155\n",
      "Epoch 2 Batch: 116 Train Loss: 0.08894740790128708\n",
      "Epoch 2 Batch: 118 Train Loss: 0.09093839675188065\n",
      "Epoch 2 Batch: 120 Train Loss: 0.07410281896591187\n",
      "Epoch 2 Batch: 122 Train Loss: 0.35480034351348877\n",
      "Epoch 2 Batch: 124 Train Loss: 0.2782512605190277\n",
      "Epoch 2 Batch: 126 Train Loss: 0.3034251928329468\n",
      "Epoch 2 Batch: 128 Train Loss: 0.38599109649658203\n",
      "Epoch 2 Batch: 130 Train Loss: 0.5803066492080688\n",
      "Epoch 2 Batch: 132 Train Loss: 0.21368081867694855\n",
      "Epoch 2 Batch: 134 Train Loss: 0.4996328353881836\n",
      "Epoch 2 Batch: 136 Train Loss: 0.08553455770015717\n",
      "Epoch 2 Batch: 138 Train Loss: 0.4599006175994873\n",
      "Epoch 2 Batch: 140 Train Loss: 0.14072412252426147\n",
      "Epoch 2 Batch: 142 Train Loss: 0.17399942874908447\n",
      "Epoch 2 Batch: 144 Train Loss: 0.07759085297584534\n",
      "Epoch 2 Batch: 146 Train Loss: 0.32034820318222046\n",
      "Epoch 2 Batch: 148 Train Loss: 0.08231523633003235\n",
      "Epoch 2 Batch: 150 Train Loss: 0.12119494378566742\n",
      "Epoch 2 Batch: 152 Train Loss: 0.09935374557971954\n",
      "Epoch 2 Batch: 154 Train Loss: 0.31204038858413696\n",
      "Epoch 2 Batch: 156 Train Loss: 0.4056646227836609\n",
      "Epoch 2 Batch: 158 Train Loss: 0.1514284312725067\n",
      "Epoch 2 Batch: 160 Train Loss: 0.0860997885465622\n",
      "Epoch 3 Batch: 2 Train Loss: 0.10291510820388794\n",
      "Epoch 3 Batch: 4 Train Loss: 0.30890655517578125\n",
      "Epoch 3 Batch: 6 Train Loss: 0.31437259912490845\n",
      "Epoch 3 Batch: 8 Train Loss: 0.09594761580228806\n",
      "Epoch 3 Batch: 10 Train Loss: 0.0641220435500145\n",
      "Epoch 3 Batch: 12 Train Loss: 0.29073575139045715\n",
      "Epoch 3 Batch: 14 Train Loss: 0.09987093508243561\n",
      "Epoch 3 Batch: 16 Train Loss: 0.15344415605068207\n",
      "Epoch 3 Batch: 18 Train Loss: 0.07090358436107635\n",
      "Epoch 3 Batch: 20 Train Loss: 0.105644091963768\n",
      "Epoch 3 Batch: 22 Train Loss: 0.051225580275058746\n",
      "Epoch 3 Batch: 24 Train Loss: 0.36680158972740173\n",
      "Epoch 3 Batch: 26 Train Loss: 0.35070711374282837\n",
      "Epoch 3 Batch: 28 Train Loss: 0.3782317638397217\n",
      "Epoch 3 Batch: 30 Train Loss: 0.3712281584739685\n",
      "Epoch 3 Batch: 32 Train Loss: 0.07703086733818054\n",
      "Epoch 3 Batch: 34 Train Loss: 0.36999747157096863\n",
      "Epoch 3 Batch: 36 Train Loss: 0.06795629113912582\n",
      "Epoch 3 Batch: 38 Train Loss: 0.3572705090045929\n",
      "Epoch 3 Batch: 40 Train Loss: 0.30791300535202026\n",
      "Epoch 3 Batch: 42 Train Loss: 0.06599122285842896\n",
      "Epoch 3 Batch: 44 Train Loss: 0.5658573508262634\n",
      "Epoch 3 Batch: 46 Train Loss: 0.17689990997314453\n",
      "Epoch 3 Batch: 48 Train Loss: 0.1281743347644806\n",
      "Epoch 3 Batch: 50 Train Loss: 0.08598139137029648\n",
      "Epoch 3 Batch: 52 Train Loss: 0.40960103273391724\n",
      "Epoch 3 Batch: 54 Train Loss: 0.3289130628108978\n",
      "Epoch 3 Batch: 56 Train Loss: 0.27016180753707886\n",
      "Epoch 3 Batch: 58 Train Loss: 0.31822457909584045\n",
      "Epoch 3 Batch: 60 Train Loss: 0.13900808990001678\n",
      "Epoch 3 Batch: 62 Train Loss: 0.14352259039878845\n",
      "Epoch 3 Batch: 64 Train Loss: 0.29864388704299927\n",
      "Epoch 3 Batch: 66 Train Loss: 0.10767810046672821\n",
      "Epoch 3 Batch: 68 Train Loss: 0.23493759334087372\n",
      "Epoch 3 Batch: 70 Train Loss: 0.1107555404305458\n",
      "Epoch 3 Batch: 72 Train Loss: 0.07276305556297302\n",
      "Epoch 3 Batch: 74 Train Loss: 0.16137996315956116\n",
      "Epoch 3 Batch: 76 Train Loss: 0.08158431202173233\n",
      "Epoch 3 Batch: 78 Train Loss: 0.3672125041484833\n",
      "Epoch 3 Batch: 80 Train Loss: 0.23867443203926086\n",
      "Epoch 3 Batch: 82 Train Loss: 0.3166142404079437\n",
      "Epoch 3 Batch: 84 Train Loss: 0.06454048305749893\n",
      "Epoch 3 Batch: 86 Train Loss: 0.0700826346874237\n",
      "Epoch 3 Batch: 88 Train Loss: 0.2932053506374359\n",
      "Epoch 3 Batch: 90 Train Loss: 0.28035637736320496\n",
      "Epoch 3 Batch: 92 Train Loss: 0.272091805934906\n",
      "Epoch 3 Batch: 94 Train Loss: 0.13048911094665527\n",
      "Epoch 3 Batch: 96 Train Loss: 0.10066799074411392\n",
      "Epoch 3 Batch: 98 Train Loss: 0.045358508825302124\n",
      "Epoch 3 Batch: 100 Train Loss: 0.3294052481651306\n",
      "Epoch 3 Batch: 102 Train Loss: 0.048804156482219696\n",
      "Epoch 3 Batch: 104 Train Loss: 0.5445955395698547\n",
      "Epoch 3 Batch: 106 Train Loss: 0.0969330221414566\n",
      "Epoch 3 Batch: 108 Train Loss: 0.11563003063201904\n",
      "Epoch 3 Batch: 110 Train Loss: 0.08254411071538925\n",
      "Epoch 3 Batch: 112 Train Loss: 0.06314932554960251\n",
      "Epoch 3 Batch: 114 Train Loss: 0.5341594219207764\n",
      "Epoch 3 Batch: 116 Train Loss: 0.06661398708820343\n",
      "Epoch 3 Batch: 118 Train Loss: 0.3201490044593811\n",
      "Epoch 3 Batch: 120 Train Loss: 0.03147674351930618\n",
      "Epoch 3 Batch: 122 Train Loss: 0.32950359582901\n",
      "Epoch 3 Batch: 124 Train Loss: 0.07796619087457657\n",
      "Epoch 3 Batch: 126 Train Loss: 0.06236584112048149\n",
      "Epoch 3 Batch: 128 Train Loss: 0.08710795640945435\n",
      "Epoch 3 Batch: 130 Train Loss: 0.07816384732723236\n",
      "Epoch 3 Batch: 132 Train Loss: 0.483509361743927\n",
      "Epoch 3 Batch: 134 Train Loss: 0.3587592840194702\n",
      "Epoch 3 Batch: 136 Train Loss: 0.5425025820732117\n",
      "Epoch 3 Batch: 138 Train Loss: 0.05162809044122696\n",
      "Epoch 3 Batch: 140 Train Loss: 0.06177353113889694\n",
      "Epoch 3 Batch: 142 Train Loss: 0.08331555873155594\n",
      "Epoch 3 Batch: 144 Train Loss: 0.07360929995775223\n",
      "Epoch 3 Batch: 146 Train Loss: 0.06365208327770233\n",
      "Epoch 3 Batch: 148 Train Loss: 0.04263228178024292\n",
      "Epoch 3 Batch: 150 Train Loss: 0.042192243039608\n",
      "Epoch 3 Batch: 152 Train Loss: 0.31577080488204956\n",
      "Epoch 3 Batch: 154 Train Loss: 0.061083823442459106\n",
      "Epoch 3 Batch: 156 Train Loss: 0.32691138982772827\n",
      "Epoch 3 Batch: 158 Train Loss: 0.4791986048221588\n",
      "Epoch 3 Batch: 160 Train Loss: 0.1042657271027565\n",
      "Epoch 4 Batch: 2 Train Loss: 0.4465307295322418\n",
      "Epoch 4 Batch: 4 Train Loss: 0.2922798991203308\n",
      "Epoch 4 Batch: 6 Train Loss: 0.30185800790786743\n",
      "Epoch 4 Batch: 8 Train Loss: 0.2667560279369354\n",
      "Epoch 4 Batch: 10 Train Loss: 0.06965698301792145\n",
      "Epoch 4 Batch: 12 Train Loss: 0.0904526561498642\n",
      "Epoch 4 Batch: 14 Train Loss: 0.06371286511421204\n",
      "Epoch 4 Batch: 16 Train Loss: 0.07319682836532593\n",
      "Epoch 4 Batch: 18 Train Loss: 0.3515826165676117\n",
      "Epoch 4 Batch: 20 Train Loss: 0.3000350892543793\n",
      "Epoch 4 Batch: 22 Train Loss: 0.0848039910197258\n",
      "Epoch 4 Batch: 24 Train Loss: 0.08749575167894363\n",
      "Epoch 4 Batch: 26 Train Loss: 0.05084959417581558\n",
      "Epoch 4 Batch: 28 Train Loss: 0.05048227310180664\n",
      "Epoch 4 Batch: 30 Train Loss: 0.07867010682821274\n",
      "Epoch 4 Batch: 32 Train Loss: 0.5428332090377808\n",
      "Epoch 4 Batch: 34 Train Loss: 0.2741338610649109\n",
      "Epoch 4 Batch: 36 Train Loss: 0.11363723129034042\n",
      "Epoch 4 Batch: 38 Train Loss: 0.034524593502283096\n",
      "Epoch 4 Batch: 40 Train Loss: 0.04153924435377121\n",
      "Epoch 4 Batch: 42 Train Loss: 0.061216242611408234\n",
      "Epoch 4 Batch: 44 Train Loss: 0.25626620650291443\n",
      "Epoch 4 Batch: 46 Train Loss: 0.05682486295700073\n",
      "Epoch 4 Batch: 48 Train Loss: 0.0830572098493576\n",
      "Epoch 4 Batch: 50 Train Loss: 0.07266811281442642\n",
      "Epoch 4 Batch: 52 Train Loss: 0.3189641535282135\n",
      "Epoch 4 Batch: 54 Train Loss: 0.27795544266700745\n",
      "Epoch 4 Batch: 56 Train Loss: 0.036669377237558365\n",
      "Epoch 4 Batch: 58 Train Loss: 0.2452520877122879\n",
      "Epoch 4 Batch: 60 Train Loss: 0.11854513734579086\n",
      "Epoch 4 Batch: 62 Train Loss: 0.31875696778297424\n",
      "Epoch 4 Batch: 64 Train Loss: 0.07288187742233276\n",
      "Epoch 4 Batch: 66 Train Loss: 0.3205999732017517\n",
      "Epoch 4 Batch: 68 Train Loss: 0.09781190752983093\n",
      "Epoch 4 Batch: 70 Train Loss: 0.28025978803634644\n",
      "Epoch 4 Batch: 72 Train Loss: 0.31177666783332825\n",
      "Epoch 4 Batch: 74 Train Loss: 0.056031204760074615\n",
      "Epoch 4 Batch: 76 Train Loss: 0.22677722573280334\n",
      "Epoch 4 Batch: 78 Train Loss: 0.05296383053064346\n",
      "Epoch 4 Batch: 80 Train Loss: 0.0857105702161789\n",
      "Epoch 4 Batch: 82 Train Loss: 0.3051217794418335\n",
      "Epoch 4 Batch: 84 Train Loss: 0.056204505264759064\n",
      "Epoch 4 Batch: 86 Train Loss: 0.4736720621585846\n",
      "Epoch 4 Batch: 88 Train Loss: 0.06567181646823883\n",
      "Epoch 4 Batch: 90 Train Loss: 0.02435271441936493\n",
      "Epoch 4 Batch: 92 Train Loss: 0.0816151425242424\n",
      "Epoch 4 Batch: 94 Train Loss: 0.06919074803590775\n",
      "Epoch 4 Batch: 96 Train Loss: 0.03779294341802597\n",
      "Epoch 4 Batch: 98 Train Loss: 0.06440375000238419\n",
      "Epoch 4 Batch: 100 Train Loss: 0.07113897055387497\n",
      "Epoch 4 Batch: 102 Train Loss: 0.06563174724578857\n",
      "Epoch 4 Batch: 104 Train Loss: 0.3603723645210266\n",
      "Epoch 4 Batch: 106 Train Loss: 0.049947138875722885\n",
      "Epoch 4 Batch: 108 Train Loss: 0.03787919878959656\n",
      "Epoch 4 Batch: 110 Train Loss: 0.04870464652776718\n",
      "Epoch 4 Batch: 112 Train Loss: 0.061505068093538284\n",
      "Epoch 4 Batch: 114 Train Loss: 0.07439650595188141\n",
      "Epoch 4 Batch: 116 Train Loss: 0.5530493259429932\n",
      "Epoch 4 Batch: 118 Train Loss: 0.046468086540699005\n",
      "Epoch 4 Batch: 120 Train Loss: 0.5498066544532776\n",
      "Epoch 4 Batch: 122 Train Loss: 0.03464481979608536\n",
      "Epoch 4 Batch: 124 Train Loss: 0.05287548899650574\n",
      "Epoch 4 Batch: 126 Train Loss: 0.3097834885120392\n",
      "Epoch 4 Batch: 128 Train Loss: 0.049047283828258514\n",
      "Epoch 4 Batch: 130 Train Loss: 0.7905495762825012\n",
      "Epoch 4 Batch: 132 Train Loss: 0.043218642473220825\n",
      "Epoch 4 Batch: 134 Train Loss: 0.09355922788381577\n",
      "Epoch 4 Batch: 136 Train Loss: 0.06836891174316406\n",
      "Epoch 4 Batch: 138 Train Loss: 0.026243075728416443\n",
      "Epoch 4 Batch: 140 Train Loss: 0.3865658640861511\n",
      "Epoch 4 Batch: 142 Train Loss: 0.5770371556282043\n",
      "Epoch 4 Batch: 144 Train Loss: 0.06144287437200546\n",
      "Epoch 4 Batch: 146 Train Loss: 0.05336400866508484\n",
      "Epoch 4 Batch: 148 Train Loss: 0.33736518025398254\n",
      "Epoch 4 Batch: 150 Train Loss: 0.038991451263427734\n",
      "Epoch 4 Batch: 152 Train Loss: 0.05880957841873169\n",
      "Epoch 4 Batch: 154 Train Loss: 0.33866938948631287\n",
      "Epoch 4 Batch: 156 Train Loss: 0.06112120673060417\n",
      "Epoch 4 Batch: 158 Train Loss: 0.6290978789329529\n",
      "Epoch 4 Batch: 160 Train Loss: 0.07989845424890518\n",
      "Epoch 5 Batch: 2 Train Loss: 0.3097689151763916\n",
      "Epoch 5 Batch: 4 Train Loss: 0.07750125229358673\n",
      "Epoch 5 Batch: 6 Train Loss: 0.2643038034439087\n",
      "Epoch 5 Batch: 8 Train Loss: 0.08925051987171173\n",
      "Epoch 5 Batch: 10 Train Loss: 0.4054035246372223\n",
      "Epoch 5 Batch: 12 Train Loss: 0.30916327238082886\n",
      "Epoch 5 Batch: 14 Train Loss: 0.2899216115474701\n",
      "Epoch 5 Batch: 16 Train Loss: 0.31195059418678284\n",
      "Epoch 5 Batch: 18 Train Loss: 0.05641709640622139\n",
      "Epoch 5 Batch: 20 Train Loss: 0.3102267384529114\n",
      "Epoch 5 Batch: 22 Train Loss: 0.05507213994860649\n",
      "Epoch 5 Batch: 24 Train Loss: 0.07204911857843399\n",
      "Epoch 5 Batch: 26 Train Loss: 0.03820013999938965\n",
      "Epoch 5 Batch: 28 Train Loss: 0.03516663610935211\n",
      "Epoch 5 Batch: 30 Train Loss: 0.350850909948349\n",
      "Epoch 5 Batch: 32 Train Loss: 0.06294684112071991\n",
      "Epoch 5 Batch: 34 Train Loss: 0.49560362100601196\n",
      "Epoch 5 Batch: 36 Train Loss: 0.08796631544828415\n",
      "Epoch 5 Batch: 38 Train Loss: 0.05743603780865669\n",
      "Epoch 5 Batch: 40 Train Loss: 0.030470389872789383\n",
      "Epoch 5 Batch: 42 Train Loss: 0.301370233297348\n",
      "Epoch 5 Batch: 44 Train Loss: 0.5032337307929993\n",
      "Epoch 5 Batch: 46 Train Loss: 0.328055739402771\n",
      "Epoch 5 Batch: 48 Train Loss: 0.0779511034488678\n",
      "Epoch 5 Batch: 50 Train Loss: 0.2974938750267029\n",
      "Epoch 5 Batch: 52 Train Loss: 0.25777631998062134\n",
      "Epoch 5 Batch: 54 Train Loss: 0.2905133068561554\n",
      "Epoch 5 Batch: 56 Train Loss: 0.0902961790561676\n",
      "Epoch 5 Batch: 58 Train Loss: 0.29168760776519775\n",
      "Epoch 5 Batch: 60 Train Loss: 0.24422378838062286\n",
      "Epoch 5 Batch: 62 Train Loss: 0.09607808291912079\n",
      "Epoch 5 Batch: 64 Train Loss: 0.05972994118928909\n",
      "Epoch 5 Batch: 66 Train Loss: 0.09226733446121216\n",
      "Epoch 5 Batch: 68 Train Loss: 0.2829330563545227\n",
      "Epoch 5 Batch: 70 Train Loss: 0.037455882877111435\n",
      "Epoch 5 Batch: 72 Train Loss: 0.3318135738372803\n",
      "Epoch 5 Batch: 74 Train Loss: 0.3286358118057251\n",
      "Epoch 5 Batch: 76 Train Loss: 0.03659916669130325\n",
      "Epoch 5 Batch: 78 Train Loss: 0.28195586800575256\n",
      "Epoch 5 Batch: 80 Train Loss: 0.07300227880477905\n",
      "Epoch 5 Batch: 82 Train Loss: 0.30885082483291626\n",
      "Epoch 5 Batch: 84 Train Loss: 0.30439382791519165\n",
      "Epoch 5 Batch: 86 Train Loss: 0.04026635363698006\n",
      "Epoch 5 Batch: 88 Train Loss: 0.05211789533495903\n",
      "Epoch 5 Batch: 90 Train Loss: 0.03938563913106918\n",
      "Epoch 5 Batch: 92 Train Loss: 0.044825516641139984\n",
      "Epoch 5 Batch: 94 Train Loss: 0.040724702179431915\n",
      "Epoch 5 Batch: 96 Train Loss: 0.027056947350502014\n",
      "Epoch 5 Batch: 98 Train Loss: 0.06397359073162079\n",
      "Epoch 5 Batch: 100 Train Loss: 0.04913705959916115\n",
      "Epoch 5 Batch: 102 Train Loss: 0.04551318660378456\n",
      "Epoch 5 Batch: 104 Train Loss: 0.584301769733429\n",
      "Epoch 5 Batch: 106 Train Loss: 0.0757354348897934\n",
      "Epoch 5 Batch: 108 Train Loss: 0.0491776280105114\n",
      "Epoch 5 Batch: 110 Train Loss: 0.041749197989702225\n",
      "Epoch 5 Batch: 112 Train Loss: 0.2730272114276886\n",
      "Epoch 5 Batch: 114 Train Loss: 0.4524803161621094\n",
      "Epoch 5 Batch: 116 Train Loss: 0.054130684584379196\n",
      "Epoch 5 Batch: 118 Train Loss: 0.04608481377363205\n",
      "Epoch 5 Batch: 120 Train Loss: 0.06692837178707123\n",
      "Epoch 5 Batch: 122 Train Loss: 0.04318198561668396\n",
      "Epoch 5 Batch: 124 Train Loss: 0.06381116062402725\n",
      "Epoch 5 Batch: 126 Train Loss: 0.29176947474479675\n",
      "Epoch 5 Batch: 128 Train Loss: 0.061998508870601654\n",
      "Epoch 5 Batch: 130 Train Loss: 0.29705697298049927\n",
      "Epoch 5 Batch: 132 Train Loss: 0.04984612017869949\n",
      "Epoch 5 Batch: 134 Train Loss: 0.059318553656339645\n",
      "Epoch 5 Batch: 136 Train Loss: 0.21476492285728455\n",
      "Epoch 5 Batch: 138 Train Loss: 0.09943254292011261\n",
      "Epoch 5 Batch: 140 Train Loss: 0.19569368660449982\n",
      "Epoch 5 Batch: 142 Train Loss: 0.04451422020792961\n",
      "Epoch 5 Batch: 144 Train Loss: 0.06354554742574692\n",
      "Epoch 5 Batch: 146 Train Loss: 0.27115142345428467\n",
      "Epoch 5 Batch: 148 Train Loss: 0.02834106609225273\n",
      "Epoch 5 Batch: 150 Train Loss: 0.5665926933288574\n",
      "Epoch 5 Batch: 152 Train Loss: 0.2861931622028351\n",
      "Epoch 5 Batch: 154 Train Loss: 0.0429241769015789\n",
      "Epoch 5 Batch: 156 Train Loss: 0.2759210765361786\n",
      "Epoch 5 Batch: 158 Train Loss: 0.2515275478363037\n",
      "Epoch 5 Batch: 160 Train Loss: 0.31312814354896545\n",
      "Epoch 6 Batch: 2 Train Loss: 0.4839666485786438\n",
      "Epoch 6 Batch: 4 Train Loss: 0.302041620016098\n",
      "Epoch 6 Batch: 6 Train Loss: 0.42682623863220215\n",
      "Epoch 6 Batch: 8 Train Loss: 0.14832596480846405\n",
      "Epoch 6 Batch: 10 Train Loss: 0.2792501747608185\n",
      "Epoch 6 Batch: 12 Train Loss: 0.05764614790678024\n",
      "Epoch 6 Batch: 14 Train Loss: 0.057652831077575684\n",
      "Epoch 6 Batch: 16 Train Loss: 0.17332282662391663\n",
      "Epoch 6 Batch: 18 Train Loss: 0.3338906764984131\n",
      "Epoch 6 Batch: 20 Train Loss: 0.04786791279911995\n",
      "Epoch 6 Batch: 22 Train Loss: 0.05159587413072586\n",
      "Epoch 6 Batch: 24 Train Loss: 0.0844818577170372\n",
      "Epoch 6 Batch: 26 Train Loss: 0.6563760042190552\n",
      "Epoch 6 Batch: 28 Train Loss: 0.15536388754844666\n",
      "Epoch 6 Batch: 30 Train Loss: 0.3039073646068573\n",
      "Epoch 6 Batch: 32 Train Loss: 0.2674311101436615\n",
      "Epoch 6 Batch: 34 Train Loss: 0.02319810912013054\n",
      "Epoch 6 Batch: 36 Train Loss: 0.05892312526702881\n",
      "Epoch 6 Batch: 38 Train Loss: 0.10693873465061188\n",
      "Epoch 6 Batch: 40 Train Loss: 0.3433031439781189\n",
      "Epoch 6 Batch: 42 Train Loss: 0.34932926297187805\n",
      "Epoch 6 Batch: 44 Train Loss: 0.31288135051727295\n",
      "Epoch 6 Batch: 46 Train Loss: 0.30927592515945435\n",
      "Epoch 6 Batch: 48 Train Loss: 0.2624211311340332\n",
      "Epoch 6 Batch: 50 Train Loss: 0.05211399868130684\n",
      "Epoch 6 Batch: 52 Train Loss: 0.07697971165180206\n",
      "Epoch 6 Batch: 54 Train Loss: 0.07231354713439941\n",
      "Epoch 6 Batch: 56 Train Loss: 0.2515011131763458\n",
      "Epoch 6 Batch: 58 Train Loss: 0.11305665969848633\n",
      "Epoch 6 Batch: 60 Train Loss: 0.2986529767513275\n",
      "Epoch 6 Batch: 62 Train Loss: 0.24711880087852478\n",
      "Epoch 6 Batch: 64 Train Loss: 0.10591299831867218\n",
      "Epoch 6 Batch: 66 Train Loss: 0.09266046434640884\n",
      "Epoch 6 Batch: 68 Train Loss: 0.24874767661094666\n",
      "Epoch 6 Batch: 70 Train Loss: 0.05329670384526253\n",
      "Epoch 6 Batch: 72 Train Loss: 0.3019026517868042\n",
      "Epoch 6 Batch: 74 Train Loss: 0.05050121620297432\n",
      "Epoch 6 Batch: 76 Train Loss: 0.5329697728157043\n",
      "Epoch 6 Batch: 78 Train Loss: 0.056845031678676605\n",
      "Epoch 6 Batch: 80 Train Loss: 0.04867346212267876\n",
      "Epoch 6 Batch: 82 Train Loss: 0.03593926876783371\n",
      "Epoch 6 Batch: 84 Train Loss: 0.3020554184913635\n",
      "Epoch 6 Batch: 86 Train Loss: 0.2829802632331848\n",
      "Epoch 6 Batch: 88 Train Loss: 0.5882055163383484\n",
      "Epoch 6 Batch: 90 Train Loss: 0.3575633764266968\n",
      "Epoch 6 Batch: 92 Train Loss: 0.10955280065536499\n",
      "Epoch 6 Batch: 94 Train Loss: 0.04637189954519272\n",
      "Epoch 6 Batch: 96 Train Loss: 0.10163843631744385\n",
      "Epoch 6 Batch: 98 Train Loss: 0.061416614800691605\n",
      "Epoch 6 Batch: 100 Train Loss: 0.06790511310100555\n",
      "Epoch 6 Batch: 102 Train Loss: 0.27282869815826416\n",
      "Epoch 6 Batch: 104 Train Loss: 0.03689861297607422\n",
      "Epoch 6 Batch: 106 Train Loss: 0.2743634283542633\n",
      "Epoch 6 Batch: 108 Train Loss: 0.27064698934555054\n",
      "Epoch 6 Batch: 110 Train Loss: 0.09088839590549469\n",
      "Epoch 6 Batch: 112 Train Loss: 0.054756779223680496\n",
      "Epoch 6 Batch: 114 Train Loss: 0.2442587912082672\n",
      "Epoch 6 Batch: 116 Train Loss: 0.024975202977657318\n",
      "Epoch 6 Batch: 118 Train Loss: 0.26675376296043396\n",
      "Epoch 6 Batch: 120 Train Loss: 0.2762978672981262\n",
      "Epoch 6 Batch: 122 Train Loss: 0.043747734278440475\n",
      "Epoch 6 Batch: 124 Train Loss: 0.02416713908314705\n",
      "Epoch 6 Batch: 126 Train Loss: 0.29082292318344116\n",
      "Epoch 6 Batch: 128 Train Loss: 0.2706514000892639\n",
      "Epoch 6 Batch: 130 Train Loss: 0.2744385600090027\n",
      "Epoch 6 Batch: 132 Train Loss: 0.13368366658687592\n",
      "Epoch 6 Batch: 134 Train Loss: 0.13990257680416107\n",
      "Epoch 6 Batch: 136 Train Loss: 0.02399735525250435\n",
      "Epoch 6 Batch: 138 Train Loss: 0.04897530376911163\n",
      "Epoch 6 Batch: 140 Train Loss: 0.37281250953674316\n",
      "Epoch 6 Batch: 142 Train Loss: 0.5899186134338379\n",
      "Epoch 6 Batch: 144 Train Loss: 0.05214722082018852\n",
      "Epoch 6 Batch: 146 Train Loss: 0.04935864359140396\n",
      "Epoch 6 Batch: 148 Train Loss: 0.33949750661849976\n",
      "Epoch 6 Batch: 150 Train Loss: 0.07227712124586105\n",
      "Epoch 6 Batch: 152 Train Loss: 0.2718244194984436\n",
      "Epoch 6 Batch: 154 Train Loss: 0.29474085569381714\n",
      "Epoch 6 Batch: 156 Train Loss: 0.07997584342956543\n",
      "Epoch 6 Batch: 158 Train Loss: 0.2843523621559143\n",
      "Epoch 6 Batch: 160 Train Loss: 0.0656810849905014\n",
      "Epoch 7 Batch: 2 Train Loss: 0.0561542809009552\n",
      "Epoch 7 Batch: 4 Train Loss: 0.03222213685512543\n",
      "Epoch 7 Batch: 6 Train Loss: 0.05552779883146286\n",
      "Epoch 7 Batch: 8 Train Loss: 0.3377133905887604\n",
      "Epoch 7 Batch: 10 Train Loss: 0.032392460852861404\n",
      "Epoch 7 Batch: 12 Train Loss: 0.26095256209373474\n",
      "Epoch 7 Batch: 14 Train Loss: 0.5541146993637085\n",
      "Epoch 7 Batch: 16 Train Loss: 0.33718353509902954\n",
      "Epoch 7 Batch: 18 Train Loss: 0.04824896901845932\n",
      "Epoch 7 Batch: 20 Train Loss: 0.47242698073387146\n",
      "Epoch 7 Batch: 22 Train Loss: 0.27268677949905396\n",
      "Epoch 7 Batch: 24 Train Loss: 0.09705071151256561\n",
      "Epoch 7 Batch: 26 Train Loss: 0.2796519696712494\n",
      "Epoch 7 Batch: 28 Train Loss: 0.27862319350242615\n",
      "Epoch 7 Batch: 30 Train Loss: 0.05291323736310005\n",
      "Epoch 7 Batch: 32 Train Loss: 0.28941312432289124\n",
      "Epoch 7 Batch: 34 Train Loss: 0.2042190581560135\n",
      "Epoch 7 Batch: 36 Train Loss: 0.7262755632400513\n",
      "Epoch 7 Batch: 38 Train Loss: 0.25073957443237305\n",
      "Epoch 7 Batch: 40 Train Loss: 0.09270010888576508\n",
      "Epoch 7 Batch: 42 Train Loss: 0.04949498176574707\n",
      "Epoch 7 Batch: 44 Train Loss: 0.046283744275569916\n",
      "Epoch 7 Batch: 46 Train Loss: 0.052798252552747726\n",
      "Epoch 7 Batch: 48 Train Loss: 0.6687856912612915\n",
      "Epoch 7 Batch: 50 Train Loss: 0.6662267446517944\n",
      "Epoch 7 Batch: 52 Train Loss: 0.046520672738552094\n",
      "Epoch 7 Batch: 54 Train Loss: 0.07152516394853592\n",
      "Epoch 7 Batch: 56 Train Loss: 0.08522863686084747\n",
      "Epoch 7 Batch: 58 Train Loss: 0.15848127007484436\n",
      "Epoch 7 Batch: 60 Train Loss: 0.29957935214042664\n",
      "Epoch 7 Batch: 62 Train Loss: 0.290098637342453\n",
      "Epoch 7 Batch: 64 Train Loss: 0.057815175503492355\n",
      "Epoch 7 Batch: 66 Train Loss: 0.3066132664680481\n",
      "Epoch 7 Batch: 68 Train Loss: 0.06414994597434998\n",
      "Epoch 7 Batch: 70 Train Loss: 0.034016676247119904\n",
      "Epoch 7 Batch: 72 Train Loss: 0.07985203713178635\n",
      "Epoch 7 Batch: 74 Train Loss: 0.07153503596782684\n",
      "Epoch 7 Batch: 76 Train Loss: 0.2817424535751343\n",
      "Epoch 7 Batch: 78 Train Loss: 0.26457884907722473\n",
      "Epoch 7 Batch: 80 Train Loss: 0.049508679658174515\n",
      "Epoch 7 Batch: 82 Train Loss: 0.27756208181381226\n",
      "Epoch 7 Batch: 84 Train Loss: 0.059404294937849045\n",
      "Epoch 7 Batch: 86 Train Loss: 0.2555801272392273\n",
      "Epoch 7 Batch: 88 Train Loss: 0.53681880235672\n",
      "Epoch 7 Batch: 90 Train Loss: 0.039842624217271805\n",
      "Epoch 7 Batch: 92 Train Loss: 0.2755057215690613\n",
      "Epoch 7 Batch: 94 Train Loss: 0.05101219564676285\n",
      "Epoch 7 Batch: 96 Train Loss: 0.08392354846000671\n",
      "Epoch 7 Batch: 98 Train Loss: 0.26772624254226685\n",
      "Epoch 7 Batch: 100 Train Loss: 0.09147701412439346\n",
      "Epoch 7 Batch: 102 Train Loss: 0.2631596624851227\n",
      "Epoch 7 Batch: 104 Train Loss: 0.05118657276034355\n",
      "Epoch 7 Batch: 106 Train Loss: 0.057706404477357864\n",
      "Epoch 7 Batch: 108 Train Loss: 0.3167307376861572\n",
      "Epoch 7 Batch: 110 Train Loss: 0.0571308359503746\n",
      "Epoch 7 Batch: 112 Train Loss: 0.058252058923244476\n",
      "Epoch 7 Batch: 114 Train Loss: 0.05839288979768753\n",
      "Epoch 7 Batch: 116 Train Loss: 0.045474614948034286\n",
      "Epoch 7 Batch: 118 Train Loss: 0.30541980266571045\n",
      "Epoch 7 Batch: 120 Train Loss: 0.5279574990272522\n",
      "Epoch 7 Batch: 122 Train Loss: 0.2512820065021515\n",
      "Epoch 7 Batch: 124 Train Loss: 0.19152705371379852\n",
      "Epoch 7 Batch: 126 Train Loss: 0.13525386154651642\n",
      "Epoch 7 Batch: 128 Train Loss: 0.3467474579811096\n",
      "Epoch 7 Batch: 130 Train Loss: 0.5575010776519775\n",
      "Epoch 7 Batch: 132 Train Loss: 0.0477571114897728\n",
      "Epoch 7 Batch: 134 Train Loss: 0.01717434823513031\n",
      "Epoch 7 Batch: 136 Train Loss: 0.03307656943798065\n",
      "Epoch 7 Batch: 138 Train Loss: 0.051119495183229446\n",
      "Epoch 7 Batch: 140 Train Loss: 0.03970639035105705\n",
      "Epoch 7 Batch: 142 Train Loss: 0.05852615833282471\n",
      "Epoch 7 Batch: 144 Train Loss: 0.036995500326156616\n",
      "Epoch 7 Batch: 146 Train Loss: 0.033088527619838715\n",
      "Epoch 7 Batch: 148 Train Loss: 0.054123807698488235\n",
      "Epoch 7 Batch: 150 Train Loss: 0.31398284435272217\n",
      "Epoch 7 Batch: 152 Train Loss: 0.34310251474380493\n",
      "Epoch 7 Batch: 154 Train Loss: 0.8598631620407104\n",
      "Epoch 7 Batch: 156 Train Loss: 0.23305010795593262\n",
      "Epoch 7 Batch: 158 Train Loss: 0.3139954209327698\n",
      "Epoch 7 Batch: 160 Train Loss: 0.09598260372877121\n",
      "Epoch 8 Batch: 2 Train Loss: 0.019127126783132553\n",
      "Epoch 8 Batch: 4 Train Loss: 0.04912986233830452\n",
      "Epoch 8 Batch: 6 Train Loss: 0.13752759993076324\n",
      "Epoch 8 Batch: 8 Train Loss: 0.5344997644424438\n",
      "Epoch 8 Batch: 10 Train Loss: 0.04405239224433899\n",
      "Epoch 8 Batch: 12 Train Loss: 0.521414041519165\n",
      "Epoch 8 Batch: 14 Train Loss: 0.16280891001224518\n",
      "Epoch 8 Batch: 16 Train Loss: 0.4878520965576172\n",
      "Epoch 8 Batch: 18 Train Loss: 0.2667185664176941\n",
      "Epoch 8 Batch: 20 Train Loss: 0.060187142342329025\n",
      "Epoch 8 Batch: 22 Train Loss: 0.060146551579236984\n",
      "Epoch 8 Batch: 24 Train Loss: 0.05251253768801689\n",
      "Epoch 8 Batch: 26 Train Loss: 0.04844188690185547\n",
      "Epoch 8 Batch: 28 Train Loss: 0.30185186862945557\n",
      "Epoch 8 Batch: 30 Train Loss: 0.04623347520828247\n",
      "Epoch 8 Batch: 32 Train Loss: 0.33284908533096313\n",
      "Epoch 8 Batch: 34 Train Loss: 0.04817502945661545\n",
      "Epoch 8 Batch: 36 Train Loss: 0.3121519088745117\n",
      "Epoch 8 Batch: 38 Train Loss: 0.305763840675354\n",
      "Epoch 8 Batch: 40 Train Loss: 0.049208320677280426\n",
      "Epoch 8 Batch: 42 Train Loss: 0.04352366179227829\n",
      "Epoch 8 Batch: 44 Train Loss: 0.06589499861001968\n",
      "Epoch 8 Batch: 46 Train Loss: 0.23168382048606873\n",
      "Epoch 8 Batch: 48 Train Loss: 0.05325738340616226\n",
      "Epoch 8 Batch: 50 Train Loss: 0.057692963629961014\n",
      "Epoch 8 Batch: 52 Train Loss: 0.2913534939289093\n",
      "Epoch 8 Batch: 54 Train Loss: 0.09050370752811432\n",
      "Epoch 8 Batch: 56 Train Loss: 0.07495369017124176\n",
      "Epoch 8 Batch: 58 Train Loss: 0.039746545255184174\n",
      "Epoch 8 Batch: 60 Train Loss: 0.2992980480194092\n",
      "Epoch 8 Batch: 62 Train Loss: 0.08596891164779663\n",
      "Epoch 8 Batch: 64 Train Loss: 0.053094424307346344\n",
      "Epoch 8 Batch: 66 Train Loss: 0.2915436327457428\n",
      "Epoch 8 Batch: 68 Train Loss: 0.05226095765829086\n",
      "Epoch 8 Batch: 70 Train Loss: 0.062036074697971344\n",
      "Epoch 8 Batch: 72 Train Loss: 0.05641115829348564\n",
      "Epoch 8 Batch: 74 Train Loss: 0.05429244786500931\n",
      "Epoch 8 Batch: 76 Train Loss: 0.03710947185754776\n",
      "Epoch 8 Batch: 78 Train Loss: 0.8558362126350403\n",
      "Epoch 8 Batch: 80 Train Loss: 0.5831660628318787\n",
      "Epoch 8 Batch: 82 Train Loss: 0.03524050861597061\n",
      "Epoch 8 Batch: 84 Train Loss: 0.05917820334434509\n",
      "Epoch 8 Batch: 86 Train Loss: 0.08238552510738373\n",
      "Epoch 8 Batch: 88 Train Loss: 0.3300628066062927\n",
      "Epoch 8 Batch: 90 Train Loss: 0.057707495987415314\n",
      "Epoch 8 Batch: 92 Train Loss: 0.052285511046648026\n",
      "Epoch 8 Batch: 94 Train Loss: 0.03282326087355614\n",
      "Epoch 8 Batch: 96 Train Loss: 0.054984886199235916\n",
      "Epoch 8 Batch: 98 Train Loss: 0.5673526525497437\n",
      "Epoch 8 Batch: 100 Train Loss: 0.06756429374217987\n",
      "Epoch 8 Batch: 102 Train Loss: 0.05270281434059143\n",
      "Epoch 8 Batch: 104 Train Loss: 0.2794213891029358\n",
      "Epoch 8 Batch: 106 Train Loss: 0.29759037494659424\n",
      "Epoch 8 Batch: 108 Train Loss: 0.3064117729663849\n",
      "Epoch 8 Batch: 110 Train Loss: 0.589267373085022\n",
      "Epoch 8 Batch: 112 Train Loss: 0.28016287088394165\n",
      "Epoch 8 Batch: 114 Train Loss: 0.060937486588954926\n",
      "Epoch 8 Batch: 116 Train Loss: 0.49161452054977417\n",
      "Epoch 8 Batch: 118 Train Loss: 0.18692541122436523\n",
      "Epoch 8 Batch: 120 Train Loss: 0.06278525292873383\n",
      "Epoch 8 Batch: 122 Train Loss: 0.029466520994901657\n",
      "Epoch 8 Batch: 124 Train Loss: 0.3061673045158386\n",
      "Epoch 8 Batch: 126 Train Loss: 0.2672565281391144\n",
      "Epoch 8 Batch: 128 Train Loss: 0.345100462436676\n",
      "Epoch 8 Batch: 130 Train Loss: 0.06172297149896622\n",
      "Epoch 8 Batch: 132 Train Loss: 0.061154477298259735\n",
      "Epoch 8 Batch: 134 Train Loss: 0.29739874601364136\n",
      "Epoch 8 Batch: 136 Train Loss: 0.06323879212141037\n",
      "Epoch 8 Batch: 138 Train Loss: 0.04371511936187744\n",
      "Epoch 8 Batch: 140 Train Loss: 0.2629280686378479\n",
      "Epoch 8 Batch: 142 Train Loss: 0.08345052599906921\n",
      "Epoch 8 Batch: 144 Train Loss: 0.5458744764328003\n",
      "Epoch 8 Batch: 146 Train Loss: 0.06544514000415802\n",
      "Epoch 8 Batch: 148 Train Loss: 0.2951100170612335\n",
      "Epoch 8 Batch: 150 Train Loss: 0.07958783954381943\n",
      "Epoch 8 Batch: 152 Train Loss: 0.4453904628753662\n",
      "Epoch 8 Batch: 154 Train Loss: 0.26498502492904663\n",
      "Epoch 8 Batch: 156 Train Loss: 0.2747173309326172\n",
      "Epoch 8 Batch: 158 Train Loss: 0.2714349329471588\n",
      "Epoch 8 Batch: 160 Train Loss: 0.020443543791770935\n",
      "Epoch 9 Batch: 2 Train Loss: 0.08134568482637405\n",
      "Epoch 9 Batch: 4 Train Loss: 0.06033497303724289\n",
      "Epoch 9 Batch: 6 Train Loss: 0.3547261655330658\n",
      "Epoch 9 Batch: 8 Train Loss: 0.047977037727832794\n",
      "Epoch 9 Batch: 10 Train Loss: 0.3048297166824341\n",
      "Epoch 9 Batch: 12 Train Loss: 0.6259128451347351\n",
      "Epoch 9 Batch: 14 Train Loss: 0.04282701760530472\n",
      "Epoch 9 Batch: 16 Train Loss: 0.04373852163553238\n",
      "Epoch 9 Batch: 18 Train Loss: 0.2463139295578003\n",
      "Epoch 9 Batch: 20 Train Loss: 0.053428828716278076\n",
      "Epoch 9 Batch: 22 Train Loss: 0.07940692454576492\n",
      "Epoch 9 Batch: 24 Train Loss: 0.27847757935523987\n",
      "Epoch 9 Batch: 26 Train Loss: 0.28400173783302307\n",
      "Epoch 9 Batch: 28 Train Loss: 0.04674575850367546\n",
      "Epoch 9 Batch: 30 Train Loss: 0.12664678692817688\n",
      "Epoch 9 Batch: 32 Train Loss: 0.07107991725206375\n",
      "Epoch 9 Batch: 34 Train Loss: 0.09156577289104462\n",
      "Epoch 9 Batch: 36 Train Loss: 0.05777531862258911\n",
      "Epoch 9 Batch: 38 Train Loss: 0.04007386416196823\n",
      "Epoch 9 Batch: 40 Train Loss: 0.5398210287094116\n",
      "Epoch 9 Batch: 42 Train Loss: 0.33071157336235046\n",
      "Epoch 9 Batch: 44 Train Loss: 0.057070326060056686\n",
      "Epoch 9 Batch: 46 Train Loss: 0.2836126983165741\n",
      "Epoch 9 Batch: 48 Train Loss: 0.3196874260902405\n",
      "Epoch 9 Batch: 50 Train Loss: 0.35176554322242737\n",
      "Epoch 9 Batch: 52 Train Loss: 0.08041620999574661\n",
      "Epoch 9 Batch: 54 Train Loss: 0.3337816298007965\n",
      "Epoch 9 Batch: 56 Train Loss: 0.06064147502183914\n",
      "Epoch 9 Batch: 58 Train Loss: 0.04954397305846214\n",
      "Epoch 9 Batch: 60 Train Loss: 0.03697136789560318\n",
      "Epoch 9 Batch: 62 Train Loss: 0.30506807565689087\n",
      "Epoch 9 Batch: 64 Train Loss: 0.026185613125562668\n",
      "Epoch 9 Batch: 66 Train Loss: 0.3108518719673157\n",
      "Epoch 9 Batch: 68 Train Loss: 0.07477770745754242\n",
      "Epoch 9 Batch: 70 Train Loss: 0.2892513871192932\n",
      "Epoch 9 Batch: 72 Train Loss: 0.05784206464886665\n",
      "Epoch 9 Batch: 74 Train Loss: 0.028456229716539383\n",
      "Epoch 9 Batch: 76 Train Loss: 0.04360377788543701\n",
      "Epoch 9 Batch: 78 Train Loss: 0.32849639654159546\n",
      "Epoch 9 Batch: 80 Train Loss: 0.0270734541118145\n",
      "Epoch 9 Batch: 82 Train Loss: 0.3036610186100006\n",
      "Epoch 9 Batch: 84 Train Loss: 0.2639826834201813\n",
      "Epoch 9 Batch: 86 Train Loss: 0.2740319073200226\n",
      "Epoch 9 Batch: 88 Train Loss: 0.052536558359861374\n",
      "Epoch 9 Batch: 90 Train Loss: 0.5459159016609192\n",
      "Epoch 9 Batch: 92 Train Loss: 0.26595473289489746\n",
      "Epoch 9 Batch: 94 Train Loss: 0.25638002157211304\n",
      "Epoch 9 Batch: 96 Train Loss: 0.2573067545890808\n",
      "Epoch 9 Batch: 98 Train Loss: 0.11888065189123154\n",
      "Epoch 9 Batch: 100 Train Loss: 0.2300805300474167\n",
      "Epoch 9 Batch: 102 Train Loss: 0.42490333318710327\n",
      "Epoch 9 Batch: 104 Train Loss: 0.4870591163635254\n",
      "Epoch 9 Batch: 106 Train Loss: 0.06706955283880234\n",
      "Epoch 9 Batch: 108 Train Loss: 0.2786575257778168\n",
      "Epoch 9 Batch: 110 Train Loss: 0.0746956467628479\n",
      "Epoch 9 Batch: 112 Train Loss: 0.32265201210975647\n",
      "Epoch 9 Batch: 114 Train Loss: 0.029721781611442566\n",
      "Epoch 9 Batch: 116 Train Loss: 0.07690359652042389\n",
      "Epoch 9 Batch: 118 Train Loss: 0.06458298861980438\n",
      "Epoch 9 Batch: 120 Train Loss: 0.03278554230928421\n",
      "Epoch 9 Batch: 122 Train Loss: 0.5539711713790894\n",
      "Epoch 9 Batch: 124 Train Loss: 0.07934647798538208\n",
      "Epoch 9 Batch: 126 Train Loss: 0.3239954113960266\n",
      "Epoch 9 Batch: 128 Train Loss: 0.08856505900621414\n",
      "Epoch 9 Batch: 130 Train Loss: 0.03637304529547691\n",
      "Epoch 9 Batch: 132 Train Loss: 0.0653972327709198\n",
      "Epoch 9 Batch: 134 Train Loss: 0.2724340856075287\n",
      "Epoch 9 Batch: 136 Train Loss: 0.03820652887225151\n",
      "Epoch 9 Batch: 138 Train Loss: 0.031887151300907135\n",
      "Epoch 9 Batch: 140 Train Loss: 0.05702129006385803\n",
      "Epoch 9 Batch: 142 Train Loss: 0.03941427543759346\n",
      "Epoch 9 Batch: 144 Train Loss: 0.34397387504577637\n",
      "Epoch 9 Batch: 146 Train Loss: 0.29584917426109314\n",
      "Epoch 9 Batch: 148 Train Loss: 0.05269196629524231\n",
      "Epoch 9 Batch: 150 Train Loss: 0.014182944782078266\n",
      "Epoch 9 Batch: 152 Train Loss: 0.5012113451957703\n",
      "Epoch 9 Batch: 154 Train Loss: 0.05023454502224922\n",
      "Epoch 9 Batch: 156 Train Loss: 0.013036753050982952\n",
      "Epoch 9 Batch: 158 Train Loss: 0.5209770202636719\n",
      "Epoch 9 Batch: 160 Train Loss: 0.3094469904899597\n",
      "Epoch 10 Batch: 2 Train Loss: 0.28778761625289917\n",
      "Epoch 10 Batch: 4 Train Loss: 0.28587478399276733\n",
      "Epoch 10 Batch: 6 Train Loss: 0.08529265969991684\n",
      "Epoch 10 Batch: 8 Train Loss: 0.24057932198047638\n",
      "Epoch 10 Batch: 10 Train Loss: 0.07631082832813263\n",
      "Epoch 10 Batch: 12 Train Loss: 0.0731862485408783\n",
      "Epoch 10 Batch: 14 Train Loss: 0.2996242642402649\n",
      "Epoch 10 Batch: 16 Train Loss: 0.03602329269051552\n",
      "Epoch 10 Batch: 18 Train Loss: 0.03278611972928047\n",
      "Epoch 10 Batch: 20 Train Loss: 0.044467952102422714\n",
      "Epoch 10 Batch: 22 Train Loss: 0.6160343289375305\n",
      "Epoch 10 Batch: 24 Train Loss: 0.3073895275592804\n",
      "Epoch 10 Batch: 26 Train Loss: 0.04151230305433273\n",
      "Epoch 10 Batch: 28 Train Loss: 0.06945805996656418\n",
      "Epoch 10 Batch: 30 Train Loss: 0.05939751863479614\n",
      "Epoch 10 Batch: 32 Train Loss: 0.557515025138855\n",
      "Epoch 10 Batch: 34 Train Loss: 0.052762530744075775\n",
      "Epoch 10 Batch: 36 Train Loss: 0.2728191912174225\n",
      "Epoch 10 Batch: 38 Train Loss: 0.0987086147069931\n",
      "Epoch 10 Batch: 40 Train Loss: 0.08396757394075394\n",
      "Epoch 10 Batch: 42 Train Loss: 0.07044931501150131\n",
      "Epoch 10 Batch: 44 Train Loss: 0.037884242832660675\n",
      "Epoch 10 Batch: 46 Train Loss: 0.046052321791648865\n",
      "Epoch 10 Batch: 48 Train Loss: 0.3490754961967468\n",
      "Epoch 10 Batch: 50 Train Loss: 0.057508211582899094\n",
      "Epoch 10 Batch: 52 Train Loss: 0.03890691697597504\n",
      "Epoch 10 Batch: 54 Train Loss: 0.024173468351364136\n",
      "Epoch 10 Batch: 56 Train Loss: 0.26666978001594543\n",
      "Epoch 10 Batch: 58 Train Loss: 0.29783734679222107\n",
      "Epoch 10 Batch: 60 Train Loss: 0.04842645674943924\n",
      "Epoch 10 Batch: 62 Train Loss: 0.04097827151417732\n",
      "Epoch 10 Batch: 64 Train Loss: 0.06121297925710678\n",
      "Epoch 10 Batch: 66 Train Loss: 0.05196639150381088\n",
      "Epoch 10 Batch: 68 Train Loss: 0.5494005084037781\n",
      "Epoch 10 Batch: 70 Train Loss: 0.03349488601088524\n",
      "Epoch 10 Batch: 72 Train Loss: 0.5445860624313354\n",
      "Epoch 10 Batch: 74 Train Loss: 0.3068806529045105\n",
      "Epoch 10 Batch: 76 Train Loss: 0.06447400897741318\n",
      "Epoch 10 Batch: 78 Train Loss: 0.0521395280957222\n",
      "Epoch 10 Batch: 80 Train Loss: 0.11803360283374786\n",
      "Epoch 10 Batch: 82 Train Loss: 0.1113697737455368\n",
      "Epoch 10 Batch: 84 Train Loss: 0.03712055832147598\n",
      "Epoch 10 Batch: 86 Train Loss: 0.3258909583091736\n",
      "Epoch 10 Batch: 88 Train Loss: 0.04365582391619682\n",
      "Epoch 10 Batch: 90 Train Loss: 0.5609644055366516\n",
      "Epoch 10 Batch: 92 Train Loss: 0.038977377116680145\n",
      "Epoch 10 Batch: 94 Train Loss: 0.02703235112130642\n",
      "Epoch 10 Batch: 96 Train Loss: 0.03713048994541168\n",
      "Epoch 10 Batch: 98 Train Loss: 0.2887544631958008\n",
      "Epoch 10 Batch: 100 Train Loss: 0.030631965026259422\n",
      "Epoch 10 Batch: 102 Train Loss: 0.03412969782948494\n",
      "Epoch 10 Batch: 104 Train Loss: 0.7532529830932617\n",
      "Epoch 10 Batch: 106 Train Loss: 0.5067633986473083\n",
      "Epoch 10 Batch: 108 Train Loss: 0.0781688392162323\n",
      "Epoch 10 Batch: 110 Train Loss: 0.4826067090034485\n",
      "Epoch 10 Batch: 112 Train Loss: 0.25513970851898193\n",
      "Epoch 10 Batch: 114 Train Loss: 0.0800875872373581\n",
      "Epoch 10 Batch: 116 Train Loss: 0.05922655016183853\n",
      "Epoch 10 Batch: 118 Train Loss: 0.07042434066534042\n",
      "Epoch 10 Batch: 120 Train Loss: 0.78270423412323\n",
      "Epoch 10 Batch: 122 Train Loss: 0.0508558563888073\n",
      "Epoch 10 Batch: 124 Train Loss: 0.03776402398943901\n",
      "Epoch 10 Batch: 126 Train Loss: 0.07034319639205933\n",
      "Epoch 10 Batch: 128 Train Loss: 0.045619890093803406\n",
      "Epoch 10 Batch: 130 Train Loss: 0.2811172604560852\n",
      "Epoch 10 Batch: 132 Train Loss: 0.05090748146176338\n",
      "Epoch 10 Batch: 134 Train Loss: 0.02171049639582634\n",
      "Epoch 10 Batch: 136 Train Loss: 0.08080239593982697\n",
      "Epoch 10 Batch: 138 Train Loss: 0.03728710860013962\n",
      "Epoch 10 Batch: 140 Train Loss: 0.04876316338777542\n",
      "Epoch 10 Batch: 142 Train Loss: 0.05732237547636032\n",
      "Epoch 10 Batch: 144 Train Loss: 0.5417099595069885\n",
      "Epoch 10 Batch: 146 Train Loss: 0.05002529174089432\n",
      "Epoch 10 Batch: 148 Train Loss: 0.5479990839958191\n",
      "Epoch 10 Batch: 150 Train Loss: 0.06366483867168427\n",
      "Epoch 10 Batch: 152 Train Loss: 0.2629196047782898\n",
      "Epoch 10 Batch: 154 Train Loss: 0.2620028853416443\n",
      "Epoch 10 Batch: 156 Train Loss: 0.2943604588508606\n",
      "Epoch 10 Batch: 158 Train Loss: 0.028254542499780655\n",
      "Epoch 10 Batch: 160 Train Loss: 0.059336233884096146\n",
      "Epoch 11 Batch: 2 Train Loss: 0.2703297734260559\n",
      "Epoch 11 Batch: 4 Train Loss: 0.268443763256073\n",
      "Epoch 11 Batch: 6 Train Loss: 0.08153850585222244\n",
      "Epoch 11 Batch: 8 Train Loss: 0.2947842478752136\n",
      "Epoch 11 Batch: 10 Train Loss: 0.0728062093257904\n",
      "Epoch 11 Batch: 12 Train Loss: 0.03626110032200813\n",
      "Epoch 11 Batch: 14 Train Loss: 0.27271413803100586\n",
      "Epoch 11 Batch: 16 Train Loss: 0.06631885468959808\n",
      "Epoch 11 Batch: 18 Train Loss: 0.28189700841903687\n",
      "Epoch 11 Batch: 20 Train Loss: 0.05479242652654648\n",
      "Epoch 11 Batch: 22 Train Loss: 0.2519260048866272\n",
      "Epoch 11 Batch: 24 Train Loss: 0.07446473836898804\n",
      "Epoch 11 Batch: 26 Train Loss: 0.28188782930374146\n",
      "Epoch 11 Batch: 28 Train Loss: 0.06797423958778381\n",
      "Epoch 11 Batch: 30 Train Loss: 0.07656154781579971\n",
      "Epoch 11 Batch: 32 Train Loss: 0.047858089208602905\n",
      "Epoch 11 Batch: 34 Train Loss: 0.284824013710022\n",
      "Epoch 11 Batch: 36 Train Loss: 0.03522162884473801\n",
      "Epoch 11 Batch: 38 Train Loss: 0.26067742705345154\n",
      "Epoch 11 Batch: 40 Train Loss: 0.3370688259601593\n",
      "Epoch 11 Batch: 42 Train Loss: 0.05342358350753784\n",
      "Epoch 11 Batch: 44 Train Loss: 0.5667810440063477\n",
      "Epoch 11 Batch: 46 Train Loss: 0.2546376585960388\n",
      "Epoch 11 Batch: 48 Train Loss: 0.29399484395980835\n",
      "Epoch 11 Batch: 50 Train Loss: 0.21716062724590302\n",
      "Epoch 11 Batch: 52 Train Loss: 0.06745383143424988\n",
      "Epoch 11 Batch: 54 Train Loss: 0.08678203076124191\n",
      "Epoch 11 Batch: 56 Train Loss: 0.028610333800315857\n",
      "Epoch 11 Batch: 58 Train Loss: 0.09722995012998581\n",
      "Epoch 11 Batch: 60 Train Loss: 0.05574299022555351\n",
      "Epoch 11 Batch: 62 Train Loss: 0.06411585956811905\n",
      "Epoch 11 Batch: 64 Train Loss: 0.2957804799079895\n",
      "Epoch 11 Batch: 66 Train Loss: 0.07633677870035172\n",
      "Epoch 11 Batch: 68 Train Loss: 0.3156166672706604\n",
      "Epoch 11 Batch: 70 Train Loss: 0.04035644605755806\n",
      "Epoch 11 Batch: 72 Train Loss: 0.045985180884599686\n",
      "Epoch 11 Batch: 74 Train Loss: 0.05069635435938835\n",
      "Epoch 11 Batch: 76 Train Loss: 0.3046511709690094\n",
      "Epoch 11 Batch: 78 Train Loss: 0.05029340833425522\n",
      "Epoch 11 Batch: 80 Train Loss: 0.041556466370821\n",
      "Epoch 11 Batch: 82 Train Loss: 0.024418696761131287\n",
      "Epoch 11 Batch: 84 Train Loss: 0.26091259717941284\n",
      "Epoch 11 Batch: 86 Train Loss: 0.015226365998387337\n",
      "Epoch 11 Batch: 88 Train Loss: 0.046044688671827316\n",
      "Epoch 11 Batch: 90 Train Loss: 0.046505726873874664\n",
      "Epoch 11 Batch: 92 Train Loss: 0.02727162465453148\n",
      "Epoch 11 Batch: 94 Train Loss: 0.010534382425248623\n",
      "Epoch 11 Batch: 96 Train Loss: 0.03458069637417793\n",
      "Epoch 11 Batch: 98 Train Loss: 0.06085339933633804\n",
      "Epoch 11 Batch: 100 Train Loss: 0.06490098685026169\n",
      "Epoch 11 Batch: 102 Train Loss: 0.3410196006298065\n",
      "Epoch 11 Batch: 104 Train Loss: 0.5181393027305603\n",
      "Epoch 11 Batch: 106 Train Loss: 0.06904800236225128\n",
      "Epoch 11 Batch: 108 Train Loss: 0.25152236223220825\n",
      "Epoch 11 Batch: 110 Train Loss: 0.22360515594482422\n",
      "Epoch 11 Batch: 112 Train Loss: 0.24972668290138245\n",
      "Epoch 11 Batch: 114 Train Loss: 0.10704879462718964\n",
      "Epoch 11 Batch: 116 Train Loss: 0.3304426372051239\n",
      "Epoch 11 Batch: 118 Train Loss: 0.050092268735170364\n",
      "Epoch 11 Batch: 120 Train Loss: 0.32416030764579773\n",
      "Epoch 11 Batch: 122 Train Loss: 0.044384486973285675\n",
      "Epoch 11 Batch: 124 Train Loss: 0.3005519211292267\n",
      "Epoch 11 Batch: 126 Train Loss: 0.05891216918826103\n",
      "Epoch 11 Batch: 128 Train Loss: 0.04190262407064438\n",
      "Epoch 11 Batch: 130 Train Loss: 0.03170003741979599\n",
      "Epoch 11 Batch: 132 Train Loss: 0.06714785099029541\n",
      "Epoch 11 Batch: 134 Train Loss: 0.31428641080856323\n",
      "Epoch 11 Batch: 136 Train Loss: 0.2974669337272644\n",
      "Epoch 11 Batch: 138 Train Loss: 0.08813444525003433\n",
      "Epoch 11 Batch: 140 Train Loss: 0.03557237982749939\n",
      "Epoch 11 Batch: 142 Train Loss: 0.04800320416688919\n",
      "Epoch 11 Batch: 144 Train Loss: 0.06469182670116425\n",
      "Epoch 11 Batch: 146 Train Loss: 0.5075278282165527\n",
      "Epoch 11 Batch: 148 Train Loss: 0.07409010827541351\n",
      "Epoch 11 Batch: 150 Train Loss: 0.27340397238731384\n",
      "Epoch 11 Batch: 152 Train Loss: 0.2619359791278839\n",
      "Epoch 11 Batch: 154 Train Loss: 0.49323636293411255\n",
      "Epoch 11 Batch: 156 Train Loss: 0.04383965954184532\n",
      "Epoch 11 Batch: 158 Train Loss: 0.08761880546808243\n",
      "Epoch 11 Batch: 160 Train Loss: 0.0667601227760315\n",
      "Epoch 12 Batch: 2 Train Loss: 0.08839883655309677\n",
      "Epoch 12 Batch: 4 Train Loss: 0.06793826818466187\n",
      "Epoch 12 Batch: 6 Train Loss: 0.038008373230695724\n",
      "Epoch 12 Batch: 8 Train Loss: 0.27330905199050903\n",
      "Epoch 12 Batch: 10 Train Loss: 0.30312275886535645\n",
      "Epoch 12 Batch: 12 Train Loss: 0.26393744349479675\n",
      "Epoch 12 Batch: 14 Train Loss: 0.2689129114151001\n",
      "Epoch 12 Batch: 16 Train Loss: 0.29670852422714233\n",
      "Epoch 12 Batch: 18 Train Loss: 0.5315683484077454\n",
      "Epoch 12 Batch: 20 Train Loss: 0.09596920013427734\n",
      "Epoch 12 Batch: 22 Train Loss: 0.2847893238067627\n",
      "Epoch 12 Batch: 24 Train Loss: 0.06014619022607803\n",
      "Epoch 12 Batch: 26 Train Loss: 0.2605312466621399\n",
      "Epoch 12 Batch: 28 Train Loss: 0.959314227104187\n",
      "Epoch 12 Batch: 30 Train Loss: 0.2752607762813568\n",
      "Epoch 12 Batch: 32 Train Loss: 0.05424206331372261\n",
      "Epoch 12 Batch: 34 Train Loss: 0.0731968879699707\n",
      "Epoch 12 Batch: 36 Train Loss: 0.5305483937263489\n",
      "Epoch 12 Batch: 38 Train Loss: 0.253089964389801\n",
      "Epoch 12 Batch: 40 Train Loss: 0.33554038405418396\n",
      "Epoch 12 Batch: 42 Train Loss: 0.06644383817911148\n",
      "Epoch 12 Batch: 44 Train Loss: 0.07909334450960159\n",
      "Epoch 12 Batch: 46 Train Loss: 0.24490976333618164\n",
      "Epoch 12 Batch: 48 Train Loss: 0.5638672113418579\n",
      "Epoch 12 Batch: 50 Train Loss: 0.33484649658203125\n",
      "Epoch 12 Batch: 52 Train Loss: 0.05621274560689926\n",
      "Epoch 12 Batch: 54 Train Loss: 0.5712408423423767\n",
      "Epoch 12 Batch: 56 Train Loss: 0.03169752284884453\n",
      "Epoch 12 Batch: 58 Train Loss: 0.4845300614833832\n",
      "Epoch 12 Batch: 60 Train Loss: 0.05066102743148804\n",
      "Epoch 12 Batch: 62 Train Loss: 0.28643983602523804\n",
      "Epoch 12 Batch: 64 Train Loss: 0.08288954198360443\n",
      "Epoch 12 Batch: 66 Train Loss: 0.09795267879962921\n",
      "Epoch 12 Batch: 68 Train Loss: 0.08222422003746033\n",
      "Epoch 12 Batch: 70 Train Loss: 0.2724933624267578\n",
      "Epoch 12 Batch: 72 Train Loss: 0.054220665246248245\n",
      "Epoch 12 Batch: 74 Train Loss: 0.30359914898872375\n",
      "Epoch 12 Batch: 76 Train Loss: 0.0738212913274765\n",
      "Epoch 12 Batch: 78 Train Loss: 0.298088401556015\n",
      "Epoch 12 Batch: 80 Train Loss: 0.06080091744661331\n",
      "Epoch 12 Batch: 82 Train Loss: 0.05838745832443237\n",
      "Epoch 12 Batch: 84 Train Loss: 0.2990745007991791\n",
      "Epoch 12 Batch: 86 Train Loss: 0.2601841390132904\n",
      "Epoch 12 Batch: 88 Train Loss: 0.06838525831699371\n",
      "Epoch 12 Batch: 90 Train Loss: 0.0331597663462162\n",
      "Epoch 12 Batch: 92 Train Loss: 0.04939179867506027\n",
      "Epoch 12 Batch: 94 Train Loss: 0.5583574175834656\n",
      "Epoch 12 Batch: 96 Train Loss: 0.2899606227874756\n",
      "Epoch 12 Batch: 98 Train Loss: 0.047215063124895096\n",
      "Epoch 12 Batch: 100 Train Loss: 0.05741008371114731\n",
      "Epoch 12 Batch: 102 Train Loss: 0.3253830671310425\n",
      "Epoch 12 Batch: 104 Train Loss: 0.2945767045021057\n",
      "Epoch 12 Batch: 106 Train Loss: 0.06773074716329575\n",
      "Epoch 12 Batch: 108 Train Loss: 0.058535706251859665\n",
      "Epoch 12 Batch: 110 Train Loss: 0.09099490940570831\n",
      "Epoch 12 Batch: 112 Train Loss: 0.3067407011985779\n",
      "Epoch 12 Batch: 114 Train Loss: 0.8157515525817871\n",
      "Epoch 12 Batch: 116 Train Loss: 0.036005984991788864\n",
      "Epoch 12 Batch: 118 Train Loss: 0.30692097544670105\n",
      "Epoch 12 Batch: 120 Train Loss: 0.05254826694726944\n",
      "Epoch 12 Batch: 122 Train Loss: 0.27880892157554626\n",
      "Epoch 12 Batch: 124 Train Loss: 0.260403573513031\n",
      "Epoch 12 Batch: 126 Train Loss: 0.04748079180717468\n",
      "Epoch 12 Batch: 128 Train Loss: 0.07709814608097076\n",
      "Epoch 12 Batch: 130 Train Loss: 0.5132449865341187\n",
      "Epoch 12 Batch: 132 Train Loss: 0.07049832493066788\n",
      "Epoch 12 Batch: 134 Train Loss: 0.06198369711637497\n",
      "Epoch 12 Batch: 136 Train Loss: 0.06587765365839005\n",
      "Epoch 12 Batch: 138 Train Loss: 0.08122467249631882\n",
      "Epoch 12 Batch: 140 Train Loss: 0.06662582606077194\n",
      "Epoch 12 Batch: 142 Train Loss: 0.300616592168808\n",
      "Epoch 12 Batch: 144 Train Loss: 0.057310719043016434\n",
      "Epoch 12 Batch: 146 Train Loss: 0.01709628477692604\n",
      "Epoch 12 Batch: 148 Train Loss: 0.6630901098251343\n",
      "Epoch 12 Batch: 150 Train Loss: 0.03962203487753868\n",
      "Epoch 12 Batch: 152 Train Loss: 0.051110345870256424\n",
      "Epoch 12 Batch: 154 Train Loss: 0.048552967607975006\n",
      "Epoch 12 Batch: 156 Train Loss: 0.03185948729515076\n",
      "Epoch 12 Batch: 158 Train Loss: 0.0362398587167263\n",
      "Epoch 12 Batch: 160 Train Loss: 0.044442638754844666\n",
      "Epoch 13 Batch: 2 Train Loss: 0.04041555896401405\n",
      "Epoch 13 Batch: 4 Train Loss: 0.05391324311494827\n",
      "Epoch 13 Batch: 6 Train Loss: 0.03758755326271057\n",
      "Epoch 13 Batch: 8 Train Loss: 0.03898731619119644\n",
      "Epoch 13 Batch: 10 Train Loss: 0.5157157182693481\n",
      "Epoch 13 Batch: 12 Train Loss: 0.2502412796020508\n",
      "Epoch 13 Batch: 14 Train Loss: 0.20264306664466858\n",
      "Epoch 13 Batch: 16 Train Loss: 0.3577219843864441\n",
      "Epoch 13 Batch: 18 Train Loss: 0.07273389399051666\n",
      "Epoch 13 Batch: 20 Train Loss: 0.25213170051574707\n",
      "Epoch 13 Batch: 22 Train Loss: 0.26214760541915894\n",
      "Epoch 13 Batch: 24 Train Loss: 0.5995541214942932\n",
      "Epoch 13 Batch: 26 Train Loss: 0.04462068900465965\n",
      "Epoch 13 Batch: 28 Train Loss: 0.014159945771098137\n",
      "Epoch 13 Batch: 30 Train Loss: 0.04054036736488342\n",
      "Epoch 13 Batch: 32 Train Loss: 0.042852308601140976\n",
      "Epoch 13 Batch: 34 Train Loss: 0.3382023274898529\n",
      "Epoch 13 Batch: 36 Train Loss: 0.06607778370380402\n",
      "Epoch 13 Batch: 38 Train Loss: 0.032635897397994995\n",
      "Epoch 13 Batch: 40 Train Loss: 0.04073172062635422\n",
      "Epoch 13 Batch: 42 Train Loss: 0.308165967464447\n",
      "Epoch 13 Batch: 44 Train Loss: 0.3538186252117157\n",
      "Epoch 13 Batch: 46 Train Loss: 0.024266162887215614\n",
      "Epoch 13 Batch: 48 Train Loss: 0.03512340039014816\n",
      "Epoch 13 Batch: 50 Train Loss: 0.271826833486557\n",
      "Epoch 13 Batch: 52 Train Loss: 0.06289797276258469\n",
      "Epoch 13 Batch: 54 Train Loss: 0.2614790201187134\n",
      "Epoch 13 Batch: 56 Train Loss: 0.5034468173980713\n",
      "Epoch 13 Batch: 58 Train Loss: 0.05589888244867325\n",
      "Epoch 13 Batch: 60 Train Loss: 0.07110050320625305\n",
      "Epoch 13 Batch: 62 Train Loss: 0.3072940707206726\n",
      "Epoch 13 Batch: 64 Train Loss: 0.045195434242486954\n",
      "Epoch 13 Batch: 66 Train Loss: 0.05817309021949768\n",
      "Epoch 13 Batch: 68 Train Loss: 0.04384166747331619\n",
      "Epoch 13 Batch: 70 Train Loss: 0.03839225322008133\n",
      "Epoch 13 Batch: 72 Train Loss: 0.04345203563570976\n",
      "Epoch 13 Batch: 74 Train Loss: 0.03305451571941376\n",
      "Epoch 13 Batch: 76 Train Loss: 0.046592578291893005\n",
      "Epoch 13 Batch: 78 Train Loss: 0.3491094708442688\n",
      "Epoch 13 Batch: 80 Train Loss: 0.026568595319986343\n",
      "Epoch 13 Batch: 82 Train Loss: 0.07314018905162811\n",
      "Epoch 13 Batch: 84 Train Loss: 0.8184682726860046\n",
      "Epoch 13 Batch: 86 Train Loss: 0.07533745467662811\n",
      "Epoch 13 Batch: 88 Train Loss: 0.10750526189804077\n",
      "Epoch 13 Batch: 90 Train Loss: 0.06455956399440765\n",
      "Epoch 13 Batch: 92 Train Loss: 0.05512819439172745\n",
      "Epoch 13 Batch: 94 Train Loss: 0.05534302443265915\n",
      "Epoch 13 Batch: 96 Train Loss: 0.24599094688892365\n",
      "Epoch 13 Batch: 98 Train Loss: 0.04440087452530861\n",
      "Epoch 13 Batch: 100 Train Loss: 0.02659732475876808\n",
      "Epoch 13 Batch: 102 Train Loss: 0.025724906474351883\n",
      "Epoch 13 Batch: 104 Train Loss: 0.32710668444633484\n",
      "Epoch 13 Batch: 106 Train Loss: 0.3641216456890106\n",
      "Epoch 13 Batch: 108 Train Loss: 0.2950313985347748\n",
      "Epoch 13 Batch: 110 Train Loss: 0.058386169373989105\n",
      "Epoch 13 Batch: 112 Train Loss: 0.5240092277526855\n",
      "Epoch 13 Batch: 114 Train Loss: 0.2950906753540039\n",
      "Epoch 13 Batch: 116 Train Loss: 0.045275524258613586\n",
      "Epoch 13 Batch: 118 Train Loss: 0.31254881620407104\n",
      "Epoch 13 Batch: 120 Train Loss: 0.05567913502454758\n",
      "Epoch 13 Batch: 122 Train Loss: 0.30116593837738037\n",
      "Epoch 13 Batch: 124 Train Loss: 0.053537894040346146\n",
      "Epoch 13 Batch: 126 Train Loss: 0.07070443779230118\n",
      "Epoch 13 Batch: 128 Train Loss: 0.30484065413475037\n",
      "Epoch 13 Batch: 130 Train Loss: 0.28037410974502563\n",
      "Epoch 13 Batch: 132 Train Loss: 0.24784556031227112\n",
      "Epoch 13 Batch: 134 Train Loss: 0.28797775506973267\n",
      "Epoch 13 Batch: 136 Train Loss: 0.45801371335983276\n",
      "Epoch 13 Batch: 138 Train Loss: 0.4698353409767151\n",
      "Epoch 13 Batch: 140 Train Loss: 0.16214224696159363\n",
      "Epoch 13 Batch: 142 Train Loss: 0.28186720609664917\n",
      "Epoch 13 Batch: 144 Train Loss: 0.033151112496852875\n",
      "Epoch 13 Batch: 146 Train Loss: 0.31695112586021423\n",
      "Epoch 13 Batch: 148 Train Loss: 0.32590728998184204\n",
      "Epoch 13 Batch: 150 Train Loss: 0.04578582942485809\n",
      "Epoch 13 Batch: 152 Train Loss: 0.10006290674209595\n",
      "Epoch 13 Batch: 154 Train Loss: 0.03731340914964676\n",
      "Epoch 13 Batch: 156 Train Loss: 0.04137972742319107\n",
      "Epoch 13 Batch: 158 Train Loss: 0.04230433702468872\n",
      "Epoch 13 Batch: 160 Train Loss: 0.3080802857875824\n",
      "Epoch 14 Batch: 2 Train Loss: 0.23822864890098572\n",
      "Epoch 14 Batch: 4 Train Loss: 0.2811102271080017\n",
      "Epoch 14 Batch: 6 Train Loss: 0.05421539396047592\n",
      "Epoch 14 Batch: 8 Train Loss: 0.07879650592803955\n",
      "Epoch 14 Batch: 10 Train Loss: 0.05653258040547371\n",
      "Epoch 14 Batch: 12 Train Loss: 0.07711298763751984\n",
      "Epoch 14 Batch: 14 Train Loss: 0.035967014729976654\n",
      "Epoch 14 Batch: 16 Train Loss: 0.27599745988845825\n",
      "Epoch 14 Batch: 18 Train Loss: 0.6022030115127563\n",
      "Epoch 14 Batch: 20 Train Loss: 0.25864431262016296\n",
      "Epoch 14 Batch: 22 Train Loss: 0.2961313724517822\n",
      "Epoch 14 Batch: 24 Train Loss: 0.05682402104139328\n",
      "Epoch 14 Batch: 26 Train Loss: 0.07442840188741684\n",
      "Epoch 14 Batch: 28 Train Loss: 0.48792487382888794\n",
      "Epoch 14 Batch: 30 Train Loss: 0.30988872051239014\n",
      "Epoch 14 Batch: 32 Train Loss: 0.2835172712802887\n",
      "Epoch 14 Batch: 34 Train Loss: 0.0864817276597023\n",
      "Epoch 14 Batch: 36 Train Loss: 0.059857167303562164\n",
      "Epoch 14 Batch: 38 Train Loss: 0.2853650450706482\n",
      "Epoch 14 Batch: 40 Train Loss: 0.03550068289041519\n",
      "Epoch 14 Batch: 42 Train Loss: 0.0527772381901741\n",
      "Epoch 14 Batch: 44 Train Loss: 0.07798079401254654\n",
      "Epoch 14 Batch: 46 Train Loss: 0.2854592502117157\n",
      "Epoch 14 Batch: 48 Train Loss: 0.027581235393881798\n",
      "Epoch 14 Batch: 50 Train Loss: 0.07491367310285568\n",
      "Epoch 14 Batch: 52 Train Loss: 0.04990745335817337\n",
      "Epoch 14 Batch: 54 Train Loss: 0.3551165759563446\n",
      "Epoch 14 Batch: 56 Train Loss: 0.31067854166030884\n",
      "Epoch 14 Batch: 58 Train Loss: 0.5294005870819092\n",
      "Epoch 14 Batch: 60 Train Loss: 0.3099643886089325\n",
      "Epoch 14 Batch: 62 Train Loss: 0.31733426451683044\n",
      "Epoch 14 Batch: 64 Train Loss: 0.027455169707536697\n",
      "Epoch 14 Batch: 66 Train Loss: 0.051340289413928986\n",
      "Epoch 14 Batch: 68 Train Loss: 0.06355302035808563\n",
      "Epoch 14 Batch: 70 Train Loss: 0.05771254748106003\n",
      "Epoch 14 Batch: 72 Train Loss: 0.04438536614179611\n",
      "Epoch 14 Batch: 74 Train Loss: 0.08530393987894058\n",
      "Epoch 14 Batch: 76 Train Loss: 0.07465486228466034\n",
      "Epoch 14 Batch: 78 Train Loss: 0.062140174210071564\n",
      "Epoch 14 Batch: 80 Train Loss: 0.09538032859563828\n",
      "Epoch 14 Batch: 82 Train Loss: 0.0859469622373581\n",
      "Epoch 14 Batch: 84 Train Loss: 0.28908663988113403\n",
      "Epoch 14 Batch: 86 Train Loss: 0.02404547668993473\n",
      "Epoch 14 Batch: 88 Train Loss: 0.29041093587875366\n",
      "Epoch 14 Batch: 90 Train Loss: 0.05792010948061943\n",
      "Epoch 14 Batch: 92 Train Loss: 0.02557467855513096\n",
      "Epoch 14 Batch: 94 Train Loss: 0.3251843750476837\n",
      "Epoch 14 Batch: 96 Train Loss: 0.2913709282875061\n",
      "Epoch 14 Batch: 98 Train Loss: 0.2767903804779053\n",
      "Epoch 14 Batch: 100 Train Loss: 0.03321366384625435\n",
      "Epoch 14 Batch: 102 Train Loss: 0.05340767651796341\n",
      "Epoch 14 Batch: 104 Train Loss: 0.08148390799760818\n",
      "Epoch 14 Batch: 106 Train Loss: 0.27815115451812744\n",
      "Epoch 14 Batch: 108 Train Loss: 0.2930663526058197\n",
      "Epoch 14 Batch: 110 Train Loss: 0.25507673621177673\n",
      "Epoch 14 Batch: 112 Train Loss: 0.0684889703989029\n",
      "Epoch 14 Batch: 114 Train Loss: 0.2639123499393463\n",
      "Epoch 14 Batch: 116 Train Loss: 0.05881791189312935\n",
      "Epoch 14 Batch: 118 Train Loss: 0.057744406163692474\n",
      "Epoch 14 Batch: 120 Train Loss: 0.09077869355678558\n",
      "Epoch 14 Batch: 122 Train Loss: 0.5110522508621216\n",
      "Epoch 14 Batch: 124 Train Loss: 0.26407551765441895\n",
      "Epoch 14 Batch: 126 Train Loss: 0.2823636531829834\n",
      "Epoch 14 Batch: 128 Train Loss: 0.08295083045959473\n",
      "Epoch 14 Batch: 130 Train Loss: 0.06798326969146729\n",
      "Epoch 14 Batch: 132 Train Loss: 0.07414504140615463\n",
      "Epoch 14 Batch: 134 Train Loss: 0.07196348905563354\n",
      "Epoch 14 Batch: 136 Train Loss: 0.03510190546512604\n",
      "Epoch 14 Batch: 138 Train Loss: 0.07457780092954636\n",
      "Epoch 14 Batch: 140 Train Loss: 0.5125161409378052\n",
      "Epoch 14 Batch: 142 Train Loss: 0.03668221831321716\n",
      "Epoch 14 Batch: 144 Train Loss: 0.03762379288673401\n",
      "Epoch 14 Batch: 146 Train Loss: 0.2786024212837219\n",
      "Epoch 14 Batch: 148 Train Loss: 0.27080920338630676\n",
      "Epoch 14 Batch: 150 Train Loss: 0.06339289247989655\n",
      "Epoch 14 Batch: 152 Train Loss: 0.0573231466114521\n",
      "Epoch 14 Batch: 154 Train Loss: 0.318411260843277\n",
      "Epoch 14 Batch: 156 Train Loss: 0.2635168433189392\n",
      "Epoch 14 Batch: 158 Train Loss: 0.5373894572257996\n",
      "Epoch 14 Batch: 160 Train Loss: 0.029983067885041237\n",
      "Epoch 15 Batch: 2 Train Loss: 0.04171159118413925\n",
      "Epoch 15 Batch: 4 Train Loss: 0.27952587604522705\n",
      "Epoch 15 Batch: 6 Train Loss: 0.3137316405773163\n",
      "Epoch 15 Batch: 8 Train Loss: 0.018749359995126724\n",
      "Epoch 15 Batch: 10 Train Loss: 0.08424617350101471\n",
      "Epoch 15 Batch: 12 Train Loss: 0.5096895694732666\n",
      "Epoch 15 Batch: 14 Train Loss: 0.28315064311027527\n",
      "Epoch 15 Batch: 16 Train Loss: 0.0661339983344078\n",
      "Epoch 15 Batch: 18 Train Loss: 0.30126285552978516\n",
      "Epoch 15 Batch: 20 Train Loss: 0.27702826261520386\n",
      "Epoch 15 Batch: 22 Train Loss: 0.1047554612159729\n",
      "Epoch 15 Batch: 24 Train Loss: 0.5340738296508789\n",
      "Epoch 15 Batch: 26 Train Loss: 0.22763200104236603\n",
      "Epoch 15 Batch: 28 Train Loss: 0.4749773144721985\n",
      "Epoch 15 Batch: 30 Train Loss: 0.07081793248653412\n",
      "Epoch 15 Batch: 32 Train Loss: 0.476229727268219\n",
      "Epoch 15 Batch: 34 Train Loss: 0.03752666339278221\n",
      "Epoch 15 Batch: 36 Train Loss: 0.034715134650468826\n",
      "Epoch 15 Batch: 38 Train Loss: 0.06623642146587372\n",
      "Epoch 15 Batch: 40 Train Loss: 0.025322938337922096\n",
      "Epoch 15 Batch: 42 Train Loss: 0.07660950720310211\n",
      "Epoch 15 Batch: 44 Train Loss: 0.29972830414772034\n",
      "Epoch 15 Batch: 46 Train Loss: 0.06571494787931442\n",
      "Epoch 15 Batch: 48 Train Loss: 0.29173994064331055\n",
      "Epoch 15 Batch: 50 Train Loss: 0.4866161346435547\n",
      "Epoch 15 Batch: 52 Train Loss: 0.058876730501651764\n",
      "Epoch 15 Batch: 54 Train Loss: 0.3020252585411072\n",
      "Epoch 15 Batch: 56 Train Loss: 0.4998915195465088\n",
      "Epoch 15 Batch: 58 Train Loss: 0.04609479755163193\n",
      "Epoch 15 Batch: 60 Train Loss: 0.05769450590014458\n",
      "Epoch 15 Batch: 62 Train Loss: 0.07996901124715805\n",
      "Epoch 15 Batch: 64 Train Loss: 0.07774543762207031\n",
      "Epoch 15 Batch: 66 Train Loss: 0.0699215680360794\n",
      "Epoch 15 Batch: 68 Train Loss: 0.09355825930833817\n",
      "Epoch 15 Batch: 70 Train Loss: 0.27933236956596375\n",
      "Epoch 15 Batch: 72 Train Loss: 0.03558538109064102\n",
      "Epoch 15 Batch: 74 Train Loss: 0.03790614753961563\n",
      "Epoch 15 Batch: 76 Train Loss: 0.31133347749710083\n",
      "Epoch 15 Batch: 78 Train Loss: 0.015019109472632408\n",
      "Epoch 15 Batch: 80 Train Loss: 0.03546697273850441\n",
      "Epoch 15 Batch: 82 Train Loss: 0.040441226214170456\n",
      "Epoch 15 Batch: 84 Train Loss: 0.027173051610589027\n",
      "Epoch 15 Batch: 86 Train Loss: 0.04759181663393974\n",
      "Epoch 15 Batch: 88 Train Loss: 0.017652790993452072\n",
      "Epoch 15 Batch: 90 Train Loss: 0.05313507467508316\n",
      "Epoch 15 Batch: 92 Train Loss: 0.5865604281425476\n",
      "Epoch 15 Batch: 94 Train Loss: 0.31628087162971497\n",
      "Epoch 15 Batch: 96 Train Loss: 0.03843099996447563\n",
      "Epoch 15 Batch: 98 Train Loss: 0.04331899434328079\n",
      "Epoch 15 Batch: 100 Train Loss: 0.30506157875061035\n",
      "Epoch 15 Batch: 102 Train Loss: 0.02976982854306698\n",
      "Epoch 15 Batch: 104 Train Loss: 0.047866396605968475\n",
      "Epoch 15 Batch: 106 Train Loss: 0.5056294798851013\n",
      "Epoch 15 Batch: 108 Train Loss: 0.018578216433525085\n",
      "Epoch 15 Batch: 110 Train Loss: 0.2629096806049347\n",
      "Epoch 15 Batch: 112 Train Loss: 0.06929253041744232\n",
      "Epoch 15 Batch: 114 Train Loss: 0.2607921361923218\n",
      "Epoch 15 Batch: 116 Train Loss: 0.08307043462991714\n",
      "Epoch 15 Batch: 118 Train Loss: 0.9136039018630981\n",
      "Epoch 15 Batch: 120 Train Loss: 0.03736976534128189\n",
      "Epoch 15 Batch: 122 Train Loss: 0.04772115498781204\n",
      "Epoch 15 Batch: 124 Train Loss: 0.40241241455078125\n",
      "Epoch 15 Batch: 126 Train Loss: 0.9300373196601868\n",
      "Epoch 15 Batch: 128 Train Loss: 0.06286311149597168\n",
      "Epoch 15 Batch: 130 Train Loss: 0.09814566373825073\n",
      "Epoch 15 Batch: 132 Train Loss: 1.4193475246429443\n",
      "Epoch 15 Batch: 134 Train Loss: 0.06074487417936325\n",
      "Epoch 15 Batch: 136 Train Loss: 0.05755500867962837\n",
      "Epoch 15 Batch: 138 Train Loss: 0.11696940660476685\n",
      "Epoch 15 Batch: 140 Train Loss: 0.34822994470596313\n",
      "Epoch 15 Batch: 142 Train Loss: 0.889500617980957\n",
      "Epoch 15 Batch: 144 Train Loss: 0.24690845608711243\n",
      "Epoch 15 Batch: 146 Train Loss: 0.2735051214694977\n",
      "Epoch 15 Batch: 148 Train Loss: 0.4355982840061188\n",
      "Epoch 15 Batch: 150 Train Loss: 0.36624544858932495\n",
      "Epoch 15 Batch: 152 Train Loss: 0.295250803232193\n",
      "Epoch 15 Batch: 154 Train Loss: 0.09094832092523575\n",
      "Epoch 15 Batch: 156 Train Loss: 0.5046675801277161\n",
      "Epoch 15 Batch: 158 Train Loss: 0.07937031984329224\n",
      "Epoch 15 Batch: 160 Train Loss: 0.11892566829919815\n",
      "Epoch 16 Batch: 2 Train Loss: 0.1173592358827591\n",
      "Epoch 16 Batch: 4 Train Loss: 0.3233981132507324\n",
      "Epoch 16 Batch: 6 Train Loss: 0.23335325717926025\n",
      "Epoch 16 Batch: 8 Train Loss: 0.38035911321640015\n",
      "Epoch 16 Batch: 10 Train Loss: 0.15082861483097076\n",
      "Epoch 16 Batch: 12 Train Loss: 0.0849398821592331\n",
      "Epoch 16 Batch: 14 Train Loss: 0.24589760601520538\n",
      "Epoch 16 Batch: 16 Train Loss: 0.10848020017147064\n",
      "Epoch 16 Batch: 18 Train Loss: 0.13702510297298431\n",
      "Epoch 16 Batch: 20 Train Loss: 0.0647420734167099\n",
      "Epoch 16 Batch: 22 Train Loss: 0.2517976760864258\n",
      "Epoch 16 Batch: 24 Train Loss: 0.27210918068885803\n",
      "Epoch 16 Batch: 26 Train Loss: 0.06533866375684738\n",
      "Epoch 16 Batch: 28 Train Loss: 0.6920841932296753\n",
      "Epoch 16 Batch: 30 Train Loss: 0.019884280860424042\n",
      "Epoch 16 Batch: 32 Train Loss: 0.04837093502283096\n",
      "Epoch 16 Batch: 34 Train Loss: 0.10386739671230316\n",
      "Epoch 16 Batch: 36 Train Loss: 0.2941994071006775\n",
      "Epoch 16 Batch: 38 Train Loss: 0.2881218492984772\n",
      "Epoch 16 Batch: 40 Train Loss: 0.08554203063249588\n",
      "Epoch 16 Batch: 42 Train Loss: 0.29576557874679565\n",
      "Epoch 16 Batch: 44 Train Loss: 0.06513785570859909\n",
      "Epoch 16 Batch: 46 Train Loss: 0.30110418796539307\n",
      "Epoch 16 Batch: 48 Train Loss: 0.7111189365386963\n",
      "Epoch 16 Batch: 50 Train Loss: 0.2968296408653259\n",
      "Epoch 16 Batch: 52 Train Loss: 0.049100104719400406\n",
      "Epoch 16 Batch: 54 Train Loss: 0.46004924178123474\n",
      "Epoch 16 Batch: 56 Train Loss: 0.046639349311590195\n",
      "Epoch 16 Batch: 58 Train Loss: 0.0736456960439682\n",
      "Epoch 16 Batch: 60 Train Loss: 0.27204430103302\n",
      "Epoch 16 Batch: 62 Train Loss: 0.26897186040878296\n",
      "Epoch 16 Batch: 64 Train Loss: 0.22946444153785706\n",
      "Epoch 16 Batch: 66 Train Loss: 0.2670818269252777\n",
      "Epoch 16 Batch: 68 Train Loss: 0.09444832801818848\n",
      "Epoch 16 Batch: 70 Train Loss: 0.074713334441185\n",
      "Epoch 16 Batch: 72 Train Loss: 0.043735306710004807\n",
      "Epoch 16 Batch: 74 Train Loss: 0.047049280256032944\n",
      "Epoch 16 Batch: 76 Train Loss: 0.10777046531438828\n",
      "Epoch 16 Batch: 78 Train Loss: 0.05828091502189636\n",
      "Epoch 16 Batch: 80 Train Loss: 0.08375626057386398\n",
      "Epoch 16 Batch: 82 Train Loss: 0.05329925939440727\n",
      "Epoch 16 Batch: 84 Train Loss: 0.07479334622621536\n",
      "Epoch 16 Batch: 86 Train Loss: 0.051330529153347015\n",
      "Epoch 16 Batch: 88 Train Loss: 0.059311456978321075\n",
      "Epoch 16 Batch: 90 Train Loss: 0.029279092326760292\n",
      "Epoch 16 Batch: 92 Train Loss: 0.28249773383140564\n",
      "Epoch 16 Batch: 94 Train Loss: 0.2750989496707916\n",
      "Epoch 16 Batch: 96 Train Loss: 0.32931646704673767\n",
      "Epoch 16 Batch: 98 Train Loss: 0.0411967858672142\n",
      "Epoch 16 Batch: 100 Train Loss: 0.04839916154742241\n",
      "Epoch 16 Batch: 102 Train Loss: 0.5487643480300903\n",
      "Epoch 16 Batch: 104 Train Loss: 0.07343209534883499\n",
      "Epoch 16 Batch: 106 Train Loss: 0.07514763623476028\n",
      "Epoch 16 Batch: 108 Train Loss: 0.2573713958263397\n",
      "Epoch 16 Batch: 110 Train Loss: 0.27185097336769104\n",
      "Epoch 16 Batch: 112 Train Loss: 0.07806229591369629\n",
      "Epoch 16 Batch: 114 Train Loss: 0.05894361063838005\n",
      "Epoch 16 Batch: 116 Train Loss: 0.056511204689741135\n",
      "Epoch 16 Batch: 118 Train Loss: 0.04472290724515915\n",
      "Epoch 16 Batch: 120 Train Loss: 0.06499527394771576\n",
      "Epoch 16 Batch: 122 Train Loss: 0.30166101455688477\n",
      "Epoch 16 Batch: 124 Train Loss: 0.05918821692466736\n",
      "Epoch 16 Batch: 126 Train Loss: 0.01833016239106655\n",
      "Epoch 16 Batch: 128 Train Loss: 0.04343453794717789\n",
      "Epoch 16 Batch: 130 Train Loss: 0.49806341528892517\n",
      "Epoch 16 Batch: 132 Train Loss: 0.3321729600429535\n",
      "Epoch 16 Batch: 134 Train Loss: 0.035007864236831665\n",
      "Epoch 16 Batch: 136 Train Loss: 0.5916043519973755\n",
      "Epoch 16 Batch: 138 Train Loss: 0.058362942188978195\n",
      "Epoch 16 Batch: 140 Train Loss: 0.050563763827085495\n",
      "Epoch 16 Batch: 142 Train Loss: 0.05181489512324333\n",
      "Epoch 16 Batch: 144 Train Loss: 0.054768841713666916\n",
      "Epoch 16 Batch: 146 Train Loss: 0.2575211822986603\n",
      "Epoch 16 Batch: 148 Train Loss: 0.047976575791835785\n",
      "Epoch 16 Batch: 150 Train Loss: 0.0454925037920475\n",
      "Epoch 16 Batch: 152 Train Loss: 0.3257293701171875\n",
      "Epoch 16 Batch: 154 Train Loss: 0.05435340479016304\n",
      "Epoch 16 Batch: 156 Train Loss: 0.2822704315185547\n",
      "Epoch 16 Batch: 158 Train Loss: 0.05923601984977722\n",
      "Epoch 16 Batch: 160 Train Loss: 0.022532034665346146\n",
      "Epoch 17 Batch: 2 Train Loss: 0.2411992847919464\n",
      "Epoch 17 Batch: 4 Train Loss: 0.09334701299667358\n",
      "Epoch 17 Batch: 6 Train Loss: 0.059035081416368484\n",
      "Epoch 17 Batch: 8 Train Loss: 0.02514633908867836\n",
      "Epoch 17 Batch: 10 Train Loss: 0.06472726166248322\n",
      "Epoch 17 Batch: 12 Train Loss: 0.27650201320648193\n",
      "Epoch 17 Batch: 14 Train Loss: 0.35107195377349854\n",
      "Epoch 17 Batch: 16 Train Loss: 0.04501449316740036\n",
      "Epoch 17 Batch: 18 Train Loss: 0.06411616504192352\n",
      "Epoch 17 Batch: 20 Train Loss: 0.04744633287191391\n",
      "Epoch 17 Batch: 22 Train Loss: 0.056847263127565384\n",
      "Epoch 17 Batch: 24 Train Loss: 0.04420704394578934\n",
      "Epoch 17 Batch: 26 Train Loss: 0.36075615882873535\n",
      "Epoch 17 Batch: 28 Train Loss: 0.04115624353289604\n",
      "Epoch 17 Batch: 30 Train Loss: 0.3005897104740143\n",
      "Epoch 17 Batch: 32 Train Loss: 0.06068659946322441\n",
      "Epoch 17 Batch: 34 Train Loss: 0.03285350650548935\n",
      "Epoch 17 Batch: 36 Train Loss: 0.023101888597011566\n",
      "Epoch 17 Batch: 38 Train Loss: 0.03719906881451607\n",
      "Epoch 17 Batch: 40 Train Loss: 0.04426399618387222\n",
      "Epoch 17 Batch: 42 Train Loss: 0.5251680612564087\n",
      "Epoch 17 Batch: 44 Train Loss: 0.29848018288612366\n",
      "Epoch 17 Batch: 46 Train Loss: 0.0615655854344368\n",
      "Epoch 17 Batch: 48 Train Loss: 0.4551888108253479\n",
      "Epoch 17 Batch: 50 Train Loss: 0.27326464653015137\n",
      "Epoch 17 Batch: 52 Train Loss: 0.27003082633018494\n",
      "Epoch 17 Batch: 54 Train Loss: 0.4312151074409485\n",
      "Epoch 17 Batch: 56 Train Loss: 0.060225725173950195\n",
      "Epoch 17 Batch: 58 Train Loss: 0.07602915912866592\n",
      "Epoch 17 Batch: 60 Train Loss: 0.12514707446098328\n",
      "Epoch 17 Batch: 62 Train Loss: 0.07268506288528442\n",
      "Epoch 17 Batch: 64 Train Loss: 0.056136250495910645\n",
      "Epoch 17 Batch: 66 Train Loss: 0.0729144960641861\n",
      "Epoch 17 Batch: 68 Train Loss: 0.07231922447681427\n",
      "Epoch 17 Batch: 70 Train Loss: 0.06565173715353012\n",
      "Epoch 17 Batch: 72 Train Loss: 0.03278961777687073\n",
      "Epoch 17 Batch: 74 Train Loss: 0.0526195652782917\n",
      "Epoch 17 Batch: 76 Train Loss: 0.3664409816265106\n",
      "Epoch 17 Batch: 78 Train Loss: 0.32880640029907227\n",
      "Epoch 17 Batch: 80 Train Loss: 0.2573995888233185\n",
      "Epoch 17 Batch: 82 Train Loss: 0.06240827962756157\n",
      "Epoch 17 Batch: 84 Train Loss: 0.3247271776199341\n",
      "Epoch 17 Batch: 86 Train Loss: 0.3146485686302185\n",
      "Epoch 17 Batch: 88 Train Loss: 0.11388754844665527\n",
      "Epoch 17 Batch: 90 Train Loss: 0.05986058712005615\n",
      "Epoch 17 Batch: 92 Train Loss: 0.09097184985876083\n",
      "Epoch 17 Batch: 94 Train Loss: 0.06492353975772858\n",
      "Epoch 17 Batch: 96 Train Loss: 0.2675274908542633\n",
      "Epoch 17 Batch: 98 Train Loss: 0.029857376590371132\n",
      "Epoch 17 Batch: 100 Train Loss: 0.027140533551573753\n",
      "Epoch 17 Batch: 102 Train Loss: 0.2687607407569885\n",
      "Epoch 17 Batch: 104 Train Loss: 0.2907421886920929\n",
      "Epoch 17 Batch: 106 Train Loss: 0.023785851895809174\n",
      "Epoch 17 Batch: 108 Train Loss: 0.2905593514442444\n",
      "Epoch 17 Batch: 110 Train Loss: 0.293253630399704\n",
      "Epoch 17 Batch: 112 Train Loss: 0.02949417755007744\n",
      "Epoch 17 Batch: 114 Train Loss: 0.29130858182907104\n",
      "Epoch 17 Batch: 116 Train Loss: 0.5077440142631531\n",
      "Epoch 17 Batch: 118 Train Loss: 0.06485003978013992\n",
      "Epoch 17 Batch: 120 Train Loss: 0.10388880968093872\n",
      "Epoch 17 Batch: 122 Train Loss: 0.25998052954673767\n",
      "Epoch 17 Batch: 124 Train Loss: 0.08551078289747238\n",
      "Epoch 17 Batch: 126 Train Loss: 0.07005716860294342\n",
      "Epoch 17 Batch: 128 Train Loss: 0.043001092970371246\n",
      "Epoch 17 Batch: 130 Train Loss: 0.06814087927341461\n",
      "Epoch 17 Batch: 132 Train Loss: 0.06246882677078247\n",
      "Epoch 17 Batch: 134 Train Loss: 0.3241737484931946\n",
      "Epoch 17 Batch: 136 Train Loss: 0.05638517066836357\n",
      "Epoch 17 Batch: 138 Train Loss: 0.53874671459198\n",
      "Epoch 17 Batch: 140 Train Loss: 0.04992329701781273\n",
      "Epoch 17 Batch: 142 Train Loss: 0.037145934998989105\n",
      "Epoch 17 Batch: 144 Train Loss: 0.5345249176025391\n",
      "Epoch 17 Batch: 146 Train Loss: 0.29670459032058716\n",
      "Epoch 17 Batch: 148 Train Loss: 0.042316097766160965\n",
      "Epoch 17 Batch: 150 Train Loss: 0.061411142349243164\n",
      "Epoch 17 Batch: 152 Train Loss: 0.28650951385498047\n",
      "Epoch 17 Batch: 154 Train Loss: 0.04601521044969559\n",
      "Epoch 17 Batch: 156 Train Loss: 0.0797366201877594\n",
      "Epoch 17 Batch: 158 Train Loss: 0.2743687331676483\n",
      "Epoch 17 Batch: 160 Train Loss: 0.06879745423793793\n",
      "Epoch 18 Batch: 2 Train Loss: 0.31481191515922546\n",
      "Epoch 18 Batch: 4 Train Loss: 0.06446268409490585\n",
      "Epoch 18 Batch: 6 Train Loss: 0.3452363610267639\n",
      "Epoch 18 Batch: 8 Train Loss: 0.07053519785404205\n",
      "Epoch 18 Batch: 10 Train Loss: 0.04803464189171791\n",
      "Epoch 18 Batch: 12 Train Loss: 0.48688140511512756\n",
      "Epoch 18 Batch: 14 Train Loss: 0.07322665303945541\n",
      "Epoch 18 Batch: 16 Train Loss: 0.2667198181152344\n",
      "Epoch 18 Batch: 18 Train Loss: 0.2617577612400055\n",
      "Epoch 18 Batch: 20 Train Loss: 0.2670719027519226\n",
      "Epoch 18 Batch: 22 Train Loss: 0.2604162096977234\n",
      "Epoch 18 Batch: 24 Train Loss: 0.04850936681032181\n",
      "Epoch 18 Batch: 26 Train Loss: 0.26872849464416504\n",
      "Epoch 18 Batch: 28 Train Loss: 0.47009390592575073\n",
      "Epoch 18 Batch: 30 Train Loss: 0.05288885906338692\n",
      "Epoch 18 Batch: 32 Train Loss: 0.05749822407960892\n",
      "Epoch 18 Batch: 34 Train Loss: 0.09208657592535019\n",
      "Epoch 18 Batch: 36 Train Loss: 0.31687578558921814\n",
      "Epoch 18 Batch: 38 Train Loss: 0.047205206006765366\n",
      "Epoch 18 Batch: 40 Train Loss: 0.30514997243881226\n",
      "Epoch 18 Batch: 42 Train Loss: 0.29987186193466187\n",
      "Epoch 18 Batch: 44 Train Loss: 0.27134841680526733\n",
      "Epoch 18 Batch: 46 Train Loss: 0.23562180995941162\n",
      "Epoch 18 Batch: 48 Train Loss: 0.06804483383893967\n",
      "Epoch 18 Batch: 50 Train Loss: 0.2908613085746765\n",
      "Epoch 18 Batch: 52 Train Loss: 0.036331143230199814\n",
      "Epoch 18 Batch: 54 Train Loss: 0.032158154994249344\n",
      "Epoch 18 Batch: 56 Train Loss: 0.07026968896389008\n",
      "Epoch 18 Batch: 58 Train Loss: 0.04701986536383629\n",
      "Epoch 18 Batch: 60 Train Loss: 0.02970249392092228\n",
      "Epoch 18 Batch: 62 Train Loss: 0.06876654922962189\n",
      "Epoch 18 Batch: 64 Train Loss: 0.04048776626586914\n",
      "Epoch 18 Batch: 66 Train Loss: 0.05872129276394844\n",
      "Epoch 18 Batch: 68 Train Loss: 0.036494798958301544\n",
      "Epoch 18 Batch: 70 Train Loss: 0.029563093557953835\n",
      "Epoch 18 Batch: 72 Train Loss: 0.025497138500213623\n",
      "Epoch 18 Batch: 74 Train Loss: 0.04215557128190994\n",
      "Epoch 18 Batch: 76 Train Loss: 0.02934150956571102\n",
      "Epoch 18 Batch: 78 Train Loss: 0.2948703169822693\n",
      "Epoch 18 Batch: 80 Train Loss: 0.6593414545059204\n",
      "Epoch 18 Batch: 82 Train Loss: 0.02591516077518463\n",
      "Epoch 18 Batch: 84 Train Loss: 0.04752213507890701\n",
      "Epoch 18 Batch: 86 Train Loss: 0.3140411376953125\n",
      "Epoch 18 Batch: 88 Train Loss: 0.31569287180900574\n",
      "Epoch 18 Batch: 90 Train Loss: 0.33257803320884705\n",
      "Epoch 18 Batch: 92 Train Loss: 0.08786245435476303\n",
      "Epoch 18 Batch: 94 Train Loss: 0.053614504635334015\n",
      "Epoch 18 Batch: 96 Train Loss: 0.04924609884619713\n",
      "Epoch 18 Batch: 98 Train Loss: 0.051388055086135864\n",
      "Epoch 18 Batch: 100 Train Loss: 0.05978197604417801\n",
      "Epoch 18 Batch: 102 Train Loss: 0.4597699046134949\n",
      "Epoch 18 Batch: 104 Train Loss: 0.2313099354505539\n",
      "Epoch 18 Batch: 106 Train Loss: 0.1873917132616043\n",
      "Epoch 18 Batch: 108 Train Loss: 0.5015918016433716\n",
      "Epoch 18 Batch: 110 Train Loss: 0.05977846309542656\n",
      "Epoch 18 Batch: 112 Train Loss: 0.038447581231594086\n",
      "Epoch 18 Batch: 114 Train Loss: 0.3891488015651703\n",
      "Epoch 18 Batch: 116 Train Loss: 0.6373778581619263\n",
      "Epoch 18 Batch: 118 Train Loss: 0.03333244100213051\n",
      "Epoch 18 Batch: 120 Train Loss: 0.3635151982307434\n",
      "Epoch 18 Batch: 122 Train Loss: 0.04843791574239731\n",
      "Epoch 18 Batch: 124 Train Loss: 0.04967775195837021\n",
      "Epoch 18 Batch: 126 Train Loss: 0.06931985914707184\n",
      "Epoch 18 Batch: 128 Train Loss: 0.4367242455482483\n",
      "Epoch 18 Batch: 130 Train Loss: 0.04027819633483887\n",
      "Epoch 18 Batch: 132 Train Loss: 0.13322314620018005\n",
      "Epoch 18 Batch: 134 Train Loss: 0.04822232946753502\n",
      "Epoch 18 Batch: 136 Train Loss: 0.5013634562492371\n",
      "Epoch 18 Batch: 138 Train Loss: 0.05597921460866928\n",
      "Epoch 18 Batch: 140 Train Loss: 0.07185684889554977\n",
      "Epoch 18 Batch: 142 Train Loss: 0.05885805934667587\n",
      "Epoch 18 Batch: 144 Train Loss: 0.05039767175912857\n",
      "Epoch 18 Batch: 146 Train Loss: 0.04254637658596039\n",
      "Epoch 18 Batch: 148 Train Loss: 0.2902321517467499\n",
      "Epoch 18 Batch: 150 Train Loss: 0.5446940660476685\n",
      "Epoch 18 Batch: 152 Train Loss: 0.027997534722089767\n",
      "Epoch 18 Batch: 154 Train Loss: 0.06689678877592087\n",
      "Epoch 18 Batch: 156 Train Loss: 0.3148183226585388\n",
      "Epoch 18 Batch: 158 Train Loss: 0.04762473329901695\n",
      "Epoch 18 Batch: 160 Train Loss: 0.2770403027534485\n",
      "Epoch 19 Batch: 2 Train Loss: 0.24371595680713654\n",
      "Epoch 19 Batch: 4 Train Loss: 0.10716462135314941\n",
      "Epoch 19 Batch: 6 Train Loss: 0.08070063591003418\n",
      "Epoch 19 Batch: 8 Train Loss: 0.10436725616455078\n",
      "Epoch 19 Batch: 10 Train Loss: 0.2786144018173218\n",
      "Epoch 19 Batch: 12 Train Loss: 0.25837820768356323\n",
      "Epoch 19 Batch: 14 Train Loss: 0.2853824496269226\n",
      "Epoch 19 Batch: 16 Train Loss: 0.0668940395116806\n",
      "Epoch 19 Batch: 18 Train Loss: 0.5461093783378601\n",
      "Epoch 19 Batch: 20 Train Loss: 0.05267426371574402\n",
      "Epoch 19 Batch: 22 Train Loss: 0.8860071301460266\n",
      "Epoch 19 Batch: 24 Train Loss: 0.3172556161880493\n",
      "Epoch 19 Batch: 26 Train Loss: 0.08912131935358047\n",
      "Epoch 19 Batch: 28 Train Loss: 0.01438161265105009\n",
      "Epoch 19 Batch: 30 Train Loss: 0.26797953248023987\n",
      "Epoch 19 Batch: 32 Train Loss: 0.08972994983196259\n",
      "Epoch 19 Batch: 34 Train Loss: 0.059048689901828766\n",
      "Epoch 19 Batch: 36 Train Loss: 0.45667919516563416\n",
      "Epoch 19 Batch: 38 Train Loss: 0.2988225221633911\n",
      "Epoch 19 Batch: 40 Train Loss: 0.3289188742637634\n",
      "Epoch 19 Batch: 42 Train Loss: 0.3025609254837036\n",
      "Epoch 19 Batch: 44 Train Loss: 0.05575598403811455\n",
      "Epoch 19 Batch: 46 Train Loss: 0.06080583482980728\n",
      "Epoch 19 Batch: 48 Train Loss: 0.08323061466217041\n",
      "Epoch 19 Batch: 50 Train Loss: 0.06025349348783493\n",
      "Epoch 19 Batch: 52 Train Loss: 0.016691943630576134\n",
      "Epoch 19 Batch: 54 Train Loss: 0.044281814247369766\n",
      "Epoch 19 Batch: 56 Train Loss: 0.3046664893627167\n",
      "Epoch 19 Batch: 58 Train Loss: 0.017151344567537308\n",
      "Epoch 19 Batch: 60 Train Loss: 0.33423829078674316\n",
      "Epoch 19 Batch: 62 Train Loss: 0.032198552042245865\n",
      "Epoch 19 Batch: 64 Train Loss: 0.30048543214797974\n",
      "Epoch 19 Batch: 66 Train Loss: 0.0254904143512249\n",
      "Epoch 19 Batch: 68 Train Loss: 0.031576432287693024\n",
      "Epoch 19 Batch: 70 Train Loss: 0.27182716131210327\n",
      "Epoch 19 Batch: 72 Train Loss: 0.3074449896812439\n",
      "Epoch 19 Batch: 74 Train Loss: 0.3303788900375366\n",
      "Epoch 19 Batch: 76 Train Loss: 0.31925302743911743\n",
      "Epoch 19 Batch: 78 Train Loss: 0.05353273078799248\n",
      "Epoch 19 Batch: 80 Train Loss: 0.30497393012046814\n",
      "Epoch 19 Batch: 82 Train Loss: 0.08472893387079239\n",
      "Epoch 19 Batch: 84 Train Loss: 0.06274944543838501\n",
      "Epoch 19 Batch: 86 Train Loss: 0.05432857945561409\n",
      "Epoch 19 Batch: 88 Train Loss: 0.03752300143241882\n",
      "Epoch 19 Batch: 90 Train Loss: 0.27620550990104675\n",
      "Epoch 19 Batch: 92 Train Loss: 0.28133779764175415\n",
      "Epoch 19 Batch: 94 Train Loss: 0.28572705388069153\n",
      "Epoch 19 Batch: 96 Train Loss: 0.04101094231009483\n",
      "Epoch 19 Batch: 98 Train Loss: 0.30475878715515137\n",
      "Epoch 19 Batch: 100 Train Loss: 0.07819265872240067\n",
      "Epoch 19 Batch: 102 Train Loss: 0.5095891356468201\n",
      "Epoch 19 Batch: 104 Train Loss: 0.2717013657093048\n",
      "Epoch 19 Batch: 106 Train Loss: 0.4367472529411316\n",
      "Epoch 19 Batch: 108 Train Loss: 0.09425286203622818\n",
      "Epoch 19 Batch: 110 Train Loss: 0.03700457885861397\n",
      "Epoch 19 Batch: 112 Train Loss: 0.3145939111709595\n",
      "Epoch 19 Batch: 114 Train Loss: 0.02712157741189003\n",
      "Epoch 19 Batch: 116 Train Loss: 0.06359488517045975\n",
      "Epoch 19 Batch: 118 Train Loss: 0.10459210723638535\n",
      "Epoch 19 Batch: 120 Train Loss: 0.13030298054218292\n",
      "Epoch 19 Batch: 122 Train Loss: 0.06561924517154694\n",
      "Epoch 19 Batch: 124 Train Loss: 0.05103468894958496\n",
      "Epoch 19 Batch: 126 Train Loss: 0.06633581221103668\n",
      "Epoch 19 Batch: 128 Train Loss: 0.05234203860163689\n",
      "Epoch 19 Batch: 130 Train Loss: 0.3185299038887024\n",
      "Epoch 19 Batch: 132 Train Loss: 0.05515594407916069\n",
      "Epoch 19 Batch: 134 Train Loss: 0.5309299826622009\n",
      "Epoch 19 Batch: 136 Train Loss: 0.04284518584609032\n",
      "Epoch 19 Batch: 138 Train Loss: 0.25981849431991577\n",
      "Epoch 19 Batch: 140 Train Loss: 0.048297278583049774\n",
      "Epoch 19 Batch: 142 Train Loss: 0.05210081487894058\n",
      "Epoch 19 Batch: 144 Train Loss: 0.3152710497379303\n",
      "Epoch 19 Batch: 146 Train Loss: 0.047463901340961456\n",
      "Epoch 19 Batch: 148 Train Loss: 0.022485604509711266\n",
      "Epoch 19 Batch: 150 Train Loss: 0.3130129277706146\n",
      "Epoch 19 Batch: 152 Train Loss: 0.04581458494067192\n",
      "Epoch 19 Batch: 154 Train Loss: 0.0710684061050415\n",
      "Epoch 19 Batch: 156 Train Loss: 0.3089810311794281\n",
      "Epoch 19 Batch: 158 Train Loss: 0.2953612804412842\n",
      "Epoch 19 Batch: 160 Train Loss: 0.3169165551662445\n",
      "Epoch 20 Batch: 2 Train Loss: 0.2636198103427887\n",
      "Epoch 20 Batch: 4 Train Loss: 0.04508988559246063\n",
      "Epoch 20 Batch: 6 Train Loss: 0.03403087705373764\n",
      "Epoch 20 Batch: 8 Train Loss: 0.026251235976815224\n",
      "Epoch 20 Batch: 10 Train Loss: 0.0668274313211441\n",
      "Epoch 20 Batch: 12 Train Loss: 0.31323349475860596\n",
      "Epoch 20 Batch: 14 Train Loss: 0.6101418733596802\n",
      "Epoch 20 Batch: 16 Train Loss: 0.3001318573951721\n",
      "Epoch 20 Batch: 18 Train Loss: 0.2753840982913971\n",
      "Epoch 20 Batch: 20 Train Loss: 0.04251699894666672\n",
      "Epoch 20 Batch: 22 Train Loss: 0.3115729093551636\n",
      "Epoch 20 Batch: 24 Train Loss: 0.04423213005065918\n",
      "Epoch 20 Batch: 26 Train Loss: 0.7899615168571472\n",
      "Epoch 20 Batch: 28 Train Loss: 0.04587579891085625\n",
      "Epoch 20 Batch: 30 Train Loss: 0.04644780233502388\n",
      "Epoch 20 Batch: 32 Train Loss: 0.05797651410102844\n",
      "Epoch 20 Batch: 34 Train Loss: 0.2652595043182373\n",
      "Epoch 20 Batch: 36 Train Loss: 0.3151417076587677\n",
      "Epoch 20 Batch: 38 Train Loss: 0.00893582496792078\n",
      "Epoch 20 Batch: 40 Train Loss: 0.5037509202957153\n",
      "Epoch 20 Batch: 42 Train Loss: 0.02423090487718582\n",
      "Epoch 20 Batch: 44 Train Loss: 0.037320345640182495\n",
      "Epoch 20 Batch: 46 Train Loss: 0.02809050679206848\n",
      "Epoch 20 Batch: 48 Train Loss: 0.26043370366096497\n",
      "Epoch 20 Batch: 50 Train Loss: 0.28126630187034607\n",
      "Epoch 20 Batch: 52 Train Loss: 0.07587145268917084\n",
      "Epoch 20 Batch: 54 Train Loss: 0.061174821108579636\n",
      "Epoch 20 Batch: 56 Train Loss: 0.0937490463256836\n",
      "Epoch 20 Batch: 58 Train Loss: 0.09676139056682587\n",
      "Epoch 20 Batch: 60 Train Loss: 0.11231201887130737\n",
      "Epoch 20 Batch: 62 Train Loss: 0.05874047800898552\n",
      "Epoch 20 Batch: 64 Train Loss: 0.0870700255036354\n",
      "Epoch 20 Batch: 66 Train Loss: 0.28066474199295044\n",
      "Epoch 20 Batch: 68 Train Loss: 0.08082839101552963\n",
      "Epoch 20 Batch: 70 Train Loss: 0.5133093595504761\n",
      "Epoch 20 Batch: 72 Train Loss: 0.07293541729450226\n",
      "Epoch 20 Batch: 74 Train Loss: 0.25422388315200806\n",
      "Epoch 20 Batch: 76 Train Loss: 0.04254496842622757\n",
      "Epoch 20 Batch: 78 Train Loss: 0.34170690178871155\n",
      "Epoch 20 Batch: 80 Train Loss: 0.30409178137779236\n",
      "Epoch 20 Batch: 82 Train Loss: 0.07597064226865768\n",
      "Epoch 20 Batch: 84 Train Loss: 0.3386174738407135\n",
      "Epoch 20 Batch: 86 Train Loss: 0.30584555864334106\n",
      "Epoch 20 Batch: 88 Train Loss: 0.04220154136419296\n",
      "Epoch 20 Batch: 90 Train Loss: 0.02851439081132412\n",
      "Epoch 20 Batch: 92 Train Loss: 0.2796902060508728\n",
      "Epoch 20 Batch: 94 Train Loss: 0.04823559150099754\n",
      "Epoch 20 Batch: 96 Train Loss: 0.27833789587020874\n",
      "Epoch 20 Batch: 98 Train Loss: 0.028814714401960373\n",
      "Epoch 20 Batch: 100 Train Loss: 0.049072086811065674\n",
      "Epoch 20 Batch: 102 Train Loss: 0.066804900765419\n",
      "Epoch 20 Batch: 104 Train Loss: 0.28065064549446106\n",
      "Epoch 20 Batch: 106 Train Loss: 0.05962764099240303\n",
      "Epoch 20 Batch: 108 Train Loss: 0.33682072162628174\n",
      "Epoch 20 Batch: 110 Train Loss: 0.04460567981004715\n",
      "Epoch 20 Batch: 112 Train Loss: 0.031697049736976624\n",
      "Epoch 20 Batch: 114 Train Loss: 0.025253113359212875\n",
      "Epoch 20 Batch: 116 Train Loss: 0.05063728615641594\n",
      "Epoch 20 Batch: 118 Train Loss: 0.28576213121414185\n",
      "Epoch 20 Batch: 120 Train Loss: 0.043876320123672485\n",
      "Epoch 20 Batch: 122 Train Loss: 0.04779784381389618\n",
      "Epoch 20 Batch: 124 Train Loss: 0.3154086470603943\n",
      "Epoch 20 Batch: 126 Train Loss: 0.04052867740392685\n",
      "Epoch 20 Batch: 128 Train Loss: 0.28553876280784607\n",
      "Epoch 20 Batch: 130 Train Loss: 0.31951695680618286\n",
      "Epoch 20 Batch: 132 Train Loss: 0.057272542268037796\n",
      "Epoch 20 Batch: 134 Train Loss: 0.299332857131958\n",
      "Epoch 20 Batch: 136 Train Loss: 0.04267067834734917\n",
      "Epoch 20 Batch: 138 Train Loss: 0.09084898233413696\n",
      "Epoch 20 Batch: 140 Train Loss: 0.06328693777322769\n",
      "Epoch 20 Batch: 142 Train Loss: 0.2795456349849701\n",
      "Epoch 20 Batch: 144 Train Loss: 0.7567195296287537\n",
      "Epoch 20 Batch: 146 Train Loss: 0.06375039368867874\n",
      "Epoch 20 Batch: 148 Train Loss: 0.3211018145084381\n",
      "Epoch 20 Batch: 150 Train Loss: 0.08113692700862885\n",
      "Epoch 20 Batch: 152 Train Loss: 0.05030157417058945\n",
      "Epoch 20 Batch: 154 Train Loss: 0.5282891988754272\n",
      "Epoch 20 Batch: 156 Train Loss: 0.3288591802120209\n",
      "Epoch 20 Batch: 158 Train Loss: 0.04283960163593292\n",
      "Epoch 20 Batch: 160 Train Loss: 0.3083704113960266\n",
      "Epoch 21 Batch: 2 Train Loss: 0.49774837493896484\n",
      "Epoch 21 Batch: 4 Train Loss: 0.09170947968959808\n",
      "Epoch 21 Batch: 6 Train Loss: 0.0233615692704916\n",
      "Epoch 21 Batch: 8 Train Loss: 0.06295667588710785\n",
      "Epoch 21 Batch: 10 Train Loss: 0.0803464725613594\n",
      "Epoch 21 Batch: 12 Train Loss: 0.04371724650263786\n",
      "Epoch 21 Batch: 14 Train Loss: 0.2572525143623352\n",
      "Epoch 21 Batch: 16 Train Loss: 0.04704440012574196\n",
      "Epoch 21 Batch: 18 Train Loss: 0.038819458335638046\n",
      "Epoch 21 Batch: 20 Train Loss: 0.04517626389861107\n",
      "Epoch 21 Batch: 22 Train Loss: 0.30174994468688965\n",
      "Epoch 21 Batch: 24 Train Loss: 0.28909072279930115\n",
      "Epoch 21 Batch: 26 Train Loss: 0.05536402389407158\n",
      "Epoch 21 Batch: 28 Train Loss: 0.28026750683784485\n",
      "Epoch 21 Batch: 30 Train Loss: 0.30244094133377075\n",
      "Epoch 21 Batch: 32 Train Loss: 0.032351795583963394\n",
      "Epoch 21 Batch: 34 Train Loss: 0.4724723696708679\n",
      "Epoch 21 Batch: 36 Train Loss: 0.277716726064682\n",
      "Epoch 21 Batch: 38 Train Loss: 0.05126868933439255\n",
      "Epoch 21 Batch: 40 Train Loss: 0.26695069670677185\n",
      "Epoch 21 Batch: 42 Train Loss: 0.05099024251103401\n",
      "Epoch 21 Batch: 44 Train Loss: 0.05838475748896599\n",
      "Epoch 21 Batch: 46 Train Loss: 0.08753545582294464\n",
      "Epoch 21 Batch: 48 Train Loss: 0.07765217125415802\n",
      "Epoch 21 Batch: 50 Train Loss: 0.5137767791748047\n",
      "Epoch 21 Batch: 52 Train Loss: 0.041421934962272644\n",
      "Epoch 21 Batch: 54 Train Loss: 0.2844085097312927\n",
      "Epoch 21 Batch: 56 Train Loss: 0.08477111905813217\n",
      "Epoch 21 Batch: 58 Train Loss: 0.28598082065582275\n",
      "Epoch 21 Batch: 60 Train Loss: 0.32942718267440796\n",
      "Epoch 21 Batch: 62 Train Loss: 0.3430132567882538\n",
      "Epoch 21 Batch: 64 Train Loss: 0.05310410261154175\n",
      "Epoch 21 Batch: 66 Train Loss: 0.04072616994380951\n",
      "Epoch 21 Batch: 68 Train Loss: 0.05256347730755806\n",
      "Epoch 21 Batch: 70 Train Loss: 0.2540133595466614\n",
      "Epoch 21 Batch: 72 Train Loss: 0.3123420476913452\n",
      "Epoch 21 Batch: 74 Train Loss: 0.29235950112342834\n",
      "Epoch 21 Batch: 76 Train Loss: 0.029221951961517334\n",
      "Epoch 21 Batch: 78 Train Loss: 0.037935204803943634\n",
      "Epoch 21 Batch: 80 Train Loss: 0.04953581839799881\n",
      "Epoch 21 Batch: 82 Train Loss: 0.06330174207687378\n",
      "Epoch 21 Batch: 84 Train Loss: 0.07285105437040329\n",
      "Epoch 21 Batch: 86 Train Loss: 0.054042596369981766\n",
      "Epoch 21 Batch: 88 Train Loss: 0.30501481890678406\n",
      "Epoch 21 Batch: 90 Train Loss: 0.06712783873081207\n",
      "Epoch 21 Batch: 92 Train Loss: 0.051112134009599686\n",
      "Epoch 21 Batch: 94 Train Loss: 0.30373820662498474\n",
      "Epoch 21 Batch: 96 Train Loss: 0.06622830033302307\n",
      "Epoch 21 Batch: 98 Train Loss: 0.024878520518541336\n",
      "Epoch 21 Batch: 100 Train Loss: 0.034797605127096176\n",
      "Epoch 21 Batch: 102 Train Loss: 0.045075785368680954\n",
      "Epoch 21 Batch: 104 Train Loss: 0.36133161187171936\n",
      "Epoch 21 Batch: 106 Train Loss: 0.3428187072277069\n",
      "Epoch 21 Batch: 108 Train Loss: 0.6442300081253052\n",
      "Epoch 21 Batch: 110 Train Loss: 0.049286168068647385\n",
      "Epoch 21 Batch: 112 Train Loss: 0.3282696008682251\n",
      "Epoch 21 Batch: 114 Train Loss: 0.3082337975502014\n",
      "Epoch 21 Batch: 116 Train Loss: 0.016316037625074387\n",
      "Epoch 21 Batch: 118 Train Loss: 0.32382288575172424\n",
      "Epoch 21 Batch: 120 Train Loss: 0.3205428421497345\n",
      "Epoch 21 Batch: 122 Train Loss: 0.08639886230230331\n",
      "Epoch 21 Batch: 124 Train Loss: 0.06807743012905121\n",
      "Epoch 21 Batch: 126 Train Loss: 0.24348223209381104\n",
      "Epoch 21 Batch: 128 Train Loss: 0.45986199378967285\n",
      "Epoch 21 Batch: 130 Train Loss: 0.0879136472940445\n",
      "Epoch 21 Batch: 132 Train Loss: 0.07662013918161392\n",
      "Epoch 21 Batch: 134 Train Loss: 0.25388458371162415\n",
      "Epoch 21 Batch: 136 Train Loss: 0.2726898789405823\n",
      "Epoch 21 Batch: 138 Train Loss: 0.6922492980957031\n",
      "Epoch 21 Batch: 140 Train Loss: 0.26303020119667053\n",
      "Epoch 21 Batch: 142 Train Loss: 0.26857438683509827\n",
      "Epoch 21 Batch: 144 Train Loss: 0.039318207651376724\n",
      "Epoch 21 Batch: 146 Train Loss: 0.040732938796281815\n",
      "Epoch 21 Batch: 148 Train Loss: 0.023874405771493912\n",
      "Epoch 21 Batch: 150 Train Loss: 0.04982578381896019\n",
      "Epoch 21 Batch: 152 Train Loss: 0.06121264770627022\n",
      "Epoch 21 Batch: 154 Train Loss: 0.07182950526475906\n",
      "Epoch 21 Batch: 156 Train Loss: 0.024154607206583023\n",
      "Epoch 21 Batch: 158 Train Loss: 0.06952761113643646\n",
      "Epoch 21 Batch: 160 Train Loss: 0.2847159206867218\n",
      "Epoch 22 Batch: 2 Train Loss: 0.053173817694187164\n",
      "Epoch 22 Batch: 4 Train Loss: 0.5337470769882202\n",
      "Epoch 22 Batch: 6 Train Loss: 0.05285513401031494\n",
      "Epoch 22 Batch: 8 Train Loss: 0.32274508476257324\n",
      "Epoch 22 Batch: 10 Train Loss: 0.05795248597860336\n",
      "Epoch 22 Batch: 12 Train Loss: 0.05206841975450516\n",
      "Epoch 22 Batch: 14 Train Loss: 0.04227469861507416\n",
      "Epoch 22 Batch: 16 Train Loss: 0.28872552514076233\n",
      "Epoch 22 Batch: 18 Train Loss: 1.095329761505127\n",
      "Epoch 22 Batch: 20 Train Loss: 0.04413270950317383\n",
      "Epoch 22 Batch: 22 Train Loss: 0.020705994218587875\n",
      "Epoch 22 Batch: 24 Train Loss: 0.25869524478912354\n",
      "Epoch 22 Batch: 26 Train Loss: 0.2986299693584442\n",
      "Epoch 22 Batch: 28 Train Loss: 0.2403942048549652\n",
      "Epoch 22 Batch: 30 Train Loss: 0.23311904072761536\n",
      "Epoch 22 Batch: 32 Train Loss: 0.31821173429489136\n",
      "Epoch 22 Batch: 34 Train Loss: 0.0864916518330574\n",
      "Epoch 22 Batch: 36 Train Loss: 0.5358144640922546\n",
      "Epoch 22 Batch: 38 Train Loss: 0.2711162269115448\n",
      "Epoch 22 Batch: 40 Train Loss: 0.06110178679227829\n",
      "Epoch 22 Batch: 42 Train Loss: 0.545747697353363\n",
      "Epoch 22 Batch: 44 Train Loss: 0.04986252635717392\n",
      "Epoch 22 Batch: 46 Train Loss: 0.304299533367157\n",
      "Epoch 22 Batch: 48 Train Loss: 0.07877038419246674\n",
      "Epoch 22 Batch: 50 Train Loss: 0.30474039912223816\n",
      "Epoch 22 Batch: 52 Train Loss: 0.0604553297162056\n",
      "Epoch 22 Batch: 54 Train Loss: 0.2906617522239685\n",
      "Epoch 22 Batch: 56 Train Loss: 0.06338117271661758\n",
      "Epoch 22 Batch: 58 Train Loss: 0.07158499956130981\n",
      "Epoch 22 Batch: 60 Train Loss: 0.018848255276679993\n",
      "Epoch 22 Batch: 62 Train Loss: 0.06661675870418549\n",
      "Epoch 22 Batch: 64 Train Loss: 0.05093344300985336\n",
      "Epoch 22 Batch: 66 Train Loss: 0.05663107708096504\n",
      "Epoch 22 Batch: 68 Train Loss: 0.0770019143819809\n",
      "Epoch 22 Batch: 70 Train Loss: 0.06825286149978638\n",
      "Epoch 22 Batch: 72 Train Loss: 0.32907187938690186\n",
      "Epoch 22 Batch: 74 Train Loss: 0.2749830484390259\n",
      "Epoch 22 Batch: 76 Train Loss: 0.01802138239145279\n",
      "Epoch 22 Batch: 78 Train Loss: 0.04169660061597824\n",
      "Epoch 22 Batch: 80 Train Loss: 0.022092223167419434\n",
      "Epoch 22 Batch: 82 Train Loss: 0.05526811629533768\n",
      "Epoch 22 Batch: 84 Train Loss: 0.3116435110569\n",
      "Epoch 22 Batch: 86 Train Loss: 0.037375859916210175\n",
      "Epoch 22 Batch: 88 Train Loss: 0.048827555030584335\n",
      "Epoch 22 Batch: 90 Train Loss: 0.3048756718635559\n",
      "Epoch 22 Batch: 92 Train Loss: 0.04553539305925369\n",
      "Epoch 22 Batch: 94 Train Loss: 0.04361085966229439\n",
      "Epoch 22 Batch: 96 Train Loss: 0.04582986980676651\n",
      "Epoch 22 Batch: 98 Train Loss: 0.26663559675216675\n",
      "Epoch 22 Batch: 100 Train Loss: 0.3150300085544586\n",
      "Epoch 22 Batch: 102 Train Loss: 0.3221918046474457\n",
      "Epoch 22 Batch: 104 Train Loss: 0.04621915891766548\n",
      "Epoch 22 Batch: 106 Train Loss: 0.06654266268014908\n",
      "Epoch 22 Batch: 108 Train Loss: 0.2983500063419342\n",
      "Epoch 22 Batch: 110 Train Loss: 0.06523703783750534\n",
      "Epoch 22 Batch: 112 Train Loss: 0.28332608938217163\n",
      "Epoch 22 Batch: 114 Train Loss: 0.04574238508939743\n",
      "Epoch 22 Batch: 116 Train Loss: 0.2503039240837097\n",
      "Epoch 22 Batch: 118 Train Loss: 0.2910258173942566\n",
      "Epoch 22 Batch: 120 Train Loss: 0.28722596168518066\n",
      "Epoch 22 Batch: 122 Train Loss: 0.0212151687592268\n",
      "Epoch 22 Batch: 124 Train Loss: 0.22483423352241516\n",
      "Epoch 22 Batch: 126 Train Loss: 0.07264303416013718\n",
      "Epoch 22 Batch: 128 Train Loss: 0.30841338634490967\n",
      "Epoch 22 Batch: 130 Train Loss: 0.3449054956436157\n",
      "Epoch 22 Batch: 132 Train Loss: 0.06037978455424309\n",
      "Epoch 22 Batch: 134 Train Loss: 0.09608081728219986\n",
      "Epoch 22 Batch: 136 Train Loss: 0.05144492909312248\n",
      "Epoch 22 Batch: 138 Train Loss: 0.06086440756917\n",
      "Epoch 22 Batch: 140 Train Loss: 0.06068449467420578\n",
      "Epoch 22 Batch: 142 Train Loss: 0.256896436214447\n",
      "Epoch 22 Batch: 144 Train Loss: 0.29963165521621704\n",
      "Epoch 22 Batch: 146 Train Loss: 0.04817721247673035\n",
      "Epoch 22 Batch: 148 Train Loss: 0.08252598345279694\n",
      "Epoch 22 Batch: 150 Train Loss: 0.03267469257116318\n",
      "Epoch 22 Batch: 152 Train Loss: 0.08302618563175201\n",
      "Epoch 22 Batch: 154 Train Loss: 0.0359659418463707\n",
      "Epoch 22 Batch: 156 Train Loss: 0.29185211658477783\n",
      "Epoch 22 Batch: 158 Train Loss: 0.05085856467485428\n",
      "Epoch 22 Batch: 160 Train Loss: 0.05804203823208809\n",
      "Epoch 23 Batch: 2 Train Loss: 0.04458702355623245\n",
      "Epoch 23 Batch: 4 Train Loss: 0.04421102628111839\n",
      "Epoch 23 Batch: 6 Train Loss: 0.030670780688524246\n",
      "Epoch 23 Batch: 8 Train Loss: 0.320916086435318\n",
      "Epoch 23 Batch: 10 Train Loss: 0.05833820253610611\n",
      "Epoch 23 Batch: 12 Train Loss: 0.3136870861053467\n",
      "Epoch 23 Batch: 14 Train Loss: 0.059158194810152054\n",
      "Epoch 23 Batch: 16 Train Loss: 0.07477392256259918\n",
      "Epoch 23 Batch: 18 Train Loss: 0.264090359210968\n",
      "Epoch 23 Batch: 20 Train Loss: 0.2919154465198517\n",
      "Epoch 23 Batch: 22 Train Loss: 0.06960418075323105\n",
      "Epoch 23 Batch: 24 Train Loss: 0.09689591079950333\n",
      "Epoch 23 Batch: 26 Train Loss: 0.049603380262851715\n",
      "Epoch 23 Batch: 28 Train Loss: 0.5138435959815979\n",
      "Epoch 23 Batch: 30 Train Loss: 0.06382645666599274\n",
      "Epoch 23 Batch: 32 Train Loss: 0.05189234018325806\n",
      "Epoch 23 Batch: 34 Train Loss: 0.07270096242427826\n",
      "Epoch 23 Batch: 36 Train Loss: 0.31451401114463806\n",
      "Epoch 23 Batch: 38 Train Loss: 0.03190135210752487\n",
      "Epoch 23 Batch: 40 Train Loss: 0.2897816598415375\n",
      "Epoch 23 Batch: 42 Train Loss: 0.5484241843223572\n",
      "Epoch 23 Batch: 44 Train Loss: 0.04785745218396187\n",
      "Epoch 23 Batch: 46 Train Loss: 0.045870453119277954\n",
      "Epoch 23 Batch: 48 Train Loss: 0.043846748769283295\n",
      "Epoch 23 Batch: 50 Train Loss: 0.03272044286131859\n",
      "Epoch 23 Batch: 52 Train Loss: 0.0529947392642498\n",
      "Epoch 23 Batch: 54 Train Loss: 0.5353256464004517\n",
      "Epoch 23 Batch: 56 Train Loss: 0.07074912637472153\n",
      "Epoch 23 Batch: 58 Train Loss: 0.2466713935136795\n",
      "Epoch 23 Batch: 60 Train Loss: 0.05155924707651138\n",
      "Epoch 23 Batch: 62 Train Loss: 0.2704247236251831\n",
      "Epoch 23 Batch: 64 Train Loss: 0.4593077301979065\n",
      "Epoch 23 Batch: 66 Train Loss: 0.48647433519363403\n",
      "Epoch 23 Batch: 68 Train Loss: 0.09872208535671234\n",
      "Epoch 23 Batch: 70 Train Loss: 0.47527942061424255\n",
      "Epoch 23 Batch: 72 Train Loss: 0.055853236466646194\n",
      "Epoch 23 Batch: 74 Train Loss: 0.26451364159584045\n",
      "Epoch 23 Batch: 76 Train Loss: 0.13202576339244843\n",
      "Epoch 23 Batch: 78 Train Loss: 0.4657478332519531\n",
      "Epoch 23 Batch: 80 Train Loss: 0.06066694110631943\n",
      "Epoch 23 Batch: 82 Train Loss: 0.24719610810279846\n",
      "Epoch 23 Batch: 84 Train Loss: 0.08983924239873886\n",
      "Epoch 23 Batch: 86 Train Loss: 0.09067820757627487\n",
      "Epoch 23 Batch: 88 Train Loss: 0.05195871740579605\n",
      "Epoch 23 Batch: 90 Train Loss: 0.0317503958940506\n",
      "Epoch 23 Batch: 92 Train Loss: 0.020438892766833305\n",
      "Epoch 23 Batch: 94 Train Loss: 0.03767386078834534\n",
      "Epoch 23 Batch: 96 Train Loss: 0.0309135764837265\n",
      "Epoch 23 Batch: 98 Train Loss: 0.07228760421276093\n",
      "Epoch 23 Batch: 100 Train Loss: 0.5991272926330566\n",
      "Epoch 23 Batch: 102 Train Loss: 0.3041408956050873\n",
      "Epoch 23 Batch: 104 Train Loss: 0.046291571110486984\n",
      "Epoch 23 Batch: 106 Train Loss: 0.04200497269630432\n",
      "Epoch 23 Batch: 108 Train Loss: 0.31700000166893005\n",
      "Epoch 23 Batch: 110 Train Loss: 0.8208036422729492\n",
      "Epoch 23 Batch: 112 Train Loss: 0.2699863314628601\n",
      "Epoch 23 Batch: 114 Train Loss: 0.30628934502601624\n",
      "Epoch 23 Batch: 116 Train Loss: 0.02908838354051113\n",
      "Epoch 23 Batch: 118 Train Loss: 0.04670371115207672\n",
      "Epoch 23 Batch: 120 Train Loss: 0.2936793267726898\n",
      "Epoch 23 Batch: 122 Train Loss: 0.06802164763212204\n",
      "Epoch 23 Batch: 124 Train Loss: 0.3008301854133606\n",
      "Epoch 23 Batch: 126 Train Loss: 0.0764203816652298\n",
      "Epoch 23 Batch: 128 Train Loss: 0.04360666126012802\n",
      "Epoch 23 Batch: 130 Train Loss: 0.27993884682655334\n",
      "Epoch 23 Batch: 132 Train Loss: 0.04033755511045456\n",
      "Epoch 23 Batch: 134 Train Loss: 0.035281918942928314\n",
      "Epoch 23 Batch: 136 Train Loss: 0.07990193367004395\n",
      "Epoch 23 Batch: 138 Train Loss: 0.06378567218780518\n",
      "Epoch 23 Batch: 140 Train Loss: 0.32714658975601196\n",
      "Epoch 23 Batch: 142 Train Loss: 0.8508976101875305\n",
      "Epoch 23 Batch: 144 Train Loss: 0.020024722442030907\n",
      "Epoch 23 Batch: 146 Train Loss: 0.0382910892367363\n",
      "Epoch 23 Batch: 148 Train Loss: 0.03132807835936546\n",
      "Epoch 23 Batch: 150 Train Loss: 0.2777957022190094\n",
      "Epoch 23 Batch: 152 Train Loss: 0.0528445839881897\n",
      "Epoch 23 Batch: 154 Train Loss: 0.32978585362434387\n",
      "Epoch 23 Batch: 156 Train Loss: 0.05455449968576431\n",
      "Epoch 23 Batch: 158 Train Loss: 0.28373807668685913\n",
      "Epoch 23 Batch: 160 Train Loss: 0.042707014828920364\n",
      "Epoch 24 Batch: 2 Train Loss: 0.2732292115688324\n",
      "Epoch 24 Batch: 4 Train Loss: 0.04185698181390762\n",
      "Epoch 24 Batch: 6 Train Loss: 0.041710902005434036\n",
      "Epoch 24 Batch: 8 Train Loss: 0.049163103103637695\n",
      "Epoch 24 Batch: 10 Train Loss: 0.031676873564720154\n",
      "Epoch 24 Batch: 12 Train Loss: 0.033439431339502335\n",
      "Epoch 24 Batch: 14 Train Loss: 0.02770397625863552\n",
      "Epoch 24 Batch: 16 Train Loss: 0.04104715958237648\n",
      "Epoch 24 Batch: 18 Train Loss: 0.29221969842910767\n",
      "Epoch 24 Batch: 20 Train Loss: 0.2735845446586609\n",
      "Epoch 24 Batch: 22 Train Loss: 0.04365687817335129\n",
      "Epoch 24 Batch: 24 Train Loss: 0.041420094668865204\n",
      "Epoch 24 Batch: 26 Train Loss: 0.05369555950164795\n",
      "Epoch 24 Batch: 28 Train Loss: 0.06496457755565643\n",
      "Epoch 24 Batch: 30 Train Loss: 0.06520538032054901\n",
      "Epoch 24 Batch: 32 Train Loss: 0.04676683992147446\n",
      "Epoch 24 Batch: 34 Train Loss: 0.03773156553506851\n",
      "Epoch 24 Batch: 36 Train Loss: 0.5966450572013855\n",
      "Epoch 24 Batch: 38 Train Loss: 0.06648688018321991\n",
      "Epoch 24 Batch: 40 Train Loss: 0.04048699513077736\n",
      "Epoch 24 Batch: 42 Train Loss: 0.29998117685317993\n",
      "Epoch 24 Batch: 44 Train Loss: 0.31754347681999207\n",
      "Epoch 24 Batch: 46 Train Loss: 0.053986161947250366\n",
      "Epoch 24 Batch: 48 Train Loss: 0.058259665966033936\n",
      "Epoch 24 Batch: 50 Train Loss: 0.07768543064594269\n",
      "Epoch 24 Batch: 52 Train Loss: 0.02091160975396633\n",
      "Epoch 24 Batch: 54 Train Loss: 0.052229344844818115\n",
      "Epoch 24 Batch: 56 Train Loss: 0.2563905417919159\n",
      "Epoch 24 Batch: 58 Train Loss: 0.05037149041891098\n",
      "Epoch 24 Batch: 60 Train Loss: 0.09181974828243256\n",
      "Epoch 24 Batch: 62 Train Loss: 0.26540035009384155\n",
      "Epoch 24 Batch: 64 Train Loss: 0.08788780122995377\n",
      "Epoch 24 Batch: 66 Train Loss: 0.2999013066291809\n",
      "Epoch 24 Batch: 68 Train Loss: 0.06271194666624069\n",
      "Epoch 24 Batch: 70 Train Loss: 0.07186005264520645\n",
      "Epoch 24 Batch: 72 Train Loss: 0.26085782051086426\n",
      "Epoch 24 Batch: 74 Train Loss: 0.07790642976760864\n",
      "Epoch 24 Batch: 76 Train Loss: 0.328601598739624\n",
      "Epoch 24 Batch: 78 Train Loss: 0.25641191005706787\n",
      "Epoch 24 Batch: 80 Train Loss: 0.3339056372642517\n",
      "Epoch 24 Batch: 82 Train Loss: 0.04190715402364731\n",
      "Epoch 24 Batch: 84 Train Loss: 0.047109976410865784\n",
      "Epoch 24 Batch: 86 Train Loss: 0.07098110020160675\n",
      "Epoch 24 Batch: 88 Train Loss: 0.734491229057312\n",
      "Epoch 24 Batch: 90 Train Loss: 0.04701409116387367\n",
      "Epoch 24 Batch: 92 Train Loss: 0.07945234328508377\n",
      "Epoch 24 Batch: 94 Train Loss: 0.04268672317266464\n",
      "Epoch 24 Batch: 96 Train Loss: 0.2919955253601074\n",
      "Epoch 24 Batch: 98 Train Loss: 0.052852462977170944\n",
      "Epoch 24 Batch: 100 Train Loss: 0.06601252406835556\n",
      "Epoch 24 Batch: 102 Train Loss: 0.2817160487174988\n",
      "Epoch 24 Batch: 104 Train Loss: 0.2804151475429535\n",
      "Epoch 24 Batch: 106 Train Loss: 0.09611799567937851\n",
      "Epoch 24 Batch: 108 Train Loss: 0.039125435054302216\n",
      "Epoch 24 Batch: 110 Train Loss: 0.0441550686955452\n",
      "Epoch 24 Batch: 112 Train Loss: 0.5074411034584045\n",
      "Epoch 24 Batch: 114 Train Loss: 0.03903525322675705\n",
      "Epoch 24 Batch: 116 Train Loss: 0.0653730034828186\n",
      "Epoch 24 Batch: 118 Train Loss: 0.2697891891002655\n",
      "Epoch 24 Batch: 120 Train Loss: 0.032792799174785614\n",
      "Epoch 24 Batch: 122 Train Loss: 0.35722681879997253\n",
      "Epoch 24 Batch: 124 Train Loss: 0.03118547424674034\n",
      "Epoch 24 Batch: 126 Train Loss: 0.03259903937578201\n",
      "Epoch 24 Batch: 128 Train Loss: 0.03820478171110153\n",
      "Epoch 24 Batch: 130 Train Loss: 0.04561721161007881\n",
      "Epoch 24 Batch: 132 Train Loss: 0.8050251007080078\n",
      "Epoch 24 Batch: 134 Train Loss: 0.04889524728059769\n",
      "Epoch 24 Batch: 136 Train Loss: 0.08594395965337753\n",
      "Epoch 24 Batch: 138 Train Loss: 0.008615926839411259\n",
      "Epoch 24 Batch: 140 Train Loss: 0.05279667302966118\n",
      "Epoch 24 Batch: 142 Train Loss: 0.5478848814964294\n",
      "Epoch 24 Batch: 144 Train Loss: 0.04238516092300415\n",
      "Epoch 24 Batch: 146 Train Loss: 0.06564565747976303\n",
      "Epoch 24 Batch: 148 Train Loss: 0.4856516718864441\n",
      "Epoch 24 Batch: 150 Train Loss: 0.09194660186767578\n",
      "Epoch 24 Batch: 152 Train Loss: 0.041446320712566376\n",
      "Epoch 24 Batch: 154 Train Loss: 0.06686313450336456\n",
      "Epoch 24 Batch: 156 Train Loss: 0.031153732910752296\n",
      "Epoch 24 Batch: 158 Train Loss: 0.29892784357070923\n",
      "Epoch 24 Batch: 160 Train Loss: 0.03928845748305321\n",
      "Epoch 25 Batch: 2 Train Loss: 0.04049507901072502\n",
      "Epoch 25 Batch: 4 Train Loss: 0.26859092712402344\n",
      "Epoch 25 Batch: 6 Train Loss: 0.06862329691648483\n",
      "Epoch 25 Batch: 8 Train Loss: 0.55106520652771\n",
      "Epoch 25 Batch: 10 Train Loss: 0.31154197454452515\n",
      "Epoch 25 Batch: 12 Train Loss: 0.3142356276512146\n",
      "Epoch 25 Batch: 14 Train Loss: 0.26368197798728943\n",
      "Epoch 25 Batch: 16 Train Loss: 0.08587392419576645\n",
      "Epoch 25 Batch: 18 Train Loss: 0.07010046392679214\n",
      "Epoch 25 Batch: 20 Train Loss: 0.06283536553382874\n",
      "Epoch 25 Batch: 22 Train Loss: 0.08158861100673676\n",
      "Epoch 25 Batch: 24 Train Loss: 0.054545897990465164\n",
      "Epoch 25 Batch: 26 Train Loss: 0.2771035134792328\n",
      "Epoch 25 Batch: 28 Train Loss: 0.06947523355484009\n",
      "Epoch 25 Batch: 30 Train Loss: 0.04385644197463989\n",
      "Epoch 25 Batch: 32 Train Loss: 0.24835439026355743\n",
      "Epoch 25 Batch: 34 Train Loss: 0.062257468700408936\n",
      "Epoch 25 Batch: 36 Train Loss: 0.042630456387996674\n",
      "Epoch 25 Batch: 38 Train Loss: 0.04013965651392937\n",
      "Epoch 25 Batch: 40 Train Loss: 0.30387991666793823\n",
      "Epoch 25 Batch: 42 Train Loss: 0.043947864323854446\n",
      "Epoch 25 Batch: 44 Train Loss: 0.31945985555648804\n",
      "Epoch 25 Batch: 46 Train Loss: 0.042353324592113495\n",
      "Epoch 25 Batch: 48 Train Loss: 0.27003204822540283\n",
      "Epoch 25 Batch: 50 Train Loss: 0.06231435388326645\n",
      "Epoch 25 Batch: 52 Train Loss: 0.3033144176006317\n",
      "Epoch 25 Batch: 54 Train Loss: 0.07512736320495605\n",
      "Epoch 25 Batch: 56 Train Loss: 0.2721410095691681\n",
      "Epoch 25 Batch: 58 Train Loss: 0.061773646622896194\n",
      "Epoch 25 Batch: 60 Train Loss: 0.26046162843704224\n",
      "Epoch 25 Batch: 62 Train Loss: 0.25402194261550903\n",
      "Epoch 25 Batch: 64 Train Loss: 0.07111169397830963\n",
      "Epoch 25 Batch: 66 Train Loss: 0.22165486216545105\n",
      "Epoch 25 Batch: 68 Train Loss: 0.09741000831127167\n",
      "Epoch 25 Batch: 70 Train Loss: 0.25966960191726685\n",
      "Epoch 25 Batch: 72 Train Loss: 0.0766676515340805\n",
      "Epoch 25 Batch: 74 Train Loss: 0.06985102593898773\n",
      "Epoch 25 Batch: 76 Train Loss: 0.07431318610906601\n",
      "Epoch 25 Batch: 78 Train Loss: 0.05858483165502548\n",
      "Epoch 25 Batch: 80 Train Loss: 0.052526332437992096\n",
      "Epoch 25 Batch: 82 Train Loss: 0.04819200560450554\n",
      "Epoch 25 Batch: 84 Train Loss: 0.5371158719062805\n",
      "Epoch 25 Batch: 86 Train Loss: 0.0520913302898407\n",
      "Epoch 25 Batch: 88 Train Loss: 0.2828819453716278\n",
      "Epoch 25 Batch: 90 Train Loss: 0.042037881910800934\n",
      "Epoch 25 Batch: 92 Train Loss: 0.042758990079164505\n",
      "Epoch 25 Batch: 94 Train Loss: 0.03410112112760544\n",
      "Epoch 25 Batch: 96 Train Loss: 0.04273241013288498\n",
      "Epoch 25 Batch: 98 Train Loss: 0.2929612994194031\n",
      "Epoch 25 Batch: 100 Train Loss: 0.018370214849710464\n",
      "Epoch 25 Batch: 102 Train Loss: 0.024896908551454544\n",
      "Epoch 25 Batch: 104 Train Loss: 0.049967989325523376\n",
      "Epoch 25 Batch: 106 Train Loss: 0.5609298944473267\n",
      "Epoch 25 Batch: 108 Train Loss: 0.3004174530506134\n",
      "Epoch 25 Batch: 110 Train Loss: 0.30311352014541626\n",
      "Epoch 25 Batch: 112 Train Loss: 0.29330942034721375\n",
      "Epoch 25 Batch: 114 Train Loss: 0.10076441615819931\n",
      "Epoch 25 Batch: 116 Train Loss: 0.08106344938278198\n",
      "Epoch 25 Batch: 118 Train Loss: 0.059153951704502106\n",
      "Epoch 25 Batch: 120 Train Loss: 0.07348153740167618\n",
      "Epoch 25 Batch: 122 Train Loss: 0.07534369081258774\n",
      "Epoch 25 Batch: 124 Train Loss: 0.03553473949432373\n",
      "Epoch 25 Batch: 126 Train Loss: 0.2965439260005951\n",
      "Epoch 25 Batch: 128 Train Loss: 0.3130185902118683\n",
      "Epoch 25 Batch: 130 Train Loss: 0.03966818004846573\n",
      "Epoch 25 Batch: 132 Train Loss: 0.26500338315963745\n",
      "Epoch 25 Batch: 134 Train Loss: 0.03985905647277832\n",
      "Epoch 25 Batch: 136 Train Loss: 0.06682826578617096\n",
      "Epoch 25 Batch: 138 Train Loss: 0.05885319784283638\n",
      "Epoch 25 Batch: 140 Train Loss: 0.27869725227355957\n",
      "Epoch 25 Batch: 142 Train Loss: 0.03731396049261093\n",
      "Epoch 25 Batch: 144 Train Loss: 0.04723571985960007\n",
      "Epoch 25 Batch: 146 Train Loss: 0.07538485527038574\n",
      "Epoch 25 Batch: 148 Train Loss: 0.2583499848842621\n",
      "Epoch 25 Batch: 150 Train Loss: 0.045299142599105835\n",
      "Epoch 25 Batch: 152 Train Loss: 0.49616631865501404\n",
      "Epoch 25 Batch: 154 Train Loss: 0.07066205143928528\n",
      "Epoch 25 Batch: 156 Train Loss: 0.06720546633005142\n",
      "Epoch 25 Batch: 158 Train Loss: 0.2453118860721588\n",
      "Epoch 25 Batch: 160 Train Loss: 0.06975854188203812\n",
      "Epoch 26 Batch: 2 Train Loss: 0.010706918314099312\n",
      "Epoch 26 Batch: 4 Train Loss: 0.06114618107676506\n",
      "Epoch 26 Batch: 6 Train Loss: 0.060398466885089874\n",
      "Epoch 26 Batch: 8 Train Loss: 0.3194119334220886\n",
      "Epoch 26 Batch: 10 Train Loss: 0.07401838153600693\n",
      "Epoch 26 Batch: 12 Train Loss: 0.042316220700740814\n",
      "Epoch 26 Batch: 14 Train Loss: 0.31861674785614014\n",
      "Epoch 26 Batch: 16 Train Loss: 0.05148697644472122\n",
      "Epoch 26 Batch: 18 Train Loss: 0.2887570261955261\n",
      "Epoch 26 Batch: 20 Train Loss: 0.5286651253700256\n",
      "Epoch 26 Batch: 22 Train Loss: 0.05144123360514641\n",
      "Epoch 26 Batch: 24 Train Loss: 0.06105198711156845\n",
      "Epoch 26 Batch: 26 Train Loss: 0.060927391052246094\n",
      "Epoch 26 Batch: 28 Train Loss: 0.07172180712223053\n",
      "Epoch 26 Batch: 30 Train Loss: 0.10497774183750153\n",
      "Epoch 26 Batch: 32 Train Loss: 0.05573888495564461\n",
      "Epoch 26 Batch: 34 Train Loss: 0.292088121175766\n",
      "Epoch 26 Batch: 36 Train Loss: 0.03836433216929436\n",
      "Epoch 26 Batch: 38 Train Loss: 0.018182143568992615\n",
      "Epoch 26 Batch: 40 Train Loss: 0.326163113117218\n",
      "Epoch 26 Batch: 42 Train Loss: 0.03605138882994652\n",
      "Epoch 26 Batch: 44 Train Loss: 0.04051176458597183\n",
      "Epoch 26 Batch: 46 Train Loss: 0.30888330936431885\n",
      "Epoch 26 Batch: 48 Train Loss: 0.058546699583530426\n",
      "Epoch 26 Batch: 50 Train Loss: 0.04042554646730423\n",
      "Epoch 26 Batch: 52 Train Loss: 0.3240439295768738\n",
      "Epoch 26 Batch: 54 Train Loss: 0.5783200263977051\n",
      "Epoch 26 Batch: 56 Train Loss: 0.05049852654337883\n",
      "Epoch 26 Batch: 58 Train Loss: 0.2692332863807678\n",
      "Epoch 26 Batch: 60 Train Loss: 0.030549371615052223\n",
      "Epoch 26 Batch: 62 Train Loss: 0.047001488506793976\n",
      "Epoch 26 Batch: 64 Train Loss: 0.538679301738739\n",
      "Epoch 26 Batch: 66 Train Loss: 0.0243215449154377\n",
      "Epoch 26 Batch: 68 Train Loss: 0.05340879037976265\n",
      "Epoch 26 Batch: 70 Train Loss: 0.046976033598184586\n",
      "Epoch 26 Batch: 72 Train Loss: 0.30253809690475464\n",
      "Epoch 26 Batch: 74 Train Loss: 0.02801779843866825\n",
      "Epoch 26 Batch: 76 Train Loss: 0.2982977032661438\n",
      "Epoch 26 Batch: 78 Train Loss: 0.0458819642663002\n",
      "Epoch 26 Batch: 80 Train Loss: 0.2760487496852875\n",
      "Epoch 26 Batch: 82 Train Loss: 0.5118434429168701\n",
      "Epoch 26 Batch: 84 Train Loss: 0.261924147605896\n",
      "Epoch 26 Batch: 86 Train Loss: 0.10194878280162811\n",
      "Epoch 26 Batch: 88 Train Loss: 0.0601905882358551\n",
      "Epoch 26 Batch: 90 Train Loss: 0.3055261969566345\n",
      "Epoch 26 Batch: 92 Train Loss: 0.02819173038005829\n",
      "Epoch 26 Batch: 94 Train Loss: 0.06065857410430908\n",
      "Epoch 26 Batch: 96 Train Loss: 0.04287618026137352\n",
      "Epoch 26 Batch: 98 Train Loss: 0.029185503721237183\n",
      "Epoch 26 Batch: 100 Train Loss: 0.27291005849838257\n",
      "Epoch 26 Batch: 102 Train Loss: 0.3390454649925232\n",
      "Epoch 26 Batch: 104 Train Loss: 0.03975826874375343\n",
      "Epoch 26 Batch: 106 Train Loss: 0.2833850681781769\n",
      "Epoch 26 Batch: 108 Train Loss: 0.02861449122428894\n",
      "Epoch 26 Batch: 110 Train Loss: 0.032324787229299545\n",
      "Epoch 26 Batch: 112 Train Loss: 0.043038081377744675\n",
      "Epoch 26 Batch: 114 Train Loss: 0.036259882152080536\n",
      "Epoch 26 Batch: 116 Train Loss: 0.5049968957901001\n",
      "Epoch 26 Batch: 118 Train Loss: 0.288826048374176\n",
      "Epoch 26 Batch: 120 Train Loss: 0.04779365286231041\n",
      "Epoch 26 Batch: 122 Train Loss: 0.27087658643722534\n",
      "Epoch 26 Batch: 124 Train Loss: 0.058938510715961456\n",
      "Epoch 26 Batch: 126 Train Loss: 0.2638072073459625\n",
      "Epoch 26 Batch: 128 Train Loss: 0.4934404492378235\n",
      "Epoch 26 Batch: 130 Train Loss: 0.08109887689352036\n",
      "Epoch 26 Batch: 132 Train Loss: 0.06880059838294983\n",
      "Epoch 26 Batch: 134 Train Loss: 0.0868557021021843\n",
      "Epoch 26 Batch: 136 Train Loss: 0.2571713328361511\n",
      "Epoch 26 Batch: 138 Train Loss: 0.06697171926498413\n",
      "Epoch 26 Batch: 140 Train Loss: 0.0715642124414444\n",
      "Epoch 26 Batch: 142 Train Loss: 0.26756685972213745\n",
      "Epoch 26 Batch: 144 Train Loss: 0.26853644847869873\n",
      "Epoch 26 Batch: 146 Train Loss: 0.056872326880693436\n",
      "Epoch 26 Batch: 148 Train Loss: 0.061843596398830414\n",
      "Epoch 26 Batch: 150 Train Loss: 0.06022966653108597\n",
      "Epoch 26 Batch: 152 Train Loss: 0.3105001449584961\n",
      "Epoch 26 Batch: 154 Train Loss: 0.2827901542186737\n",
      "Epoch 26 Batch: 156 Train Loss: 0.5984178781509399\n",
      "Epoch 26 Batch: 158 Train Loss: 0.3240424692630768\n",
      "Epoch 26 Batch: 160 Train Loss: 0.040000297129154205\n",
      "Epoch 27 Batch: 2 Train Loss: 0.32744383811950684\n",
      "Epoch 27 Batch: 4 Train Loss: 0.3048667311668396\n",
      "Epoch 27 Batch: 6 Train Loss: 0.03530297800898552\n",
      "Epoch 27 Batch: 8 Train Loss: 0.08899742364883423\n",
      "Epoch 27 Batch: 10 Train Loss: 0.07173381000757217\n",
      "Epoch 27 Batch: 12 Train Loss: 0.27471253275871277\n",
      "Epoch 27 Batch: 14 Train Loss: 0.28922995924949646\n",
      "Epoch 27 Batch: 16 Train Loss: 0.026793796569108963\n",
      "Epoch 27 Batch: 18 Train Loss: 0.07201961427927017\n",
      "Epoch 27 Batch: 20 Train Loss: 0.049765538424253464\n",
      "Epoch 27 Batch: 22 Train Loss: 0.32334035634994507\n",
      "Epoch 27 Batch: 24 Train Loss: 0.2670462131500244\n",
      "Epoch 27 Batch: 26 Train Loss: 0.2625879645347595\n",
      "Epoch 27 Batch: 28 Train Loss: 0.471454381942749\n",
      "Epoch 27 Batch: 30 Train Loss: 0.08366525918245316\n",
      "Epoch 27 Batch: 32 Train Loss: 0.05370970815420151\n",
      "Epoch 27 Batch: 34 Train Loss: 0.06238403171300888\n",
      "Epoch 27 Batch: 36 Train Loss: 0.10358840227127075\n",
      "Epoch 27 Batch: 38 Train Loss: 0.09690473973751068\n",
      "Epoch 27 Batch: 40 Train Loss: 0.4818265438079834\n",
      "Epoch 27 Batch: 42 Train Loss: 0.05501728132367134\n",
      "Epoch 27 Batch: 44 Train Loss: 0.28113722801208496\n",
      "Epoch 27 Batch: 46 Train Loss: 0.31824344396591187\n",
      "Epoch 27 Batch: 48 Train Loss: 0.2574205696582794\n",
      "Epoch 27 Batch: 50 Train Loss: 0.06000571697950363\n",
      "Epoch 27 Batch: 52 Train Loss: 0.06394852697849274\n",
      "Epoch 27 Batch: 54 Train Loss: 0.3094746172428131\n",
      "Epoch 27 Batch: 56 Train Loss: 0.06328897178173065\n",
      "Epoch 27 Batch: 58 Train Loss: 0.05837859585881233\n",
      "Epoch 27 Batch: 60 Train Loss: 0.27425235509872437\n",
      "Epoch 27 Batch: 62 Train Loss: 0.2902407944202423\n",
      "Epoch 27 Batch: 64 Train Loss: 0.045577626675367355\n",
      "Epoch 27 Batch: 66 Train Loss: 0.07320525497198105\n",
      "Epoch 27 Batch: 68 Train Loss: 0.04597720503807068\n",
      "Epoch 27 Batch: 70 Train Loss: 0.3153832256793976\n",
      "Epoch 27 Batch: 72 Train Loss: 0.039722152054309845\n",
      "Epoch 27 Batch: 74 Train Loss: 0.048195529729127884\n",
      "Epoch 27 Batch: 76 Train Loss: 0.3024633228778839\n",
      "Epoch 27 Batch: 78 Train Loss: 0.08356313407421112\n",
      "Epoch 27 Batch: 80 Train Loss: 0.054106734693050385\n",
      "Epoch 27 Batch: 82 Train Loss: 0.030045196413993835\n",
      "Epoch 27 Batch: 84 Train Loss: 0.0404847078025341\n",
      "Epoch 27 Batch: 86 Train Loss: 0.05563206598162651\n",
      "Epoch 27 Batch: 88 Train Loss: 0.3103344142436981\n",
      "Epoch 27 Batch: 90 Train Loss: 0.024974307045340538\n",
      "Epoch 27 Batch: 92 Train Loss: 0.05417722463607788\n",
      "Epoch 27 Batch: 94 Train Loss: 0.26664644479751587\n",
      "Epoch 27 Batch: 96 Train Loss: 0.31309008598327637\n",
      "Epoch 27 Batch: 98 Train Loss: 0.038295213133096695\n",
      "Epoch 27 Batch: 100 Train Loss: 0.02650662697851658\n",
      "Epoch 27 Batch: 102 Train Loss: 0.5514329075813293\n",
      "Epoch 27 Batch: 104 Train Loss: 0.04221739247441292\n",
      "Epoch 27 Batch: 106 Train Loss: 0.08089629560709\n",
      "Epoch 27 Batch: 108 Train Loss: 0.06732238829135895\n",
      "Epoch 27 Batch: 110 Train Loss: 0.05696297436952591\n",
      "Epoch 27 Batch: 112 Train Loss: 0.3134915232658386\n",
      "Epoch 27 Batch: 114 Train Loss: 0.018180999904870987\n",
      "Epoch 27 Batch: 116 Train Loss: 0.04077266901731491\n",
      "Epoch 27 Batch: 118 Train Loss: 0.04463142156600952\n",
      "Epoch 27 Batch: 120 Train Loss: 0.07114584743976593\n",
      "Epoch 27 Batch: 122 Train Loss: 0.2890911102294922\n",
      "Epoch 27 Batch: 124 Train Loss: 0.3080716133117676\n",
      "Epoch 27 Batch: 126 Train Loss: 0.08650317043066025\n",
      "Epoch 27 Batch: 128 Train Loss: 0.4695866107940674\n",
      "Epoch 27 Batch: 130 Train Loss: 0.04659661650657654\n",
      "Epoch 27 Batch: 132 Train Loss: 0.24544355273246765\n",
      "Epoch 27 Batch: 134 Train Loss: 0.5270506143569946\n",
      "Epoch 27 Batch: 136 Train Loss: 0.05748424679040909\n",
      "Epoch 27 Batch: 138 Train Loss: 0.09048645943403244\n",
      "Epoch 27 Batch: 140 Train Loss: 0.3052230477333069\n",
      "Epoch 27 Batch: 142 Train Loss: 0.04758458212018013\n",
      "Epoch 27 Batch: 144 Train Loss: 0.2659135162830353\n",
      "Epoch 27 Batch: 146 Train Loss: 0.04614648595452309\n",
      "Epoch 27 Batch: 148 Train Loss: 0.05724092572927475\n",
      "Epoch 27 Batch: 150 Train Loss: 0.27251172065734863\n",
      "Epoch 27 Batch: 152 Train Loss: 0.5128620862960815\n",
      "Epoch 27 Batch: 154 Train Loss: 0.027175456285476685\n",
      "Epoch 27 Batch: 156 Train Loss: 0.06167701631784439\n",
      "Epoch 27 Batch: 158 Train Loss: 0.0760636180639267\n",
      "Epoch 27 Batch: 160 Train Loss: 0.037818994373083115\n",
      "Epoch 28 Batch: 2 Train Loss: 0.038193196058273315\n",
      "Epoch 28 Batch: 4 Train Loss: 0.0406893827021122\n",
      "Epoch 28 Batch: 6 Train Loss: 0.5237414240837097\n",
      "Epoch 28 Batch: 8 Train Loss: 0.06333495676517487\n",
      "Epoch 28 Batch: 10 Train Loss: 0.06264448910951614\n",
      "Epoch 28 Batch: 12 Train Loss: 0.06082829087972641\n",
      "Epoch 28 Batch: 14 Train Loss: 0.292370080947876\n",
      "Epoch 28 Batch: 16 Train Loss: 0.05135134607553482\n",
      "Epoch 28 Batch: 18 Train Loss: 0.03992987051606178\n",
      "Epoch 28 Batch: 20 Train Loss: 0.045905835926532745\n",
      "Epoch 28 Batch: 22 Train Loss: 0.04570357874035835\n",
      "Epoch 28 Batch: 24 Train Loss: 0.05100967362523079\n",
      "Epoch 28 Batch: 26 Train Loss: 0.026197265833616257\n",
      "Epoch 28 Batch: 28 Train Loss: 0.27344033122062683\n",
      "Epoch 28 Batch: 30 Train Loss: 0.041239380836486816\n",
      "Epoch 28 Batch: 32 Train Loss: 0.045444488525390625\n",
      "Epoch 28 Batch: 34 Train Loss: 0.03927115350961685\n",
      "Epoch 28 Batch: 36 Train Loss: 0.04101622477173805\n",
      "Epoch 28 Batch: 38 Train Loss: 0.0601053312420845\n",
      "Epoch 28 Batch: 40 Train Loss: 0.03984793648123741\n",
      "Epoch 28 Batch: 42 Train Loss: 0.27057531476020813\n",
      "Epoch 28 Batch: 44 Train Loss: 0.247660830616951\n",
      "Epoch 28 Batch: 46 Train Loss: 0.31169241666793823\n",
      "Epoch 28 Batch: 48 Train Loss: 0.06249411031603813\n",
      "Epoch 28 Batch: 50 Train Loss: 0.057641953229904175\n",
      "Epoch 28 Batch: 52 Train Loss: 0.28539976477622986\n",
      "Epoch 28 Batch: 54 Train Loss: 0.5257622599601746\n",
      "Epoch 28 Batch: 56 Train Loss: 0.26501354575157166\n",
      "Epoch 28 Batch: 58 Train Loss: 0.044632721692323685\n",
      "Epoch 28 Batch: 60 Train Loss: 0.28445515036582947\n",
      "Epoch 28 Batch: 62 Train Loss: 0.06825294345617294\n",
      "Epoch 28 Batch: 64 Train Loss: 0.2815968096256256\n",
      "Epoch 28 Batch: 66 Train Loss: 0.08721188455820084\n",
      "Epoch 28 Batch: 68 Train Loss: 0.11358056217432022\n",
      "Epoch 28 Batch: 70 Train Loss: 0.052544064819812775\n",
      "Epoch 28 Batch: 72 Train Loss: 0.28899991512298584\n",
      "Epoch 28 Batch: 74 Train Loss: 0.026099061593413353\n",
      "Epoch 28 Batch: 76 Train Loss: 0.33755701780319214\n",
      "Epoch 28 Batch: 78 Train Loss: 0.06874950230121613\n",
      "Epoch 28 Batch: 80 Train Loss: 0.048417892307043076\n",
      "Epoch 28 Batch: 82 Train Loss: 0.2768549621105194\n",
      "Epoch 28 Batch: 84 Train Loss: 0.28695446252822876\n",
      "Epoch 28 Batch: 86 Train Loss: 0.0536494255065918\n",
      "Epoch 28 Batch: 88 Train Loss: 0.26678940653800964\n",
      "Epoch 28 Batch: 90 Train Loss: 0.509787917137146\n",
      "Epoch 28 Batch: 92 Train Loss: 0.07207440584897995\n",
      "Epoch 28 Batch: 94 Train Loss: 0.07055381685495377\n",
      "Epoch 28 Batch: 96 Train Loss: 0.06437418609857559\n",
      "Epoch 28 Batch: 98 Train Loss: 0.4831472337245941\n",
      "Epoch 28 Batch: 100 Train Loss: 0.2676566243171692\n",
      "Epoch 28 Batch: 102 Train Loss: 0.4891510009765625\n",
      "Epoch 28 Batch: 104 Train Loss: 0.2859042286872864\n",
      "Epoch 28 Batch: 106 Train Loss: 0.12179110199213028\n",
      "Epoch 28 Batch: 108 Train Loss: 0.08132599294185638\n",
      "Epoch 28 Batch: 110 Train Loss: 0.8535903692245483\n",
      "Epoch 28 Batch: 112 Train Loss: 0.4680996537208557\n",
      "Epoch 28 Batch: 114 Train Loss: 0.28894907236099243\n",
      "Epoch 28 Batch: 116 Train Loss: 0.12397488206624985\n",
      "Epoch 28 Batch: 118 Train Loss: 0.030307402834296227\n",
      "Epoch 28 Batch: 120 Train Loss: 0.058771632611751556\n",
      "Epoch 28 Batch: 122 Train Loss: 0.13706400990486145\n",
      "Epoch 28 Batch: 124 Train Loss: 0.2725829482078552\n",
      "Epoch 28 Batch: 126 Train Loss: 0.09984101355075836\n",
      "Epoch 28 Batch: 128 Train Loss: 0.0658435970544815\n",
      "Epoch 28 Batch: 130 Train Loss: 0.061727117747068405\n",
      "Epoch 28 Batch: 132 Train Loss: 0.08362586796283722\n",
      "Epoch 28 Batch: 134 Train Loss: 0.29485392570495605\n",
      "Epoch 28 Batch: 136 Train Loss: 0.06394736468791962\n",
      "Epoch 28 Batch: 138 Train Loss: 0.042496029287576675\n",
      "Epoch 28 Batch: 140 Train Loss: 0.3111833930015564\n",
      "Epoch 28 Batch: 142 Train Loss: 0.05074573680758476\n",
      "Epoch 28 Batch: 144 Train Loss: 0.049426477402448654\n",
      "Epoch 28 Batch: 146 Train Loss: 0.05061211436986923\n",
      "Epoch 28 Batch: 148 Train Loss: 0.2896382212638855\n",
      "Epoch 28 Batch: 150 Train Loss: 0.03667788207530975\n",
      "Epoch 28 Batch: 152 Train Loss: 0.03066498413681984\n",
      "Epoch 28 Batch: 154 Train Loss: 0.5453116297721863\n",
      "Epoch 28 Batch: 156 Train Loss: 0.531359851360321\n",
      "Epoch 28 Batch: 158 Train Loss: 0.29309239983558655\n",
      "Epoch 28 Batch: 160 Train Loss: 0.05382513254880905\n",
      "Epoch 29 Batch: 2 Train Loss: 0.30984801054000854\n",
      "Epoch 29 Batch: 4 Train Loss: 0.29545626044273376\n",
      "Epoch 29 Batch: 6 Train Loss: 0.03716548904776573\n",
      "Epoch 29 Batch: 8 Train Loss: 0.07574833929538727\n",
      "Epoch 29 Batch: 10 Train Loss: 0.06362573802471161\n",
      "Epoch 29 Batch: 12 Train Loss: 0.05100461095571518\n",
      "Epoch 29 Batch: 14 Train Loss: 0.07847505062818527\n",
      "Epoch 29 Batch: 16 Train Loss: 0.04589753597974777\n",
      "Epoch 29 Batch: 18 Train Loss: 0.0377717986702919\n",
      "Epoch 29 Batch: 20 Train Loss: 0.031463705003261566\n",
      "Epoch 29 Batch: 22 Train Loss: 0.3297852575778961\n",
      "Epoch 29 Batch: 24 Train Loss: 0.2918018102645874\n",
      "Epoch 29 Batch: 26 Train Loss: 0.0327058844268322\n",
      "Epoch 29 Batch: 28 Train Loss: 0.026083583012223244\n",
      "Epoch 29 Batch: 30 Train Loss: 0.029819006100296974\n",
      "Epoch 29 Batch: 32 Train Loss: 0.342084139585495\n",
      "Epoch 29 Batch: 34 Train Loss: 0.9722501635551453\n",
      "Epoch 29 Batch: 36 Train Loss: 0.5994130373001099\n",
      "Epoch 29 Batch: 38 Train Loss: 0.046061478555202484\n",
      "Epoch 29 Batch: 40 Train Loss: 0.3403894901275635\n",
      "Epoch 29 Batch: 42 Train Loss: 0.04717941954731941\n",
      "Epoch 29 Batch: 44 Train Loss: 0.5005711317062378\n",
      "Epoch 29 Batch: 46 Train Loss: 0.05993383005261421\n",
      "Epoch 29 Batch: 48 Train Loss: 0.10054860264062881\n",
      "Epoch 29 Batch: 50 Train Loss: 0.030941713601350784\n",
      "Epoch 29 Batch: 52 Train Loss: 0.33414414525032043\n",
      "Epoch 29 Batch: 54 Train Loss: 0.043220460414886475\n",
      "Epoch 29 Batch: 56 Train Loss: 0.025781214237213135\n",
      "Epoch 29 Batch: 58 Train Loss: 0.0325949490070343\n",
      "Epoch 29 Batch: 60 Train Loss: 0.055340081453323364\n",
      "Epoch 29 Batch: 62 Train Loss: 0.05814676731824875\n",
      "Epoch 29 Batch: 64 Train Loss: 0.08208819478750229\n",
      "Epoch 29 Batch: 66 Train Loss: 0.26046282052993774\n",
      "Epoch 29 Batch: 68 Train Loss: 0.2905924916267395\n",
      "Epoch 29 Batch: 70 Train Loss: 0.04335084185004234\n",
      "Epoch 29 Batch: 72 Train Loss: 0.3905046582221985\n",
      "Epoch 29 Batch: 74 Train Loss: 0.09003245085477829\n",
      "Epoch 29 Batch: 76 Train Loss: 0.2994714677333832\n",
      "Epoch 29 Batch: 78 Train Loss: 0.04117881506681442\n",
      "Epoch 29 Batch: 80 Train Loss: 0.1382661759853363\n",
      "Epoch 29 Batch: 82 Train Loss: 0.05959077924489975\n",
      "Epoch 29 Batch: 84 Train Loss: 0.049806494265794754\n",
      "Epoch 29 Batch: 86 Train Loss: 0.5748817920684814\n",
      "Epoch 29 Batch: 88 Train Loss: 0.47400927543640137\n",
      "Epoch 29 Batch: 90 Train Loss: 0.28721582889556885\n",
      "Epoch 29 Batch: 92 Train Loss: 0.027294810861349106\n",
      "Epoch 29 Batch: 94 Train Loss: 0.2785648703575134\n",
      "Epoch 29 Batch: 96 Train Loss: 0.04665321484208107\n",
      "Epoch 29 Batch: 98 Train Loss: 0.07909494638442993\n",
      "Epoch 29 Batch: 100 Train Loss: 0.07084283232688904\n",
      "Epoch 29 Batch: 102 Train Loss: 0.29402655363082886\n",
      "Epoch 29 Batch: 104 Train Loss: 0.0388895645737648\n",
      "Epoch 29 Batch: 106 Train Loss: 0.6677390336990356\n",
      "Epoch 29 Batch: 108 Train Loss: 0.38121527433395386\n",
      "Epoch 29 Batch: 110 Train Loss: 0.3529488146305084\n",
      "Epoch 29 Batch: 112 Train Loss: 0.49014919996261597\n",
      "Epoch 29 Batch: 114 Train Loss: 0.03628211095929146\n",
      "Epoch 29 Batch: 116 Train Loss: 0.32431986927986145\n",
      "Epoch 29 Batch: 118 Train Loss: 0.3095238208770752\n",
      "Epoch 29 Batch: 120 Train Loss: 0.025925124064087868\n",
      "Epoch 29 Batch: 122 Train Loss: 0.06209757924079895\n",
      "Epoch 29 Batch: 124 Train Loss: 0.29107865691185\n",
      "Epoch 29 Batch: 126 Train Loss: 0.5434019565582275\n",
      "Epoch 29 Batch: 128 Train Loss: 0.5430053472518921\n",
      "Epoch 29 Batch: 130 Train Loss: 0.27256089448928833\n",
      "Epoch 29 Batch: 132 Train Loss: 0.04930227994918823\n",
      "Epoch 29 Batch: 134 Train Loss: 0.02902374602854252\n",
      "Epoch 29 Batch: 136 Train Loss: 0.09042055904865265\n",
      "Epoch 29 Batch: 138 Train Loss: 0.11776785552501678\n",
      "Epoch 29 Batch: 140 Train Loss: 0.07457845658063889\n",
      "Epoch 29 Batch: 142 Train Loss: 0.07811705768108368\n",
      "Epoch 29 Batch: 144 Train Loss: 0.21487736701965332\n",
      "Epoch 29 Batch: 146 Train Loss: 0.09653018414974213\n",
      "Epoch 29 Batch: 148 Train Loss: 0.07506795227527618\n",
      "Epoch 29 Batch: 150 Train Loss: 0.08498071134090424\n",
      "Epoch 29 Batch: 152 Train Loss: 0.08209116011857986\n",
      "Epoch 29 Batch: 154 Train Loss: 0.48953065276145935\n",
      "Epoch 29 Batch: 156 Train Loss: 0.29723042249679565\n",
      "Epoch 29 Batch: 158 Train Loss: 0.05690848082304001\n",
      "Epoch 29 Batch: 160 Train Loss: 0.25632452964782715\n",
      "Epoch 30 Batch: 2 Train Loss: 0.032968275249004364\n",
      "Epoch 30 Batch: 4 Train Loss: 0.2943479120731354\n",
      "Epoch 30 Batch: 6 Train Loss: 0.0604192316532135\n",
      "Epoch 30 Batch: 8 Train Loss: 0.2592521905899048\n",
      "Epoch 30 Batch: 10 Train Loss: 0.014926685020327568\n",
      "Epoch 30 Batch: 12 Train Loss: 0.27180784940719604\n",
      "Epoch 30 Batch: 14 Train Loss: 0.06466367095708847\n",
      "Epoch 30 Batch: 16 Train Loss: 0.06130502372980118\n",
      "Epoch 30 Batch: 18 Train Loss: 0.2607570290565491\n",
      "Epoch 30 Batch: 20 Train Loss: 0.09356088936328888\n",
      "Epoch 30 Batch: 22 Train Loss: 0.05875595659017563\n",
      "Epoch 30 Batch: 24 Train Loss: 0.06891441345214844\n",
      "Epoch 30 Batch: 26 Train Loss: 0.03145996481180191\n",
      "Epoch 30 Batch: 28 Train Loss: 0.09196609258651733\n",
      "Epoch 30 Batch: 30 Train Loss: 0.04624608904123306\n",
      "Epoch 30 Batch: 32 Train Loss: 0.25102880597114563\n",
      "Epoch 30 Batch: 34 Train Loss: 0.07385358959436417\n",
      "Epoch 30 Batch: 36 Train Loss: 0.292614609003067\n",
      "Epoch 30 Batch: 38 Train Loss: 0.07250581681728363\n",
      "Epoch 30 Batch: 40 Train Loss: 0.06343948841094971\n",
      "Epoch 30 Batch: 42 Train Loss: 0.49130159616470337\n",
      "Epoch 30 Batch: 44 Train Loss: 0.287628710269928\n",
      "Epoch 30 Batch: 46 Train Loss: 0.2523067593574524\n",
      "Epoch 30 Batch: 48 Train Loss: 0.06304849684238434\n",
      "Epoch 30 Batch: 50 Train Loss: 0.0702301487326622\n",
      "Epoch 30 Batch: 52 Train Loss: 0.08709640800952911\n",
      "Epoch 30 Batch: 54 Train Loss: 0.2965249717235565\n",
      "Epoch 30 Batch: 56 Train Loss: 0.3133402466773987\n",
      "Epoch 30 Batch: 58 Train Loss: 0.054125793278217316\n",
      "Epoch 30 Batch: 60 Train Loss: 0.26607546210289\n",
      "Epoch 30 Batch: 62 Train Loss: 0.0696471780538559\n",
      "Epoch 30 Batch: 64 Train Loss: 0.25848388671875\n",
      "Epoch 30 Batch: 66 Train Loss: 0.28407108783721924\n",
      "Epoch 30 Batch: 68 Train Loss: 0.08531340956687927\n",
      "Epoch 30 Batch: 70 Train Loss: 0.05963538959622383\n",
      "Epoch 30 Batch: 72 Train Loss: 0.06283138692378998\n",
      "Epoch 30 Batch: 74 Train Loss: 0.056653160601854324\n",
      "Epoch 30 Batch: 76 Train Loss: 0.6838642358779907\n",
      "Epoch 30 Batch: 78 Train Loss: 0.32155200839042664\n",
      "Epoch 30 Batch: 80 Train Loss: 0.039160534739494324\n",
      "Epoch 30 Batch: 82 Train Loss: 0.05351077765226364\n",
      "Epoch 30 Batch: 84 Train Loss: 0.29113253951072693\n",
      "Epoch 30 Batch: 86 Train Loss: 0.04695449396967888\n",
      "Epoch 30 Batch: 88 Train Loss: 0.05507221817970276\n",
      "Epoch 30 Batch: 90 Train Loss: 0.2927525043487549\n",
      "Epoch 30 Batch: 92 Train Loss: 0.05559874698519707\n",
      "Epoch 30 Batch: 94 Train Loss: 0.015863405540585518\n",
      "Epoch 30 Batch: 96 Train Loss: 0.03358939662575722\n",
      "Epoch 30 Batch: 98 Train Loss: 0.05349520593881607\n",
      "Epoch 30 Batch: 100 Train Loss: 0.30078810453414917\n",
      "Epoch 30 Batch: 102 Train Loss: 0.29313796758651733\n",
      "Epoch 30 Batch: 104 Train Loss: 0.04785413667559624\n",
      "Epoch 30 Batch: 106 Train Loss: 0.0660053938627243\n",
      "Epoch 30 Batch: 108 Train Loss: 0.28765755891799927\n",
      "Epoch 30 Batch: 110 Train Loss: 0.023236721754074097\n",
      "Epoch 30 Batch: 112 Train Loss: 0.06310240924358368\n",
      "Epoch 30 Batch: 114 Train Loss: 0.2516162395477295\n",
      "Epoch 30 Batch: 116 Train Loss: 0.04708268493413925\n",
      "Epoch 30 Batch: 118 Train Loss: 0.3162289261817932\n",
      "Epoch 30 Batch: 120 Train Loss: 0.03469831123948097\n",
      "Epoch 30 Batch: 122 Train Loss: 0.054723650217056274\n",
      "Epoch 30 Batch: 124 Train Loss: 0.046397484838962555\n",
      "Epoch 30 Batch: 126 Train Loss: 0.3040560483932495\n",
      "Epoch 30 Batch: 128 Train Loss: 0.04508127272129059\n",
      "Epoch 30 Batch: 130 Train Loss: 0.02589239738881588\n",
      "Epoch 30 Batch: 132 Train Loss: 0.2914392650127411\n",
      "Epoch 30 Batch: 134 Train Loss: 0.07122202217578888\n",
      "Epoch 30 Batch: 136 Train Loss: 0.5346770286560059\n",
      "Epoch 30 Batch: 138 Train Loss: 0.31988525390625\n",
      "Epoch 30 Batch: 140 Train Loss: 0.07602662593126297\n",
      "Epoch 30 Batch: 142 Train Loss: 0.054441798478364944\n",
      "Epoch 30 Batch: 144 Train Loss: 0.2870126962661743\n",
      "Epoch 30 Batch: 146 Train Loss: 0.274949848651886\n",
      "Epoch 30 Batch: 148 Train Loss: 0.7915260195732117\n",
      "Epoch 30 Batch: 150 Train Loss: 0.02234547957777977\n",
      "Epoch 30 Batch: 152 Train Loss: 0.04429594799876213\n",
      "Epoch 30 Batch: 154 Train Loss: 0.053742676973342896\n",
      "Epoch 30 Batch: 156 Train Loss: 0.06621746718883514\n",
      "Epoch 30 Batch: 158 Train Loss: 0.30780458450317383\n",
      "Epoch 30 Batch: 160 Train Loss: 0.061349790543317795\n",
      "Epoch 31 Batch: 2 Train Loss: 0.2594372630119324\n",
      "Epoch 31 Batch: 4 Train Loss: 0.5222650766372681\n",
      "Epoch 31 Batch: 6 Train Loss: 0.0535208061337471\n",
      "Epoch 31 Batch: 8 Train Loss: 0.06427882611751556\n",
      "Epoch 31 Batch: 10 Train Loss: 0.25715991854667664\n",
      "Epoch 31 Batch: 12 Train Loss: 0.1087663397192955\n",
      "Epoch 31 Batch: 14 Train Loss: 0.05962543562054634\n",
      "Epoch 31 Batch: 16 Train Loss: 0.0685441792011261\n",
      "Epoch 31 Batch: 18 Train Loss: 0.09872618317604065\n",
      "Epoch 31 Batch: 20 Train Loss: 0.08078963309526443\n",
      "Epoch 31 Batch: 22 Train Loss: 0.07198530435562134\n",
      "Epoch 31 Batch: 24 Train Loss: 0.05350927263498306\n",
      "Epoch 31 Batch: 26 Train Loss: 0.0318228080868721\n",
      "Epoch 31 Batch: 28 Train Loss: 0.27463242411613464\n",
      "Epoch 31 Batch: 30 Train Loss: 0.03772609308362007\n",
      "Epoch 31 Batch: 32 Train Loss: 0.5757167339324951\n",
      "Epoch 31 Batch: 34 Train Loss: 0.534477710723877\n",
      "Epoch 31 Batch: 36 Train Loss: 0.29449573159217834\n",
      "Epoch 31 Batch: 38 Train Loss: 0.29669851064682007\n",
      "Epoch 31 Batch: 40 Train Loss: 0.2808127999305725\n",
      "Epoch 31 Batch: 42 Train Loss: 0.061802297830581665\n",
      "Epoch 31 Batch: 44 Train Loss: 0.09932129830121994\n",
      "Epoch 31 Batch: 46 Train Loss: 0.49282002449035645\n",
      "Epoch 31 Batch: 48 Train Loss: 0.3214434087276459\n",
      "Epoch 31 Batch: 50 Train Loss: 0.2271120548248291\n",
      "Epoch 31 Batch: 52 Train Loss: 0.1161901131272316\n",
      "Epoch 31 Batch: 54 Train Loss: 0.08505658060312271\n",
      "Epoch 31 Batch: 56 Train Loss: 0.0447746142745018\n",
      "Epoch 31 Batch: 58 Train Loss: 0.055402420461177826\n",
      "Epoch 31 Batch: 60 Train Loss: 0.2650076746940613\n",
      "Epoch 31 Batch: 62 Train Loss: 0.09607196599245071\n",
      "Epoch 31 Batch: 64 Train Loss: 0.06975573301315308\n",
      "Epoch 31 Batch: 66 Train Loss: 0.07288665324449539\n",
      "Epoch 31 Batch: 68 Train Loss: 0.27538540959358215\n",
      "Epoch 31 Batch: 70 Train Loss: 0.3259137272834778\n",
      "Epoch 31 Batch: 72 Train Loss: 0.03977451100945473\n",
      "Epoch 31 Batch: 74 Train Loss: 0.05852431058883667\n",
      "Epoch 31 Batch: 76 Train Loss: 0.04044562578201294\n",
      "Epoch 31 Batch: 78 Train Loss: 0.31773683428764343\n",
      "Epoch 31 Batch: 80 Train Loss: 0.032676488161087036\n",
      "Epoch 31 Batch: 82 Train Loss: 0.02446317858994007\n",
      "Epoch 31 Batch: 84 Train Loss: 0.04371344670653343\n",
      "Epoch 31 Batch: 86 Train Loss: 0.2862291932106018\n",
      "Epoch 31 Batch: 88 Train Loss: 0.03335617855191231\n",
      "Epoch 31 Batch: 90 Train Loss: 0.02680782973766327\n",
      "Epoch 31 Batch: 92 Train Loss: 0.26016896963119507\n",
      "Epoch 31 Batch: 94 Train Loss: 0.025696352124214172\n",
      "Epoch 31 Batch: 96 Train Loss: 0.02999575063586235\n",
      "Epoch 31 Batch: 98 Train Loss: 0.028603706508874893\n",
      "Epoch 31 Batch: 100 Train Loss: 0.3273751735687256\n",
      "Epoch 31 Batch: 102 Train Loss: 0.2773684561252594\n",
      "Epoch 31 Batch: 104 Train Loss: 0.3210698366165161\n",
      "Epoch 31 Batch: 106 Train Loss: 0.032084982842206955\n",
      "Epoch 31 Batch: 108 Train Loss: 0.3313484489917755\n",
      "Epoch 31 Batch: 110 Train Loss: 0.27637431025505066\n",
      "Epoch 31 Batch: 112 Train Loss: 0.08334159106016159\n",
      "Epoch 31 Batch: 114 Train Loss: 0.09411658346652985\n",
      "Epoch 31 Batch: 116 Train Loss: 0.07301183044910431\n",
      "Epoch 31 Batch: 118 Train Loss: 0.04703493043780327\n",
      "Epoch 31 Batch: 120 Train Loss: 0.48264437913894653\n",
      "Epoch 31 Batch: 122 Train Loss: 0.8953801989555359\n",
      "Epoch 31 Batch: 124 Train Loss: 0.07862818241119385\n",
      "Epoch 31 Batch: 126 Train Loss: 0.12113665044307709\n",
      "Epoch 31 Batch: 128 Train Loss: 0.2588326930999756\n",
      "Epoch 31 Batch: 130 Train Loss: 0.05813400074839592\n",
      "Epoch 31 Batch: 132 Train Loss: 0.08291112631559372\n",
      "Epoch 31 Batch: 134 Train Loss: 0.28042060136795044\n",
      "Epoch 31 Batch: 136 Train Loss: 0.2779408395290375\n",
      "Epoch 31 Batch: 138 Train Loss: 0.06573490798473358\n",
      "Epoch 31 Batch: 140 Train Loss: 0.06591780483722687\n",
      "Epoch 31 Batch: 142 Train Loss: 0.0823284238576889\n",
      "Epoch 31 Batch: 144 Train Loss: 0.037741973996162415\n",
      "Epoch 31 Batch: 146 Train Loss: 0.0579686164855957\n",
      "Epoch 31 Batch: 148 Train Loss: 0.019960839301347733\n",
      "Epoch 31 Batch: 150 Train Loss: 0.2636946737766266\n",
      "Epoch 31 Batch: 152 Train Loss: 0.040018074214458466\n",
      "Epoch 31 Batch: 154 Train Loss: 0.01606403850018978\n",
      "Epoch 31 Batch: 156 Train Loss: 0.2919495701789856\n",
      "Epoch 31 Batch: 158 Train Loss: 0.03929780423641205\n",
      "Epoch 31 Batch: 160 Train Loss: 0.31118205189704895\n",
      "Epoch 32 Batch: 2 Train Loss: 0.036198940128088\n",
      "Epoch 32 Batch: 4 Train Loss: 0.04703453928232193\n",
      "Epoch 32 Batch: 6 Train Loss: 0.02382647432386875\n",
      "Epoch 32 Batch: 8 Train Loss: 0.3444591164588928\n",
      "Epoch 32 Batch: 10 Train Loss: 0.50154048204422\n",
      "Epoch 32 Batch: 12 Train Loss: 0.02372046932578087\n",
      "Epoch 32 Batch: 14 Train Loss: 0.034427184611558914\n",
      "Epoch 32 Batch: 16 Train Loss: 0.05179803818464279\n",
      "Epoch 32 Batch: 18 Train Loss: 0.8150351643562317\n",
      "Epoch 32 Batch: 20 Train Loss: 0.04031751677393913\n",
      "Epoch 32 Batch: 22 Train Loss: 0.3132503926753998\n",
      "Epoch 32 Batch: 24 Train Loss: 0.04114106670022011\n",
      "Epoch 32 Batch: 26 Train Loss: 0.06660045683383942\n",
      "Epoch 32 Batch: 28 Train Loss: 0.06088403984904289\n",
      "Epoch 32 Batch: 30 Train Loss: 0.06781971454620361\n",
      "Epoch 32 Batch: 32 Train Loss: 0.034528084099292755\n",
      "Epoch 32 Batch: 34 Train Loss: 0.05580013990402222\n",
      "Epoch 32 Batch: 36 Train Loss: 0.050172965973615646\n",
      "Epoch 32 Batch: 38 Train Loss: 0.026933226734399796\n",
      "Epoch 32 Batch: 40 Train Loss: 0.051286935806274414\n",
      "Epoch 32 Batch: 42 Train Loss: 0.036045171320438385\n",
      "Epoch 32 Batch: 44 Train Loss: 0.039789557456970215\n",
      "Epoch 32 Batch: 46 Train Loss: 0.05192568153142929\n",
      "Epoch 32 Batch: 48 Train Loss: 0.30499380826950073\n",
      "Epoch 32 Batch: 50 Train Loss: 0.04201304540038109\n",
      "Epoch 32 Batch: 52 Train Loss: 0.03247331455349922\n",
      "Epoch 32 Batch: 54 Train Loss: 0.5623050928115845\n",
      "Epoch 32 Batch: 56 Train Loss: 0.0329497829079628\n",
      "Epoch 32 Batch: 58 Train Loss: 0.51584792137146\n",
      "Epoch 32 Batch: 60 Train Loss: 0.24698975682258606\n",
      "Epoch 32 Batch: 62 Train Loss: 0.28383195400238037\n",
      "Epoch 32 Batch: 64 Train Loss: 0.44637173414230347\n",
      "Epoch 32 Batch: 66 Train Loss: 0.09295117110013962\n",
      "Epoch 32 Batch: 68 Train Loss: 0.10184057056903839\n",
      "Epoch 32 Batch: 70 Train Loss: 0.10849344730377197\n",
      "Epoch 32 Batch: 72 Train Loss: 0.5025200843811035\n",
      "Epoch 32 Batch: 74 Train Loss: 0.2929893434047699\n",
      "Epoch 32 Batch: 76 Train Loss: 0.3053974509239197\n",
      "Epoch 32 Batch: 78 Train Loss: 0.04445769637823105\n",
      "Epoch 32 Batch: 80 Train Loss: 0.682745635509491\n",
      "Epoch 32 Batch: 82 Train Loss: 0.07124025374650955\n",
      "Epoch 32 Batch: 84 Train Loss: 0.10328128188848495\n",
      "Epoch 32 Batch: 86 Train Loss: 0.05926109105348587\n",
      "Epoch 32 Batch: 88 Train Loss: 0.07975360751152039\n",
      "Epoch 32 Batch: 90 Train Loss: 0.06185726448893547\n",
      "Epoch 32 Batch: 92 Train Loss: 0.07415270060300827\n",
      "Epoch 32 Batch: 94 Train Loss: 0.030667942017316818\n",
      "Epoch 32 Batch: 96 Train Loss: 0.29686886072158813\n",
      "Epoch 32 Batch: 98 Train Loss: 0.04651167243719101\n",
      "Epoch 32 Batch: 100 Train Loss: 0.31443578004837036\n",
      "Epoch 32 Batch: 102 Train Loss: 0.04738146811723709\n",
      "Epoch 32 Batch: 104 Train Loss: 0.3328520655632019\n",
      "Epoch 32 Batch: 106 Train Loss: 0.04608675837516785\n",
      "Epoch 32 Batch: 108 Train Loss: 0.5394299030303955\n",
      "Epoch 32 Batch: 110 Train Loss: 0.05126636102795601\n",
      "Epoch 32 Batch: 112 Train Loss: 0.0239536315202713\n",
      "Epoch 32 Batch: 114 Train Loss: 0.03805312141776085\n",
      "Epoch 32 Batch: 116 Train Loss: 0.5081635117530823\n",
      "Epoch 32 Batch: 118 Train Loss: 0.06484267860651016\n",
      "Epoch 32 Batch: 120 Train Loss: 0.11020706593990326\n",
      "Epoch 32 Batch: 122 Train Loss: 0.266539603471756\n",
      "Epoch 32 Batch: 124 Train Loss: 0.04417607560753822\n",
      "Epoch 32 Batch: 126 Train Loss: 0.4801625609397888\n",
      "Epoch 32 Batch: 128 Train Loss: 0.2895148694515228\n",
      "Epoch 32 Batch: 130 Train Loss: 0.08659467101097107\n",
      "Epoch 32 Batch: 132 Train Loss: 0.05277808755636215\n",
      "Epoch 32 Batch: 134 Train Loss: 0.2729766070842743\n",
      "Epoch 32 Batch: 136 Train Loss: 0.05532338470220566\n",
      "Epoch 32 Batch: 138 Train Loss: 0.04707023501396179\n",
      "Epoch 32 Batch: 140 Train Loss: 0.042456503957509995\n",
      "Epoch 32 Batch: 142 Train Loss: 0.03473591059446335\n",
      "Epoch 32 Batch: 144 Train Loss: 0.3154042661190033\n",
      "Epoch 32 Batch: 146 Train Loss: 0.30104461312294006\n",
      "Epoch 32 Batch: 148 Train Loss: 0.29597148299217224\n",
      "Epoch 32 Batch: 150 Train Loss: 0.04047847166657448\n",
      "Epoch 32 Batch: 152 Train Loss: 0.0406189039349556\n",
      "Epoch 32 Batch: 154 Train Loss: 0.04048255831003189\n",
      "Epoch 32 Batch: 156 Train Loss: 0.2627243399620056\n",
      "Epoch 32 Batch: 158 Train Loss: 0.02593422494828701\n",
      "Epoch 32 Batch: 160 Train Loss: 0.07176805287599564\n",
      "Epoch 33 Batch: 2 Train Loss: 0.25189346075057983\n",
      "Epoch 33 Batch: 4 Train Loss: 0.4664640426635742\n",
      "Epoch 33 Batch: 6 Train Loss: 0.06940533965826035\n",
      "Epoch 33 Batch: 8 Train Loss: 0.28220468759536743\n",
      "Epoch 33 Batch: 10 Train Loss: 0.074822798371315\n",
      "Epoch 33 Batch: 12 Train Loss: 0.037963736802339554\n",
      "Epoch 33 Batch: 14 Train Loss: 0.04034370183944702\n",
      "Epoch 33 Batch: 16 Train Loss: 0.06596089899539948\n",
      "Epoch 33 Batch: 18 Train Loss: 0.03825272247195244\n",
      "Epoch 33 Batch: 20 Train Loss: 0.04828335717320442\n",
      "Epoch 33 Batch: 22 Train Loss: 0.049050312489271164\n",
      "Epoch 33 Batch: 24 Train Loss: 0.03592585399746895\n",
      "Epoch 33 Batch: 26 Train Loss: 0.2741037905216217\n",
      "Epoch 33 Batch: 28 Train Loss: 0.042618729174137115\n",
      "Epoch 33 Batch: 30 Train Loss: 0.8194189071655273\n",
      "Epoch 33 Batch: 32 Train Loss: 0.02819070592522621\n",
      "Epoch 33 Batch: 34 Train Loss: 0.3120279312133789\n",
      "Epoch 33 Batch: 36 Train Loss: 0.27807116508483887\n",
      "Epoch 33 Batch: 38 Train Loss: 0.04058249667286873\n",
      "Epoch 33 Batch: 40 Train Loss: 0.962422251701355\n",
      "Epoch 33 Batch: 42 Train Loss: 0.09038800746202469\n",
      "Epoch 33 Batch: 44 Train Loss: 0.4917353093624115\n",
      "Epoch 33 Batch: 46 Train Loss: 0.27034419775009155\n",
      "Epoch 33 Batch: 48 Train Loss: 0.08789239823818207\n",
      "Epoch 33 Batch: 50 Train Loss: 0.0768694207072258\n",
      "Epoch 33 Batch: 52 Train Loss: 0.25563371181488037\n",
      "Epoch 33 Batch: 54 Train Loss: 0.09990441799163818\n",
      "Epoch 33 Batch: 56 Train Loss: 0.06716056913137436\n",
      "Epoch 33 Batch: 58 Train Loss: 0.2793748378753662\n",
      "Epoch 33 Batch: 60 Train Loss: 0.2894771695137024\n",
      "Epoch 33 Batch: 62 Train Loss: 0.2805767059326172\n",
      "Epoch 33 Batch: 64 Train Loss: 0.05520669370889664\n",
      "Epoch 33 Batch: 66 Train Loss: 0.04306928440928459\n",
      "Epoch 33 Batch: 68 Train Loss: 0.30262643098831177\n",
      "Epoch 33 Batch: 70 Train Loss: 0.0299075935035944\n",
      "Epoch 33 Batch: 72 Train Loss: 0.08095155656337738\n",
      "Epoch 33 Batch: 74 Train Loss: 0.321102499961853\n",
      "Epoch 33 Batch: 76 Train Loss: 0.03382231667637825\n",
      "Epoch 33 Batch: 78 Train Loss: 0.05083081126213074\n",
      "Epoch 33 Batch: 80 Train Loss: 0.039137471467256546\n",
      "Epoch 33 Batch: 82 Train Loss: 0.29674774408340454\n",
      "Epoch 33 Batch: 84 Train Loss: 0.06117880344390869\n",
      "Epoch 33 Batch: 86 Train Loss: 0.0322515144944191\n",
      "Epoch 33 Batch: 88 Train Loss: 0.0393822155892849\n",
      "Epoch 33 Batch: 90 Train Loss: 0.30974870920181274\n",
      "Epoch 33 Batch: 92 Train Loss: 0.026738261803984642\n",
      "Epoch 33 Batch: 94 Train Loss: 0.2657949924468994\n",
      "Epoch 33 Batch: 96 Train Loss: 0.052631013095378876\n",
      "Epoch 33 Batch: 98 Train Loss: 0.04191073402762413\n",
      "Epoch 33 Batch: 100 Train Loss: 0.07123611867427826\n",
      "Epoch 33 Batch: 102 Train Loss: 0.03673205524682999\n",
      "Epoch 33 Batch: 104 Train Loss: 0.07093296200037003\n",
      "Epoch 33 Batch: 106 Train Loss: 0.2598193287849426\n",
      "Epoch 33 Batch: 108 Train Loss: 0.568558394908905\n",
      "Epoch 33 Batch: 110 Train Loss: 0.2693503499031067\n",
      "Epoch 33 Batch: 112 Train Loss: 0.3051052689552307\n",
      "Epoch 33 Batch: 114 Train Loss: 0.3027592599391937\n",
      "Epoch 33 Batch: 116 Train Loss: 0.34375208616256714\n",
      "Epoch 33 Batch: 118 Train Loss: 0.024370893836021423\n",
      "Epoch 33 Batch: 120 Train Loss: 0.057056158781051636\n",
      "Epoch 33 Batch: 122 Train Loss: 0.27847403287887573\n",
      "Epoch 33 Batch: 124 Train Loss: 0.30178213119506836\n",
      "Epoch 33 Batch: 126 Train Loss: 0.05406472831964493\n",
      "Epoch 33 Batch: 128 Train Loss: 0.04375617951154709\n",
      "Epoch 33 Batch: 130 Train Loss: 0.28971031308174133\n",
      "Epoch 33 Batch: 132 Train Loss: 0.05537445470690727\n",
      "Epoch 33 Batch: 134 Train Loss: 0.2772655189037323\n",
      "Epoch 33 Batch: 136 Train Loss: 0.054140008985996246\n",
      "Epoch 33 Batch: 138 Train Loss: 0.054389018565416336\n",
      "Epoch 33 Batch: 140 Train Loss: 0.2858082056045532\n",
      "Epoch 33 Batch: 142 Train Loss: 0.5458351373672485\n",
      "Epoch 33 Batch: 144 Train Loss: 0.2815711796283722\n",
      "Epoch 33 Batch: 146 Train Loss: 0.4831114709377289\n",
      "Epoch 33 Batch: 148 Train Loss: 0.06874777376651764\n",
      "Epoch 33 Batch: 150 Train Loss: 0.28744828701019287\n",
      "Epoch 33 Batch: 152 Train Loss: 0.2664709687232971\n",
      "Epoch 33 Batch: 154 Train Loss: 0.32047054171562195\n",
      "Epoch 33 Batch: 156 Train Loss: 0.07801970094442368\n",
      "Epoch 33 Batch: 158 Train Loss: 0.06036282703280449\n",
      "Epoch 33 Batch: 160 Train Loss: 0.2976343035697937\n",
      "Epoch 34 Batch: 2 Train Loss: 0.08737359195947647\n",
      "Epoch 34 Batch: 4 Train Loss: 0.0629441887140274\n",
      "Epoch 34 Batch: 6 Train Loss: 0.04229789227247238\n",
      "Epoch 34 Batch: 8 Train Loss: 0.28828370571136475\n",
      "Epoch 34 Batch: 10 Train Loss: 0.04171710088849068\n",
      "Epoch 34 Batch: 12 Train Loss: 0.2700832784175873\n",
      "Epoch 34 Batch: 14 Train Loss: 0.03932288661599159\n",
      "Epoch 34 Batch: 16 Train Loss: 0.05678342655301094\n",
      "Epoch 34 Batch: 18 Train Loss: 0.052552592009305954\n",
      "Epoch 34 Batch: 20 Train Loss: 0.04621706157922745\n",
      "Epoch 34 Batch: 22 Train Loss: 0.3149304687976837\n",
      "Epoch 34 Batch: 24 Train Loss: 0.030385509133338928\n",
      "Epoch 34 Batch: 26 Train Loss: 0.019483545795083046\n",
      "Epoch 34 Batch: 28 Train Loss: 0.3473880887031555\n",
      "Epoch 34 Batch: 30 Train Loss: 0.04597385972738266\n",
      "Epoch 34 Batch: 32 Train Loss: 0.333747923374176\n",
      "Epoch 34 Batch: 34 Train Loss: 0.024594807997345924\n",
      "Epoch 34 Batch: 36 Train Loss: 0.2700483500957489\n",
      "Epoch 34 Batch: 38 Train Loss: 0.03961118310689926\n",
      "Epoch 34 Batch: 40 Train Loss: 0.5974047780036926\n",
      "Epoch 34 Batch: 42 Train Loss: 0.25205737352371216\n",
      "Epoch 34 Batch: 44 Train Loss: 0.03404228016734123\n",
      "Epoch 34 Batch: 46 Train Loss: 0.02546689473092556\n",
      "Epoch 34 Batch: 48 Train Loss: 0.05459229275584221\n",
      "Epoch 34 Batch: 50 Train Loss: 0.027511948719620705\n",
      "Epoch 34 Batch: 52 Train Loss: 0.03684253618121147\n",
      "Epoch 34 Batch: 54 Train Loss: 0.029322704300284386\n",
      "Epoch 34 Batch: 56 Train Loss: 0.03687875345349312\n",
      "Epoch 34 Batch: 58 Train Loss: 0.31721583008766174\n",
      "Epoch 34 Batch: 60 Train Loss: 0.31802135705947876\n",
      "Epoch 34 Batch: 62 Train Loss: 0.04004015028476715\n",
      "Epoch 34 Batch: 64 Train Loss: 0.051752619445323944\n",
      "Epoch 34 Batch: 66 Train Loss: 0.5107235312461853\n",
      "Epoch 34 Batch: 68 Train Loss: 0.04532821476459503\n",
      "Epoch 34 Batch: 70 Train Loss: 0.06797867268323898\n",
      "Epoch 34 Batch: 72 Train Loss: 0.28526586294174194\n",
      "Epoch 34 Batch: 74 Train Loss: 0.05747547745704651\n",
      "Epoch 34 Batch: 76 Train Loss: 0.04322497919201851\n",
      "Epoch 34 Batch: 78 Train Loss: 0.05660172179341316\n",
      "Epoch 34 Batch: 80 Train Loss: 0.0806368738412857\n",
      "Epoch 34 Batch: 82 Train Loss: 0.2965927720069885\n",
      "Epoch 34 Batch: 84 Train Loss: 0.05835939198732376\n",
      "Epoch 34 Batch: 86 Train Loss: 0.06491327285766602\n",
      "Epoch 34 Batch: 88 Train Loss: 0.04243713989853859\n",
      "Epoch 34 Batch: 90 Train Loss: 0.05859944969415665\n",
      "Epoch 34 Batch: 92 Train Loss: 0.28025829792022705\n",
      "Epoch 34 Batch: 94 Train Loss: 0.045537643134593964\n",
      "Epoch 34 Batch: 96 Train Loss: 0.2720104157924652\n",
      "Epoch 34 Batch: 98 Train Loss: 0.029604589566588402\n",
      "Epoch 34 Batch: 100 Train Loss: 0.3210197985172272\n",
      "Epoch 34 Batch: 102 Train Loss: 0.3478778898715973\n",
      "Epoch 34 Batch: 104 Train Loss: 0.01411330234259367\n",
      "Epoch 34 Batch: 106 Train Loss: 0.5329831838607788\n",
      "Epoch 34 Batch: 108 Train Loss: 0.02827085182070732\n",
      "Epoch 34 Batch: 110 Train Loss: 0.06758089363574982\n",
      "Epoch 34 Batch: 112 Train Loss: 0.5307687520980835\n",
      "Epoch 34 Batch: 114 Train Loss: 0.07501878589391708\n",
      "Epoch 34 Batch: 116 Train Loss: 0.08798058331012726\n",
      "Epoch 34 Batch: 118 Train Loss: 0.2766347825527191\n",
      "Epoch 34 Batch: 120 Train Loss: 0.05715516209602356\n",
      "Epoch 34 Batch: 122 Train Loss: 0.08640509843826294\n",
      "Epoch 34 Batch: 124 Train Loss: 0.047221217304468155\n",
      "Epoch 34 Batch: 126 Train Loss: 0.06470435857772827\n",
      "Epoch 34 Batch: 128 Train Loss: 0.04330148547887802\n",
      "Epoch 34 Batch: 130 Train Loss: 0.2595583498477936\n",
      "Epoch 34 Batch: 132 Train Loss: 0.03237229958176613\n",
      "Epoch 34 Batch: 134 Train Loss: 0.04217074066400528\n",
      "Epoch 34 Batch: 136 Train Loss: 0.3284759223461151\n",
      "Epoch 34 Batch: 138 Train Loss: 0.5354414582252502\n",
      "Epoch 34 Batch: 140 Train Loss: 0.08473353832960129\n",
      "Epoch 34 Batch: 142 Train Loss: 0.08477914333343506\n",
      "Epoch 34 Batch: 144 Train Loss: 0.08061442524194717\n",
      "Epoch 34 Batch: 146 Train Loss: 0.0814637690782547\n",
      "Epoch 34 Batch: 148 Train Loss: 0.46759796142578125\n",
      "Epoch 34 Batch: 150 Train Loss: 0.03746599704027176\n",
      "Epoch 34 Batch: 152 Train Loss: 0.07217354327440262\n",
      "Epoch 34 Batch: 154 Train Loss: 0.06166096404194832\n",
      "Epoch 34 Batch: 156 Train Loss: 0.3128514587879181\n",
      "Epoch 34 Batch: 158 Train Loss: 0.03830829635262489\n",
      "Epoch 34 Batch: 160 Train Loss: 0.27804917097091675\n",
      "Epoch 35 Batch: 2 Train Loss: 0.2839128375053406\n",
      "Epoch 35 Batch: 4 Train Loss: 0.5042667984962463\n",
      "Epoch 35 Batch: 6 Train Loss: 0.27103888988494873\n",
      "Epoch 35 Batch: 8 Train Loss: 0.3167186677455902\n",
      "Epoch 35 Batch: 10 Train Loss: 0.04777028039097786\n",
      "Epoch 35 Batch: 12 Train Loss: 0.27260714769363403\n",
      "Epoch 35 Batch: 14 Train Loss: 0.30161064863204956\n",
      "Epoch 35 Batch: 16 Train Loss: 0.05324960872530937\n",
      "Epoch 35 Batch: 18 Train Loss: 0.08940956741571426\n",
      "Epoch 35 Batch: 20 Train Loss: 0.2898303270339966\n",
      "Epoch 35 Batch: 22 Train Loss: 0.2755427658557892\n",
      "Epoch 35 Batch: 24 Train Loss: 0.251203715801239\n",
      "Epoch 35 Batch: 26 Train Loss: 0.48319777846336365\n",
      "Epoch 35 Batch: 28 Train Loss: 0.25914186239242554\n",
      "Epoch 35 Batch: 30 Train Loss: 0.09410829842090607\n",
      "Epoch 35 Batch: 32 Train Loss: 0.21158650517463684\n",
      "Epoch 35 Batch: 34 Train Loss: 0.27520081400871277\n",
      "Epoch 35 Batch: 36 Train Loss: 0.26442208886146545\n",
      "Epoch 35 Batch: 38 Train Loss: 0.06825907528400421\n",
      "Epoch 35 Batch: 40 Train Loss: 0.07750628888607025\n",
      "Epoch 35 Batch: 42 Train Loss: 0.04953974485397339\n",
      "Epoch 35 Batch: 44 Train Loss: 0.45351848006248474\n",
      "Epoch 35 Batch: 46 Train Loss: 0.28384214639663696\n",
      "Epoch 35 Batch: 48 Train Loss: 0.07988275587558746\n",
      "Epoch 35 Batch: 50 Train Loss: 0.09893761575222015\n",
      "Epoch 35 Batch: 52 Train Loss: 0.05239417403936386\n",
      "Epoch 35 Batch: 54 Train Loss: 0.03222433477640152\n",
      "Epoch 35 Batch: 56 Train Loss: 0.06107745319604874\n",
      "Epoch 35 Batch: 58 Train Loss: 0.05673549324274063\n",
      "Epoch 35 Batch: 60 Train Loss: 0.05325378105044365\n",
      "Epoch 35 Batch: 62 Train Loss: 0.07049846649169922\n",
      "Epoch 35 Batch: 64 Train Loss: 0.023309778422117233\n",
      "Epoch 35 Batch: 66 Train Loss: 0.032355472445487976\n",
      "Epoch 35 Batch: 68 Train Loss: 0.056204844266176224\n",
      "Epoch 35 Batch: 70 Train Loss: 0.05865117907524109\n",
      "Epoch 35 Batch: 72 Train Loss: 0.26674574613571167\n",
      "Epoch 35 Batch: 74 Train Loss: 0.08009803295135498\n",
      "Epoch 35 Batch: 76 Train Loss: 0.051850151270627975\n",
      "Epoch 35 Batch: 78 Train Loss: 0.34310364723205566\n",
      "Epoch 35 Batch: 80 Train Loss: 0.2972497045993805\n",
      "Epoch 35 Batch: 82 Train Loss: 0.2679732143878937\n",
      "Epoch 35 Batch: 84 Train Loss: 0.3070827126502991\n",
      "Epoch 35 Batch: 86 Train Loss: 0.031480468809604645\n",
      "Epoch 35 Batch: 88 Train Loss: 0.029164915904402733\n",
      "Epoch 35 Batch: 90 Train Loss: 0.03710278123617172\n",
      "Epoch 35 Batch: 92 Train Loss: 0.05667920038104057\n",
      "Epoch 35 Batch: 94 Train Loss: 0.3147953152656555\n",
      "Epoch 35 Batch: 96 Train Loss: 0.07249400019645691\n",
      "Epoch 35 Batch: 98 Train Loss: 0.06514014303684235\n",
      "Epoch 35 Batch: 100 Train Loss: 0.26788532733917236\n",
      "Epoch 35 Batch: 102 Train Loss: 0.5050294995307922\n",
      "Epoch 35 Batch: 104 Train Loss: 0.07545335590839386\n",
      "Epoch 35 Batch: 106 Train Loss: 0.06683520972728729\n",
      "Epoch 35 Batch: 108 Train Loss: 0.03622271865606308\n",
      "Epoch 35 Batch: 110 Train Loss: 0.07432228326797485\n",
      "Epoch 35 Batch: 112 Train Loss: 0.05123777315020561\n",
      "Epoch 35 Batch: 114 Train Loss: 0.31346648931503296\n",
      "Epoch 35 Batch: 116 Train Loss: 0.3036046326160431\n",
      "Epoch 35 Batch: 118 Train Loss: 0.522993266582489\n",
      "Epoch 35 Batch: 120 Train Loss: 0.058548130095005035\n",
      "Epoch 35 Batch: 122 Train Loss: 0.0498206727206707\n",
      "Epoch 35 Batch: 124 Train Loss: 0.0572974868118763\n",
      "Epoch 35 Batch: 126 Train Loss: 0.23649117350578308\n",
      "Epoch 35 Batch: 128 Train Loss: 0.10584495961666107\n",
      "Epoch 35 Batch: 130 Train Loss: 0.04852820187807083\n",
      "Epoch 35 Batch: 132 Train Loss: 0.31235426664352417\n",
      "Epoch 35 Batch: 134 Train Loss: 0.0604175440967083\n",
      "Epoch 35 Batch: 136 Train Loss: 0.3069053888320923\n",
      "Epoch 35 Batch: 138 Train Loss: 0.07230736315250397\n",
      "Epoch 35 Batch: 140 Train Loss: 0.06155959516763687\n",
      "Epoch 35 Batch: 142 Train Loss: 0.56736820936203\n",
      "Epoch 35 Batch: 144 Train Loss: 0.06097552925348282\n",
      "Epoch 35 Batch: 146 Train Loss: 0.024597641080617905\n",
      "Epoch 35 Batch: 148 Train Loss: 0.025375237688422203\n",
      "Epoch 35 Batch: 150 Train Loss: 0.05137963220477104\n",
      "Epoch 35 Batch: 152 Train Loss: 0.05176166445016861\n",
      "Epoch 35 Batch: 154 Train Loss: 0.5699067711830139\n",
      "Epoch 35 Batch: 156 Train Loss: 0.061779189854860306\n",
      "Epoch 35 Batch: 158 Train Loss: 0.051571525633335114\n",
      "Epoch 35 Batch: 160 Train Loss: 0.049720585346221924\n",
      "Epoch 36 Batch: 2 Train Loss: 0.5893003940582275\n",
      "Epoch 36 Batch: 4 Train Loss: 0.8332402110099792\n",
      "Epoch 36 Batch: 6 Train Loss: 0.3194158673286438\n",
      "Epoch 36 Batch: 8 Train Loss: 0.04725509136915207\n",
      "Epoch 36 Batch: 10 Train Loss: 0.06968482583761215\n",
      "Epoch 36 Batch: 12 Train Loss: 0.07678963243961334\n",
      "Epoch 36 Batch: 14 Train Loss: 0.05070086196064949\n",
      "Epoch 36 Batch: 16 Train Loss: 0.05759938433766365\n",
      "Epoch 36 Batch: 18 Train Loss: 0.07751751691102982\n",
      "Epoch 36 Batch: 20 Train Loss: 0.031240368261933327\n",
      "Epoch 36 Batch: 22 Train Loss: 0.048857174813747406\n",
      "Epoch 36 Batch: 24 Train Loss: 0.06177804619073868\n",
      "Epoch 36 Batch: 26 Train Loss: 0.06343476474285126\n",
      "Epoch 36 Batch: 28 Train Loss: 0.05107667297124863\n",
      "Epoch 36 Batch: 30 Train Loss: 0.03810172900557518\n",
      "Epoch 36 Batch: 32 Train Loss: 0.30472856760025024\n",
      "Epoch 36 Batch: 34 Train Loss: 0.2929941713809967\n",
      "Epoch 36 Batch: 36 Train Loss: 0.0535600371658802\n",
      "Epoch 36 Batch: 38 Train Loss: 0.039752211421728134\n",
      "Epoch 36 Batch: 40 Train Loss: 0.03835916891694069\n",
      "Epoch 36 Batch: 42 Train Loss: 0.02439255639910698\n",
      "Epoch 36 Batch: 44 Train Loss: 0.3154662251472473\n",
      "Epoch 36 Batch: 46 Train Loss: 0.3201773166656494\n",
      "Epoch 36 Batch: 48 Train Loss: 0.03209420666098595\n",
      "Epoch 36 Batch: 50 Train Loss: 0.2972811460494995\n",
      "Epoch 36 Batch: 52 Train Loss: 0.0414121076464653\n",
      "Epoch 36 Batch: 54 Train Loss: 0.040943581610918045\n",
      "Epoch 36 Batch: 56 Train Loss: 0.04883681982755661\n",
      "Epoch 36 Batch: 58 Train Loss: 0.32465308904647827\n",
      "Epoch 36 Batch: 60 Train Loss: 0.05439281463623047\n",
      "Epoch 36 Batch: 62 Train Loss: 0.04054494947195053\n",
      "Epoch 36 Batch: 64 Train Loss: 0.035834748297929764\n",
      "Epoch 36 Batch: 66 Train Loss: 0.04105880856513977\n",
      "Epoch 36 Batch: 68 Train Loss: 0.038896895945072174\n",
      "Epoch 36 Batch: 70 Train Loss: 0.3113604485988617\n",
      "Epoch 36 Batch: 72 Train Loss: 0.0712023377418518\n",
      "Epoch 36 Batch: 74 Train Loss: 0.046181343495845795\n",
      "Epoch 36 Batch: 76 Train Loss: 0.070235975086689\n",
      "Epoch 36 Batch: 78 Train Loss: 0.26528874039649963\n",
      "Epoch 36 Batch: 80 Train Loss: 0.2613351047039032\n",
      "Epoch 36 Batch: 82 Train Loss: 0.30758294463157654\n",
      "Epoch 36 Batch: 84 Train Loss: 0.03514720872044563\n",
      "Epoch 36 Batch: 86 Train Loss: 0.043428659439086914\n",
      "Epoch 36 Batch: 88 Train Loss: 0.051507264375686646\n",
      "Epoch 36 Batch: 90 Train Loss: 0.3221171796321869\n",
      "Epoch 36 Batch: 92 Train Loss: 0.27702847123146057\n",
      "Epoch 36 Batch: 94 Train Loss: 0.3311130106449127\n",
      "Epoch 36 Batch: 96 Train Loss: 0.26249438524246216\n",
      "Epoch 36 Batch: 98 Train Loss: 0.062403857707977295\n",
      "Epoch 36 Batch: 100 Train Loss: 0.25617510080337524\n",
      "Epoch 36 Batch: 102 Train Loss: 0.08144569396972656\n",
      "Epoch 36 Batch: 104 Train Loss: 0.06092982366681099\n",
      "Epoch 36 Batch: 106 Train Loss: 0.03774365410208702\n",
      "Epoch 36 Batch: 108 Train Loss: 0.06373880803585052\n",
      "Epoch 36 Batch: 110 Train Loss: 0.43476778268814087\n",
      "Epoch 36 Batch: 112 Train Loss: 0.041580479592084885\n",
      "Epoch 36 Batch: 114 Train Loss: 0.0874403566122055\n",
      "Epoch 36 Batch: 116 Train Loss: 0.05137413740158081\n",
      "Epoch 36 Batch: 118 Train Loss: 0.07046238332986832\n",
      "Epoch 36 Batch: 120 Train Loss: 0.039754729717969894\n",
      "Epoch 36 Batch: 122 Train Loss: 0.03153461590409279\n",
      "Epoch 36 Batch: 124 Train Loss: 0.27755677700042725\n",
      "Epoch 36 Batch: 126 Train Loss: 0.2627905309200287\n",
      "Epoch 36 Batch: 128 Train Loss: 0.05663447454571724\n",
      "Epoch 36 Batch: 130 Train Loss: 0.03596985340118408\n",
      "Epoch 36 Batch: 132 Train Loss: 0.05297163128852844\n",
      "Epoch 36 Batch: 134 Train Loss: 0.05652512237429619\n",
      "Epoch 36 Batch: 136 Train Loss: 0.2926645874977112\n",
      "Epoch 36 Batch: 138 Train Loss: 0.05757371708750725\n",
      "Epoch 36 Batch: 140 Train Loss: 0.3223820626735687\n",
      "Epoch 36 Batch: 142 Train Loss: 0.5812755227088928\n",
      "Epoch 36 Batch: 144 Train Loss: 0.3287554681301117\n",
      "Epoch 36 Batch: 146 Train Loss: 0.2802971303462982\n",
      "Epoch 36 Batch: 148 Train Loss: 0.2652493119239807\n",
      "Epoch 36 Batch: 150 Train Loss: 0.5007415413856506\n",
      "Epoch 36 Batch: 152 Train Loss: 0.09396649897098541\n",
      "Epoch 36 Batch: 154 Train Loss: 0.26070767641067505\n",
      "Epoch 36 Batch: 156 Train Loss: 0.2590089738368988\n",
      "Epoch 36 Batch: 158 Train Loss: 0.2847785949707031\n",
      "Epoch 36 Batch: 160 Train Loss: 0.28375959396362305\n",
      "Epoch 37 Batch: 2 Train Loss: 0.03265345096588135\n",
      "Epoch 37 Batch: 4 Train Loss: 0.0987928956747055\n",
      "Epoch 37 Batch: 6 Train Loss: 0.49901747703552246\n",
      "Epoch 37 Batch: 8 Train Loss: 0.08030726760625839\n",
      "Epoch 37 Batch: 10 Train Loss: 0.07356450706720352\n",
      "Epoch 37 Batch: 12 Train Loss: 0.08746702969074249\n",
      "Epoch 37 Batch: 14 Train Loss: 0.2627106010913849\n",
      "Epoch 37 Batch: 16 Train Loss: 0.2955648601055145\n",
      "Epoch 37 Batch: 18 Train Loss: 0.28623056411743164\n",
      "Epoch 37 Batch: 20 Train Loss: 0.04193488508462906\n",
      "Epoch 37 Batch: 22 Train Loss: 0.04736856371164322\n",
      "Epoch 37 Batch: 24 Train Loss: 0.05923189967870712\n",
      "Epoch 37 Batch: 26 Train Loss: 0.236715167760849\n",
      "Epoch 37 Batch: 28 Train Loss: 0.08251171559095383\n",
      "Epoch 37 Batch: 30 Train Loss: 0.0469416044652462\n",
      "Epoch 37 Batch: 32 Train Loss: 0.2949219048023224\n",
      "Epoch 37 Batch: 34 Train Loss: 0.2668692469596863\n",
      "Epoch 37 Batch: 36 Train Loss: 0.06686003506183624\n",
      "Epoch 37 Batch: 38 Train Loss: 0.47723522782325745\n",
      "Epoch 37 Batch: 40 Train Loss: 0.0654667541384697\n",
      "Epoch 37 Batch: 42 Train Loss: 0.07747222483158112\n",
      "Epoch 37 Batch: 44 Train Loss: 0.018692784011363983\n",
      "Epoch 37 Batch: 46 Train Loss: 0.02733856812119484\n",
      "Epoch 37 Batch: 48 Train Loss: 0.3418014943599701\n",
      "Epoch 37 Batch: 50 Train Loss: 0.28985947370529175\n",
      "Epoch 37 Batch: 52 Train Loss: 0.046249277889728546\n",
      "Epoch 37 Batch: 54 Train Loss: 0.04013240337371826\n",
      "Epoch 37 Batch: 56 Train Loss: 0.06601018458604813\n",
      "Epoch 37 Batch: 58 Train Loss: 0.055635321885347366\n",
      "Epoch 37 Batch: 60 Train Loss: 0.035727448761463165\n",
      "Epoch 37 Batch: 62 Train Loss: 0.3004796802997589\n",
      "Epoch 37 Batch: 64 Train Loss: 0.5885955095291138\n",
      "Epoch 37 Batch: 66 Train Loss: 0.272747278213501\n",
      "Epoch 37 Batch: 68 Train Loss: 0.05948963016271591\n",
      "Epoch 37 Batch: 70 Train Loss: 0.03546500951051712\n",
      "Epoch 37 Batch: 72 Train Loss: 0.3120560050010681\n",
      "Epoch 37 Batch: 74 Train Loss: 0.8405952453613281\n",
      "Epoch 37 Batch: 76 Train Loss: 0.31015822291374207\n",
      "Epoch 37 Batch: 78 Train Loss: 0.08661127835512161\n",
      "Epoch 37 Batch: 80 Train Loss: 0.274946391582489\n",
      "Epoch 37 Batch: 82 Train Loss: 0.09007149189710617\n",
      "Epoch 37 Batch: 84 Train Loss: 0.06405086815357208\n",
      "Epoch 37 Batch: 86 Train Loss: 0.28868332505226135\n",
      "Epoch 37 Batch: 88 Train Loss: 0.25913485884666443\n",
      "Epoch 37 Batch: 90 Train Loss: 0.04210386425256729\n",
      "Epoch 37 Batch: 92 Train Loss: 0.2718060612678528\n",
      "Epoch 37 Batch: 94 Train Loss: 0.04478204622864723\n",
      "Epoch 37 Batch: 96 Train Loss: 0.04764199256896973\n",
      "Epoch 37 Batch: 98 Train Loss: 0.056102775037288666\n",
      "Epoch 37 Batch: 100 Train Loss: 0.07175293564796448\n",
      "Epoch 37 Batch: 102 Train Loss: 0.292466938495636\n",
      "Epoch 37 Batch: 104 Train Loss: 0.02819647826254368\n",
      "Epoch 37 Batch: 106 Train Loss: 0.3118954300880432\n",
      "Epoch 37 Batch: 108 Train Loss: 0.2488798201084137\n",
      "Epoch 37 Batch: 110 Train Loss: 0.2666219174861908\n",
      "Epoch 37 Batch: 112 Train Loss: 0.2931126654148102\n",
      "Epoch 37 Batch: 114 Train Loss: 0.04341476038098335\n",
      "Epoch 37 Batch: 116 Train Loss: 0.052720196545124054\n",
      "Epoch 37 Batch: 118 Train Loss: 0.03144324943423271\n",
      "Epoch 37 Batch: 120 Train Loss: 0.06808395683765411\n",
      "Epoch 37 Batch: 122 Train Loss: 0.048499833792448044\n",
      "Epoch 37 Batch: 124 Train Loss: 0.035797275602817535\n",
      "Epoch 37 Batch: 126 Train Loss: 0.03414042294025421\n",
      "Epoch 37 Batch: 128 Train Loss: 0.042193103581666946\n",
      "Epoch 37 Batch: 130 Train Loss: 0.2840162515640259\n",
      "Epoch 37 Batch: 132 Train Loss: 0.3082389235496521\n",
      "Epoch 37 Batch: 134 Train Loss: 0.31724926829338074\n",
      "Epoch 37 Batch: 136 Train Loss: 0.2900020480155945\n",
      "Epoch 37 Batch: 138 Train Loss: 0.048015810549259186\n",
      "Epoch 37 Batch: 140 Train Loss: 0.04295153170824051\n",
      "Epoch 37 Batch: 142 Train Loss: 0.3453177809715271\n",
      "Epoch 37 Batch: 144 Train Loss: 0.3058018088340759\n",
      "Epoch 37 Batch: 146 Train Loss: 0.5414665937423706\n",
      "Epoch 37 Batch: 148 Train Loss: 0.2996656596660614\n",
      "Epoch 37 Batch: 150 Train Loss: 0.27326780557632446\n",
      "Epoch 37 Batch: 152 Train Loss: 0.06465382874011993\n",
      "Epoch 37 Batch: 154 Train Loss: 0.2624111771583557\n",
      "Epoch 37 Batch: 156 Train Loss: 0.04721217602491379\n",
      "Epoch 37 Batch: 158 Train Loss: 0.5089676976203918\n",
      "Epoch 37 Batch: 160 Train Loss: 0.29302430152893066\n",
      "Epoch 38 Batch: 2 Train Loss: 0.4494590759277344\n",
      "Epoch 38 Batch: 4 Train Loss: 0.10597105324268341\n",
      "Epoch 38 Batch: 6 Train Loss: 0.10354630649089813\n",
      "Epoch 38 Batch: 8 Train Loss: 0.2601594030857086\n",
      "Epoch 38 Batch: 10 Train Loss: 0.2184457778930664\n",
      "Epoch 38 Batch: 12 Train Loss: 0.03031555749475956\n",
      "Epoch 38 Batch: 14 Train Loss: 0.06925623118877411\n",
      "Epoch 38 Batch: 16 Train Loss: 0.056921541690826416\n",
      "Epoch 38 Batch: 18 Train Loss: 0.2746028006076813\n",
      "Epoch 38 Batch: 20 Train Loss: 0.057960063219070435\n",
      "Epoch 38 Batch: 22 Train Loss: 0.3109186291694641\n",
      "Epoch 38 Batch: 24 Train Loss: 0.2968018054962158\n",
      "Epoch 38 Batch: 26 Train Loss: 0.47047218680381775\n",
      "Epoch 38 Batch: 28 Train Loss: 0.28497934341430664\n",
      "Epoch 38 Batch: 30 Train Loss: 0.054923005402088165\n",
      "Epoch 38 Batch: 32 Train Loss: 0.08596689254045486\n",
      "Epoch 38 Batch: 34 Train Loss: 0.08329102396965027\n",
      "Epoch 38 Batch: 36 Train Loss: 0.5140058994293213\n",
      "Epoch 38 Batch: 38 Train Loss: 0.06116244196891785\n",
      "Epoch 38 Batch: 40 Train Loss: 0.10276713222265244\n",
      "Epoch 38 Batch: 42 Train Loss: 0.27843040227890015\n",
      "Epoch 38 Batch: 44 Train Loss: 0.04335562512278557\n",
      "Epoch 38 Batch: 46 Train Loss: 0.06922262907028198\n",
      "Epoch 38 Batch: 48 Train Loss: 0.09696581959724426\n",
      "Epoch 38 Batch: 50 Train Loss: 0.07268693298101425\n",
      "Epoch 38 Batch: 52 Train Loss: 0.2664281725883484\n",
      "Epoch 38 Batch: 54 Train Loss: 0.4831565320491791\n",
      "Epoch 38 Batch: 56 Train Loss: 0.029905950650572777\n",
      "Epoch 38 Batch: 58 Train Loss: 0.05761639401316643\n",
      "Epoch 38 Batch: 60 Train Loss: 0.30377399921417236\n",
      "Epoch 38 Batch: 62 Train Loss: 0.49122318625450134\n",
      "Epoch 38 Batch: 64 Train Loss: 0.07501311600208282\n",
      "Epoch 38 Batch: 66 Train Loss: 0.03518819808959961\n",
      "Epoch 38 Batch: 68 Train Loss: 0.029125500470399857\n",
      "Epoch 38 Batch: 70 Train Loss: 0.3096388578414917\n",
      "Epoch 38 Batch: 72 Train Loss: 0.047644153237342834\n",
      "Epoch 38 Batch: 74 Train Loss: 0.521506667137146\n",
      "Epoch 38 Batch: 76 Train Loss: 0.290502667427063\n",
      "Epoch 38 Batch: 78 Train Loss: 0.020688796415925026\n",
      "Epoch 38 Batch: 80 Train Loss: 0.2680889070034027\n",
      "Epoch 38 Batch: 82 Train Loss: 0.08461672067642212\n",
      "Epoch 38 Batch: 84 Train Loss: 0.07187076658010483\n",
      "Epoch 38 Batch: 86 Train Loss: 0.09443892538547516\n",
      "Epoch 38 Batch: 88 Train Loss: 0.3133191168308258\n",
      "Epoch 38 Batch: 90 Train Loss: 0.03258168324828148\n",
      "Epoch 38 Batch: 92 Train Loss: 0.04204172268509865\n",
      "Epoch 38 Batch: 94 Train Loss: 0.0419103279709816\n",
      "Epoch 38 Batch: 96 Train Loss: 0.024762388318777084\n",
      "Epoch 38 Batch: 98 Train Loss: 0.034284479916095734\n",
      "Epoch 38 Batch: 100 Train Loss: 0.035910800099372864\n",
      "Epoch 38 Batch: 102 Train Loss: 0.029418345540761948\n",
      "Epoch 38 Batch: 104 Train Loss: 0.04754728823900223\n",
      "Epoch 38 Batch: 106 Train Loss: 0.6327875852584839\n",
      "Epoch 38 Batch: 108 Train Loss: 0.3086094260215759\n",
      "Epoch 38 Batch: 110 Train Loss: 0.04117882624268532\n",
      "Epoch 38 Batch: 112 Train Loss: 0.3094165027141571\n",
      "Epoch 38 Batch: 114 Train Loss: 0.5126967430114746\n",
      "Epoch 38 Batch: 116 Train Loss: 0.044244349002838135\n",
      "Epoch 38 Batch: 118 Train Loss: 0.0521809346973896\n",
      "Epoch 38 Batch: 120 Train Loss: 0.05162492394447327\n",
      "Epoch 38 Batch: 122 Train Loss: 0.07616089284420013\n",
      "Epoch 38 Batch: 124 Train Loss: 0.057870201766490936\n",
      "Epoch 38 Batch: 126 Train Loss: 0.053402841091156006\n",
      "Epoch 38 Batch: 128 Train Loss: 0.04865261912345886\n",
      "Epoch 38 Batch: 130 Train Loss: 0.028620615601539612\n",
      "Epoch 38 Batch: 132 Train Loss: 0.050846271216869354\n",
      "Epoch 38 Batch: 134 Train Loss: 0.021156031638383865\n",
      "Epoch 38 Batch: 136 Train Loss: 0.5109320878982544\n",
      "Epoch 38 Batch: 138 Train Loss: 0.06524896621704102\n",
      "Epoch 38 Batch: 140 Train Loss: 0.03404095396399498\n",
      "Epoch 38 Batch: 142 Train Loss: 0.04262726753950119\n",
      "Epoch 38 Batch: 144 Train Loss: 0.045594222843647\n",
      "Epoch 38 Batch: 146 Train Loss: 0.04352012276649475\n",
      "Epoch 38 Batch: 148 Train Loss: 0.02456667087972164\n",
      "Epoch 38 Batch: 150 Train Loss: 0.3088855445384979\n",
      "Epoch 38 Batch: 152 Train Loss: 0.30057305097579956\n",
      "Epoch 38 Batch: 154 Train Loss: 0.32501640915870667\n",
      "Epoch 38 Batch: 156 Train Loss: 0.03782949969172478\n",
      "Epoch 38 Batch: 158 Train Loss: 0.05100078508257866\n",
      "Epoch 38 Batch: 160 Train Loss: 0.3144676387310028\n",
      "Epoch 39 Batch: 2 Train Loss: 0.29363736510276794\n",
      "Epoch 39 Batch: 4 Train Loss: 0.28310710191726685\n",
      "Epoch 39 Batch: 6 Train Loss: 0.27788206934928894\n",
      "Epoch 39 Batch: 8 Train Loss: 0.08952713012695312\n",
      "Epoch 39 Batch: 10 Train Loss: 0.032764069736003876\n",
      "Epoch 39 Batch: 12 Train Loss: 0.03525378927588463\n",
      "Epoch 39 Batch: 14 Train Loss: 0.06495053321123123\n",
      "Epoch 39 Batch: 16 Train Loss: 0.07595428079366684\n",
      "Epoch 39 Batch: 18 Train Loss: 0.03846915811300278\n",
      "Epoch 39 Batch: 20 Train Loss: 0.0427284836769104\n",
      "Epoch 39 Batch: 22 Train Loss: 0.06245248764753342\n",
      "Epoch 39 Batch: 24 Train Loss: 0.28069713711738586\n",
      "Epoch 39 Batch: 26 Train Loss: 0.29726821184158325\n",
      "Epoch 39 Batch: 28 Train Loss: 0.035518791526556015\n",
      "Epoch 39 Batch: 30 Train Loss: 0.6009105443954468\n",
      "Epoch 39 Batch: 32 Train Loss: 0.3145532011985779\n",
      "Epoch 39 Batch: 34 Train Loss: 0.05340931937098503\n",
      "Epoch 39 Batch: 36 Train Loss: 0.012680290266871452\n",
      "Epoch 39 Batch: 38 Train Loss: 0.05551321059465408\n",
      "Epoch 39 Batch: 40 Train Loss: 0.030932873487472534\n",
      "Epoch 39 Batch: 42 Train Loss: 0.5599651336669922\n",
      "Epoch 39 Batch: 44 Train Loss: 0.2664521336555481\n",
      "Epoch 39 Batch: 46 Train Loss: 0.2581840455532074\n",
      "Epoch 39 Batch: 48 Train Loss: 0.29263606667518616\n",
      "Epoch 39 Batch: 50 Train Loss: 0.04093059152364731\n",
      "Epoch 39 Batch: 52 Train Loss: 0.32924437522888184\n",
      "Epoch 39 Batch: 54 Train Loss: 0.061513565480709076\n",
      "Epoch 39 Batch: 56 Train Loss: 0.2625013291835785\n",
      "Epoch 39 Batch: 58 Train Loss: 0.08204333484172821\n",
      "Epoch 39 Batch: 60 Train Loss: 0.07301051914691925\n",
      "Epoch 39 Batch: 62 Train Loss: 0.27171987295150757\n",
      "Epoch 39 Batch: 64 Train Loss: 0.07128854095935822\n",
      "Epoch 39 Batch: 66 Train Loss: 0.07950785756111145\n",
      "Epoch 39 Batch: 68 Train Loss: 0.47731608152389526\n",
      "Epoch 39 Batch: 70 Train Loss: 0.07153665274381638\n",
      "Epoch 39 Batch: 72 Train Loss: 0.08939360827207565\n",
      "Epoch 39 Batch: 74 Train Loss: 0.06977304071187973\n",
      "Epoch 39 Batch: 76 Train Loss: 0.2814604640007019\n",
      "Epoch 39 Batch: 78 Train Loss: 0.07128205895423889\n",
      "Epoch 39 Batch: 80 Train Loss: 0.08934028446674347\n",
      "Epoch 39 Batch: 82 Train Loss: 0.043186407536268234\n",
      "Epoch 39 Batch: 84 Train Loss: 0.08216973394155502\n",
      "Epoch 39 Batch: 86 Train Loss: 0.2619016170501709\n",
      "Epoch 39 Batch: 88 Train Loss: 0.48930850625038147\n",
      "Epoch 39 Batch: 90 Train Loss: 0.03535589575767517\n",
      "Epoch 39 Batch: 92 Train Loss: 0.29070809483528137\n",
      "Epoch 39 Batch: 94 Train Loss: 0.2671145796775818\n",
      "Epoch 39 Batch: 96 Train Loss: 0.27334314584732056\n",
      "Epoch 39 Batch: 98 Train Loss: 0.07585740089416504\n",
      "Epoch 39 Batch: 100 Train Loss: 0.06535527855157852\n",
      "Epoch 39 Batch: 102 Train Loss: 0.04012526944279671\n",
      "Epoch 39 Batch: 104 Train Loss: 0.040200889110565186\n",
      "Epoch 39 Batch: 106 Train Loss: 0.04888663440942764\n",
      "Epoch 39 Batch: 108 Train Loss: 0.05924684926867485\n",
      "Epoch 39 Batch: 110 Train Loss: 0.264335036277771\n",
      "Epoch 39 Batch: 112 Train Loss: 0.04619333893060684\n",
      "Epoch 39 Batch: 114 Train Loss: 0.03998429700732231\n",
      "Epoch 39 Batch: 116 Train Loss: 0.05370957776904106\n",
      "Epoch 39 Batch: 118 Train Loss: 0.05424797534942627\n",
      "Epoch 39 Batch: 120 Train Loss: 0.2800821363925934\n",
      "Epoch 39 Batch: 122 Train Loss: 0.548770010471344\n",
      "Epoch 39 Batch: 124 Train Loss: 0.044684380292892456\n",
      "Epoch 39 Batch: 126 Train Loss: 0.30232053995132446\n",
      "Epoch 39 Batch: 128 Train Loss: 0.05159735679626465\n",
      "Epoch 39 Batch: 130 Train Loss: 0.2740355432033539\n",
      "Epoch 39 Batch: 132 Train Loss: 0.05572466179728508\n",
      "Epoch 39 Batch: 134 Train Loss: 0.08153095096349716\n",
      "Epoch 39 Batch: 136 Train Loss: 0.2764648497104645\n",
      "Epoch 39 Batch: 138 Train Loss: 0.10182646661996841\n",
      "Epoch 39 Batch: 140 Train Loss: 0.2778753638267517\n",
      "Epoch 39 Batch: 142 Train Loss: 0.07544388622045517\n",
      "Epoch 39 Batch: 144 Train Loss: 0.06170274689793587\n",
      "Epoch 39 Batch: 146 Train Loss: 0.11092495918273926\n",
      "Epoch 39 Batch: 148 Train Loss: 0.04220261424779892\n",
      "Epoch 39 Batch: 150 Train Loss: 0.059755515307188034\n",
      "Epoch 39 Batch: 152 Train Loss: 0.04402093216776848\n",
      "Epoch 39 Batch: 154 Train Loss: 0.29670250415802\n",
      "Epoch 39 Batch: 156 Train Loss: 0.040166936814785004\n",
      "Epoch 39 Batch: 158 Train Loss: 0.5159493684768677\n",
      "Epoch 39 Batch: 160 Train Loss: 0.03684390336275101\n",
      "Epoch 40 Batch: 2 Train Loss: 0.05018327385187149\n",
      "Epoch 40 Batch: 4 Train Loss: 0.2985531687736511\n",
      "Epoch 40 Batch: 6 Train Loss: 0.5606773495674133\n",
      "Epoch 40 Batch: 8 Train Loss: 0.31130367517471313\n",
      "Epoch 40 Batch: 10 Train Loss: 0.0697387158870697\n",
      "Epoch 40 Batch: 12 Train Loss: 0.026165183633565903\n",
      "Epoch 40 Batch: 14 Train Loss: 0.038436684757471085\n",
      "Epoch 40 Batch: 16 Train Loss: 0.0385516993701458\n",
      "Epoch 40 Batch: 18 Train Loss: 0.03586698696017265\n",
      "Epoch 40 Batch: 20 Train Loss: 0.04220455512404442\n",
      "Epoch 40 Batch: 22 Train Loss: 0.043487101793289185\n",
      "Epoch 40 Batch: 24 Train Loss: 0.05895451456308365\n",
      "Epoch 40 Batch: 26 Train Loss: 0.02137025073170662\n",
      "Epoch 40 Batch: 28 Train Loss: 0.5596855878829956\n",
      "Epoch 40 Batch: 30 Train Loss: 0.30101683735847473\n",
      "Epoch 40 Batch: 32 Train Loss: 0.040561288595199585\n",
      "Epoch 40 Batch: 34 Train Loss: 0.037484060972929\n",
      "Epoch 40 Batch: 36 Train Loss: 0.5824570655822754\n",
      "Epoch 40 Batch: 38 Train Loss: 0.28516680002212524\n",
      "Epoch 40 Batch: 40 Train Loss: 0.03801899030804634\n",
      "Epoch 40 Batch: 42 Train Loss: 0.0796184316277504\n",
      "Epoch 40 Batch: 44 Train Loss: 0.054822780191898346\n",
      "Epoch 40 Batch: 46 Train Loss: 0.0436706580221653\n",
      "Epoch 40 Batch: 48 Train Loss: 0.03147311508655548\n",
      "Epoch 40 Batch: 50 Train Loss: 0.051576532423496246\n",
      "Epoch 40 Batch: 52 Train Loss: 0.06030959635972977\n",
      "Epoch 40 Batch: 54 Train Loss: 0.03999858722090721\n",
      "Epoch 40 Batch: 56 Train Loss: 0.5040682554244995\n",
      "Epoch 40 Batch: 58 Train Loss: 0.5125788450241089\n",
      "Epoch 40 Batch: 60 Train Loss: 0.5285050272941589\n",
      "Epoch 40 Batch: 62 Train Loss: 0.05018752068281174\n",
      "Epoch 40 Batch: 64 Train Loss: 0.2668742835521698\n",
      "Epoch 40 Batch: 66 Train Loss: 0.30522218346595764\n",
      "Epoch 40 Batch: 68 Train Loss: 0.060358863323926926\n",
      "Epoch 40 Batch: 70 Train Loss: 0.437505304813385\n",
      "Epoch 40 Batch: 72 Train Loss: 0.08602850139141083\n",
      "Epoch 40 Batch: 74 Train Loss: 0.07365299016237259\n",
      "Epoch 40 Batch: 76 Train Loss: 0.06170247122645378\n",
      "Epoch 40 Batch: 78 Train Loss: 0.04333962872624397\n",
      "Epoch 40 Batch: 80 Train Loss: 0.0727190300822258\n",
      "Epoch 40 Batch: 82 Train Loss: 0.04843930900096893\n",
      "Epoch 40 Batch: 84 Train Loss: 0.039202794432640076\n",
      "Epoch 40 Batch: 86 Train Loss: 0.29360300302505493\n",
      "Epoch 40 Batch: 88 Train Loss: 0.03765116259455681\n",
      "Epoch 40 Batch: 90 Train Loss: 0.2916276156902313\n",
      "Epoch 40 Batch: 92 Train Loss: 0.2914925515651703\n",
      "Epoch 40 Batch: 94 Train Loss: 0.2830590307712555\n",
      "Epoch 40 Batch: 96 Train Loss: 0.055547088384628296\n",
      "Epoch 40 Batch: 98 Train Loss: 0.06548281013965607\n",
      "Epoch 40 Batch: 100 Train Loss: 0.5885147452354431\n",
      "Epoch 40 Batch: 102 Train Loss: 0.5312504768371582\n",
      "Epoch 40 Batch: 104 Train Loss: 0.2993907332420349\n",
      "Epoch 40 Batch: 106 Train Loss: 0.04562541842460632\n",
      "Epoch 40 Batch: 108 Train Loss: 0.04013557359576225\n",
      "Epoch 40 Batch: 110 Train Loss: 0.2726784944534302\n",
      "Epoch 40 Batch: 112 Train Loss: 0.30701276659965515\n",
      "Epoch 40 Batch: 114 Train Loss: 0.48798972368240356\n",
      "Epoch 40 Batch: 116 Train Loss: 0.050978947430849075\n",
      "Epoch 40 Batch: 118 Train Loss: 0.3037794828414917\n",
      "Epoch 40 Batch: 120 Train Loss: 0.04963773116469383\n",
      "Epoch 40 Batch: 122 Train Loss: 0.05822817608714104\n",
      "Epoch 40 Batch: 124 Train Loss: 0.07493280619382858\n",
      "Epoch 40 Batch: 126 Train Loss: 0.46515876054763794\n",
      "Epoch 40 Batch: 128 Train Loss: 0.03976035490632057\n",
      "Epoch 40 Batch: 130 Train Loss: 0.2413814514875412\n",
      "Epoch 40 Batch: 132 Train Loss: 0.06214061379432678\n",
      "Epoch 40 Batch: 134 Train Loss: 0.28558653593063354\n",
      "Epoch 40 Batch: 136 Train Loss: 0.07802633941173553\n",
      "Epoch 40 Batch: 138 Train Loss: 0.10042355954647064\n",
      "Epoch 40 Batch: 140 Train Loss: 0.05843888968229294\n",
      "Epoch 40 Batch: 142 Train Loss: 0.07350251078605652\n",
      "Epoch 40 Batch: 144 Train Loss: 0.04178674891591072\n",
      "Epoch 40 Batch: 146 Train Loss: 0.06680800020694733\n",
      "Epoch 40 Batch: 148 Train Loss: 0.03270963206887245\n",
      "Epoch 40 Batch: 150 Train Loss: 0.28773003816604614\n",
      "Epoch 40 Batch: 152 Train Loss: 0.3260192573070526\n",
      "Epoch 40 Batch: 154 Train Loss: 0.04589541628956795\n",
      "Epoch 40 Batch: 156 Train Loss: 0.04538015276193619\n",
      "Epoch 40 Batch: 158 Train Loss: 0.28481781482696533\n",
      "Epoch 40 Batch: 160 Train Loss: 0.03236309438943863\n",
      "Epoch 41 Batch: 2 Train Loss: 0.03566988557577133\n",
      "Epoch 41 Batch: 4 Train Loss: 0.0448269285261631\n",
      "Epoch 41 Batch: 6 Train Loss: 0.29856961965560913\n",
      "Epoch 41 Batch: 8 Train Loss: 0.08198639750480652\n",
      "Epoch 41 Batch: 10 Train Loss: 0.06474268436431885\n",
      "Epoch 41 Batch: 12 Train Loss: 0.046518873423337936\n",
      "Epoch 41 Batch: 14 Train Loss: 0.2711300849914551\n",
      "Epoch 41 Batch: 16 Train Loss: 0.06472821533679962\n",
      "Epoch 41 Batch: 18 Train Loss: 0.03190955892205238\n",
      "Epoch 41 Batch: 20 Train Loss: 0.04439995065331459\n",
      "Epoch 41 Batch: 22 Train Loss: 0.26763609051704407\n",
      "Epoch 41 Batch: 24 Train Loss: 0.046408761292696\n",
      "Epoch 41 Batch: 26 Train Loss: 0.058253198862075806\n",
      "Epoch 41 Batch: 28 Train Loss: 0.2959357500076294\n",
      "Epoch 41 Batch: 30 Train Loss: 0.037293870002031326\n",
      "Epoch 41 Batch: 32 Train Loss: 0.3169609606266022\n",
      "Epoch 41 Batch: 34 Train Loss: 0.04316692426800728\n",
      "Epoch 41 Batch: 36 Train Loss: 0.03721120208501816\n",
      "Epoch 41 Batch: 38 Train Loss: 0.2657177746295929\n",
      "Epoch 41 Batch: 40 Train Loss: 0.2620309889316559\n",
      "Epoch 41 Batch: 42 Train Loss: 0.3138604760169983\n",
      "Epoch 41 Batch: 44 Train Loss: 0.4822360575199127\n",
      "Epoch 41 Batch: 46 Train Loss: 0.04971389099955559\n",
      "Epoch 41 Batch: 48 Train Loss: 0.0448012501001358\n",
      "Epoch 41 Batch: 50 Train Loss: 0.30071017146110535\n",
      "Epoch 41 Batch: 52 Train Loss: 0.0689028650522232\n",
      "Epoch 41 Batch: 54 Train Loss: 0.05356447026133537\n",
      "Epoch 41 Batch: 56 Train Loss: 0.28219667077064514\n",
      "Epoch 41 Batch: 58 Train Loss: 0.4808889925479889\n",
      "Epoch 41 Batch: 60 Train Loss: 0.05719003826379776\n",
      "Epoch 41 Batch: 62 Train Loss: 0.0958109050989151\n",
      "Epoch 41 Batch: 64 Train Loss: 0.08774155378341675\n",
      "Epoch 41 Batch: 66 Train Loss: 0.08573298156261444\n",
      "Epoch 41 Batch: 68 Train Loss: 0.029911845922470093\n",
      "Epoch 41 Batch: 70 Train Loss: 0.03479064255952835\n",
      "Epoch 41 Batch: 72 Train Loss: 0.503457248210907\n",
      "Epoch 41 Batch: 74 Train Loss: 0.5014570355415344\n",
      "Epoch 41 Batch: 76 Train Loss: 0.044810373336076736\n",
      "Epoch 41 Batch: 78 Train Loss: 0.05091317370533943\n",
      "Epoch 41 Batch: 80 Train Loss: 0.08462590724229813\n",
      "Epoch 41 Batch: 82 Train Loss: 0.08292390406131744\n",
      "Epoch 41 Batch: 84 Train Loss: 0.30329567193984985\n",
      "Epoch 41 Batch: 86 Train Loss: 0.06569229811429977\n",
      "Epoch 41 Batch: 88 Train Loss: 0.30157461762428284\n",
      "Epoch 41 Batch: 90 Train Loss: 0.03925849497318268\n",
      "Epoch 41 Batch: 92 Train Loss: 0.0625726506114006\n",
      "Epoch 41 Batch: 94 Train Loss: 0.0436946339905262\n",
      "Epoch 41 Batch: 96 Train Loss: 0.04970730468630791\n",
      "Epoch 41 Batch: 98 Train Loss: 0.06935127079486847\n",
      "Epoch 41 Batch: 100 Train Loss: 0.04549063369631767\n",
      "Epoch 41 Batch: 102 Train Loss: 0.04650462418794632\n",
      "Epoch 41 Batch: 104 Train Loss: 0.0322408564388752\n",
      "Epoch 41 Batch: 106 Train Loss: 0.02471022866666317\n",
      "Epoch 41 Batch: 108 Train Loss: 0.04289243370294571\n",
      "Epoch 41 Batch: 110 Train Loss: 0.29882296919822693\n",
      "Epoch 41 Batch: 112 Train Loss: 0.3116340935230255\n",
      "Epoch 41 Batch: 114 Train Loss: 0.037982381880283356\n",
      "Epoch 41 Batch: 116 Train Loss: 0.028811689466238022\n",
      "Epoch 41 Batch: 118 Train Loss: 0.32782113552093506\n",
      "Epoch 41 Batch: 120 Train Loss: 0.03416328504681587\n",
      "Epoch 41 Batch: 122 Train Loss: 0.041636113077402115\n",
      "Epoch 41 Batch: 124 Train Loss: 0.2700534164905548\n",
      "Epoch 41 Batch: 126 Train Loss: 0.035615868866443634\n",
      "Epoch 41 Batch: 128 Train Loss: 0.267288476228714\n",
      "Epoch 41 Batch: 130 Train Loss: 0.27885937690734863\n",
      "Epoch 41 Batch: 132 Train Loss: 0.07481463253498077\n",
      "Epoch 41 Batch: 134 Train Loss: 0.03502175211906433\n",
      "Epoch 41 Batch: 136 Train Loss: 0.7752426266670227\n",
      "Epoch 41 Batch: 138 Train Loss: 0.04219750687479973\n",
      "Epoch 41 Batch: 140 Train Loss: 0.038338035345077515\n",
      "Epoch 41 Batch: 142 Train Loss: 0.5952361822128296\n",
      "Epoch 41 Batch: 144 Train Loss: 0.05562470480799675\n",
      "Epoch 41 Batch: 146 Train Loss: 0.023237401619553566\n",
      "Epoch 41 Batch: 148 Train Loss: 0.28688937425613403\n",
      "Epoch 41 Batch: 150 Train Loss: 0.24964170157909393\n",
      "Epoch 41 Batch: 152 Train Loss: 0.28313881158828735\n",
      "Epoch 41 Batch: 154 Train Loss: 0.5049139261245728\n",
      "Epoch 41 Batch: 156 Train Loss: 0.07304415106773376\n",
      "Epoch 41 Batch: 158 Train Loss: 0.26418331265449524\n",
      "Epoch 41 Batch: 160 Train Loss: 0.04223718121647835\n",
      "Epoch 42 Batch: 2 Train Loss: 0.28411802649497986\n",
      "Epoch 42 Batch: 4 Train Loss: 0.07957346737384796\n",
      "Epoch 42 Batch: 6 Train Loss: 0.05266086384654045\n",
      "Epoch 42 Batch: 8 Train Loss: 0.2529981732368469\n",
      "Epoch 42 Batch: 10 Train Loss: 0.25377365946769714\n",
      "Epoch 42 Batch: 12 Train Loss: 0.2985970973968506\n",
      "Epoch 42 Batch: 14 Train Loss: 0.29042065143585205\n",
      "Epoch 42 Batch: 16 Train Loss: 0.29001903533935547\n",
      "Epoch 42 Batch: 18 Train Loss: 0.5273091197013855\n",
      "Epoch 42 Batch: 20 Train Loss: 0.28320029377937317\n",
      "Epoch 42 Batch: 22 Train Loss: 0.28318390250205994\n",
      "Epoch 42 Batch: 24 Train Loss: 0.047904063016176224\n",
      "Epoch 42 Batch: 26 Train Loss: 0.12877890467643738\n",
      "Epoch 42 Batch: 28 Train Loss: 0.449476420879364\n",
      "Epoch 42 Batch: 30 Train Loss: 0.06750687956809998\n",
      "Epoch 42 Batch: 32 Train Loss: 0.056209273636341095\n",
      "Epoch 42 Batch: 34 Train Loss: 0.42909544706344604\n",
      "Epoch 42 Batch: 36 Train Loss: 0.04365852102637291\n",
      "Epoch 42 Batch: 38 Train Loss: 0.450067937374115\n",
      "Epoch 42 Batch: 40 Train Loss: 0.27392417192459106\n",
      "Epoch 42 Batch: 42 Train Loss: 0.23245282471179962\n",
      "Epoch 42 Batch: 44 Train Loss: 0.06200523301959038\n",
      "Epoch 42 Batch: 46 Train Loss: 0.07653205841779709\n",
      "Epoch 42 Batch: 48 Train Loss: 0.2979009747505188\n",
      "Epoch 42 Batch: 50 Train Loss: 0.0475187823176384\n",
      "Epoch 42 Batch: 52 Train Loss: 0.08136138319969177\n",
      "Epoch 42 Batch: 54 Train Loss: 0.06369189918041229\n",
      "Epoch 42 Batch: 56 Train Loss: 0.06199197843670845\n",
      "Epoch 42 Batch: 58 Train Loss: 0.027941545471549034\n",
      "Epoch 42 Batch: 60 Train Loss: 0.3027803301811218\n",
      "Epoch 42 Batch: 62 Train Loss: 0.29976481199264526\n",
      "Epoch 42 Batch: 64 Train Loss: 0.30475279688835144\n",
      "Epoch 42 Batch: 66 Train Loss: 0.559154748916626\n",
      "Epoch 42 Batch: 68 Train Loss: 0.49056434631347656\n",
      "Epoch 42 Batch: 70 Train Loss: 0.25325915217399597\n",
      "Epoch 42 Batch: 72 Train Loss: 0.047636520117521286\n",
      "Epoch 42 Batch: 74 Train Loss: 0.05192744731903076\n",
      "Epoch 42 Batch: 76 Train Loss: 0.2868553400039673\n",
      "Epoch 42 Batch: 78 Train Loss: 0.08724305778741837\n",
      "Epoch 42 Batch: 80 Train Loss: 0.31360170245170593\n",
      "Epoch 42 Batch: 82 Train Loss: 0.09367953985929489\n",
      "Epoch 42 Batch: 84 Train Loss: 0.11515440046787262\n",
      "Epoch 42 Batch: 86 Train Loss: 0.05350154638290405\n",
      "Epoch 42 Batch: 88 Train Loss: 0.07576484978199005\n",
      "Epoch 42 Batch: 90 Train Loss: 0.06774253398180008\n",
      "Epoch 42 Batch: 92 Train Loss: 0.3112935721874237\n",
      "Epoch 42 Batch: 94 Train Loss: 0.0692894235253334\n",
      "Epoch 42 Batch: 96 Train Loss: 0.5508683323860168\n",
      "Epoch 42 Batch: 98 Train Loss: 0.33302879333496094\n",
      "Epoch 42 Batch: 100 Train Loss: 0.05034791678190231\n",
      "Epoch 42 Batch: 102 Train Loss: 0.06493132561445236\n",
      "Epoch 42 Batch: 104 Train Loss: 0.0548275001347065\n",
      "Epoch 42 Batch: 106 Train Loss: 0.041824474930763245\n",
      "Epoch 42 Batch: 108 Train Loss: 0.31303757429122925\n",
      "Epoch 42 Batch: 110 Train Loss: 0.05038652569055557\n",
      "Epoch 42 Batch: 112 Train Loss: 0.04484767094254494\n",
      "Epoch 42 Batch: 114 Train Loss: 0.03981277346611023\n",
      "Epoch 42 Batch: 116 Train Loss: 0.03393315151333809\n",
      "Epoch 42 Batch: 118 Train Loss: 0.036470506340265274\n",
      "Epoch 42 Batch: 120 Train Loss: 0.06191723421216011\n",
      "Epoch 42 Batch: 122 Train Loss: 0.025870656594634056\n",
      "Epoch 42 Batch: 124 Train Loss: 0.05907924100756645\n",
      "Epoch 42 Batch: 126 Train Loss: 0.046630583703517914\n",
      "Epoch 42 Batch: 128 Train Loss: 0.04115675762295723\n",
      "Epoch 42 Batch: 130 Train Loss: 0.04350785166025162\n",
      "Epoch 42 Batch: 132 Train Loss: 0.04328526183962822\n",
      "Epoch 42 Batch: 134 Train Loss: 0.2742280960083008\n",
      "Epoch 42 Batch: 136 Train Loss: 0.039123423397541046\n",
      "Epoch 42 Batch: 138 Train Loss: 0.045629359781742096\n",
      "Epoch 42 Batch: 140 Train Loss: 0.5010083913803101\n",
      "Epoch 42 Batch: 142 Train Loss: 0.27783024311065674\n",
      "Epoch 42 Batch: 144 Train Loss: 0.0632023811340332\n",
      "Epoch 42 Batch: 146 Train Loss: 0.06899182498455048\n",
      "Epoch 42 Batch: 148 Train Loss: 0.045202918350696564\n",
      "Epoch 42 Batch: 150 Train Loss: 0.2560252547264099\n",
      "Epoch 42 Batch: 152 Train Loss: 0.07160995900630951\n",
      "Epoch 42 Batch: 154 Train Loss: 0.09739607572555542\n",
      "Epoch 42 Batch: 156 Train Loss: 0.04363424330949783\n",
      "Epoch 42 Batch: 158 Train Loss: 0.31714722514152527\n",
      "Epoch 42 Batch: 160 Train Loss: 0.03546512499451637\n",
      "Epoch 43 Batch: 2 Train Loss: 0.2781522572040558\n",
      "Epoch 43 Batch: 4 Train Loss: 0.045198507606983185\n",
      "Epoch 43 Batch: 6 Train Loss: 0.05963569134473801\n",
      "Epoch 43 Batch: 8 Train Loss: 0.5350933074951172\n",
      "Epoch 43 Batch: 10 Train Loss: 0.03252929821610451\n",
      "Epoch 43 Batch: 12 Train Loss: 0.06092948839068413\n",
      "Epoch 43 Batch: 14 Train Loss: 0.5438550710678101\n",
      "Epoch 43 Batch: 16 Train Loss: 0.32778826355934143\n",
      "Epoch 43 Batch: 18 Train Loss: 0.05996595695614815\n",
      "Epoch 43 Batch: 20 Train Loss: 0.2834254205226898\n",
      "Epoch 43 Batch: 22 Train Loss: 0.0691351443529129\n",
      "Epoch 43 Batch: 24 Train Loss: 0.2837691307067871\n",
      "Epoch 43 Batch: 26 Train Loss: 0.0537632517516613\n",
      "Epoch 43 Batch: 28 Train Loss: 0.05223247408866882\n",
      "Epoch 43 Batch: 30 Train Loss: 0.2654464542865753\n",
      "Epoch 43 Batch: 32 Train Loss: 0.2993437647819519\n",
      "Epoch 43 Batch: 34 Train Loss: 0.0817672610282898\n",
      "Epoch 43 Batch: 36 Train Loss: 0.02104465663433075\n",
      "Epoch 43 Batch: 38 Train Loss: 0.29083165526390076\n",
      "Epoch 43 Batch: 40 Train Loss: 0.31434372067451477\n",
      "Epoch 43 Batch: 42 Train Loss: 0.024107709527015686\n",
      "Epoch 43 Batch: 44 Train Loss: 0.4956679344177246\n",
      "Epoch 43 Batch: 46 Train Loss: 0.24088242650032043\n",
      "Epoch 43 Batch: 48 Train Loss: 0.04271497577428818\n",
      "Epoch 43 Batch: 50 Train Loss: 0.07915661484003067\n",
      "Epoch 43 Batch: 52 Train Loss: 0.09748001396656036\n",
      "Epoch 43 Batch: 54 Train Loss: 0.042445193976163864\n",
      "Epoch 43 Batch: 56 Train Loss: 0.0694497749209404\n",
      "Epoch 43 Batch: 58 Train Loss: 0.0640147477388382\n",
      "Epoch 43 Batch: 60 Train Loss: 0.2878391146659851\n",
      "Epoch 43 Batch: 62 Train Loss: 0.30304813385009766\n",
      "Epoch 43 Batch: 64 Train Loss: 0.5287729501724243\n",
      "Epoch 43 Batch: 66 Train Loss: 0.0525876060128212\n",
      "Epoch 43 Batch: 68 Train Loss: 0.046945374459028244\n",
      "Epoch 43 Batch: 70 Train Loss: 0.02860446646809578\n",
      "Epoch 43 Batch: 72 Train Loss: 0.04469669237732887\n",
      "Epoch 43 Batch: 74 Train Loss: 0.04704487696290016\n",
      "Epoch 43 Batch: 76 Train Loss: 0.08137470483779907\n",
      "Epoch 43 Batch: 78 Train Loss: 0.2770257592201233\n",
      "Epoch 43 Batch: 80 Train Loss: 0.02210552617907524\n",
      "Epoch 43 Batch: 82 Train Loss: 0.4981016516685486\n",
      "Epoch 43 Batch: 84 Train Loss: 0.0571226067841053\n",
      "Epoch 43 Batch: 86 Train Loss: 0.04525753855705261\n",
      "Epoch 43 Batch: 88 Train Loss: 0.05704789608716965\n",
      "Epoch 43 Batch: 90 Train Loss: 0.04522889852523804\n",
      "Epoch 43 Batch: 92 Train Loss: 0.5357640981674194\n",
      "Epoch 43 Batch: 94 Train Loss: 0.036891382187604904\n",
      "Epoch 43 Batch: 96 Train Loss: 0.2864993214607239\n",
      "Epoch 43 Batch: 98 Train Loss: 0.05074256658554077\n",
      "Epoch 43 Batch: 100 Train Loss: 0.05042834207415581\n",
      "Epoch 43 Batch: 102 Train Loss: 0.08026440441608429\n",
      "Epoch 43 Batch: 104 Train Loss: 0.028677111491560936\n",
      "Epoch 43 Batch: 106 Train Loss: 0.2629470229148865\n",
      "Epoch 43 Batch: 108 Train Loss: 0.010178456082940102\n",
      "Epoch 43 Batch: 110 Train Loss: 0.5265399813652039\n",
      "Epoch 43 Batch: 112 Train Loss: 0.055876873433589935\n",
      "Epoch 43 Batch: 114 Train Loss: 0.29793688654899597\n",
      "Epoch 43 Batch: 116 Train Loss: 0.279915988445282\n",
      "Epoch 43 Batch: 118 Train Loss: 0.06364908069372177\n",
      "Epoch 43 Batch: 120 Train Loss: 0.050680529326200485\n",
      "Epoch 43 Batch: 122 Train Loss: 0.04753149673342705\n",
      "Epoch 43 Batch: 124 Train Loss: 0.5469781160354614\n",
      "Epoch 43 Batch: 126 Train Loss: 0.05039185285568237\n",
      "Epoch 43 Batch: 128 Train Loss: 0.06087130308151245\n",
      "Epoch 43 Batch: 130 Train Loss: 0.30335789918899536\n",
      "Epoch 43 Batch: 132 Train Loss: 0.5069009065628052\n",
      "Epoch 43 Batch: 134 Train Loss: 0.05359784513711929\n",
      "Epoch 43 Batch: 136 Train Loss: 0.2771678864955902\n",
      "Epoch 43 Batch: 138 Train Loss: 0.2922055125236511\n",
      "Epoch 43 Batch: 140 Train Loss: 0.0702003464102745\n",
      "Epoch 43 Batch: 142 Train Loss: 0.05120648071169853\n",
      "Epoch 43 Batch: 144 Train Loss: 0.03985055163502693\n",
      "Epoch 43 Batch: 146 Train Loss: 0.0536838062107563\n",
      "Epoch 43 Batch: 148 Train Loss: 0.2785066068172455\n",
      "Epoch 43 Batch: 150 Train Loss: 0.05622462183237076\n",
      "Epoch 43 Batch: 152 Train Loss: 0.30007562041282654\n",
      "Epoch 43 Batch: 154 Train Loss: 0.2936679720878601\n",
      "Epoch 43 Batch: 156 Train Loss: 0.044114455580711365\n",
      "Epoch 43 Batch: 158 Train Loss: 0.027520200237631798\n",
      "Epoch 43 Batch: 160 Train Loss: 0.05270518735051155\n",
      "Epoch 44 Batch: 2 Train Loss: 0.048620183020830154\n",
      "Epoch 44 Batch: 4 Train Loss: 0.0583503320813179\n",
      "Epoch 44 Batch: 6 Train Loss: 0.04219536855816841\n",
      "Epoch 44 Batch: 8 Train Loss: 0.053710032254457474\n",
      "Epoch 44 Batch: 10 Train Loss: 0.04847192019224167\n",
      "Epoch 44 Batch: 12 Train Loss: 0.04477786272764206\n",
      "Epoch 44 Batch: 14 Train Loss: 0.7893326878547668\n",
      "Epoch 44 Batch: 16 Train Loss: 0.06237628310918808\n",
      "Epoch 44 Batch: 18 Train Loss: 0.052052684128284454\n",
      "Epoch 44 Batch: 20 Train Loss: 0.31299126148223877\n",
      "Epoch 44 Batch: 22 Train Loss: 0.2710195779800415\n",
      "Epoch 44 Batch: 24 Train Loss: 0.054962851107120514\n",
      "Epoch 44 Batch: 26 Train Loss: 0.04804728552699089\n",
      "Epoch 44 Batch: 28 Train Loss: 0.5673221349716187\n",
      "Epoch 44 Batch: 30 Train Loss: 0.5032503008842468\n",
      "Epoch 44 Batch: 32 Train Loss: 0.04101463034749031\n",
      "Epoch 44 Batch: 34 Train Loss: 0.05185059458017349\n",
      "Epoch 44 Batch: 36 Train Loss: 0.04087936133146286\n",
      "Epoch 44 Batch: 38 Train Loss: 0.04867764562368393\n",
      "Epoch 44 Batch: 40 Train Loss: 0.037947479635477066\n",
      "Epoch 44 Batch: 42 Train Loss: 0.031137680634856224\n",
      "Epoch 44 Batch: 44 Train Loss: 0.3061433434486389\n",
      "Epoch 44 Batch: 46 Train Loss: 0.2865578532218933\n",
      "Epoch 44 Batch: 48 Train Loss: 0.036317501217126846\n",
      "Epoch 44 Batch: 50 Train Loss: 0.04334014654159546\n",
      "Epoch 44 Batch: 52 Train Loss: 0.034573446959257126\n",
      "Epoch 44 Batch: 54 Train Loss: 0.2984018921852112\n",
      "Epoch 44 Batch: 56 Train Loss: 0.01838831976056099\n",
      "Epoch 44 Batch: 58 Train Loss: 0.3155849575996399\n",
      "Epoch 44 Batch: 60 Train Loss: 0.3335498571395874\n",
      "Epoch 44 Batch: 62 Train Loss: 0.3046547472476959\n",
      "Epoch 44 Batch: 64 Train Loss: 0.2698686718940735\n",
      "Epoch 44 Batch: 66 Train Loss: 0.3087032437324524\n",
      "Epoch 44 Batch: 68 Train Loss: 0.04068313166499138\n",
      "Epoch 44 Batch: 70 Train Loss: 0.0661044493317604\n",
      "Epoch 44 Batch: 72 Train Loss: 0.015734665095806122\n",
      "Epoch 44 Batch: 74 Train Loss: 0.0515173003077507\n",
      "Epoch 44 Batch: 76 Train Loss: 0.04131074622273445\n",
      "Epoch 44 Batch: 78 Train Loss: 0.3017377257347107\n",
      "Epoch 44 Batch: 80 Train Loss: 0.03018583357334137\n",
      "Epoch 44 Batch: 82 Train Loss: 0.32945114374160767\n",
      "Epoch 44 Batch: 84 Train Loss: 0.06654481589794159\n",
      "Epoch 44 Batch: 86 Train Loss: 0.052400995045900345\n",
      "Epoch 44 Batch: 88 Train Loss: 0.25918787717819214\n",
      "Epoch 44 Batch: 90 Train Loss: 0.2958300709724426\n",
      "Epoch 44 Batch: 92 Train Loss: 0.3173089623451233\n",
      "Epoch 44 Batch: 94 Train Loss: 0.29710152745246887\n",
      "Epoch 44 Batch: 96 Train Loss: 0.3293156921863556\n",
      "Epoch 44 Batch: 98 Train Loss: 0.31397753953933716\n",
      "Epoch 44 Batch: 100 Train Loss: 0.056905560195446014\n",
      "Epoch 44 Batch: 102 Train Loss: 0.018932703882455826\n",
      "Epoch 44 Batch: 104 Train Loss: 0.31165939569473267\n",
      "Epoch 44 Batch: 106 Train Loss: 0.07153923809528351\n",
      "Epoch 44 Batch: 108 Train Loss: 0.03886991739273071\n",
      "Epoch 44 Batch: 110 Train Loss: 0.10637928545475006\n",
      "Epoch 44 Batch: 112 Train Loss: 0.2613864243030548\n",
      "Epoch 44 Batch: 114 Train Loss: 0.2522176206111908\n",
      "Epoch 44 Batch: 116 Train Loss: 0.2563658058643341\n",
      "Epoch 44 Batch: 118 Train Loss: 0.6754268407821655\n",
      "Epoch 44 Batch: 120 Train Loss: 0.03760073706507683\n",
      "Epoch 44 Batch: 122 Train Loss: 0.10645654052495956\n",
      "Epoch 44 Batch: 124 Train Loss: 0.07605169713497162\n",
      "Epoch 44 Batch: 126 Train Loss: 0.07885771989822388\n",
      "Epoch 44 Batch: 128 Train Loss: 0.2662178874015808\n",
      "Epoch 44 Batch: 130 Train Loss: 0.7155480980873108\n",
      "Epoch 44 Batch: 132 Train Loss: 0.07908184826374054\n",
      "Epoch 44 Batch: 134 Train Loss: 0.28387510776519775\n",
      "Epoch 44 Batch: 136 Train Loss: 0.05208243802189827\n",
      "Epoch 44 Batch: 138 Train Loss: 0.06964384019374847\n",
      "Epoch 44 Batch: 140 Train Loss: 0.06601754575967789\n",
      "Epoch 44 Batch: 142 Train Loss: 0.29998379945755005\n",
      "Epoch 44 Batch: 144 Train Loss: 0.029573488980531693\n",
      "Epoch 44 Batch: 146 Train Loss: 0.2724529504776001\n",
      "Epoch 44 Batch: 148 Train Loss: 0.06851359456777573\n",
      "Epoch 44 Batch: 150 Train Loss: 0.5009430646896362\n",
      "Epoch 44 Batch: 152 Train Loss: 0.26538926362991333\n",
      "Epoch 44 Batch: 154 Train Loss: 0.2601515054702759\n",
      "Epoch 44 Batch: 156 Train Loss: 0.05536451190710068\n",
      "Epoch 44 Batch: 158 Train Loss: 0.08105261623859406\n",
      "Epoch 44 Batch: 160 Train Loss: 0.2878296375274658\n",
      "Epoch 45 Batch: 2 Train Loss: 0.07256277650594711\n",
      "Epoch 45 Batch: 4 Train Loss: 0.270796000957489\n",
      "Epoch 45 Batch: 6 Train Loss: 0.040798865258693695\n",
      "Epoch 45 Batch: 8 Train Loss: 0.03428059071302414\n",
      "Epoch 45 Batch: 10 Train Loss: 0.32088133692741394\n",
      "Epoch 45 Batch: 12 Train Loss: 0.034626878798007965\n",
      "Epoch 45 Batch: 14 Train Loss: 0.2694626450538635\n",
      "Epoch 45 Batch: 16 Train Loss: 0.07350574433803558\n",
      "Epoch 45 Batch: 18 Train Loss: 0.016092214733362198\n",
      "Epoch 45 Batch: 20 Train Loss: 0.29426321387290955\n",
      "Epoch 45 Batch: 22 Train Loss: 0.5375860929489136\n",
      "Epoch 45 Batch: 24 Train Loss: 0.02102617546916008\n",
      "Epoch 45 Batch: 26 Train Loss: 0.2966201901435852\n",
      "Epoch 45 Batch: 28 Train Loss: 0.2905588746070862\n",
      "Epoch 45 Batch: 30 Train Loss: 0.07265233248472214\n",
      "Epoch 45 Batch: 32 Train Loss: 0.09568854421377182\n",
      "Epoch 45 Batch: 34 Train Loss: 0.27149486541748047\n",
      "Epoch 45 Batch: 36 Train Loss: 0.424365371465683\n",
      "Epoch 45 Batch: 38 Train Loss: 0.05348903685808182\n",
      "Epoch 45 Batch: 40 Train Loss: 0.3339589238166809\n",
      "Epoch 45 Batch: 42 Train Loss: 0.14131952822208405\n",
      "Epoch 45 Batch: 44 Train Loss: 0.29907703399658203\n",
      "Epoch 45 Batch: 46 Train Loss: 0.07673020660877228\n",
      "Epoch 45 Batch: 48 Train Loss: 0.2618800103664398\n",
      "Epoch 45 Batch: 50 Train Loss: 0.11078277975320816\n",
      "Epoch 45 Batch: 52 Train Loss: 0.27541932463645935\n",
      "Epoch 45 Batch: 54 Train Loss: 0.24559488892555237\n",
      "Epoch 45 Batch: 56 Train Loss: 0.5467778444290161\n",
      "Epoch 45 Batch: 58 Train Loss: 0.27121323347091675\n",
      "Epoch 45 Batch: 60 Train Loss: 0.05040612071752548\n",
      "Epoch 45 Batch: 62 Train Loss: 0.09323003888130188\n",
      "Epoch 45 Batch: 64 Train Loss: 0.43607187271118164\n",
      "Epoch 45 Batch: 66 Train Loss: 0.286458283662796\n",
      "Epoch 45 Batch: 68 Train Loss: 0.07067997753620148\n",
      "Epoch 45 Batch: 70 Train Loss: 0.0697920098900795\n",
      "Epoch 45 Batch: 72 Train Loss: 0.9817654490470886\n",
      "Epoch 45 Batch: 74 Train Loss: 1.1806784868240356\n",
      "Epoch 45 Batch: 76 Train Loss: 0.40338534116744995\n",
      "Epoch 45 Batch: 78 Train Loss: 0.27519503235816956\n",
      "Epoch 45 Batch: 80 Train Loss: 0.6028028726577759\n",
      "Epoch 45 Batch: 82 Train Loss: 0.10442344844341278\n",
      "Epoch 45 Batch: 84 Train Loss: 0.041218966245651245\n",
      "Epoch 45 Batch: 86 Train Loss: 0.26430076360702515\n",
      "Epoch 45 Batch: 88 Train Loss: 0.14401085674762726\n",
      "Epoch 45 Batch: 90 Train Loss: 0.2477763593196869\n",
      "Epoch 45 Batch: 92 Train Loss: 0.2687031924724579\n",
      "Epoch 45 Batch: 94 Train Loss: 0.33436113595962524\n",
      "Epoch 45 Batch: 96 Train Loss: 0.21991916000843048\n",
      "Epoch 45 Batch: 98 Train Loss: 0.18670277297496796\n",
      "Epoch 45 Batch: 100 Train Loss: 0.13064061105251312\n",
      "Epoch 45 Batch: 102 Train Loss: 0.10251233726739883\n",
      "Epoch 45 Batch: 104 Train Loss: 0.05729122832417488\n",
      "Epoch 45 Batch: 106 Train Loss: 0.05136309936642647\n",
      "Epoch 45 Batch: 108 Train Loss: 0.10436155647039413\n",
      "Epoch 45 Batch: 110 Train Loss: 0.49838003516197205\n",
      "Epoch 45 Batch: 112 Train Loss: 0.07165129482746124\n",
      "Epoch 45 Batch: 114 Train Loss: 0.036922696977853775\n",
      "Epoch 45 Batch: 116 Train Loss: 0.07093041390180588\n",
      "Epoch 45 Batch: 118 Train Loss: 0.04636407643556595\n",
      "Epoch 45 Batch: 120 Train Loss: 0.5009635090827942\n",
      "Epoch 45 Batch: 122 Train Loss: 0.04854171350598335\n",
      "Epoch 45 Batch: 124 Train Loss: 0.0848507508635521\n",
      "Epoch 45 Batch: 126 Train Loss: 0.07064962387084961\n",
      "Epoch 45 Batch: 128 Train Loss: 0.0480780266225338\n",
      "Epoch 45 Batch: 130 Train Loss: 0.04632468521595001\n",
      "Epoch 45 Batch: 132 Train Loss: 0.04419003427028656\n",
      "Epoch 45 Batch: 134 Train Loss: 0.3047260642051697\n",
      "Epoch 45 Batch: 136 Train Loss: 0.04721720516681671\n",
      "Epoch 45 Batch: 138 Train Loss: 0.04884284734725952\n",
      "Epoch 45 Batch: 140 Train Loss: 0.0629291832447052\n",
      "Epoch 45 Batch: 142 Train Loss: 0.05277431756258011\n",
      "Epoch 45 Batch: 144 Train Loss: 0.03725064918398857\n",
      "Epoch 45 Batch: 146 Train Loss: 0.03381163254380226\n",
      "Epoch 45 Batch: 148 Train Loss: 0.0335916206240654\n",
      "Epoch 45 Batch: 150 Train Loss: 0.5625113248825073\n",
      "Epoch 45 Batch: 152 Train Loss: 0.03454584628343582\n",
      "Epoch 45 Batch: 154 Train Loss: 0.02614409103989601\n",
      "Epoch 45 Batch: 156 Train Loss: 0.05523210018873215\n",
      "Epoch 45 Batch: 158 Train Loss: 0.2897774279117584\n",
      "Epoch 45 Batch: 160 Train Loss: 0.30207815766334534\n",
      "Epoch 46 Batch: 2 Train Loss: 0.28429800271987915\n",
      "Epoch 46 Batch: 4 Train Loss: 0.0315958634018898\n",
      "Epoch 46 Batch: 6 Train Loss: 0.031293027102947235\n",
      "Epoch 46 Batch: 8 Train Loss: 0.05617803335189819\n",
      "Epoch 46 Batch: 10 Train Loss: 0.0812365934252739\n",
      "Epoch 46 Batch: 12 Train Loss: 0.06189652159810066\n",
      "Epoch 46 Batch: 14 Train Loss: 0.07563942670822144\n",
      "Epoch 46 Batch: 16 Train Loss: 0.061879150569438934\n",
      "Epoch 46 Batch: 18 Train Loss: 0.07085566222667694\n",
      "Epoch 46 Batch: 20 Train Loss: 0.2961825430393219\n",
      "Epoch 46 Batch: 22 Train Loss: 0.030334223061800003\n",
      "Epoch 46 Batch: 24 Train Loss: 0.07119086384773254\n",
      "Epoch 46 Batch: 26 Train Loss: 0.04768671467900276\n",
      "Epoch 46 Batch: 28 Train Loss: 0.0596809908747673\n",
      "Epoch 46 Batch: 30 Train Loss: 0.30153632164001465\n",
      "Epoch 46 Batch: 32 Train Loss: 0.03502706438302994\n",
      "Epoch 46 Batch: 34 Train Loss: 0.30643022060394287\n",
      "Epoch 46 Batch: 36 Train Loss: 0.3047889471054077\n",
      "Epoch 46 Batch: 38 Train Loss: 0.04891417175531387\n",
      "Epoch 46 Batch: 40 Train Loss: 0.2908015251159668\n",
      "Epoch 46 Batch: 42 Train Loss: 0.28038057684898376\n",
      "Epoch 46 Batch: 44 Train Loss: 0.07229150831699371\n",
      "Epoch 46 Batch: 46 Train Loss: 0.2704121470451355\n",
      "Epoch 46 Batch: 48 Train Loss: 0.5134851336479187\n",
      "Epoch 46 Batch: 50 Train Loss: 0.09134091436862946\n",
      "Epoch 46 Batch: 52 Train Loss: 0.31314408779144287\n",
      "Epoch 46 Batch: 54 Train Loss: 0.5162450075149536\n",
      "Epoch 46 Batch: 56 Train Loss: 0.022472059354186058\n",
      "Epoch 46 Batch: 58 Train Loss: 0.036135997623205185\n",
      "Epoch 46 Batch: 60 Train Loss: 0.06696335971355438\n",
      "Epoch 46 Batch: 62 Train Loss: 0.051826201379299164\n",
      "Epoch 46 Batch: 64 Train Loss: 0.06983999907970428\n",
      "Epoch 46 Batch: 66 Train Loss: 0.6664581298828125\n",
      "Epoch 46 Batch: 68 Train Loss: 0.22588594257831573\n",
      "Epoch 46 Batch: 70 Train Loss: 0.06298189610242844\n",
      "Epoch 46 Batch: 72 Train Loss: 0.08323320001363754\n",
      "Epoch 46 Batch: 74 Train Loss: 0.04985860735177994\n",
      "Epoch 46 Batch: 76 Train Loss: 0.10388736426830292\n",
      "Epoch 46 Batch: 78 Train Loss: 0.29374009370803833\n",
      "Epoch 46 Batch: 80 Train Loss: 0.2613966464996338\n",
      "Epoch 46 Batch: 82 Train Loss: 0.07342825829982758\n",
      "Epoch 46 Batch: 84 Train Loss: 0.057800404727458954\n",
      "Epoch 46 Batch: 86 Train Loss: 0.08768002688884735\n",
      "Epoch 46 Batch: 88 Train Loss: 0.2840004563331604\n",
      "Epoch 46 Batch: 90 Train Loss: 0.2552149295806885\n",
      "Epoch 46 Batch: 92 Train Loss: 0.0848185271024704\n",
      "Epoch 46 Batch: 94 Train Loss: 0.27112624049186707\n",
      "Epoch 46 Batch: 96 Train Loss: 0.06730304658412933\n",
      "Epoch 46 Batch: 98 Train Loss: 0.2698178291320801\n",
      "Epoch 46 Batch: 100 Train Loss: 0.059227049350738525\n",
      "Epoch 46 Batch: 102 Train Loss: 0.574657142162323\n",
      "Epoch 46 Batch: 104 Train Loss: 0.06822522729635239\n",
      "Epoch 46 Batch: 106 Train Loss: 0.060628682374954224\n",
      "Epoch 46 Batch: 108 Train Loss: 0.28670454025268555\n",
      "Epoch 46 Batch: 110 Train Loss: 0.06297066807746887\n",
      "Epoch 46 Batch: 112 Train Loss: 0.4746360778808594\n",
      "Epoch 46 Batch: 114 Train Loss: 0.06500223278999329\n",
      "Epoch 46 Batch: 116 Train Loss: 0.05798481032252312\n",
      "Epoch 46 Batch: 118 Train Loss: 0.2685900330543518\n",
      "Epoch 46 Batch: 120 Train Loss: 0.05608828738331795\n",
      "Epoch 46 Batch: 122 Train Loss: 0.06867571175098419\n",
      "Epoch 46 Batch: 124 Train Loss: 0.5171878933906555\n",
      "Epoch 46 Batch: 126 Train Loss: 0.043544523417949677\n",
      "Epoch 46 Batch: 128 Train Loss: 0.031462348997592926\n",
      "Epoch 46 Batch: 130 Train Loss: 0.058401919901371\n",
      "Epoch 46 Batch: 132 Train Loss: 0.04303266480565071\n",
      "Epoch 46 Batch: 134 Train Loss: 0.05045134946703911\n",
      "Epoch 46 Batch: 136 Train Loss: 0.02753717079758644\n",
      "Epoch 46 Batch: 138 Train Loss: 0.526266872882843\n",
      "Epoch 46 Batch: 140 Train Loss: 0.04483342915773392\n",
      "Epoch 46 Batch: 142 Train Loss: 0.04534078761935234\n",
      "Epoch 46 Batch: 144 Train Loss: 0.0500197596848011\n",
      "Epoch 46 Batch: 146 Train Loss: 0.020950164645910263\n",
      "Epoch 46 Batch: 148 Train Loss: 0.0320291742682457\n",
      "Epoch 46 Batch: 150 Train Loss: 0.04690767452120781\n",
      "Epoch 46 Batch: 152 Train Loss: 0.3201906681060791\n",
      "Epoch 46 Batch: 154 Train Loss: 0.30930954217910767\n",
      "Epoch 46 Batch: 156 Train Loss: 0.3243432641029358\n",
      "Epoch 46 Batch: 158 Train Loss: 0.04032561555504799\n",
      "Epoch 46 Batch: 160 Train Loss: 0.6161165833473206\n",
      "Epoch 47 Batch: 2 Train Loss: 0.8242460489273071\n",
      "Epoch 47 Batch: 4 Train Loss: 0.06090259552001953\n",
      "Epoch 47 Batch: 6 Train Loss: 0.04259657487273216\n",
      "Epoch 47 Batch: 8 Train Loss: 0.03807218000292778\n",
      "Epoch 47 Batch: 10 Train Loss: 0.04772808402776718\n",
      "Epoch 47 Batch: 12 Train Loss: 0.051096249371767044\n",
      "Epoch 47 Batch: 14 Train Loss: 0.31176140904426575\n",
      "Epoch 47 Batch: 16 Train Loss: 0.0529320128262043\n",
      "Epoch 47 Batch: 18 Train Loss: 0.05701042339205742\n",
      "Epoch 47 Batch: 20 Train Loss: 0.064744733273983\n",
      "Epoch 47 Batch: 22 Train Loss: 0.2602764666080475\n",
      "Epoch 47 Batch: 24 Train Loss: 0.057880204170942307\n",
      "Epoch 47 Batch: 26 Train Loss: 0.2748815417289734\n",
      "Epoch 47 Batch: 28 Train Loss: 0.3134859800338745\n",
      "Epoch 47 Batch: 30 Train Loss: 0.043831102550029755\n",
      "Epoch 47 Batch: 32 Train Loss: 0.5250078439712524\n",
      "Epoch 47 Batch: 34 Train Loss: 0.06647490710020065\n",
      "Epoch 47 Batch: 36 Train Loss: 0.3076108694076538\n",
      "Epoch 47 Batch: 38 Train Loss: 0.04696742445230484\n",
      "Epoch 47 Batch: 40 Train Loss: 0.05185544490814209\n",
      "Epoch 47 Batch: 42 Train Loss: 0.07120959460735321\n",
      "Epoch 47 Batch: 44 Train Loss: 0.039955221116542816\n",
      "Epoch 47 Batch: 46 Train Loss: 0.04571657255291939\n",
      "Epoch 47 Batch: 48 Train Loss: 0.5196840167045593\n",
      "Epoch 47 Batch: 50 Train Loss: 0.0359031967818737\n",
      "Epoch 47 Batch: 52 Train Loss: 0.04535184055566788\n",
      "Epoch 47 Batch: 54 Train Loss: 0.5576445460319519\n",
      "Epoch 47 Batch: 56 Train Loss: 0.05588831380009651\n",
      "Epoch 47 Batch: 58 Train Loss: 0.03536059334874153\n",
      "Epoch 47 Batch: 60 Train Loss: 0.27915292978286743\n",
      "Epoch 47 Batch: 62 Train Loss: 0.06340165436267853\n",
      "Epoch 47 Batch: 64 Train Loss: 0.057555317878723145\n",
      "Epoch 47 Batch: 66 Train Loss: 0.035857878625392914\n",
      "Epoch 47 Batch: 68 Train Loss: 0.07020550966262817\n",
      "Epoch 47 Batch: 70 Train Loss: 0.2970297038555145\n",
      "Epoch 47 Batch: 72 Train Loss: 0.04822058603167534\n",
      "Epoch 47 Batch: 74 Train Loss: 0.2772343158721924\n",
      "Epoch 47 Batch: 76 Train Loss: 0.033047694712877274\n",
      "Epoch 47 Batch: 78 Train Loss: 0.013981044292449951\n",
      "Epoch 47 Batch: 80 Train Loss: 0.06661158800125122\n",
      "Epoch 47 Batch: 82 Train Loss: 0.02976282313466072\n",
      "Epoch 47 Batch: 84 Train Loss: 0.028904173523187637\n",
      "Epoch 47 Batch: 86 Train Loss: 0.031099244952201843\n",
      "Epoch 47 Batch: 88 Train Loss: 0.05798335745930672\n",
      "Epoch 47 Batch: 90 Train Loss: 0.33531802892684937\n",
      "Epoch 47 Batch: 92 Train Loss: 0.3012197017669678\n",
      "Epoch 47 Batch: 94 Train Loss: 0.2802627682685852\n",
      "Epoch 47 Batch: 96 Train Loss: 0.023929251357913017\n",
      "Epoch 47 Batch: 98 Train Loss: 0.2983396649360657\n",
      "Epoch 47 Batch: 100 Train Loss: 0.02023591473698616\n",
      "Epoch 47 Batch: 102 Train Loss: 0.2860458791255951\n",
      "Epoch 47 Batch: 104 Train Loss: 0.05016444995999336\n",
      "Epoch 47 Batch: 106 Train Loss: 0.05306854844093323\n",
      "Epoch 47 Batch: 108 Train Loss: 0.29975512623786926\n",
      "Epoch 47 Batch: 110 Train Loss: 0.74763023853302\n",
      "Epoch 47 Batch: 112 Train Loss: 0.24003000557422638\n",
      "Epoch 47 Batch: 114 Train Loss: 0.2943194508552551\n",
      "Epoch 47 Batch: 116 Train Loss: 0.30238670110702515\n",
      "Epoch 47 Batch: 118 Train Loss: 0.2656197249889374\n",
      "Epoch 47 Batch: 120 Train Loss: 0.043526411056518555\n",
      "Epoch 47 Batch: 122 Train Loss: 0.07884394377470016\n",
      "Epoch 47 Batch: 124 Train Loss: 0.059330664575099945\n",
      "Epoch 47 Batch: 126 Train Loss: 0.04380996897816658\n",
      "Epoch 47 Batch: 128 Train Loss: 0.4963078498840332\n",
      "Epoch 47 Batch: 130 Train Loss: 0.03167165070772171\n",
      "Epoch 47 Batch: 132 Train Loss: 0.051250994205474854\n",
      "Epoch 47 Batch: 134 Train Loss: 0.5926880836486816\n",
      "Epoch 47 Batch: 136 Train Loss: 0.057231802493333817\n",
      "Epoch 47 Batch: 138 Train Loss: 0.07735992968082428\n",
      "Epoch 47 Batch: 140 Train Loss: 0.057937514036893845\n",
      "Epoch 47 Batch: 142 Train Loss: 0.045193709433078766\n",
      "Epoch 47 Batch: 144 Train Loss: 0.0575600191950798\n",
      "Epoch 47 Batch: 146 Train Loss: 0.4880211353302002\n",
      "Epoch 47 Batch: 148 Train Loss: 0.5095402598381042\n",
      "Epoch 47 Batch: 150 Train Loss: 0.054108522832393646\n",
      "Epoch 47 Batch: 152 Train Loss: 0.0679360181093216\n",
      "Epoch 47 Batch: 154 Train Loss: 0.05239998176693916\n",
      "Epoch 47 Batch: 156 Train Loss: 0.06333643198013306\n",
      "Epoch 47 Batch: 158 Train Loss: 0.30343496799468994\n",
      "Epoch 47 Batch: 160 Train Loss: 0.039104387164115906\n",
      "Epoch 48 Batch: 2 Train Loss: 0.07173635065555573\n",
      "Epoch 48 Batch: 4 Train Loss: 0.09244991093873978\n",
      "Epoch 48 Batch: 6 Train Loss: 0.08482520282268524\n",
      "Epoch 48 Batch: 8 Train Loss: 0.0696391761302948\n",
      "Epoch 48 Batch: 10 Train Loss: 0.061604153364896774\n",
      "Epoch 48 Batch: 12 Train Loss: 0.02439107373356819\n",
      "Epoch 48 Batch: 14 Train Loss: 0.026537487283349037\n",
      "Epoch 48 Batch: 16 Train Loss: 0.02315608039498329\n",
      "Epoch 48 Batch: 18 Train Loss: 0.03684902936220169\n",
      "Epoch 48 Batch: 20 Train Loss: 0.28639304637908936\n",
      "Epoch 48 Batch: 22 Train Loss: 0.585184633731842\n",
      "Epoch 48 Batch: 24 Train Loss: 0.01231305580586195\n",
      "Epoch 48 Batch: 26 Train Loss: 0.29701846837997437\n",
      "Epoch 48 Batch: 28 Train Loss: 0.02996562421321869\n",
      "Epoch 48 Batch: 30 Train Loss: 0.03125113993883133\n",
      "Epoch 48 Batch: 32 Train Loss: 0.05820118263363838\n",
      "Epoch 48 Batch: 34 Train Loss: 0.30548354983329773\n",
      "Epoch 48 Batch: 36 Train Loss: 0.03348076716065407\n",
      "Epoch 48 Batch: 38 Train Loss: 0.025025129318237305\n",
      "Epoch 48 Batch: 40 Train Loss: 0.036935195326805115\n",
      "Epoch 48 Batch: 42 Train Loss: 0.0592687726020813\n",
      "Epoch 48 Batch: 44 Train Loss: 0.02910217083990574\n",
      "Epoch 48 Batch: 46 Train Loss: 0.01759505644440651\n",
      "Epoch 48 Batch: 48 Train Loss: 0.29267948865890503\n",
      "Epoch 48 Batch: 50 Train Loss: 0.5449777841567993\n",
      "Epoch 48 Batch: 52 Train Loss: 0.2831297516822815\n",
      "Epoch 48 Batch: 54 Train Loss: 0.3022546172142029\n",
      "Epoch 48 Batch: 56 Train Loss: 0.30844491720199585\n",
      "Epoch 48 Batch: 58 Train Loss: 0.30160146951675415\n",
      "Epoch 48 Batch: 60 Train Loss: 0.04098876193165779\n",
      "Epoch 48 Batch: 62 Train Loss: 0.2716820240020752\n",
      "Epoch 48 Batch: 64 Train Loss: 0.08955243974924088\n",
      "Epoch 48 Batch: 66 Train Loss: 0.263510137796402\n",
      "Epoch 48 Batch: 68 Train Loss: 0.26936498284339905\n",
      "Epoch 48 Batch: 70 Train Loss: 0.011784895323216915\n",
      "Epoch 48 Batch: 72 Train Loss: 0.09080146253108978\n",
      "Epoch 48 Batch: 74 Train Loss: 0.3161082863807678\n",
      "Epoch 48 Batch: 76 Train Loss: 0.28528642654418945\n",
      "Epoch 48 Batch: 78 Train Loss: 0.1223701611161232\n",
      "Epoch 48 Batch: 80 Train Loss: 0.2972496747970581\n",
      "Epoch 48 Batch: 82 Train Loss: 0.01387757621705532\n",
      "Epoch 48 Batch: 84 Train Loss: 0.04215574637055397\n",
      "Epoch 48 Batch: 86 Train Loss: 0.04249167814850807\n",
      "Epoch 48 Batch: 88 Train Loss: 0.2632903754711151\n",
      "Epoch 48 Batch: 90 Train Loss: 0.3017359673976898\n",
      "Epoch 48 Batch: 92 Train Loss: 0.30585017800331116\n",
      "Epoch 48 Batch: 94 Train Loss: 0.06398145854473114\n",
      "Epoch 48 Batch: 96 Train Loss: 0.024652844294905663\n",
      "Epoch 48 Batch: 98 Train Loss: 0.26701146364212036\n",
      "Epoch 48 Batch: 100 Train Loss: 0.02482961118221283\n",
      "Epoch 48 Batch: 102 Train Loss: 0.06593665480613708\n",
      "Epoch 48 Batch: 104 Train Loss: 0.02930053509771824\n",
      "Epoch 48 Batch: 106 Train Loss: 0.05771663784980774\n",
      "Epoch 48 Batch: 108 Train Loss: 0.3065667152404785\n",
      "Epoch 48 Batch: 110 Train Loss: 0.012698261067271233\n",
      "Epoch 48 Batch: 112 Train Loss: 0.0561867281794548\n",
      "Epoch 48 Batch: 114 Train Loss: 0.29210740327835083\n",
      "Epoch 48 Batch: 116 Train Loss: 0.061736661940813065\n",
      "Epoch 48 Batch: 118 Train Loss: 0.057928722351789474\n",
      "Epoch 48 Batch: 120 Train Loss: 0.5568886995315552\n",
      "Epoch 48 Batch: 122 Train Loss: 0.04629120975732803\n",
      "Epoch 48 Batch: 124 Train Loss: 0.08374647796154022\n",
      "Epoch 48 Batch: 126 Train Loss: 0.2581528425216675\n",
      "Epoch 48 Batch: 128 Train Loss: 0.2774357199668884\n",
      "Epoch 48 Batch: 130 Train Loss: 0.2701078951358795\n",
      "Epoch 48 Batch: 132 Train Loss: 0.2976001501083374\n",
      "Epoch 48 Batch: 134 Train Loss: 0.04912492260336876\n",
      "Epoch 48 Batch: 136 Train Loss: 0.06351260095834732\n",
      "Epoch 48 Batch: 138 Train Loss: 0.27161985635757446\n",
      "Epoch 48 Batch: 140 Train Loss: 0.3086926341056824\n",
      "Epoch 48 Batch: 142 Train Loss: 0.5162554979324341\n",
      "Epoch 48 Batch: 144 Train Loss: 0.07099326699972153\n",
      "Epoch 48 Batch: 146 Train Loss: 0.31559160351753235\n",
      "Epoch 48 Batch: 148 Train Loss: 0.06902839243412018\n",
      "Epoch 48 Batch: 150 Train Loss: 0.4839157164096832\n",
      "Epoch 48 Batch: 152 Train Loss: 0.27368953824043274\n",
      "Epoch 48 Batch: 154 Train Loss: 0.08471927046775818\n",
      "Epoch 48 Batch: 156 Train Loss: 0.03652017563581467\n",
      "Epoch 48 Batch: 158 Train Loss: 0.0484730526804924\n",
      "Epoch 48 Batch: 160 Train Loss: 0.2829805910587311\n",
      "Epoch 49 Batch: 2 Train Loss: 0.2860773503780365\n",
      "Epoch 49 Batch: 4 Train Loss: 0.046138714998960495\n",
      "Epoch 49 Batch: 6 Train Loss: 0.29893869161605835\n",
      "Epoch 49 Batch: 8 Train Loss: 0.2531040608882904\n",
      "Epoch 49 Batch: 10 Train Loss: 0.2868150472640991\n",
      "Epoch 49 Batch: 12 Train Loss: 0.22122690081596375\n",
      "Epoch 49 Batch: 14 Train Loss: 0.09807664155960083\n",
      "Epoch 49 Batch: 16 Train Loss: 0.2750002145767212\n",
      "Epoch 49 Batch: 18 Train Loss: 0.2860342264175415\n",
      "Epoch 49 Batch: 20 Train Loss: 0.04914121329784393\n",
      "Epoch 49 Batch: 22 Train Loss: 0.26487088203430176\n",
      "Epoch 49 Batch: 24 Train Loss: 0.04811633378267288\n",
      "Epoch 49 Batch: 26 Train Loss: 0.05426918715238571\n",
      "Epoch 49 Batch: 28 Train Loss: 0.30351537466049194\n",
      "Epoch 49 Batch: 30 Train Loss: 0.05902940779924393\n",
      "Epoch 49 Batch: 32 Train Loss: 0.05767117068171501\n",
      "Epoch 49 Batch: 34 Train Loss: 0.3077351450920105\n",
      "Epoch 49 Batch: 36 Train Loss: 0.06447412818670273\n",
      "Epoch 49 Batch: 38 Train Loss: 0.04158959537744522\n",
      "Epoch 49 Batch: 40 Train Loss: 0.050051938742399216\n",
      "Epoch 49 Batch: 42 Train Loss: 0.3073400855064392\n",
      "Epoch 49 Batch: 44 Train Loss: 0.020641911774873734\n",
      "Epoch 49 Batch: 46 Train Loss: 0.041204579174518585\n",
      "Epoch 49 Batch: 48 Train Loss: 0.04626414552330971\n",
      "Epoch 49 Batch: 50 Train Loss: 0.04733295366168022\n",
      "Epoch 49 Batch: 52 Train Loss: 0.29828572273254395\n",
      "Epoch 49 Batch: 54 Train Loss: 0.059145160019397736\n",
      "Epoch 49 Batch: 56 Train Loss: 0.29637497663497925\n",
      "Epoch 49 Batch: 58 Train Loss: 0.5521149635314941\n",
      "Epoch 49 Batch: 60 Train Loss: 0.29849663376808167\n",
      "Epoch 49 Batch: 62 Train Loss: 0.06837062537670135\n",
      "Epoch 49 Batch: 64 Train Loss: 0.03385138139128685\n",
      "Epoch 49 Batch: 66 Train Loss: 0.043459564447402954\n",
      "Epoch 49 Batch: 68 Train Loss: 0.02599460445344448\n",
      "Epoch 49 Batch: 70 Train Loss: 0.0680738240480423\n",
      "Epoch 49 Batch: 72 Train Loss: 0.27067604660987854\n",
      "Epoch 49 Batch: 74 Train Loss: 0.28723829984664917\n",
      "Epoch 49 Batch: 76 Train Loss: 0.06420110166072845\n",
      "Epoch 49 Batch: 78 Train Loss: 0.07124130427837372\n",
      "Epoch 49 Batch: 80 Train Loss: 0.017893938347697258\n",
      "Epoch 49 Batch: 82 Train Loss: 0.30762213468551636\n",
      "Epoch 49 Batch: 84 Train Loss: 0.061891186982393265\n",
      "Epoch 49 Batch: 86 Train Loss: 0.07785056531429291\n",
      "Epoch 49 Batch: 88 Train Loss: 0.06873223930597305\n",
      "Epoch 49 Batch: 90 Train Loss: 0.061699967831373215\n",
      "Epoch 49 Batch: 92 Train Loss: 0.5198315382003784\n",
      "Epoch 49 Batch: 94 Train Loss: 0.04440924897789955\n",
      "Epoch 49 Batch: 96 Train Loss: 0.05842835456132889\n",
      "Epoch 49 Batch: 98 Train Loss: 0.045590054243803024\n",
      "Epoch 49 Batch: 100 Train Loss: 0.04100094735622406\n",
      "Epoch 49 Batch: 102 Train Loss: 0.26995420455932617\n",
      "Epoch 49 Batch: 104 Train Loss: 0.05529054254293442\n",
      "Epoch 49 Batch: 106 Train Loss: 0.30665338039398193\n",
      "Epoch 49 Batch: 108 Train Loss: 0.30629974603652954\n",
      "Epoch 49 Batch: 110 Train Loss: 0.04380172863602638\n",
      "Epoch 49 Batch: 112 Train Loss: 0.33235520124435425\n",
      "Epoch 49 Batch: 114 Train Loss: 0.034184545278549194\n",
      "Epoch 49 Batch: 116 Train Loss: 0.3058858811855316\n",
      "Epoch 49 Batch: 118 Train Loss: 0.058693598955869675\n",
      "Epoch 49 Batch: 120 Train Loss: 0.0402948334813118\n",
      "Epoch 49 Batch: 122 Train Loss: 0.2992296516895294\n",
      "Epoch 49 Batch: 124 Train Loss: 0.030386533588171005\n",
      "Epoch 49 Batch: 126 Train Loss: 0.05041690543293953\n",
      "Epoch 49 Batch: 128 Train Loss: 0.03892470896244049\n",
      "Epoch 49 Batch: 130 Train Loss: 0.2995961904525757\n",
      "Epoch 49 Batch: 132 Train Loss: 0.3258616626262665\n",
      "Epoch 49 Batch: 134 Train Loss: 0.042030803859233856\n",
      "Epoch 49 Batch: 136 Train Loss: 0.07035686075687408\n",
      "Epoch 49 Batch: 138 Train Loss: 0.27278393507003784\n",
      "Epoch 49 Batch: 140 Train Loss: 0.033318985253572464\n",
      "Epoch 49 Batch: 142 Train Loss: 0.29910212755203247\n",
      "Epoch 49 Batch: 144 Train Loss: 0.7744945287704468\n",
      "Epoch 49 Batch: 146 Train Loss: 0.05199538543820381\n",
      "Epoch 49 Batch: 148 Train Loss: 0.04778694361448288\n",
      "Epoch 49 Batch: 150 Train Loss: 0.5009609460830688\n",
      "Epoch 49 Batch: 152 Train Loss: 0.7057226896286011\n",
      "Epoch 49 Batch: 154 Train Loss: 0.05833035707473755\n",
      "Epoch 49 Batch: 156 Train Loss: 0.1500099152326584\n",
      "Epoch 49 Batch: 158 Train Loss: 0.07665417343378067\n",
      "Epoch 49 Batch: 160 Train Loss: 0.25344938039779663\n",
      "Epoch 50 Batch: 2 Train Loss: 0.2561419904232025\n",
      "Epoch 50 Batch: 4 Train Loss: 0.07745356857776642\n",
      "Epoch 50 Batch: 6 Train Loss: 0.07941912114620209\n",
      "Epoch 50 Batch: 8 Train Loss: 0.2794729173183441\n",
      "Epoch 50 Batch: 10 Train Loss: 0.05767924711108208\n",
      "Epoch 50 Batch: 12 Train Loss: 0.2896321713924408\n",
      "Epoch 50 Batch: 14 Train Loss: 0.4888538420200348\n",
      "Epoch 50 Batch: 16 Train Loss: 0.07203638553619385\n",
      "Epoch 50 Batch: 18 Train Loss: 0.05288970470428467\n",
      "Epoch 50 Batch: 20 Train Loss: 0.06278283894062042\n",
      "Epoch 50 Batch: 22 Train Loss: 0.06927794218063354\n",
      "Epoch 50 Batch: 24 Train Loss: 0.26594090461730957\n",
      "Epoch 50 Batch: 26 Train Loss: 0.03843039274215698\n",
      "Epoch 50 Batch: 28 Train Loss: 0.05717693641781807\n",
      "Epoch 50 Batch: 30 Train Loss: 0.3251362144947052\n",
      "Epoch 50 Batch: 32 Train Loss: 0.05745374411344528\n",
      "Epoch 50 Batch: 34 Train Loss: 0.04128766059875488\n",
      "Epoch 50 Batch: 36 Train Loss: 0.05027122423052788\n",
      "Epoch 50 Batch: 38 Train Loss: 0.016655199229717255\n",
      "Epoch 50 Batch: 40 Train Loss: 0.022765744477510452\n",
      "Epoch 50 Batch: 42 Train Loss: 0.308906227350235\n",
      "Epoch 50 Batch: 44 Train Loss: 0.02918086014688015\n",
      "Epoch 50 Batch: 46 Train Loss: 0.04456467181444168\n",
      "Epoch 50 Batch: 48 Train Loss: 0.3088456392288208\n",
      "Epoch 50 Batch: 50 Train Loss: 0.6301639080047607\n",
      "Epoch 50 Batch: 52 Train Loss: 0.029416922479867935\n",
      "Epoch 50 Batch: 54 Train Loss: 0.3103627860546112\n",
      "Epoch 50 Batch: 56 Train Loss: 0.06797633320093155\n",
      "Epoch 50 Batch: 58 Train Loss: 0.01675979234278202\n",
      "Epoch 50 Batch: 60 Train Loss: 0.03684302791953087\n",
      "Epoch 50 Batch: 62 Train Loss: 0.5297179222106934\n",
      "Epoch 50 Batch: 64 Train Loss: 0.3194647431373596\n",
      "Epoch 50 Batch: 66 Train Loss: 0.05739480257034302\n",
      "Epoch 50 Batch: 68 Train Loss: 0.2779304087162018\n",
      "Epoch 50 Batch: 70 Train Loss: 0.31351640820503235\n",
      "Epoch 50 Batch: 72 Train Loss: 0.26391395926475525\n",
      "Epoch 50 Batch: 74 Train Loss: 0.30317217111587524\n",
      "Epoch 50 Batch: 76 Train Loss: 0.28248751163482666\n",
      "Epoch 50 Batch: 78 Train Loss: 0.1062433272600174\n",
      "Epoch 50 Batch: 80 Train Loss: 0.06014425680041313\n",
      "Epoch 50 Batch: 82 Train Loss: 0.08700726926326752\n",
      "Epoch 50 Batch: 84 Train Loss: 0.25449472665786743\n",
      "Epoch 50 Batch: 86 Train Loss: 0.26921191811561584\n",
      "Epoch 50 Batch: 88 Train Loss: 0.24392381310462952\n",
      "Epoch 50 Batch: 90 Train Loss: 0.07925088703632355\n",
      "Epoch 50 Batch: 92 Train Loss: 0.06772824376821518\n",
      "Epoch 50 Batch: 94 Train Loss: 0.3108418881893158\n",
      "Epoch 50 Batch: 96 Train Loss: 0.05080803483724594\n",
      "Epoch 50 Batch: 98 Train Loss: 0.035996213555336\n",
      "Epoch 50 Batch: 100 Train Loss: 0.051767341792583466\n",
      "Epoch 50 Batch: 102 Train Loss: 0.046930551528930664\n",
      "Epoch 50 Batch: 104 Train Loss: 0.03969760984182358\n",
      "Epoch 50 Batch: 106 Train Loss: 0.29884329438209534\n",
      "Epoch 50 Batch: 108 Train Loss: 0.05189727991819382\n",
      "Epoch 50 Batch: 110 Train Loss: 0.3056860566139221\n",
      "Epoch 50 Batch: 112 Train Loss: 0.30069249868392944\n",
      "Epoch 50 Batch: 114 Train Loss: 0.5420577526092529\n",
      "Epoch 50 Batch: 116 Train Loss: 0.02526148036122322\n",
      "Epoch 50 Batch: 118 Train Loss: 0.2735592722892761\n",
      "Epoch 50 Batch: 120 Train Loss: 0.06075306609272957\n",
      "Epoch 50 Batch: 122 Train Loss: 0.31308504939079285\n",
      "Epoch 50 Batch: 124 Train Loss: 0.05442291498184204\n",
      "Epoch 50 Batch: 126 Train Loss: 0.5034539103507996\n",
      "Epoch 50 Batch: 128 Train Loss: 0.0983341783285141\n",
      "Epoch 50 Batch: 130 Train Loss: 0.28677603602409363\n",
      "Epoch 50 Batch: 132 Train Loss: 0.2877514362335205\n",
      "Epoch 50 Batch: 134 Train Loss: 0.25161704421043396\n",
      "Epoch 50 Batch: 136 Train Loss: 0.09234585613012314\n",
      "Epoch 50 Batch: 138 Train Loss: 0.26231908798217773\n",
      "Epoch 50 Batch: 140 Train Loss: 0.3039039075374603\n",
      "Epoch 50 Batch: 142 Train Loss: 0.08268482983112335\n",
      "Epoch 50 Batch: 144 Train Loss: 0.4700215458869934\n",
      "Epoch 50 Batch: 146 Train Loss: 0.11620040237903595\n",
      "Epoch 50 Batch: 148 Train Loss: 0.2900608479976654\n",
      "Epoch 50 Batch: 150 Train Loss: 0.04825275018811226\n",
      "Epoch 50 Batch: 152 Train Loss: 0.08127976953983307\n",
      "Epoch 50 Batch: 154 Train Loss: 0.3015463948249817\n",
      "Epoch 50 Batch: 156 Train Loss: 0.2906554341316223\n",
      "Epoch 50 Batch: 158 Train Loss: 0.2904130816459656\n",
      "Epoch 50 Batch: 160 Train Loss: 0.06394964456558228\n",
      "Epoch 51 Batch: 2 Train Loss: 0.2718413174152374\n",
      "Epoch 51 Batch: 4 Train Loss: 0.30208075046539307\n",
      "Epoch 51 Batch: 6 Train Loss: 0.043961383402347565\n",
      "Epoch 51 Batch: 8 Train Loss: 0.28799358010292053\n",
      "Epoch 51 Batch: 10 Train Loss: 0.2884819209575653\n",
      "Epoch 51 Batch: 12 Train Loss: 0.31299370527267456\n",
      "Epoch 51 Batch: 14 Train Loss: 0.33192136883735657\n",
      "Epoch 51 Batch: 16 Train Loss: 0.3060227036476135\n",
      "Epoch 51 Batch: 18 Train Loss: 0.04256591200828552\n",
      "Epoch 51 Batch: 20 Train Loss: 0.024998683482408524\n",
      "Epoch 51 Batch: 22 Train Loss: 0.29520633816719055\n",
      "Epoch 51 Batch: 24 Train Loss: 0.06143976375460625\n",
      "Epoch 51 Batch: 26 Train Loss: 0.04570888355374336\n",
      "Epoch 51 Batch: 28 Train Loss: 0.32005009055137634\n",
      "Epoch 51 Batch: 30 Train Loss: 0.4849851131439209\n",
      "Epoch 51 Batch: 32 Train Loss: 0.504952609539032\n",
      "Epoch 51 Batch: 34 Train Loss: 0.05763646960258484\n",
      "Epoch 51 Batch: 36 Train Loss: 0.27880993485450745\n",
      "Epoch 51 Batch: 38 Train Loss: 0.06492576748132706\n",
      "Epoch 51 Batch: 40 Train Loss: 0.30657655000686646\n",
      "Epoch 51 Batch: 42 Train Loss: 0.09553875029087067\n",
      "Epoch 51 Batch: 44 Train Loss: 0.0562773160636425\n",
      "Epoch 51 Batch: 46 Train Loss: 0.2578583359718323\n",
      "Epoch 51 Batch: 48 Train Loss: 0.25807708501815796\n",
      "Epoch 51 Batch: 50 Train Loss: 0.2859140634536743\n",
      "Epoch 51 Batch: 52 Train Loss: 0.08082669228315353\n",
      "Epoch 51 Batch: 54 Train Loss: 0.034440912306308746\n",
      "Epoch 51 Batch: 56 Train Loss: 0.5396679639816284\n",
      "Epoch 51 Batch: 58 Train Loss: 0.03572937101125717\n",
      "Epoch 51 Batch: 60 Train Loss: 0.06609508395195007\n",
      "Epoch 51 Batch: 62 Train Loss: 0.2875813841819763\n",
      "Epoch 51 Batch: 64 Train Loss: 0.26837414503097534\n",
      "Epoch 51 Batch: 66 Train Loss: 0.06292204558849335\n",
      "Epoch 51 Batch: 68 Train Loss: 0.06674420833587646\n",
      "Epoch 51 Batch: 70 Train Loss: 0.44674867391586304\n",
      "Epoch 51 Batch: 72 Train Loss: 0.0931740552186966\n",
      "Epoch 51 Batch: 74 Train Loss: 0.08037678897380829\n",
      "Epoch 51 Batch: 76 Train Loss: 0.5124958753585815\n",
      "Epoch 51 Batch: 78 Train Loss: 0.29365190863609314\n",
      "Epoch 51 Batch: 80 Train Loss: 0.2897152304649353\n",
      "Epoch 51 Batch: 82 Train Loss: 0.04271426051855087\n",
      "Epoch 51 Batch: 84 Train Loss: 0.32083481550216675\n",
      "Epoch 51 Batch: 86 Train Loss: 0.04930693656206131\n",
      "Epoch 51 Batch: 88 Train Loss: 0.6863023042678833\n",
      "Epoch 51 Batch: 90 Train Loss: 0.061608172953128815\n",
      "Epoch 51 Batch: 92 Train Loss: 0.05479142814874649\n",
      "Epoch 51 Batch: 94 Train Loss: 0.03923095017671585\n",
      "Epoch 51 Batch: 96 Train Loss: 0.05279328674077988\n",
      "Epoch 51 Batch: 98 Train Loss: 0.07809065282344818\n",
      "Epoch 51 Batch: 100 Train Loss: 0.0899137631058693\n",
      "Epoch 51 Batch: 102 Train Loss: 0.08916632831096649\n",
      "Epoch 51 Batch: 104 Train Loss: 0.5085404515266418\n",
      "Epoch 51 Batch: 106 Train Loss: 0.05219714716076851\n",
      "Epoch 51 Batch: 108 Train Loss: 0.05825284868478775\n",
      "Epoch 51 Batch: 110 Train Loss: 0.2724592089653015\n",
      "Epoch 51 Batch: 112 Train Loss: 0.06816170364618301\n",
      "Epoch 51 Batch: 114 Train Loss: 0.03340338543057442\n",
      "Epoch 51 Batch: 116 Train Loss: 0.03547590598464012\n",
      "Epoch 51 Batch: 118 Train Loss: 0.0526273176074028\n",
      "Epoch 51 Batch: 120 Train Loss: 0.31103628873825073\n",
      "Epoch 51 Batch: 122 Train Loss: 0.29808229207992554\n",
      "Epoch 51 Batch: 124 Train Loss: 0.32343459129333496\n",
      "Epoch 51 Batch: 126 Train Loss: 0.02674582600593567\n",
      "Epoch 51 Batch: 128 Train Loss: 0.29784566164016724\n",
      "Epoch 51 Batch: 130 Train Loss: 0.04243915528059006\n",
      "Epoch 51 Batch: 132 Train Loss: 0.03121194615960121\n",
      "Epoch 51 Batch: 134 Train Loss: 0.05079711601138115\n",
      "Epoch 51 Batch: 136 Train Loss: 0.5578858256340027\n",
      "Epoch 51 Batch: 138 Train Loss: 0.3000887632369995\n",
      "Epoch 51 Batch: 140 Train Loss: 0.06073799729347229\n",
      "Epoch 51 Batch: 142 Train Loss: 0.03496865555644035\n",
      "Epoch 51 Batch: 144 Train Loss: 0.0344710648059845\n",
      "Epoch 51 Batch: 146 Train Loss: 0.04621455818414688\n",
      "Epoch 51 Batch: 148 Train Loss: 0.032970525324344635\n",
      "Epoch 51 Batch: 150 Train Loss: 0.01790366694331169\n",
      "Epoch 51 Batch: 152 Train Loss: 0.3116195797920227\n",
      "Epoch 51 Batch: 154 Train Loss: 0.2582714259624481\n",
      "Epoch 51 Batch: 156 Train Loss: 0.3210345506668091\n",
      "Epoch 51 Batch: 158 Train Loss: 0.017341168597340584\n",
      "Epoch 51 Batch: 160 Train Loss: 0.03017411194741726\n",
      "Epoch 52 Batch: 2 Train Loss: 0.06953980028629303\n",
      "Epoch 52 Batch: 4 Train Loss: 0.03435208275914192\n",
      "Epoch 52 Batch: 6 Train Loss: 0.28820210695266724\n",
      "Epoch 52 Batch: 8 Train Loss: 0.5007277131080627\n",
      "Epoch 52 Batch: 10 Train Loss: 0.0639810711145401\n",
      "Epoch 52 Batch: 12 Train Loss: 0.06790798157453537\n",
      "Epoch 52 Batch: 14 Train Loss: 0.29206162691116333\n",
      "Epoch 52 Batch: 16 Train Loss: 0.06202968955039978\n",
      "Epoch 52 Batch: 18 Train Loss: 0.05119051784276962\n",
      "Epoch 52 Batch: 20 Train Loss: 0.2997756004333496\n",
      "Epoch 52 Batch: 22 Train Loss: 0.04961930960416794\n",
      "Epoch 52 Batch: 24 Train Loss: 0.24440228939056396\n",
      "Epoch 52 Batch: 26 Train Loss: 0.3071332573890686\n",
      "Epoch 52 Batch: 28 Train Loss: 0.06503601372241974\n",
      "Epoch 52 Batch: 30 Train Loss: 0.059699974954128265\n",
      "Epoch 52 Batch: 32 Train Loss: 0.04481910914182663\n",
      "Epoch 52 Batch: 34 Train Loss: 0.2509830594062805\n",
      "Epoch 52 Batch: 36 Train Loss: 0.4994196891784668\n",
      "Epoch 52 Batch: 38 Train Loss: 0.03145035356283188\n",
      "Epoch 52 Batch: 40 Train Loss: 0.27850478887557983\n",
      "Epoch 52 Batch: 42 Train Loss: 0.3002868592739105\n",
      "Epoch 52 Batch: 44 Train Loss: 0.05102727934718132\n",
      "Epoch 52 Batch: 46 Train Loss: 0.09076981991529465\n",
      "Epoch 52 Batch: 48 Train Loss: 0.05484485626220703\n",
      "Epoch 52 Batch: 50 Train Loss: 0.274211585521698\n",
      "Epoch 52 Batch: 52 Train Loss: 0.29813605546951294\n",
      "Epoch 52 Batch: 54 Train Loss: 0.25423663854599\n",
      "Epoch 52 Batch: 56 Train Loss: 0.08589854091405869\n",
      "Epoch 52 Batch: 58 Train Loss: 0.043155379593372345\n",
      "Epoch 52 Batch: 60 Train Loss: 0.0844789445400238\n",
      "Epoch 52 Batch: 62 Train Loss: 0.25724413990974426\n",
      "Epoch 52 Batch: 64 Train Loss: 0.28022947907447815\n",
      "Epoch 52 Batch: 66 Train Loss: 0.09091080725193024\n",
      "Epoch 52 Batch: 68 Train Loss: 0.2674473226070404\n",
      "Epoch 52 Batch: 70 Train Loss: 0.2895825207233429\n",
      "Epoch 52 Batch: 72 Train Loss: 0.04065827280282974\n",
      "Epoch 52 Batch: 74 Train Loss: 0.5465067625045776\n",
      "Epoch 52 Batch: 76 Train Loss: 0.04516240954399109\n",
      "Epoch 52 Batch: 78 Train Loss: 0.0674004778265953\n",
      "Epoch 52 Batch: 80 Train Loss: 0.0677962601184845\n",
      "Epoch 52 Batch: 82 Train Loss: 0.05331401154398918\n",
      "Epoch 52 Batch: 84 Train Loss: 0.281491219997406\n",
      "Epoch 52 Batch: 86 Train Loss: 0.04848238825798035\n",
      "Epoch 52 Batch: 88 Train Loss: 0.08771831542253494\n",
      "Epoch 52 Batch: 90 Train Loss: 0.04985498636960983\n",
      "Epoch 52 Batch: 92 Train Loss: 0.2672545909881592\n",
      "Epoch 52 Batch: 94 Train Loss: 0.05870170146226883\n",
      "Epoch 52 Batch: 96 Train Loss: 0.30427756905555725\n",
      "Epoch 52 Batch: 98 Train Loss: 0.06144890934228897\n",
      "Epoch 52 Batch: 100 Train Loss: 0.03766787424683571\n",
      "Epoch 52 Batch: 102 Train Loss: 0.2667240500450134\n",
      "Epoch 52 Batch: 104 Train Loss: 0.07268629968166351\n",
      "Epoch 52 Batch: 106 Train Loss: 0.03646321967244148\n",
      "Epoch 52 Batch: 108 Train Loss: 0.28815922141075134\n",
      "Epoch 52 Batch: 110 Train Loss: 0.058272115886211395\n",
      "Epoch 52 Batch: 112 Train Loss: 0.03376346081495285\n",
      "Epoch 52 Batch: 114 Train Loss: 0.04878780245780945\n",
      "Epoch 52 Batch: 116 Train Loss: 0.042106613516807556\n",
      "Epoch 52 Batch: 118 Train Loss: 0.04419344663619995\n",
      "Epoch 52 Batch: 120 Train Loss: 0.3120914101600647\n",
      "Epoch 52 Batch: 122 Train Loss: 0.3214263617992401\n",
      "Epoch 52 Batch: 124 Train Loss: 0.043079450726509094\n",
      "Epoch 52 Batch: 126 Train Loss: 0.01791742444038391\n",
      "Epoch 52 Batch: 128 Train Loss: 0.3308399021625519\n",
      "Epoch 52 Batch: 130 Train Loss: 0.05257049947977066\n",
      "Epoch 52 Batch: 132 Train Loss: 0.03121115267276764\n",
      "Epoch 52 Batch: 134 Train Loss: 0.6663683652877808\n",
      "Epoch 52 Batch: 136 Train Loss: 0.05656731128692627\n",
      "Epoch 52 Batch: 138 Train Loss: 0.035545460879802704\n",
      "Epoch 52 Batch: 140 Train Loss: 0.026934761554002762\n",
      "Epoch 52 Batch: 142 Train Loss: 0.5809385180473328\n",
      "Epoch 52 Batch: 144 Train Loss: 0.06033334136009216\n",
      "Epoch 52 Batch: 146 Train Loss: 0.02634950913488865\n",
      "Epoch 52 Batch: 148 Train Loss: 0.02442901022732258\n",
      "Epoch 52 Batch: 150 Train Loss: 0.7607187032699585\n",
      "Epoch 52 Batch: 152 Train Loss: 0.048470258712768555\n",
      "Epoch 52 Batch: 154 Train Loss: 0.2665628492832184\n",
      "Epoch 52 Batch: 156 Train Loss: 0.29050832986831665\n",
      "Epoch 52 Batch: 158 Train Loss: 0.062163956463336945\n",
      "Epoch 52 Batch: 160 Train Loss: 0.25759458541870117\n",
      "Epoch 53 Batch: 2 Train Loss: 0.04661177843809128\n",
      "Epoch 53 Batch: 4 Train Loss: 0.26444584131240845\n",
      "Epoch 53 Batch: 6 Train Loss: 0.492108017206192\n",
      "Epoch 53 Batch: 8 Train Loss: 0.0846279188990593\n",
      "Epoch 53 Batch: 10 Train Loss: 0.2713022232055664\n",
      "Epoch 53 Batch: 12 Train Loss: 0.2647921144962311\n",
      "Epoch 53 Batch: 14 Train Loss: 0.29666972160339355\n",
      "Epoch 53 Batch: 16 Train Loss: 0.24708378314971924\n",
      "Epoch 53 Batch: 18 Train Loss: 0.27288880944252014\n",
      "Epoch 53 Batch: 20 Train Loss: 0.09718837589025497\n",
      "Epoch 53 Batch: 22 Train Loss: 0.041620463132858276\n",
      "Epoch 53 Batch: 24 Train Loss: 0.09360276162624359\n",
      "Epoch 53 Batch: 26 Train Loss: 0.05437887832522392\n",
      "Epoch 53 Batch: 28 Train Loss: 0.0613505057990551\n",
      "Epoch 53 Batch: 30 Train Loss: 0.05179606005549431\n",
      "Epoch 53 Batch: 32 Train Loss: 0.499807208776474\n",
      "Epoch 53 Batch: 34 Train Loss: 0.2812788486480713\n",
      "Epoch 53 Batch: 36 Train Loss: 0.04535481333732605\n",
      "Epoch 53 Batch: 38 Train Loss: 0.07860423624515533\n",
      "Epoch 53 Batch: 40 Train Loss: 0.06965750455856323\n",
      "Epoch 53 Batch: 42 Train Loss: 0.062215398997068405\n",
      "Epoch 53 Batch: 44 Train Loss: 0.507513165473938\n",
      "Epoch 53 Batch: 46 Train Loss: 0.2846066951751709\n",
      "Epoch 53 Batch: 48 Train Loss: 0.04349498078227043\n",
      "Epoch 53 Batch: 50 Train Loss: 0.05086453631520271\n",
      "Epoch 53 Batch: 52 Train Loss: 0.51241534948349\n",
      "Epoch 53 Batch: 54 Train Loss: 0.5441271662712097\n",
      "Epoch 53 Batch: 56 Train Loss: 0.3076472878456116\n",
      "Epoch 53 Batch: 58 Train Loss: 0.03954039886593819\n",
      "Epoch 53 Batch: 60 Train Loss: 0.27331438660621643\n",
      "Epoch 53 Batch: 62 Train Loss: 0.2683274447917938\n",
      "Epoch 53 Batch: 64 Train Loss: 0.06591568887233734\n",
      "Epoch 53 Batch: 66 Train Loss: 0.27540212869644165\n",
      "Epoch 53 Batch: 68 Train Loss: 0.25255346298217773\n",
      "Epoch 53 Batch: 70 Train Loss: 0.4628147482872009\n",
      "Epoch 53 Batch: 72 Train Loss: 0.43398720026016235\n",
      "Epoch 53 Batch: 74 Train Loss: 0.06258365511894226\n",
      "Epoch 53 Batch: 76 Train Loss: 0.08772750198841095\n",
      "Epoch 53 Batch: 78 Train Loss: 0.47353726625442505\n",
      "Epoch 53 Batch: 80 Train Loss: 0.08233334124088287\n",
      "Epoch 53 Batch: 82 Train Loss: 0.03640550747513771\n",
      "Epoch 53 Batch: 84 Train Loss: 0.03346797823905945\n",
      "Epoch 53 Batch: 86 Train Loss: 0.06490214169025421\n",
      "Epoch 53 Batch: 88 Train Loss: 0.0536455400288105\n",
      "Epoch 53 Batch: 90 Train Loss: 0.049282632768154144\n",
      "Epoch 53 Batch: 92 Train Loss: 0.047371070832014084\n",
      "Epoch 53 Batch: 94 Train Loss: 0.5446454882621765\n",
      "Epoch 53 Batch: 96 Train Loss: 0.06686337292194366\n",
      "Epoch 53 Batch: 98 Train Loss: 0.055250417441129684\n",
      "Epoch 53 Batch: 100 Train Loss: 0.2925170958042145\n",
      "Epoch 53 Batch: 102 Train Loss: 0.05370083451271057\n",
      "Epoch 53 Batch: 104 Train Loss: 0.031709492206573486\n",
      "Epoch 53 Batch: 106 Train Loss: 0.04471542686223984\n",
      "Epoch 53 Batch: 108 Train Loss: 0.30253902077674866\n",
      "Epoch 53 Batch: 110 Train Loss: 0.021323423832654953\n",
      "Epoch 53 Batch: 112 Train Loss: 0.3122592866420746\n",
      "Epoch 53 Batch: 114 Train Loss: 0.05613257735967636\n",
      "Epoch 53 Batch: 116 Train Loss: 0.028995806351304054\n",
      "Epoch 53 Batch: 118 Train Loss: 0.2867511212825775\n",
      "Epoch 53 Batch: 120 Train Loss: 0.2988686263561249\n",
      "Epoch 53 Batch: 122 Train Loss: 0.04056699946522713\n",
      "Epoch 53 Batch: 124 Train Loss: 0.035942092537879944\n",
      "Epoch 53 Batch: 126 Train Loss: 0.03925550729036331\n",
      "Epoch 53 Batch: 128 Train Loss: 0.02265338785946369\n",
      "Epoch 53 Batch: 130 Train Loss: 0.032977934926748276\n",
      "Epoch 53 Batch: 132 Train Loss: 0.033952340483665466\n",
      "Epoch 53 Batch: 134 Train Loss: 0.010982229374349117\n",
      "Epoch 53 Batch: 136 Train Loss: 0.6404691934585571\n",
      "Epoch 53 Batch: 138 Train Loss: 0.04159706458449364\n",
      "Epoch 53 Batch: 140 Train Loss: 0.028622910380363464\n",
      "Epoch 53 Batch: 142 Train Loss: 0.03709124028682709\n",
      "Epoch 53 Batch: 144 Train Loss: 0.021972164511680603\n",
      "Epoch 53 Batch: 146 Train Loss: 0.028445342555642128\n",
      "Epoch 53 Batch: 148 Train Loss: 0.282623827457428\n",
      "Epoch 53 Batch: 150 Train Loss: 0.055516500025987625\n",
      "Epoch 53 Batch: 152 Train Loss: 0.02537040039896965\n",
      "Epoch 53 Batch: 154 Train Loss: 0.3345831334590912\n",
      "Epoch 53 Batch: 156 Train Loss: 0.034973692148923874\n",
      "Epoch 53 Batch: 158 Train Loss: 0.04000318795442581\n",
      "Epoch 53 Batch: 160 Train Loss: 0.31506550312042236\n",
      "Epoch 54 Batch: 2 Train Loss: 0.03698670119047165\n",
      "Epoch 54 Batch: 4 Train Loss: 0.039258234202861786\n",
      "Epoch 54 Batch: 6 Train Loss: 0.05479864403605461\n",
      "Epoch 54 Batch: 8 Train Loss: 0.07849497348070145\n",
      "Epoch 54 Batch: 10 Train Loss: 0.06681350618600845\n",
      "Epoch 54 Batch: 12 Train Loss: 0.07042038440704346\n",
      "Epoch 54 Batch: 14 Train Loss: 0.5408653616905212\n",
      "Epoch 54 Batch: 16 Train Loss: 0.24631011486053467\n",
      "Epoch 54 Batch: 18 Train Loss: 0.27242910861968994\n",
      "Epoch 54 Batch: 20 Train Loss: 0.061562348157167435\n",
      "Epoch 54 Batch: 22 Train Loss: 0.09797791391611099\n",
      "Epoch 54 Batch: 24 Train Loss: 0.08513037860393524\n",
      "Epoch 54 Batch: 26 Train Loss: 0.3004346489906311\n",
      "Epoch 54 Batch: 28 Train Loss: 0.039872486144304276\n",
      "Epoch 54 Batch: 30 Train Loss: 0.05114219710230827\n",
      "Epoch 54 Batch: 32 Train Loss: 0.5612006783485413\n",
      "Epoch 54 Batch: 34 Train Loss: 0.028012480586767197\n",
      "Epoch 54 Batch: 36 Train Loss: 0.047826915979385376\n",
      "Epoch 54 Batch: 38 Train Loss: 0.03429469093680382\n",
      "Epoch 54 Batch: 40 Train Loss: 0.02129364386200905\n",
      "Epoch 54 Batch: 42 Train Loss: 0.31050848960876465\n",
      "Epoch 54 Batch: 44 Train Loss: 0.30847322940826416\n",
      "Epoch 54 Batch: 46 Train Loss: 0.292295902967453\n",
      "Epoch 54 Batch: 48 Train Loss: 0.31794363260269165\n",
      "Epoch 54 Batch: 50 Train Loss: 0.02610560692846775\n",
      "Epoch 54 Batch: 52 Train Loss: 0.026426082476973534\n",
      "Epoch 54 Batch: 54 Train Loss: 0.27518025040626526\n",
      "Epoch 54 Batch: 56 Train Loss: 0.04563476890325546\n",
      "Epoch 54 Batch: 58 Train Loss: 0.7700197100639343\n",
      "Epoch 54 Batch: 60 Train Loss: 0.07215233892202377\n",
      "Epoch 54 Batch: 62 Train Loss: 0.03942585736513138\n",
      "Epoch 54 Batch: 64 Train Loss: 0.040455758571624756\n",
      "Epoch 54 Batch: 66 Train Loss: 0.32369476556777954\n",
      "Epoch 54 Batch: 68 Train Loss: 0.3067757487297058\n",
      "Epoch 54 Batch: 70 Train Loss: 0.06033818796277046\n",
      "Epoch 54 Batch: 72 Train Loss: 0.29567793011665344\n",
      "Epoch 54 Batch: 74 Train Loss: 0.2733321189880371\n",
      "Epoch 54 Batch: 76 Train Loss: 0.05700470134615898\n",
      "Epoch 54 Batch: 78 Train Loss: 0.2592175006866455\n",
      "Epoch 54 Batch: 80 Train Loss: 0.27765873074531555\n",
      "Epoch 54 Batch: 82 Train Loss: 0.05105045437812805\n",
      "Epoch 54 Batch: 84 Train Loss: 0.07871876657009125\n",
      "Epoch 54 Batch: 86 Train Loss: 0.047252316027879715\n",
      "Epoch 54 Batch: 88 Train Loss: 0.08371950685977936\n",
      "Epoch 54 Batch: 90 Train Loss: 0.7447806596755981\n",
      "Epoch 54 Batch: 92 Train Loss: 0.04617699980735779\n",
      "Epoch 54 Batch: 94 Train Loss: 0.06810270249843597\n",
      "Epoch 54 Batch: 96 Train Loss: 0.32001179456710815\n",
      "Epoch 54 Batch: 98 Train Loss: 0.050393544137477875\n",
      "Epoch 54 Batch: 100 Train Loss: 0.03768300265073776\n",
      "Epoch 54 Batch: 102 Train Loss: 0.018658580258488655\n",
      "Epoch 54 Batch: 104 Train Loss: 0.06440100073814392\n",
      "Epoch 54 Batch: 106 Train Loss: 0.06807149946689606\n",
      "Epoch 54 Batch: 108 Train Loss: 0.5059354901313782\n",
      "Epoch 54 Batch: 110 Train Loss: 0.5293065309524536\n",
      "Epoch 54 Batch: 112 Train Loss: 0.04873840883374214\n",
      "Epoch 54 Batch: 114 Train Loss: 0.06403575837612152\n",
      "Epoch 54 Batch: 116 Train Loss: 0.04460002854466438\n",
      "Epoch 54 Batch: 118 Train Loss: 0.055072106420993805\n",
      "Epoch 54 Batch: 120 Train Loss: 0.2926653027534485\n",
      "Epoch 54 Batch: 122 Train Loss: 0.044405825436115265\n",
      "Epoch 54 Batch: 124 Train Loss: 0.03283027932047844\n",
      "Epoch 54 Batch: 126 Train Loss: 0.03839487209916115\n",
      "Epoch 54 Batch: 128 Train Loss: 0.04463072493672371\n",
      "Epoch 54 Batch: 130 Train Loss: 0.581339955329895\n",
      "Epoch 54 Batch: 132 Train Loss: 0.5559288263320923\n",
      "Epoch 54 Batch: 134 Train Loss: 0.037655968219041824\n",
      "Epoch 54 Batch: 136 Train Loss: 0.06806132197380066\n",
      "Epoch 54 Batch: 138 Train Loss: 0.48449334502220154\n",
      "Epoch 54 Batch: 140 Train Loss: 0.2889776825904846\n",
      "Epoch 54 Batch: 142 Train Loss: 0.06359092146158218\n",
      "Epoch 54 Batch: 144 Train Loss: 0.05167149379849434\n",
      "Epoch 54 Batch: 146 Train Loss: 0.28196126222610474\n",
      "Epoch 54 Batch: 148 Train Loss: 0.05072341486811638\n",
      "Epoch 54 Batch: 150 Train Loss: 0.06631455570459366\n",
      "Epoch 54 Batch: 152 Train Loss: 0.24973125755786896\n",
      "Epoch 54 Batch: 154 Train Loss: 0.2758740782737732\n",
      "Epoch 54 Batch: 156 Train Loss: 0.2587619125843048\n",
      "Epoch 54 Batch: 158 Train Loss: 0.08705612272024155\n",
      "Epoch 54 Batch: 160 Train Loss: 0.0267555620521307\n",
      "Epoch 55 Batch: 2 Train Loss: 0.049497298896312714\n",
      "Epoch 55 Batch: 4 Train Loss: 0.30602139234542847\n",
      "Epoch 55 Batch: 6 Train Loss: 0.47318440675735474\n",
      "Epoch 55 Batch: 8 Train Loss: 0.08313073217868805\n",
      "Epoch 55 Batch: 10 Train Loss: 0.482391893863678\n",
      "Epoch 55 Batch: 12 Train Loss: 0.2835347354412079\n",
      "Epoch 55 Batch: 14 Train Loss: 0.21400079131126404\n",
      "Epoch 55 Batch: 16 Train Loss: 0.044080477207899094\n",
      "Epoch 55 Batch: 18 Train Loss: 0.26772230863571167\n",
      "Epoch 55 Batch: 20 Train Loss: 0.11242121458053589\n",
      "Epoch 55 Batch: 22 Train Loss: 0.25047412514686584\n",
      "Epoch 55 Batch: 24 Train Loss: 0.4933062493801117\n",
      "Epoch 55 Batch: 26 Train Loss: 0.062257636338472366\n",
      "Epoch 55 Batch: 28 Train Loss: 0.08278679102659225\n",
      "Epoch 55 Batch: 30 Train Loss: 0.3228396773338318\n",
      "Epoch 55 Batch: 32 Train Loss: 0.2759745717048645\n",
      "Epoch 55 Batch: 34 Train Loss: 0.027306411415338516\n",
      "Epoch 55 Batch: 36 Train Loss: 0.06864689290523529\n",
      "Epoch 55 Batch: 38 Train Loss: 0.03159565478563309\n",
      "Epoch 55 Batch: 40 Train Loss: 0.27480781078338623\n",
      "Epoch 55 Batch: 42 Train Loss: 0.026045242324471474\n",
      "Epoch 55 Batch: 44 Train Loss: 0.026026267558336258\n",
      "Epoch 55 Batch: 46 Train Loss: 0.32706117630004883\n",
      "Epoch 55 Batch: 48 Train Loss: 0.31130486726760864\n",
      "Epoch 55 Batch: 50 Train Loss: 0.28674033284187317\n",
      "Epoch 55 Batch: 52 Train Loss: 0.31678053736686707\n",
      "Epoch 55 Batch: 54 Train Loss: 0.04246140643954277\n",
      "Epoch 55 Batch: 56 Train Loss: 0.030959483236074448\n",
      "Epoch 55 Batch: 58 Train Loss: 0.07104144990444183\n",
      "Epoch 55 Batch: 60 Train Loss: 0.06432204693555832\n",
      "Epoch 55 Batch: 62 Train Loss: 0.07363738119602203\n",
      "Epoch 55 Batch: 64 Train Loss: 0.039494290947914124\n",
      "Epoch 55 Batch: 66 Train Loss: 0.2714596092700958\n",
      "Epoch 55 Batch: 68 Train Loss: 0.5000280141830444\n",
      "Epoch 55 Batch: 70 Train Loss: 0.09396771341562271\n",
      "Epoch 55 Batch: 72 Train Loss: 0.2780839204788208\n",
      "Epoch 55 Batch: 74 Train Loss: 0.27717700600624084\n",
      "Epoch 55 Batch: 76 Train Loss: 0.016679059714078903\n",
      "Epoch 55 Batch: 78 Train Loss: 0.030419737100601196\n",
      "Epoch 55 Batch: 80 Train Loss: 0.04788577929139137\n",
      "Epoch 55 Batch: 82 Train Loss: 0.2696894109249115\n",
      "Epoch 55 Batch: 84 Train Loss: 0.34815701842308044\n",
      "Epoch 55 Batch: 86 Train Loss: 0.017306119203567505\n",
      "Epoch 55 Batch: 88 Train Loss: 0.2801287770271301\n",
      "Epoch 55 Batch: 90 Train Loss: 0.0552060604095459\n",
      "Epoch 55 Batch: 92 Train Loss: 0.3233537971973419\n",
      "Epoch 55 Batch: 94 Train Loss: 0.08058972656726837\n",
      "Epoch 55 Batch: 96 Train Loss: 0.08388401567935944\n",
      "Epoch 55 Batch: 98 Train Loss: 0.258899986743927\n",
      "Epoch 55 Batch: 100 Train Loss: 0.07604943215847015\n",
      "Epoch 55 Batch: 102 Train Loss: 0.30834054946899414\n",
      "Epoch 55 Batch: 104 Train Loss: 0.2760775685310364\n",
      "Epoch 55 Batch: 106 Train Loss: 0.7002212405204773\n",
      "Epoch 55 Batch: 108 Train Loss: 0.07903335243463516\n",
      "Epoch 55 Batch: 110 Train Loss: 0.011773246340453625\n",
      "Epoch 55 Batch: 112 Train Loss: 0.04883073642849922\n",
      "Epoch 55 Batch: 114 Train Loss: 0.305257111787796\n",
      "Epoch 55 Batch: 116 Train Loss: 0.06817881762981415\n",
      "Epoch 55 Batch: 118 Train Loss: 0.0663919672369957\n",
      "Epoch 55 Batch: 120 Train Loss: 0.018083130940794945\n",
      "Epoch 55 Batch: 122 Train Loss: 0.28948572278022766\n",
      "Epoch 55 Batch: 124 Train Loss: 0.3138597309589386\n",
      "Epoch 55 Batch: 126 Train Loss: 0.03341863676905632\n",
      "Epoch 55 Batch: 128 Train Loss: 0.2870294451713562\n",
      "Epoch 55 Batch: 130 Train Loss: 0.3333408236503601\n",
      "Epoch 55 Batch: 132 Train Loss: 0.2736175060272217\n",
      "Epoch 55 Batch: 134 Train Loss: 0.287297785282135\n",
      "Epoch 55 Batch: 136 Train Loss: 0.0642143040895462\n",
      "Epoch 55 Batch: 138 Train Loss: 0.05791193246841431\n",
      "Epoch 55 Batch: 140 Train Loss: 0.05793412774801254\n",
      "Epoch 55 Batch: 142 Train Loss: 0.043465755879879\n",
      "Epoch 55 Batch: 144 Train Loss: 0.0708850771188736\n",
      "Epoch 55 Batch: 146 Train Loss: 0.05945981666445732\n",
      "Epoch 55 Batch: 148 Train Loss: 0.07388394325971603\n",
      "Epoch 55 Batch: 150 Train Loss: 0.7228586673736572\n",
      "Epoch 55 Batch: 152 Train Loss: 0.05023304373025894\n",
      "Epoch 55 Batch: 154 Train Loss: 0.03423565626144409\n",
      "Epoch 55 Batch: 156 Train Loss: 0.052970826625823975\n",
      "Epoch 55 Batch: 158 Train Loss: 0.032124098390340805\n",
      "Epoch 55 Batch: 160 Train Loss: 0.05958540365099907\n",
      "Epoch 56 Batch: 2 Train Loss: 0.04982759803533554\n",
      "Epoch 56 Batch: 4 Train Loss: 0.27938929200172424\n",
      "Epoch 56 Batch: 6 Train Loss: 0.2375851571559906\n",
      "Epoch 56 Batch: 8 Train Loss: 0.062607042491436\n",
      "Epoch 56 Batch: 10 Train Loss: 0.28586551547050476\n",
      "Epoch 56 Batch: 12 Train Loss: 0.013556474819779396\n",
      "Epoch 56 Batch: 14 Train Loss: 0.06157642602920532\n",
      "Epoch 56 Batch: 16 Train Loss: 0.23194456100463867\n",
      "Epoch 56 Batch: 18 Train Loss: 0.05454765632748604\n",
      "Epoch 56 Batch: 20 Train Loss: 0.052813977003097534\n",
      "Epoch 56 Batch: 22 Train Loss: 0.08778214454650879\n",
      "Epoch 56 Batch: 24 Train Loss: 0.08601990342140198\n",
      "Epoch 56 Batch: 26 Train Loss: 0.021095234900712967\n",
      "Epoch 56 Batch: 28 Train Loss: 0.05197751522064209\n",
      "Epoch 56 Batch: 30 Train Loss: 0.5272918343544006\n",
      "Epoch 56 Batch: 32 Train Loss: 0.07075904309749603\n",
      "Epoch 56 Batch: 34 Train Loss: 0.02036100998520851\n",
      "Epoch 56 Batch: 36 Train Loss: 0.02846841886639595\n",
      "Epoch 56 Batch: 38 Train Loss: 0.05510856956243515\n",
      "Epoch 56 Batch: 40 Train Loss: 0.04802077263593674\n",
      "Epoch 56 Batch: 42 Train Loss: 0.052596576511859894\n",
      "Epoch 56 Batch: 44 Train Loss: 0.28841227293014526\n",
      "Epoch 56 Batch: 46 Train Loss: 0.020800117403268814\n",
      "Epoch 56 Batch: 48 Train Loss: 0.04423993080854416\n",
      "Epoch 56 Batch: 50 Train Loss: 0.27687984704971313\n",
      "Epoch 56 Batch: 52 Train Loss: 0.323689728975296\n",
      "Epoch 56 Batch: 54 Train Loss: 0.012307004071772099\n",
      "Epoch 56 Batch: 56 Train Loss: 0.021975044161081314\n",
      "Epoch 56 Batch: 58 Train Loss: 0.9402536153793335\n",
      "Epoch 56 Batch: 60 Train Loss: 0.3390287458896637\n",
      "Epoch 56 Batch: 62 Train Loss: 0.021138956770300865\n",
      "Epoch 56 Batch: 64 Train Loss: 0.278106153011322\n",
      "Epoch 56 Batch: 66 Train Loss: 0.03447926789522171\n",
      "Epoch 56 Batch: 68 Train Loss: 0.5793169736862183\n",
      "Epoch 56 Batch: 70 Train Loss: 0.04275773465633392\n",
      "Epoch 56 Batch: 72 Train Loss: 0.0666990876197815\n",
      "Epoch 56 Batch: 74 Train Loss: 0.5076812505722046\n",
      "Epoch 56 Batch: 76 Train Loss: 0.2903762757778168\n",
      "Epoch 56 Batch: 78 Train Loss: 0.02587972953915596\n",
      "Epoch 56 Batch: 80 Train Loss: 0.2754526138305664\n",
      "Epoch 56 Batch: 82 Train Loss: 0.05771782249212265\n",
      "Epoch 56 Batch: 84 Train Loss: 0.0963263064622879\n",
      "Epoch 56 Batch: 86 Train Loss: 0.2790668308734894\n",
      "Epoch 56 Batch: 88 Train Loss: 0.07279590517282486\n",
      "Epoch 56 Batch: 90 Train Loss: 0.26906639337539673\n",
      "Epoch 56 Batch: 92 Train Loss: 0.08922126889228821\n",
      "Epoch 56 Batch: 94 Train Loss: 0.2560504078865051\n",
      "Epoch 56 Batch: 96 Train Loss: 0.05332209914922714\n",
      "Epoch 56 Batch: 98 Train Loss: 0.09816443175077438\n",
      "Epoch 56 Batch: 100 Train Loss: 0.0477277971804142\n",
      "Epoch 56 Batch: 102 Train Loss: 0.06897000223398209\n",
      "Epoch 56 Batch: 104 Train Loss: 0.06922847777605057\n",
      "Epoch 56 Batch: 106 Train Loss: 0.01941085234284401\n",
      "Epoch 56 Batch: 108 Train Loss: 0.04572693631052971\n",
      "Epoch 56 Batch: 110 Train Loss: 0.30157747864723206\n",
      "Epoch 56 Batch: 112 Train Loss: 0.032546259462833405\n",
      "Epoch 56 Batch: 114 Train Loss: 0.04879799485206604\n",
      "Epoch 56 Batch: 116 Train Loss: 0.034115564078092575\n",
      "Epoch 56 Batch: 118 Train Loss: 0.600416362285614\n",
      "Epoch 56 Batch: 120 Train Loss: 0.0460255965590477\n",
      "Epoch 56 Batch: 122 Train Loss: 0.05714448541402817\n",
      "Epoch 56 Batch: 124 Train Loss: 0.32207080721855164\n",
      "Epoch 56 Batch: 126 Train Loss: 0.28710511326789856\n",
      "Epoch 56 Batch: 128 Train Loss: 0.3040372133255005\n",
      "Epoch 56 Batch: 130 Train Loss: 0.31814447045326233\n",
      "Epoch 56 Batch: 132 Train Loss: 0.3341657221317291\n",
      "Epoch 56 Batch: 134 Train Loss: 0.5256911516189575\n",
      "Epoch 56 Batch: 136 Train Loss: 0.04705701023340225\n",
      "Epoch 56 Batch: 138 Train Loss: 0.5067161321640015\n",
      "Epoch 56 Batch: 140 Train Loss: 0.050216250121593475\n",
      "Epoch 56 Batch: 142 Train Loss: 0.2856014370918274\n",
      "Epoch 56 Batch: 144 Train Loss: 0.045072875916957855\n",
      "Epoch 56 Batch: 146 Train Loss: 0.25246354937553406\n",
      "Epoch 56 Batch: 148 Train Loss: 0.043880004435777664\n",
      "Epoch 56 Batch: 150 Train Loss: 0.31756848096847534\n",
      "Epoch 56 Batch: 152 Train Loss: 0.3027872145175934\n",
      "Epoch 56 Batch: 154 Train Loss: 0.7340707182884216\n",
      "Epoch 56 Batch: 156 Train Loss: 0.5182603001594543\n",
      "Epoch 56 Batch: 158 Train Loss: 0.061861276626586914\n",
      "Epoch 56 Batch: 160 Train Loss: 0.08799588680267334\n",
      "Epoch 57 Batch: 2 Train Loss: 0.03602154552936554\n",
      "Epoch 57 Batch: 4 Train Loss: 0.27179381251335144\n",
      "Epoch 57 Batch: 6 Train Loss: 0.05639497563242912\n",
      "Epoch 57 Batch: 8 Train Loss: 0.26905161142349243\n",
      "Epoch 57 Batch: 10 Train Loss: 0.26858705282211304\n",
      "Epoch 57 Batch: 12 Train Loss: 0.053734730929136276\n",
      "Epoch 57 Batch: 14 Train Loss: 0.041945263743400574\n",
      "Epoch 57 Batch: 16 Train Loss: 0.27978333830833435\n",
      "Epoch 57 Batch: 18 Train Loss: 0.1016920655965805\n",
      "Epoch 57 Batch: 20 Train Loss: 0.05617622658610344\n",
      "Epoch 57 Batch: 22 Train Loss: 0.07913083583116531\n",
      "Epoch 57 Batch: 24 Train Loss: 0.09805267304182053\n",
      "Epoch 57 Batch: 26 Train Loss: 0.07423306256532669\n",
      "Epoch 57 Batch: 28 Train Loss: 0.26380205154418945\n",
      "Epoch 57 Batch: 30 Train Loss: 0.05083732679486275\n",
      "Epoch 57 Batch: 32 Train Loss: 0.2988615036010742\n",
      "Epoch 57 Batch: 34 Train Loss: 0.05931653827428818\n",
      "Epoch 57 Batch: 36 Train Loss: 0.27695637941360474\n",
      "Epoch 57 Batch: 38 Train Loss: 0.03709765523672104\n",
      "Epoch 57 Batch: 40 Train Loss: 0.27864429354667664\n",
      "Epoch 57 Batch: 42 Train Loss: 0.2989630103111267\n",
      "Epoch 57 Batch: 44 Train Loss: 0.03192903846502304\n",
      "Epoch 57 Batch: 46 Train Loss: 0.03952614217996597\n",
      "Epoch 57 Batch: 48 Train Loss: 0.2784764766693115\n",
      "Epoch 57 Batch: 50 Train Loss: 0.030752545222640038\n",
      "Epoch 57 Batch: 52 Train Loss: 0.03253960609436035\n",
      "Epoch 57 Batch: 54 Train Loss: 0.05532649904489517\n",
      "Epoch 57 Batch: 56 Train Loss: 0.29546183347702026\n",
      "Epoch 57 Batch: 58 Train Loss: 0.04936976358294487\n",
      "Epoch 57 Batch: 60 Train Loss: 0.0540999099612236\n",
      "Epoch 57 Batch: 62 Train Loss: 0.3054862916469574\n",
      "Epoch 57 Batch: 64 Train Loss: 0.03349946066737175\n",
      "Epoch 57 Batch: 66 Train Loss: 0.270052433013916\n",
      "Epoch 57 Batch: 68 Train Loss: 0.04076503589749336\n",
      "Epoch 57 Batch: 70 Train Loss: 0.06043016165494919\n",
      "Epoch 57 Batch: 72 Train Loss: 0.05704822018742561\n",
      "Epoch 57 Batch: 74 Train Loss: 0.04487953335046768\n",
      "Epoch 57 Batch: 76 Train Loss: 0.050327323377132416\n",
      "Epoch 57 Batch: 78 Train Loss: 0.056731365621089935\n",
      "Epoch 57 Batch: 80 Train Loss: 0.05183456093072891\n",
      "Epoch 57 Batch: 82 Train Loss: 0.04403627663850784\n",
      "Epoch 57 Batch: 84 Train Loss: 0.28662896156311035\n",
      "Epoch 57 Batch: 86 Train Loss: 0.2980877459049225\n",
      "Epoch 57 Batch: 88 Train Loss: 0.3116472363471985\n",
      "Epoch 57 Batch: 90 Train Loss: 0.015832597389817238\n",
      "Epoch 57 Batch: 92 Train Loss: 0.026622910052537918\n",
      "Epoch 57 Batch: 94 Train Loss: 0.30478042364120483\n",
      "Epoch 57 Batch: 96 Train Loss: 0.2979206144809723\n",
      "Epoch 57 Batch: 98 Train Loss: 0.037400126457214355\n",
      "Epoch 57 Batch: 100 Train Loss: 0.05750702694058418\n",
      "Epoch 57 Batch: 102 Train Loss: 0.03543438762426376\n",
      "Epoch 57 Batch: 104 Train Loss: 0.28725194931030273\n",
      "Epoch 57 Batch: 106 Train Loss: 0.049991633743047714\n",
      "Epoch 57 Batch: 108 Train Loss: 0.2574465572834015\n",
      "Epoch 57 Batch: 110 Train Loss: 0.024495840072631836\n",
      "Epoch 57 Batch: 112 Train Loss: 0.062176406383514404\n",
      "Epoch 57 Batch: 114 Train Loss: 0.3233007788658142\n",
      "Epoch 57 Batch: 116 Train Loss: 0.29624778032302856\n",
      "Epoch 57 Batch: 118 Train Loss: 0.3112500309944153\n",
      "Epoch 57 Batch: 120 Train Loss: 0.33475548028945923\n",
      "Epoch 57 Batch: 122 Train Loss: 0.2692103385925293\n",
      "Epoch 57 Batch: 124 Train Loss: 0.26440897583961487\n",
      "Epoch 57 Batch: 126 Train Loss: 0.5192058086395264\n",
      "Epoch 57 Batch: 128 Train Loss: 0.2743055820465088\n",
      "Epoch 57 Batch: 130 Train Loss: 0.05579264834523201\n",
      "Epoch 57 Batch: 132 Train Loss: 0.0656430795788765\n",
      "Epoch 57 Batch: 134 Train Loss: 0.06777895987033844\n",
      "Epoch 57 Batch: 136 Train Loss: 0.09345649182796478\n",
      "Epoch 57 Batch: 138 Train Loss: 0.31145861744880676\n",
      "Epoch 57 Batch: 140 Train Loss: 0.1060696467757225\n",
      "Epoch 57 Batch: 142 Train Loss: 0.08528037369251251\n",
      "Epoch 57 Batch: 144 Train Loss: 0.2647770941257477\n",
      "Epoch 57 Batch: 146 Train Loss: 0.32711851596832275\n",
      "Epoch 57 Batch: 148 Train Loss: 0.48515015840530396\n",
      "Epoch 57 Batch: 150 Train Loss: 0.28788965940475464\n",
      "Epoch 57 Batch: 152 Train Loss: 0.07529183477163315\n",
      "Epoch 57 Batch: 154 Train Loss: 0.2797955572605133\n",
      "Epoch 57 Batch: 156 Train Loss: 0.30169790983200073\n",
      "Epoch 57 Batch: 158 Train Loss: 0.02606840990483761\n",
      "Epoch 57 Batch: 160 Train Loss: 0.24135568737983704\n",
      "Epoch 58 Batch: 2 Train Loss: 0.0501125268638134\n",
      "Epoch 58 Batch: 4 Train Loss: 0.03147532418370247\n",
      "Epoch 58 Batch: 6 Train Loss: 0.06003691628575325\n",
      "Epoch 58 Batch: 8 Train Loss: 0.2522861361503601\n",
      "Epoch 58 Batch: 10 Train Loss: 0.3054795265197754\n",
      "Epoch 58 Batch: 12 Train Loss: 0.03361504152417183\n",
      "Epoch 58 Batch: 14 Train Loss: 0.04774032533168793\n",
      "Epoch 58 Batch: 16 Train Loss: 0.0428999662399292\n",
      "Epoch 58 Batch: 18 Train Loss: 0.3176913857460022\n",
      "Epoch 58 Batch: 20 Train Loss: 0.28490468859672546\n",
      "Epoch 58 Batch: 22 Train Loss: 0.2818172872066498\n",
      "Epoch 58 Batch: 24 Train Loss: 0.020413296297192574\n",
      "Epoch 58 Batch: 26 Train Loss: 0.30309373140335083\n",
      "Epoch 58 Batch: 28 Train Loss: 0.07048406451940536\n",
      "Epoch 58 Batch: 30 Train Loss: 0.2988649010658264\n",
      "Epoch 58 Batch: 32 Train Loss: 0.05463723465800285\n",
      "Epoch 58 Batch: 34 Train Loss: 0.05553129315376282\n",
      "Epoch 58 Batch: 36 Train Loss: 0.03389783203601837\n",
      "Epoch 58 Batch: 38 Train Loss: 0.07092957198619843\n",
      "Epoch 58 Batch: 40 Train Loss: 0.26482218503952026\n",
      "Epoch 58 Batch: 42 Train Loss: 0.27361899614334106\n",
      "Epoch 58 Batch: 44 Train Loss: 0.040708839893341064\n",
      "Epoch 58 Batch: 46 Train Loss: 0.07430767267942429\n",
      "Epoch 58 Batch: 48 Train Loss: 0.047702521085739136\n",
      "Epoch 58 Batch: 50 Train Loss: 0.0929950699210167\n",
      "Epoch 58 Batch: 52 Train Loss: 0.04340421408414841\n",
      "Epoch 58 Batch: 54 Train Loss: 0.06953687220811844\n",
      "Epoch 58 Batch: 56 Train Loss: 0.04641633480787277\n",
      "Epoch 58 Batch: 58 Train Loss: 0.028209447860717773\n",
      "Epoch 58 Batch: 60 Train Loss: 0.29384127259254456\n",
      "Epoch 58 Batch: 62 Train Loss: 0.5613481998443604\n",
      "Epoch 58 Batch: 64 Train Loss: 0.5604590773582458\n",
      "Epoch 58 Batch: 66 Train Loss: 0.29079005122184753\n",
      "Epoch 58 Batch: 68 Train Loss: 0.044220443814992905\n",
      "Epoch 58 Batch: 70 Train Loss: 0.07336169481277466\n",
      "Epoch 58 Batch: 72 Train Loss: 0.03314536437392235\n",
      "Epoch 58 Batch: 74 Train Loss: 0.045192208141088486\n",
      "Epoch 58 Batch: 76 Train Loss: 0.037830233573913574\n",
      "Epoch 58 Batch: 78 Train Loss: 0.5630563497543335\n",
      "Epoch 58 Batch: 80 Train Loss: 0.06139339134097099\n",
      "Epoch 58 Batch: 82 Train Loss: 0.04395001009106636\n",
      "Epoch 58 Batch: 84 Train Loss: 0.024423247203230858\n",
      "Epoch 58 Batch: 86 Train Loss: 0.2758485674858093\n",
      "Epoch 58 Batch: 88 Train Loss: 0.04558528959751129\n",
      "Epoch 58 Batch: 90 Train Loss: 0.03190772980451584\n",
      "Epoch 58 Batch: 92 Train Loss: 0.06556826829910278\n",
      "Epoch 58 Batch: 94 Train Loss: 0.055435292422771454\n",
      "Epoch 58 Batch: 96 Train Loss: 0.290316104888916\n",
      "Epoch 58 Batch: 98 Train Loss: 0.5059512257575989\n",
      "Epoch 58 Batch: 100 Train Loss: 0.0641636997461319\n",
      "Epoch 58 Batch: 102 Train Loss: 0.03407551348209381\n",
      "Epoch 58 Batch: 104 Train Loss: 0.5443496108055115\n",
      "Epoch 58 Batch: 106 Train Loss: 0.28448158502578735\n",
      "Epoch 58 Batch: 108 Train Loss: 0.29683414101600647\n",
      "Epoch 58 Batch: 110 Train Loss: 0.06401905417442322\n",
      "Epoch 58 Batch: 112 Train Loss: 0.27042922377586365\n",
      "Epoch 58 Batch: 114 Train Loss: 0.30642610788345337\n",
      "Epoch 58 Batch: 116 Train Loss: 0.05537489801645279\n",
      "Epoch 58 Batch: 118 Train Loss: 0.273576557636261\n",
      "Epoch 58 Batch: 120 Train Loss: 0.2796885073184967\n",
      "Epoch 58 Batch: 122 Train Loss: 0.03430703654885292\n",
      "Epoch 58 Batch: 124 Train Loss: 0.03317723423242569\n",
      "Epoch 58 Batch: 126 Train Loss: 0.30351656675338745\n",
      "Epoch 58 Batch: 128 Train Loss: 0.27056971192359924\n",
      "Epoch 58 Batch: 130 Train Loss: 0.06036944314837456\n",
      "Epoch 58 Batch: 132 Train Loss: 0.05829835683107376\n",
      "Epoch 58 Batch: 134 Train Loss: 0.0489230714738369\n",
      "Epoch 58 Batch: 136 Train Loss: 0.08922117203474045\n",
      "Epoch 58 Batch: 138 Train Loss: 0.07832279056310654\n",
      "Epoch 58 Batch: 140 Train Loss: 0.06324970722198486\n",
      "Epoch 58 Batch: 142 Train Loss: 0.28720349073410034\n",
      "Epoch 58 Batch: 144 Train Loss: 0.039986345916986465\n",
      "Epoch 58 Batch: 146 Train Loss: 0.04162660241127014\n",
      "Epoch 58 Batch: 148 Train Loss: 0.5235126614570618\n",
      "Epoch 58 Batch: 150 Train Loss: 0.04820985347032547\n",
      "Epoch 58 Batch: 152 Train Loss: 0.04265698790550232\n",
      "Epoch 58 Batch: 154 Train Loss: 0.5655754804611206\n",
      "Epoch 58 Batch: 156 Train Loss: 0.30587708950042725\n",
      "Epoch 58 Batch: 158 Train Loss: 0.07366656512022018\n",
      "Epoch 58 Batch: 160 Train Loss: 0.052411776036024094\n",
      "Epoch 59 Batch: 2 Train Loss: 0.30878716707229614\n",
      "Epoch 59 Batch: 4 Train Loss: 0.30292871594429016\n",
      "Epoch 59 Batch: 6 Train Loss: 0.3203190863132477\n",
      "Epoch 59 Batch: 8 Train Loss: 0.06086384132504463\n",
      "Epoch 59 Batch: 10 Train Loss: 0.06624259054660797\n",
      "Epoch 59 Batch: 12 Train Loss: 0.26160430908203125\n",
      "Epoch 59 Batch: 14 Train Loss: 0.057410500943660736\n",
      "Epoch 59 Batch: 16 Train Loss: 0.4963935911655426\n",
      "Epoch 59 Batch: 18 Train Loss: 0.058100879192352295\n",
      "Epoch 59 Batch: 20 Train Loss: 0.054254066199064255\n",
      "Epoch 59 Batch: 22 Train Loss: 0.47240209579467773\n",
      "Epoch 59 Batch: 24 Train Loss: 0.2826089560985565\n",
      "Epoch 59 Batch: 26 Train Loss: 0.08015871047973633\n",
      "Epoch 59 Batch: 28 Train Loss: 0.06601804494857788\n",
      "Epoch 59 Batch: 30 Train Loss: 0.08966664969921112\n",
      "Epoch 59 Batch: 32 Train Loss: 0.06422414630651474\n",
      "Epoch 59 Batch: 34 Train Loss: 0.08578477799892426\n",
      "Epoch 59 Batch: 36 Train Loss: 0.05903204530477524\n",
      "Epoch 59 Batch: 38 Train Loss: 0.05476982519030571\n",
      "Epoch 59 Batch: 40 Train Loss: 0.30279141664505005\n",
      "Epoch 59 Batch: 42 Train Loss: 0.04400714114308357\n",
      "Epoch 59 Batch: 44 Train Loss: 0.054219938814640045\n",
      "Epoch 59 Batch: 46 Train Loss: 0.31444844603538513\n",
      "Epoch 59 Batch: 48 Train Loss: 0.029439156875014305\n",
      "Epoch 59 Batch: 50 Train Loss: 0.04640685394406319\n",
      "Epoch 59 Batch: 52 Train Loss: 0.031660404056310654\n",
      "Epoch 59 Batch: 54 Train Loss: 0.30537480115890503\n",
      "Epoch 59 Batch: 56 Train Loss: 0.027000408619642258\n",
      "Epoch 59 Batch: 58 Train Loss: 0.3017820715904236\n",
      "Epoch 59 Batch: 60 Train Loss: 0.31685447692871094\n",
      "Epoch 59 Batch: 62 Train Loss: 0.30064070224761963\n",
      "Epoch 59 Batch: 64 Train Loss: 0.6047425270080566\n",
      "Epoch 59 Batch: 66 Train Loss: 0.30621883273124695\n",
      "Epoch 59 Batch: 68 Train Loss: 0.024604206904768944\n",
      "Epoch 59 Batch: 70 Train Loss: 0.041047148406505585\n",
      "Epoch 59 Batch: 72 Train Loss: 0.03684350848197937\n",
      "Epoch 59 Batch: 74 Train Loss: 0.04136725515127182\n",
      "Epoch 59 Batch: 76 Train Loss: 0.5651289224624634\n",
      "Epoch 59 Batch: 78 Train Loss: 0.033044155687093735\n",
      "Epoch 59 Batch: 80 Train Loss: 0.28353866934776306\n",
      "Epoch 59 Batch: 82 Train Loss: 0.08492688089609146\n",
      "Epoch 59 Batch: 84 Train Loss: 0.2674500644207001\n",
      "Epoch 59 Batch: 86 Train Loss: 0.30880993604660034\n",
      "Epoch 59 Batch: 88 Train Loss: 0.053609855473041534\n",
      "Epoch 59 Batch: 90 Train Loss: 0.05330676585435867\n",
      "Epoch 59 Batch: 92 Train Loss: 0.2605305314064026\n",
      "Epoch 59 Batch: 94 Train Loss: 0.2413887083530426\n",
      "Epoch 59 Batch: 96 Train Loss: 0.2637999951839447\n",
      "Epoch 59 Batch: 98 Train Loss: 0.2878354787826538\n",
      "Epoch 59 Batch: 100 Train Loss: 0.0360354483127594\n",
      "Epoch 59 Batch: 102 Train Loss: 0.2834462821483612\n",
      "Epoch 59 Batch: 104 Train Loss: 0.09931482374668121\n",
      "Epoch 59 Batch: 106 Train Loss: 0.0833839401602745\n",
      "Epoch 59 Batch: 108 Train Loss: 0.055149566382169724\n",
      "Epoch 59 Batch: 110 Train Loss: 0.5235759019851685\n",
      "Epoch 59 Batch: 112 Train Loss: 0.2640477418899536\n",
      "Epoch 59 Batch: 114 Train Loss: 0.04820742458105087\n",
      "Epoch 59 Batch: 116 Train Loss: 0.09189797937870026\n",
      "Epoch 59 Batch: 118 Train Loss: 0.07056893408298492\n",
      "Epoch 59 Batch: 120 Train Loss: 0.2615167796611786\n",
      "Epoch 59 Batch: 122 Train Loss: 0.07469218224287033\n",
      "Epoch 59 Batch: 124 Train Loss: 0.5154712200164795\n",
      "Epoch 59 Batch: 126 Train Loss: 0.055206604301929474\n",
      "Epoch 59 Batch: 128 Train Loss: 0.061176151037216187\n",
      "Epoch 59 Batch: 130 Train Loss: 0.06828293949365616\n",
      "Epoch 59 Batch: 132 Train Loss: 0.07924672961235046\n",
      "Epoch 59 Batch: 134 Train Loss: 0.04619673267006874\n",
      "Epoch 59 Batch: 136 Train Loss: 0.3110376000404358\n",
      "Epoch 59 Batch: 138 Train Loss: 0.048247504979372025\n",
      "Epoch 59 Batch: 140 Train Loss: 0.037588931620121\n",
      "Epoch 59 Batch: 142 Train Loss: 0.5123821496963501\n",
      "Epoch 59 Batch: 144 Train Loss: 0.3086492419242859\n",
      "Epoch 59 Batch: 146 Train Loss: 0.02537946030497551\n",
      "Epoch 59 Batch: 148 Train Loss: 0.027834514155983925\n",
      "Epoch 59 Batch: 150 Train Loss: 0.2809846103191376\n",
      "Epoch 59 Batch: 152 Train Loss: 0.3113551437854767\n",
      "Epoch 59 Batch: 154 Train Loss: 0.29836827516555786\n",
      "Epoch 59 Batch: 156 Train Loss: 0.03260188549757004\n",
      "Epoch 59 Batch: 158 Train Loss: 0.06459338963031769\n",
      "Epoch 59 Batch: 160 Train Loss: 0.571561336517334\n",
      "Epoch 60 Batch: 2 Train Loss: 0.06065364554524422\n",
      "Epoch 60 Batch: 4 Train Loss: 0.5535851716995239\n",
      "Epoch 60 Batch: 6 Train Loss: 0.07977049797773361\n",
      "Epoch 60 Batch: 8 Train Loss: 0.057924579828977585\n",
      "Epoch 60 Batch: 10 Train Loss: 0.055029671639204025\n",
      "Epoch 60 Batch: 12 Train Loss: 0.061831820756196976\n",
      "Epoch 60 Batch: 14 Train Loss: 0.304765522480011\n",
      "Epoch 60 Batch: 16 Train Loss: 0.06540219485759735\n",
      "Epoch 60 Batch: 18 Train Loss: 0.04829475283622742\n",
      "Epoch 60 Batch: 20 Train Loss: 0.29714441299438477\n",
      "Epoch 60 Batch: 22 Train Loss: 0.30151206254959106\n",
      "Epoch 60 Batch: 24 Train Loss: 0.05381147190928459\n",
      "Epoch 60 Batch: 26 Train Loss: 0.03239484503865242\n",
      "Epoch 60 Batch: 28 Train Loss: 0.27641943097114563\n",
      "Epoch 60 Batch: 30 Train Loss: 0.31711000204086304\n",
      "Epoch 60 Batch: 32 Train Loss: 0.3067196011543274\n",
      "Epoch 60 Batch: 34 Train Loss: 0.08259771764278412\n",
      "Epoch 60 Batch: 36 Train Loss: 0.04422403499484062\n",
      "Epoch 60 Batch: 38 Train Loss: 0.30575576424598694\n",
      "Epoch 60 Batch: 40 Train Loss: 0.31484565138816833\n",
      "Epoch 60 Batch: 42 Train Loss: 0.04693964868783951\n",
      "Epoch 60 Batch: 44 Train Loss: 0.09219345450401306\n",
      "Epoch 60 Batch: 46 Train Loss: 0.2997909486293793\n",
      "Epoch 60 Batch: 48 Train Loss: 0.03908117860555649\n",
      "Epoch 60 Batch: 50 Train Loss: 0.048462919890880585\n",
      "Epoch 60 Batch: 52 Train Loss: 0.03736216574907303\n",
      "Epoch 60 Batch: 54 Train Loss: 0.03697032481431961\n",
      "Epoch 60 Batch: 56 Train Loss: 0.31655800342559814\n",
      "Epoch 60 Batch: 58 Train Loss: 0.2718704342842102\n",
      "Epoch 60 Batch: 60 Train Loss: 0.048889048397541046\n",
      "Epoch 60 Batch: 62 Train Loss: 0.04538077861070633\n",
      "Epoch 60 Batch: 64 Train Loss: 0.03837106376886368\n",
      "Epoch 60 Batch: 66 Train Loss: 0.3069309592247009\n",
      "Epoch 60 Batch: 68 Train Loss: 0.31986886262893677\n",
      "Epoch 60 Batch: 70 Train Loss: 0.022135160863399506\n",
      "Epoch 60 Batch: 72 Train Loss: 0.04473695904016495\n",
      "Epoch 60 Batch: 74 Train Loss: 0.28584688901901245\n",
      "Epoch 60 Batch: 76 Train Loss: 0.06970562785863876\n",
      "Epoch 60 Batch: 78 Train Loss: 0.06006311625242233\n",
      "Epoch 60 Batch: 80 Train Loss: 0.27545422315597534\n",
      "Epoch 60 Batch: 82 Train Loss: 0.26619991660118103\n",
      "Epoch 60 Batch: 84 Train Loss: 0.057368356734514236\n",
      "Epoch 60 Batch: 86 Train Loss: 0.278941810131073\n",
      "Epoch 60 Batch: 88 Train Loss: 0.5471824407577515\n",
      "Epoch 60 Batch: 90 Train Loss: 0.2693294286727905\n",
      "Epoch 60 Batch: 92 Train Loss: 0.029082637280225754\n",
      "Epoch 60 Batch: 94 Train Loss: 0.06197088956832886\n",
      "Epoch 60 Batch: 96 Train Loss: 0.04547233134508133\n",
      "Epoch 60 Batch: 98 Train Loss: 0.2848087549209595\n",
      "Epoch 60 Batch: 100 Train Loss: 0.2834874391555786\n",
      "Epoch 60 Batch: 102 Train Loss: 0.2579731345176697\n",
      "Epoch 60 Batch: 104 Train Loss: 0.05599675327539444\n",
      "Epoch 60 Batch: 106 Train Loss: 0.053218863904476166\n",
      "Epoch 60 Batch: 108 Train Loss: 0.0757526308298111\n",
      "Epoch 60 Batch: 110 Train Loss: 0.0775238424539566\n",
      "Epoch 60 Batch: 112 Train Loss: 0.31519585847854614\n",
      "Epoch 60 Batch: 114 Train Loss: 0.2784896790981293\n",
      "Epoch 60 Batch: 116 Train Loss: 0.25520747900009155\n",
      "Epoch 60 Batch: 118 Train Loss: 0.05321831256151199\n",
      "Epoch 60 Batch: 120 Train Loss: 0.2976928651332855\n",
      "Epoch 60 Batch: 122 Train Loss: 0.051763344556093216\n",
      "Epoch 60 Batch: 124 Train Loss: 0.020402655005455017\n",
      "Epoch 60 Batch: 126 Train Loss: 0.07094581425189972\n",
      "Epoch 60 Batch: 128 Train Loss: 0.04857459291815758\n",
      "Epoch 60 Batch: 130 Train Loss: 0.05379743501543999\n",
      "Epoch 60 Batch: 132 Train Loss: 0.01659570261836052\n",
      "Epoch 60 Batch: 134 Train Loss: 0.05789164826273918\n",
      "Epoch 60 Batch: 136 Train Loss: 0.5487499833106995\n",
      "Epoch 60 Batch: 138 Train Loss: 0.5760403871536255\n",
      "Epoch 60 Batch: 140 Train Loss: 0.05434365198016167\n",
      "Epoch 60 Batch: 142 Train Loss: 0.29100966453552246\n",
      "Epoch 60 Batch: 144 Train Loss: 0.2814784646034241\n",
      "Epoch 60 Batch: 146 Train Loss: 0.5334423184394836\n",
      "Epoch 60 Batch: 148 Train Loss: 0.032769836485385895\n",
      "Epoch 60 Batch: 150 Train Loss: 0.05359046906232834\n",
      "Epoch 60 Batch: 152 Train Loss: 0.2858610451221466\n",
      "Epoch 60 Batch: 154 Train Loss: 0.2857000231742859\n",
      "Epoch 60 Batch: 156 Train Loss: 0.4986514449119568\n",
      "Epoch 60 Batch: 158 Train Loss: 0.03608058765530586\n",
      "Epoch 60 Batch: 160 Train Loss: 0.27684682607650757\n",
      "Epoch 61 Batch: 2 Train Loss: 0.06654408574104309\n",
      "Epoch 61 Batch: 4 Train Loss: 0.24310994148254395\n",
      "Epoch 61 Batch: 6 Train Loss: 0.07915656268596649\n",
      "Epoch 61 Batch: 8 Train Loss: 0.44522958993911743\n",
      "Epoch 61 Batch: 10 Train Loss: 0.08169130980968475\n",
      "Epoch 61 Batch: 12 Train Loss: 0.2859620153903961\n",
      "Epoch 61 Batch: 14 Train Loss: 0.28495579957962036\n",
      "Epoch 61 Batch: 16 Train Loss: 0.07242284715175629\n",
      "Epoch 61 Batch: 18 Train Loss: 0.04980183392763138\n",
      "Epoch 61 Batch: 20 Train Loss: 0.29999232292175293\n",
      "Epoch 61 Batch: 22 Train Loss: 0.06292225420475006\n",
      "Epoch 61 Batch: 24 Train Loss: 0.045028265565633774\n",
      "Epoch 61 Batch: 26 Train Loss: 0.05093039199709892\n",
      "Epoch 61 Batch: 28 Train Loss: 0.30289894342422485\n",
      "Epoch 61 Batch: 30 Train Loss: 0.046643130481243134\n",
      "Epoch 61 Batch: 32 Train Loss: 0.038687050342559814\n",
      "Epoch 61 Batch: 34 Train Loss: 0.029040014371275902\n",
      "Epoch 61 Batch: 36 Train Loss: 0.29295456409454346\n",
      "Epoch 61 Batch: 38 Train Loss: 0.03692068159580231\n",
      "Epoch 61 Batch: 40 Train Loss: 0.3241937458515167\n",
      "Epoch 61 Batch: 42 Train Loss: 0.016450222581624985\n",
      "Epoch 61 Batch: 44 Train Loss: 0.05267757177352905\n",
      "Epoch 61 Batch: 46 Train Loss: 0.08046942949295044\n",
      "Epoch 61 Batch: 48 Train Loss: 0.06922824680805206\n",
      "Epoch 61 Batch: 50 Train Loss: 0.04839865118265152\n",
      "Epoch 61 Batch: 52 Train Loss: 0.28542813658714294\n",
      "Epoch 61 Batch: 54 Train Loss: 0.047849155962467194\n",
      "Epoch 61 Batch: 56 Train Loss: 0.5190883278846741\n",
      "Epoch 61 Batch: 58 Train Loss: 0.07196085155010223\n",
      "Epoch 61 Batch: 60 Train Loss: 0.08584832400083542\n",
      "Epoch 61 Batch: 62 Train Loss: 0.09623105823993683\n",
      "Epoch 61 Batch: 64 Train Loss: 0.056695520877838135\n",
      "Epoch 61 Batch: 66 Train Loss: 0.27101442217826843\n",
      "Epoch 61 Batch: 68 Train Loss: 0.06181742995977402\n",
      "Epoch 61 Batch: 70 Train Loss: 0.07721792161464691\n",
      "Epoch 61 Batch: 72 Train Loss: 0.5140382051467896\n",
      "Epoch 61 Batch: 74 Train Loss: 0.49993810057640076\n",
      "Epoch 61 Batch: 76 Train Loss: 0.27886486053466797\n",
      "Epoch 61 Batch: 78 Train Loss: 0.25095000863075256\n",
      "Epoch 61 Batch: 80 Train Loss: 0.06589818745851517\n",
      "Epoch 61 Batch: 82 Train Loss: 0.06377249211072922\n",
      "Epoch 61 Batch: 84 Train Loss: 0.07765509188175201\n",
      "Epoch 61 Batch: 86 Train Loss: 0.49568063020706177\n",
      "Epoch 61 Batch: 88 Train Loss: 0.2761411666870117\n",
      "Epoch 61 Batch: 90 Train Loss: 0.04388119652867317\n",
      "Epoch 61 Batch: 92 Train Loss: 0.05457555130124092\n",
      "Epoch 61 Batch: 94 Train Loss: 0.27616438269615173\n",
      "Epoch 61 Batch: 96 Train Loss: 0.283771276473999\n",
      "Epoch 61 Batch: 98 Train Loss: 0.282585084438324\n",
      "Epoch 61 Batch: 100 Train Loss: 0.27805066108703613\n",
      "Epoch 61 Batch: 102 Train Loss: 0.2718605697154999\n",
      "Epoch 61 Batch: 104 Train Loss: 0.29318535327911377\n",
      "Epoch 61 Batch: 106 Train Loss: 0.11383944749832153\n",
      "Epoch 61 Batch: 108 Train Loss: 0.08313357084989548\n",
      "Epoch 61 Batch: 110 Train Loss: 0.06713323295116425\n",
      "Epoch 61 Batch: 112 Train Loss: 0.054065655916929245\n",
      "Epoch 61 Batch: 114 Train Loss: 0.29549455642700195\n",
      "Epoch 61 Batch: 116 Train Loss: 0.04546007141470909\n",
      "Epoch 61 Batch: 118 Train Loss: 0.29087522625923157\n",
      "Epoch 61 Batch: 120 Train Loss: 0.05567134544253349\n",
      "Epoch 61 Batch: 122 Train Loss: 0.059351254254579544\n",
      "Epoch 61 Batch: 124 Train Loss: 0.0673571527004242\n",
      "Epoch 61 Batch: 126 Train Loss: 0.28745824098587036\n",
      "Epoch 61 Batch: 128 Train Loss: 0.028446156531572342\n",
      "Epoch 61 Batch: 130 Train Loss: 0.04136588051915169\n",
      "Epoch 61 Batch: 132 Train Loss: 0.04570065438747406\n",
      "Epoch 61 Batch: 134 Train Loss: 0.016452282667160034\n",
      "Epoch 61 Batch: 136 Train Loss: 0.04741217941045761\n",
      "Epoch 61 Batch: 138 Train Loss: 0.2983441948890686\n",
      "Epoch 61 Batch: 140 Train Loss: 0.2871840298175812\n",
      "Epoch 61 Batch: 142 Train Loss: 0.030457545071840286\n",
      "Epoch 61 Batch: 144 Train Loss: 0.031033581122756004\n",
      "Epoch 61 Batch: 146 Train Loss: 0.021581288427114487\n",
      "Epoch 61 Batch: 148 Train Loss: 0.3095169961452484\n",
      "Epoch 61 Batch: 150 Train Loss: 0.05205044150352478\n",
      "Epoch 61 Batch: 152 Train Loss: 0.047224193811416626\n",
      "Epoch 61 Batch: 154 Train Loss: 0.5334746837615967\n",
      "Epoch 61 Batch: 156 Train Loss: 0.5559585690498352\n",
      "Epoch 61 Batch: 158 Train Loss: 0.07875372469425201\n",
      "Epoch 61 Batch: 160 Train Loss: 0.036836590617895126\n",
      "Epoch 62 Batch: 2 Train Loss: 0.0971677154302597\n",
      "Epoch 62 Batch: 4 Train Loss: 0.043582163751125336\n",
      "Epoch 62 Batch: 6 Train Loss: 0.08109889924526215\n",
      "Epoch 62 Batch: 8 Train Loss: 0.041739702224731445\n",
      "Epoch 62 Batch: 10 Train Loss: 0.0716458186507225\n",
      "Epoch 62 Batch: 12 Train Loss: 0.5228248834609985\n",
      "Epoch 62 Batch: 14 Train Loss: 0.05682659149169922\n",
      "Epoch 62 Batch: 16 Train Loss: 0.2702750265598297\n",
      "Epoch 62 Batch: 18 Train Loss: 0.06895391643047333\n",
      "Epoch 62 Batch: 20 Train Loss: 0.06044023483991623\n",
      "Epoch 62 Batch: 22 Train Loss: 0.09020113945007324\n",
      "Epoch 62 Batch: 24 Train Loss: 0.03484639897942543\n",
      "Epoch 62 Batch: 26 Train Loss: 0.0375225804746151\n",
      "Epoch 62 Batch: 28 Train Loss: 0.81434565782547\n",
      "Epoch 62 Batch: 30 Train Loss: 0.02667156793177128\n",
      "Epoch 62 Batch: 32 Train Loss: 0.061246491968631744\n",
      "Epoch 62 Batch: 34 Train Loss: 0.05363553762435913\n",
      "Epoch 62 Batch: 36 Train Loss: 0.05900634080171585\n",
      "Epoch 62 Batch: 38 Train Loss: 0.05893012136220932\n",
      "Epoch 62 Batch: 40 Train Loss: 0.3095940947532654\n",
      "Epoch 62 Batch: 42 Train Loss: 0.025113126263022423\n",
      "Epoch 62 Batch: 44 Train Loss: 0.28570756316185\n",
      "Epoch 62 Batch: 46 Train Loss: 0.042457882314920425\n",
      "Epoch 62 Batch: 48 Train Loss: 0.06224090978503227\n",
      "Epoch 62 Batch: 50 Train Loss: 0.06131163239479065\n",
      "Epoch 62 Batch: 52 Train Loss: 0.28701984882354736\n",
      "Epoch 62 Batch: 54 Train Loss: 0.03932409733533859\n",
      "Epoch 62 Batch: 56 Train Loss: 0.3241455554962158\n",
      "Epoch 62 Batch: 58 Train Loss: 0.07652018964290619\n",
      "Epoch 62 Batch: 60 Train Loss: 0.5433439016342163\n",
      "Epoch 62 Batch: 62 Train Loss: 0.032259732484817505\n",
      "Epoch 62 Batch: 64 Train Loss: 0.021518955007195473\n",
      "Epoch 62 Batch: 66 Train Loss: 0.04589858278632164\n",
      "Epoch 62 Batch: 68 Train Loss: 0.2734382748603821\n",
      "Epoch 62 Batch: 70 Train Loss: 0.038864560425281525\n",
      "Epoch 62 Batch: 72 Train Loss: 0.04519589990377426\n",
      "Epoch 62 Batch: 74 Train Loss: 0.03661070391535759\n",
      "Epoch 62 Batch: 76 Train Loss: 0.0376875065267086\n",
      "Epoch 62 Batch: 78 Train Loss: 0.020997872576117516\n",
      "Epoch 62 Batch: 80 Train Loss: 0.3172129988670349\n",
      "Epoch 62 Batch: 82 Train Loss: 0.03345943242311478\n",
      "Epoch 62 Batch: 84 Train Loss: 1.2075135707855225\n",
      "Epoch 62 Batch: 86 Train Loss: 0.029838088899850845\n",
      "Epoch 62 Batch: 88 Train Loss: 0.020966146141290665\n",
      "Epoch 62 Batch: 90 Train Loss: 0.2861325740814209\n",
      "Epoch 62 Batch: 92 Train Loss: 0.30601781606674194\n",
      "Epoch 62 Batch: 94 Train Loss: 0.03754306212067604\n",
      "Epoch 62 Batch: 96 Train Loss: 0.04360879212617874\n",
      "Epoch 62 Batch: 98 Train Loss: 0.3020588755607605\n",
      "Epoch 62 Batch: 100 Train Loss: 0.027325451374053955\n",
      "Epoch 62 Batch: 102 Train Loss: 0.5580334067344666\n",
      "Epoch 62 Batch: 104 Train Loss: 0.3176276385784149\n",
      "Epoch 62 Batch: 106 Train Loss: 0.30153846740722656\n",
      "Epoch 62 Batch: 108 Train Loss: 0.28947457671165466\n",
      "Epoch 62 Batch: 110 Train Loss: 0.34291690587997437\n",
      "Epoch 62 Batch: 112 Train Loss: 0.053253673017024994\n",
      "Epoch 62 Batch: 114 Train Loss: 0.08390837907791138\n",
      "Epoch 62 Batch: 116 Train Loss: 0.25846606492996216\n",
      "Epoch 62 Batch: 118 Train Loss: 0.037680745124816895\n",
      "Epoch 62 Batch: 120 Train Loss: 0.11100955307483673\n",
      "Epoch 62 Batch: 122 Train Loss: 0.6008136868476868\n",
      "Epoch 62 Batch: 124 Train Loss: 0.07398758828639984\n",
      "Epoch 62 Batch: 126 Train Loss: 0.9036146402359009\n",
      "Epoch 62 Batch: 128 Train Loss: 0.42985978722572327\n",
      "Epoch 62 Batch: 130 Train Loss: 0.6770745515823364\n",
      "Epoch 62 Batch: 132 Train Loss: 0.22197496891021729\n",
      "Epoch 62 Batch: 134 Train Loss: 0.4890133738517761\n",
      "Epoch 62 Batch: 136 Train Loss: 0.4946540892124176\n",
      "Epoch 62 Batch: 138 Train Loss: 0.6925206780433655\n",
      "Epoch 62 Batch: 140 Train Loss: 0.23941592872142792\n",
      "Epoch 62 Batch: 142 Train Loss: 0.8516227006912231\n",
      "Epoch 62 Batch: 144 Train Loss: 0.08019585907459259\n",
      "Epoch 62 Batch: 146 Train Loss: 0.09296710789203644\n",
      "Epoch 62 Batch: 148 Train Loss: 0.25539839267730713\n",
      "Epoch 62 Batch: 150 Train Loss: 0.2286507785320282\n",
      "Epoch 62 Batch: 152 Train Loss: 0.2588270902633667\n",
      "Epoch 62 Batch: 154 Train Loss: 0.12187360227108002\n",
      "Epoch 62 Batch: 156 Train Loss: 0.09858474880456924\n",
      "Epoch 62 Batch: 158 Train Loss: 0.2762630581855774\n",
      "Epoch 62 Batch: 160 Train Loss: 0.1536218822002411\n",
      "Epoch 63 Batch: 2 Train Loss: 0.13753779232501984\n",
      "Epoch 63 Batch: 4 Train Loss: 0.08409427106380463\n",
      "Epoch 63 Batch: 6 Train Loss: 0.08234189450740814\n",
      "Epoch 63 Batch: 8 Train Loss: 0.2173171043395996\n",
      "Epoch 63 Batch: 10 Train Loss: 0.26491740345954895\n",
      "Epoch 63 Batch: 12 Train Loss: 0.08758282661437988\n",
      "Epoch 63 Batch: 14 Train Loss: 0.13763464987277985\n",
      "Epoch 63 Batch: 16 Train Loss: 0.09903081506490707\n",
      "Epoch 63 Batch: 18 Train Loss: 0.26938769221305847\n",
      "Epoch 63 Batch: 20 Train Loss: 0.07616845518350601\n",
      "Epoch 63 Batch: 22 Train Loss: 0.06319205462932587\n",
      "Epoch 63 Batch: 24 Train Loss: 0.08309653401374817\n",
      "Epoch 63 Batch: 26 Train Loss: 0.057565610855817795\n",
      "Epoch 63 Batch: 28 Train Loss: 0.06781373918056488\n",
      "Epoch 63 Batch: 30 Train Loss: 0.3290883004665375\n",
      "Epoch 63 Batch: 32 Train Loss: 0.29784470796585083\n",
      "Epoch 63 Batch: 34 Train Loss: 0.24337466061115265\n",
      "Epoch 63 Batch: 36 Train Loss: 0.29406580328941345\n",
      "Epoch 63 Batch: 38 Train Loss: 0.24682359397411346\n",
      "Epoch 63 Batch: 40 Train Loss: 0.05154594033956528\n",
      "Epoch 63 Batch: 42 Train Loss: 0.052296511828899384\n",
      "Epoch 63 Batch: 44 Train Loss: 0.061962854117155075\n",
      "Epoch 63 Batch: 46 Train Loss: 0.07095547765493393\n",
      "Epoch 63 Batch: 48 Train Loss: 0.07718637585639954\n",
      "Epoch 63 Batch: 50 Train Loss: 0.044060979038476944\n",
      "Epoch 63 Batch: 52 Train Loss: 0.03839422017335892\n",
      "Epoch 63 Batch: 54 Train Loss: 0.2820596992969513\n",
      "Epoch 63 Batch: 56 Train Loss: 0.041986364871263504\n",
      "Epoch 63 Batch: 58 Train Loss: 0.035622596740722656\n",
      "Epoch 63 Batch: 60 Train Loss: 0.03153809905052185\n",
      "Epoch 63 Batch: 62 Train Loss: 0.06711351126432419\n",
      "Epoch 63 Batch: 64 Train Loss: 0.046274445950984955\n",
      "Epoch 63 Batch: 66 Train Loss: 0.03517637029290199\n",
      "Epoch 63 Batch: 68 Train Loss: 0.02801678515970707\n",
      "Epoch 63 Batch: 70 Train Loss: 0.28117841482162476\n",
      "Epoch 63 Batch: 72 Train Loss: 0.021825432777404785\n",
      "Epoch 63 Batch: 74 Train Loss: 0.057873111218214035\n",
      "Epoch 63 Batch: 76 Train Loss: 0.8350067138671875\n",
      "Epoch 63 Batch: 78 Train Loss: 0.0514683835208416\n",
      "Epoch 63 Batch: 80 Train Loss: 0.5423135161399841\n",
      "Epoch 63 Batch: 82 Train Loss: 0.04173828288912773\n",
      "Epoch 63 Batch: 84 Train Loss: 0.044671494513750076\n",
      "Epoch 63 Batch: 86 Train Loss: 0.2923402190208435\n",
      "Epoch 63 Batch: 88 Train Loss: 0.04460298269987106\n",
      "Epoch 63 Batch: 90 Train Loss: 0.047399092465639114\n",
      "Epoch 63 Batch: 92 Train Loss: 0.05095483735203743\n",
      "Epoch 63 Batch: 94 Train Loss: 0.5697625279426575\n",
      "Epoch 63 Batch: 96 Train Loss: 0.09143505245447159\n",
      "Epoch 63 Batch: 98 Train Loss: 0.07252538204193115\n",
      "Epoch 63 Batch: 100 Train Loss: 0.06293121725320816\n",
      "Epoch 63 Batch: 102 Train Loss: 0.24139411747455597\n",
      "Epoch 63 Batch: 104 Train Loss: 0.305504709482193\n",
      "Epoch 63 Batch: 106 Train Loss: 0.06020360067486763\n",
      "Epoch 63 Batch: 108 Train Loss: 0.279110312461853\n",
      "Epoch 63 Batch: 110 Train Loss: 0.00954313762485981\n",
      "Epoch 63 Batch: 112 Train Loss: 0.04776912182569504\n",
      "Epoch 63 Batch: 114 Train Loss: 0.03307688981294632\n",
      "Epoch 63 Batch: 116 Train Loss: 0.06097688153386116\n",
      "Epoch 63 Batch: 118 Train Loss: 0.30276089906692505\n",
      "Epoch 63 Batch: 120 Train Loss: 0.5268585681915283\n",
      "Epoch 63 Batch: 122 Train Loss: 0.09000486880540848\n",
      "Epoch 63 Batch: 124 Train Loss: 0.28617146611213684\n",
      "Epoch 63 Batch: 126 Train Loss: 0.07030388712882996\n",
      "Epoch 63 Batch: 128 Train Loss: 0.27131056785583496\n",
      "Epoch 63 Batch: 130 Train Loss: 0.07256951183080673\n",
      "Epoch 63 Batch: 132 Train Loss: 0.29411041736602783\n",
      "Epoch 63 Batch: 134 Train Loss: 0.45640283823013306\n",
      "Epoch 63 Batch: 136 Train Loss: 0.4921678900718689\n",
      "Epoch 63 Batch: 138 Train Loss: 0.06089324504137039\n",
      "Epoch 63 Batch: 140 Train Loss: 0.5141116380691528\n",
      "Epoch 63 Batch: 142 Train Loss: 0.06878270953893661\n",
      "Epoch 63 Batch: 144 Train Loss: 0.43695908784866333\n",
      "Epoch 63 Batch: 146 Train Loss: 0.05663396045565605\n",
      "Epoch 63 Batch: 148 Train Loss: 0.08740067481994629\n",
      "Epoch 63 Batch: 150 Train Loss: 0.0961366519331932\n",
      "Epoch 63 Batch: 152 Train Loss: 0.2497960329055786\n",
      "Epoch 63 Batch: 154 Train Loss: 0.03892049938440323\n",
      "Epoch 63 Batch: 156 Train Loss: 0.06308959424495697\n",
      "Epoch 63 Batch: 158 Train Loss: 0.05817371606826782\n",
      "Epoch 63 Batch: 160 Train Loss: 0.28126537799835205\n",
      "Epoch 64 Batch: 2 Train Loss: 0.032041437923908234\n",
      "Epoch 64 Batch: 4 Train Loss: 0.020509714260697365\n",
      "Epoch 64 Batch: 6 Train Loss: 0.07068073749542236\n",
      "Epoch 64 Batch: 8 Train Loss: 0.04079025983810425\n",
      "Epoch 64 Batch: 10 Train Loss: 0.27773183584213257\n",
      "Epoch 64 Batch: 12 Train Loss: 0.039387062191963196\n",
      "Epoch 64 Batch: 14 Train Loss: 0.28009334206581116\n",
      "Epoch 64 Batch: 16 Train Loss: 0.3034891188144684\n",
      "Epoch 64 Batch: 18 Train Loss: 0.5304234027862549\n",
      "Epoch 64 Batch: 20 Train Loss: 0.2917080521583557\n",
      "Epoch 64 Batch: 22 Train Loss: 0.2775231897830963\n",
      "Epoch 64 Batch: 24 Train Loss: 0.0554678849875927\n",
      "Epoch 64 Batch: 26 Train Loss: 0.05497027188539505\n",
      "Epoch 64 Batch: 28 Train Loss: 0.09008129686117172\n",
      "Epoch 64 Batch: 30 Train Loss: 0.29588761925697327\n",
      "Epoch 64 Batch: 32 Train Loss: 0.2759854197502136\n",
      "Epoch 64 Batch: 34 Train Loss: 0.07281064987182617\n",
      "Epoch 64 Batch: 36 Train Loss: 0.05263931304216385\n",
      "Epoch 64 Batch: 38 Train Loss: 0.05234695225954056\n",
      "Epoch 64 Batch: 40 Train Loss: 0.28438371419906616\n",
      "Epoch 64 Batch: 42 Train Loss: 0.5264668464660645\n",
      "Epoch 64 Batch: 44 Train Loss: 0.041330736130476\n",
      "Epoch 64 Batch: 46 Train Loss: 0.06239893287420273\n",
      "Epoch 64 Batch: 48 Train Loss: 0.25814878940582275\n",
      "Epoch 64 Batch: 50 Train Loss: 0.2862163782119751\n",
      "Epoch 64 Batch: 52 Train Loss: 0.031073594465851784\n",
      "Epoch 64 Batch: 54 Train Loss: 0.2443661391735077\n",
      "Epoch 64 Batch: 56 Train Loss: 0.29008400440216064\n",
      "Epoch 64 Batch: 58 Train Loss: 0.03252817690372467\n",
      "Epoch 64 Batch: 60 Train Loss: 0.06560923159122467\n",
      "Epoch 64 Batch: 62 Train Loss: 0.0973941758275032\n",
      "Epoch 64 Batch: 64 Train Loss: 0.06032879278063774\n",
      "Epoch 64 Batch: 66 Train Loss: 0.037390969693660736\n",
      "Epoch 64 Batch: 68 Train Loss: 0.05062628537416458\n",
      "Epoch 64 Batch: 70 Train Loss: 0.05326120927929878\n",
      "Epoch 64 Batch: 72 Train Loss: 0.03759650141000748\n",
      "Epoch 64 Batch: 74 Train Loss: 0.08952634036540985\n",
      "Epoch 64 Batch: 76 Train Loss: 0.269785076379776\n",
      "Epoch 64 Batch: 78 Train Loss: 0.03789738565683365\n",
      "Epoch 64 Batch: 80 Train Loss: 0.0575430803000927\n",
      "Epoch 64 Batch: 82 Train Loss: 0.05385584756731987\n",
      "Epoch 64 Batch: 84 Train Loss: 0.2811213433742523\n",
      "Epoch 64 Batch: 86 Train Loss: 0.03969355300068855\n",
      "Epoch 64 Batch: 88 Train Loss: 0.02243049256503582\n",
      "Epoch 64 Batch: 90 Train Loss: 0.023244962096214294\n",
      "Epoch 64 Batch: 92 Train Loss: 0.07380937039852142\n",
      "Epoch 64 Batch: 94 Train Loss: 0.052802763879299164\n",
      "Epoch 64 Batch: 96 Train Loss: 0.015668394044041634\n",
      "Epoch 64 Batch: 98 Train Loss: 0.3275063633918762\n",
      "Epoch 64 Batch: 100 Train Loss: 0.539722204208374\n",
      "Epoch 64 Batch: 102 Train Loss: 0.0579935722053051\n",
      "Epoch 64 Batch: 104 Train Loss: 0.03887851908802986\n",
      "Epoch 64 Batch: 106 Train Loss: 0.3207789957523346\n",
      "Epoch 64 Batch: 108 Train Loss: 0.03383148834109306\n",
      "Epoch 64 Batch: 110 Train Loss: 0.5624209642410278\n",
      "Epoch 64 Batch: 112 Train Loss: 0.3160707652568817\n",
      "Epoch 64 Batch: 114 Train Loss: 0.30901533365249634\n",
      "Epoch 64 Batch: 116 Train Loss: 0.29271599650382996\n",
      "Epoch 64 Batch: 118 Train Loss: 0.5273590683937073\n",
      "Epoch 64 Batch: 120 Train Loss: 0.042664699256420135\n",
      "Epoch 64 Batch: 122 Train Loss: 0.04502546042203903\n",
      "Epoch 64 Batch: 124 Train Loss: 0.7166513204574585\n",
      "Epoch 64 Batch: 126 Train Loss: 0.06503777205944061\n",
      "Epoch 64 Batch: 128 Train Loss: 0.5061720609664917\n",
      "Epoch 64 Batch: 130 Train Loss: 0.09179689735174179\n",
      "Epoch 64 Batch: 132 Train Loss: 0.4811094403266907\n",
      "Epoch 64 Batch: 134 Train Loss: 0.29425403475761414\n",
      "Epoch 64 Batch: 136 Train Loss: 0.07170967757701874\n",
      "Epoch 64 Batch: 138 Train Loss: 0.28469279408454895\n",
      "Epoch 64 Batch: 140 Train Loss: 0.06397174298763275\n",
      "Epoch 64 Batch: 142 Train Loss: 0.0854707881808281\n",
      "Epoch 64 Batch: 144 Train Loss: 0.05245765298604965\n",
      "Epoch 64 Batch: 146 Train Loss: 0.05513434857130051\n",
      "Epoch 64 Batch: 148 Train Loss: 0.5201365947723389\n",
      "Epoch 64 Batch: 150 Train Loss: 0.06674700230360031\n",
      "Epoch 64 Batch: 152 Train Loss: 0.25547075271606445\n",
      "Epoch 64 Batch: 154 Train Loss: 0.2846270203590393\n",
      "Epoch 64 Batch: 156 Train Loss: 0.038752418011426926\n",
      "Epoch 64 Batch: 158 Train Loss: 0.2946600914001465\n",
      "Epoch 64 Batch: 160 Train Loss: 0.2568414509296417\n",
      "Epoch 65 Batch: 2 Train Loss: 0.05764715000987053\n",
      "Epoch 65 Batch: 4 Train Loss: 0.06278898566961288\n",
      "Epoch 65 Batch: 6 Train Loss: 0.05322444438934326\n",
      "Epoch 65 Batch: 8 Train Loss: 0.042914070188999176\n",
      "Epoch 65 Batch: 10 Train Loss: 0.04444609954953194\n",
      "Epoch 65 Batch: 12 Train Loss: 0.29088008403778076\n",
      "Epoch 65 Batch: 14 Train Loss: 0.06207651644945145\n",
      "Epoch 65 Batch: 16 Train Loss: 0.5368212461471558\n",
      "Epoch 65 Batch: 18 Train Loss: 0.04714591056108475\n",
      "Epoch 65 Batch: 20 Train Loss: 0.053606998175382614\n",
      "Epoch 65 Batch: 22 Train Loss: 0.2639855742454529\n",
      "Epoch 65 Batch: 24 Train Loss: 0.062182337045669556\n",
      "Epoch 65 Batch: 26 Train Loss: 0.27964693307876587\n",
      "Epoch 65 Batch: 28 Train Loss: 0.06015073135495186\n",
      "Epoch 65 Batch: 30 Train Loss: 0.5099746584892273\n",
      "Epoch 65 Batch: 32 Train Loss: 0.08024357259273529\n",
      "Epoch 65 Batch: 34 Train Loss: 0.08843252062797546\n",
      "Epoch 65 Batch: 36 Train Loss: 0.07604660838842392\n",
      "Epoch 65 Batch: 38 Train Loss: 0.27285435795783997\n",
      "Epoch 65 Batch: 40 Train Loss: 0.2641235888004303\n",
      "Epoch 65 Batch: 42 Train Loss: 0.04991866648197174\n",
      "Epoch 65 Batch: 44 Train Loss: 0.09410746395587921\n",
      "Epoch 65 Batch: 46 Train Loss: 0.2686431109905243\n",
      "Epoch 65 Batch: 48 Train Loss: 0.2770959734916687\n",
      "Epoch 65 Batch: 50 Train Loss: 0.055839039385318756\n",
      "Epoch 65 Batch: 52 Train Loss: 0.28536275029182434\n",
      "Epoch 65 Batch: 54 Train Loss: 0.07919909060001373\n",
      "Epoch 65 Batch: 56 Train Loss: 0.28323909640312195\n",
      "Epoch 65 Batch: 58 Train Loss: 0.08180464804172516\n",
      "Epoch 65 Batch: 60 Train Loss: 0.0338318906724453\n",
      "Epoch 65 Batch: 62 Train Loss: 0.02583712339401245\n",
      "Epoch 65 Batch: 64 Train Loss: 0.025534475222229958\n",
      "Epoch 65 Batch: 66 Train Loss: 0.2635654807090759\n",
      "Epoch 65 Batch: 68 Train Loss: 0.2806502878665924\n",
      "Epoch 65 Batch: 70 Train Loss: 0.05120549350976944\n",
      "Epoch 65 Batch: 72 Train Loss: 0.03165704756975174\n",
      "Epoch 65 Batch: 74 Train Loss: 0.2918494939804077\n",
      "Epoch 65 Batch: 76 Train Loss: 0.034288350492715836\n",
      "Epoch 65 Batch: 78 Train Loss: 0.28841665387153625\n",
      "Epoch 65 Batch: 80 Train Loss: 0.04270118847489357\n",
      "Epoch 65 Batch: 82 Train Loss: 0.04904690757393837\n",
      "Epoch 65 Batch: 84 Train Loss: 0.06709402799606323\n",
      "Epoch 65 Batch: 86 Train Loss: 0.040551599115133286\n",
      "Epoch 65 Batch: 88 Train Loss: 0.008860728703439236\n",
      "Epoch 65 Batch: 90 Train Loss: 0.05576976388692856\n",
      "Epoch 65 Batch: 92 Train Loss: 0.02546662651002407\n",
      "Epoch 65 Batch: 94 Train Loss: 0.2989789545536041\n",
      "Epoch 65 Batch: 96 Train Loss: 0.02939467690885067\n",
      "Epoch 65 Batch: 98 Train Loss: 0.06394130736589432\n",
      "Epoch 65 Batch: 100 Train Loss: 0.044860389083623886\n",
      "Epoch 65 Batch: 102 Train Loss: 0.014828486368060112\n",
      "Epoch 65 Batch: 104 Train Loss: 0.26645249128341675\n",
      "Epoch 65 Batch: 106 Train Loss: 0.03691533952951431\n",
      "Epoch 65 Batch: 108 Train Loss: 0.06314222514629364\n",
      "Epoch 65 Batch: 110 Train Loss: 0.06701210886240005\n",
      "Epoch 65 Batch: 112 Train Loss: 0.29593849182128906\n",
      "Epoch 65 Batch: 114 Train Loss: 0.3165406584739685\n",
      "Epoch 65 Batch: 116 Train Loss: 0.050152916461229324\n",
      "Epoch 65 Batch: 118 Train Loss: 0.031717441976070404\n",
      "Epoch 65 Batch: 120 Train Loss: 0.31695839762687683\n",
      "Epoch 65 Batch: 122 Train Loss: 0.25879016518592834\n",
      "Epoch 65 Batch: 124 Train Loss: 0.2775328457355499\n",
      "Epoch 65 Batch: 126 Train Loss: 0.28761976957321167\n",
      "Epoch 65 Batch: 128 Train Loss: 0.04556985944509506\n",
      "Epoch 65 Batch: 130 Train Loss: 0.0928427055478096\n",
      "Epoch 65 Batch: 132 Train Loss: 0.5446356534957886\n",
      "Epoch 65 Batch: 134 Train Loss: 0.5426200032234192\n",
      "Epoch 65 Batch: 136 Train Loss: 0.0482482835650444\n",
      "Epoch 65 Batch: 138 Train Loss: 0.04705435410141945\n",
      "Epoch 65 Batch: 140 Train Loss: 0.061812229454517365\n",
      "Epoch 65 Batch: 142 Train Loss: 0.24588246643543243\n",
      "Epoch 65 Batch: 144 Train Loss: 0.06426820904016495\n",
      "Epoch 65 Batch: 146 Train Loss: 0.05692073702812195\n",
      "Epoch 65 Batch: 148 Train Loss: 0.2768594026565552\n",
      "Epoch 65 Batch: 150 Train Loss: 0.06639251112937927\n",
      "Epoch 65 Batch: 152 Train Loss: 0.31020233035087585\n",
      "Epoch 65 Batch: 154 Train Loss: 0.281792014837265\n",
      "Epoch 65 Batch: 156 Train Loss: 0.01987040974199772\n",
      "Epoch 65 Batch: 158 Train Loss: 0.07967223227024078\n",
      "Epoch 65 Batch: 160 Train Loss: 0.2607097327709198\n",
      "Epoch 66 Batch: 2 Train Loss: 0.09941259026527405\n",
      "Epoch 66 Batch: 4 Train Loss: 0.25907620787620544\n",
      "Epoch 66 Batch: 6 Train Loss: 0.22626462578773499\n",
      "Epoch 66 Batch: 8 Train Loss: 0.05800863355398178\n",
      "Epoch 66 Batch: 10 Train Loss: 0.2861301302909851\n",
      "Epoch 66 Batch: 12 Train Loss: 0.29170462489128113\n",
      "Epoch 66 Batch: 14 Train Loss: 0.036872781813144684\n",
      "Epoch 66 Batch: 16 Train Loss: 0.24595525860786438\n",
      "Epoch 66 Batch: 18 Train Loss: 0.04469143599271774\n",
      "Epoch 66 Batch: 20 Train Loss: 0.04931174963712692\n",
      "Epoch 66 Batch: 22 Train Loss: 0.0832892581820488\n",
      "Epoch 66 Batch: 24 Train Loss: 0.03260689601302147\n",
      "Epoch 66 Batch: 26 Train Loss: 0.4969707429409027\n",
      "Epoch 66 Batch: 28 Train Loss: 0.26686346530914307\n",
      "Epoch 66 Batch: 30 Train Loss: 0.29421883821487427\n",
      "Epoch 66 Batch: 32 Train Loss: 0.07004904747009277\n",
      "Epoch 66 Batch: 34 Train Loss: 0.27710583806037903\n",
      "Epoch 66 Batch: 36 Train Loss: 0.0948995053768158\n",
      "Epoch 66 Batch: 38 Train Loss: 0.27584606409072876\n",
      "Epoch 66 Batch: 40 Train Loss: 0.10806592553853989\n",
      "Epoch 66 Batch: 42 Train Loss: 0.294943630695343\n",
      "Epoch 66 Batch: 44 Train Loss: 0.07349241524934769\n",
      "Epoch 66 Batch: 46 Train Loss: 0.019766801968216896\n",
      "Epoch 66 Batch: 48 Train Loss: 0.057346880435943604\n",
      "Epoch 66 Batch: 50 Train Loss: 0.03458498790860176\n",
      "Epoch 66 Batch: 52 Train Loss: 0.04515228793025017\n",
      "Epoch 66 Batch: 54 Train Loss: 0.016890665516257286\n",
      "Epoch 66 Batch: 56 Train Loss: 0.04113982245326042\n",
      "Epoch 66 Batch: 58 Train Loss: 0.2975071966648102\n",
      "Epoch 66 Batch: 60 Train Loss: 0.5699506998062134\n",
      "Epoch 66 Batch: 62 Train Loss: 0.020439304411411285\n",
      "Epoch 66 Batch: 64 Train Loss: 0.5500169992446899\n",
      "Epoch 66 Batch: 66 Train Loss: 0.04840947315096855\n",
      "Epoch 66 Batch: 68 Train Loss: 0.04261137917637825\n",
      "Epoch 66 Batch: 70 Train Loss: 0.31893986463546753\n",
      "Epoch 66 Batch: 72 Train Loss: 0.28763872385025024\n",
      "Epoch 66 Batch: 74 Train Loss: 0.05644239857792854\n",
      "Epoch 66 Batch: 76 Train Loss: 0.7863196730613708\n",
      "Epoch 66 Batch: 78 Train Loss: 0.04026290401816368\n",
      "Epoch 66 Batch: 80 Train Loss: 0.08508166670799255\n",
      "Epoch 66 Batch: 82 Train Loss: 0.07369326055049896\n",
      "Epoch 66 Batch: 84 Train Loss: 0.06520476192235947\n",
      "Epoch 66 Batch: 86 Train Loss: 0.3032468855381012\n",
      "Epoch 66 Batch: 88 Train Loss: 0.08080841600894928\n",
      "Epoch 66 Batch: 90 Train Loss: 0.2769099175930023\n",
      "Epoch 66 Batch: 92 Train Loss: 0.3002702295780182\n",
      "Epoch 66 Batch: 94 Train Loss: 0.039933446794748306\n",
      "Epoch 66 Batch: 96 Train Loss: 0.49943679571151733\n",
      "Epoch 66 Batch: 98 Train Loss: 0.29519256949424744\n",
      "Epoch 66 Batch: 100 Train Loss: 0.2593727707862854\n",
      "Epoch 66 Batch: 102 Train Loss: 0.05166752263903618\n",
      "Epoch 66 Batch: 104 Train Loss: 0.09218748658895493\n",
      "Epoch 66 Batch: 106 Train Loss: 0.07870898395776749\n",
      "Epoch 66 Batch: 108 Train Loss: 0.06644664704799652\n",
      "Epoch 66 Batch: 110 Train Loss: 0.08241686224937439\n",
      "Epoch 66 Batch: 112 Train Loss: 0.08929461240768433\n",
      "Epoch 66 Batch: 114 Train Loss: 0.07533042132854462\n",
      "Epoch 66 Batch: 116 Train Loss: 0.06911247223615646\n",
      "Epoch 66 Batch: 118 Train Loss: 0.04394742101430893\n",
      "Epoch 66 Batch: 120 Train Loss: 0.046334777027368546\n",
      "Epoch 66 Batch: 122 Train Loss: 0.05526920408010483\n",
      "Epoch 66 Batch: 124 Train Loss: 0.05497708171606064\n",
      "Epoch 66 Batch: 126 Train Loss: 0.046863604336977005\n",
      "Epoch 66 Batch: 128 Train Loss: 0.5383341908454895\n",
      "Epoch 66 Batch: 130 Train Loss: 0.24737712740898132\n",
      "Epoch 66 Batch: 132 Train Loss: 0.06797821074724197\n",
      "Epoch 66 Batch: 134 Train Loss: 0.05620202422142029\n",
      "Epoch 66 Batch: 136 Train Loss: 0.05833517387509346\n",
      "Epoch 66 Batch: 138 Train Loss: 0.2750413119792938\n",
      "Epoch 66 Batch: 140 Train Loss: 0.06268281489610672\n",
      "Epoch 66 Batch: 142 Train Loss: 0.02568890154361725\n",
      "Epoch 66 Batch: 144 Train Loss: 0.038155052810907364\n",
      "Epoch 66 Batch: 146 Train Loss: 0.3088764548301697\n",
      "Epoch 66 Batch: 148 Train Loss: 0.03220752626657486\n",
      "Epoch 66 Batch: 150 Train Loss: 0.042440976947546005\n",
      "Epoch 66 Batch: 152 Train Loss: 0.030087733641266823\n",
      "Epoch 66 Batch: 154 Train Loss: 0.057259827852249146\n",
      "Epoch 66 Batch: 156 Train Loss: 0.30299967527389526\n",
      "Epoch 66 Batch: 158 Train Loss: 0.31165611743927\n",
      "Epoch 66 Batch: 160 Train Loss: 0.06882648169994354\n",
      "Epoch 67 Batch: 2 Train Loss: 0.2750294804573059\n",
      "Epoch 67 Batch: 4 Train Loss: 0.06011009216308594\n",
      "Epoch 67 Batch: 6 Train Loss: 0.06590797752141953\n",
      "Epoch 67 Batch: 8 Train Loss: 0.03458546847105026\n",
      "Epoch 67 Batch: 10 Train Loss: 0.055165477097034454\n",
      "Epoch 67 Batch: 12 Train Loss: 0.04862203076481819\n",
      "Epoch 67 Batch: 14 Train Loss: 0.05551156401634216\n",
      "Epoch 67 Batch: 16 Train Loss: 0.051771797239780426\n",
      "Epoch 67 Batch: 18 Train Loss: 0.5278191566467285\n",
      "Epoch 67 Batch: 20 Train Loss: 0.03581728786230087\n",
      "Epoch 67 Batch: 22 Train Loss: 0.2710105776786804\n",
      "Epoch 67 Batch: 24 Train Loss: 0.29790839552879333\n",
      "Epoch 67 Batch: 26 Train Loss: 0.039709050208330154\n",
      "Epoch 67 Batch: 28 Train Loss: 0.03900078311562538\n",
      "Epoch 67 Batch: 30 Train Loss: 0.0454614982008934\n",
      "Epoch 67 Batch: 32 Train Loss: 0.2941906750202179\n",
      "Epoch 67 Batch: 34 Train Loss: 0.5310472249984741\n",
      "Epoch 67 Batch: 36 Train Loss: 0.04691779240965843\n",
      "Epoch 67 Batch: 38 Train Loss: 0.07115326821804047\n",
      "Epoch 67 Batch: 40 Train Loss: 0.2498379647731781\n",
      "Epoch 67 Batch: 42 Train Loss: 0.020165694877505302\n",
      "Epoch 67 Batch: 44 Train Loss: 0.5121944546699524\n",
      "Epoch 67 Batch: 46 Train Loss: 0.29105934500694275\n",
      "Epoch 67 Batch: 48 Train Loss: 0.0475727841258049\n",
      "Epoch 67 Batch: 50 Train Loss: 0.28821659088134766\n",
      "Epoch 67 Batch: 52 Train Loss: 0.07404021173715591\n",
      "Epoch 67 Batch: 54 Train Loss: 0.09270258247852325\n",
      "Epoch 67 Batch: 56 Train Loss: 0.03655575215816498\n",
      "Epoch 67 Batch: 58 Train Loss: 0.2515985667705536\n",
      "Epoch 67 Batch: 60 Train Loss: 0.05523954704403877\n",
      "Epoch 67 Batch: 62 Train Loss: 0.5399033427238464\n",
      "Epoch 67 Batch: 64 Train Loss: 0.2610476315021515\n",
      "Epoch 67 Batch: 66 Train Loss: 0.03615373745560646\n",
      "Epoch 67 Batch: 68 Train Loss: 0.5309180617332458\n",
      "Epoch 67 Batch: 70 Train Loss: 0.05599217489361763\n",
      "Epoch 67 Batch: 72 Train Loss: 0.09692856669425964\n",
      "Epoch 67 Batch: 74 Train Loss: 0.27933427691459656\n",
      "Epoch 67 Batch: 76 Train Loss: 0.2833636999130249\n",
      "Epoch 67 Batch: 78 Train Loss: 0.07059059292078018\n",
      "Epoch 67 Batch: 80 Train Loss: 0.07323286682367325\n",
      "Epoch 67 Batch: 82 Train Loss: 0.030413981527090073\n",
      "Epoch 67 Batch: 84 Train Loss: 0.31574827432632446\n",
      "Epoch 67 Batch: 86 Train Loss: 0.28683027625083923\n",
      "Epoch 67 Batch: 88 Train Loss: 0.07479976862668991\n",
      "Epoch 67 Batch: 90 Train Loss: 0.04713950306177139\n",
      "Epoch 67 Batch: 92 Train Loss: 0.06735121458768845\n",
      "Epoch 67 Batch: 94 Train Loss: 0.3199673295021057\n",
      "Epoch 67 Batch: 96 Train Loss: 0.011800025589764118\n",
      "Epoch 67 Batch: 98 Train Loss: 0.01941734552383423\n",
      "Epoch 67 Batch: 100 Train Loss: 0.03615112230181694\n",
      "Epoch 67 Batch: 102 Train Loss: 0.06758806854486465\n",
      "Epoch 67 Batch: 104 Train Loss: 0.05312773585319519\n",
      "Epoch 67 Batch: 106 Train Loss: 0.05922657251358032\n",
      "Epoch 67 Batch: 108 Train Loss: 0.5426233410835266\n",
      "Epoch 67 Batch: 110 Train Loss: 0.0351981446146965\n",
      "Epoch 67 Batch: 112 Train Loss: 0.29009464383125305\n",
      "Epoch 67 Batch: 114 Train Loss: 0.04490907862782478\n",
      "Epoch 67 Batch: 116 Train Loss: 0.27344444394111633\n",
      "Epoch 67 Batch: 118 Train Loss: 0.2717015743255615\n",
      "Epoch 67 Batch: 120 Train Loss: 0.292267382144928\n",
      "Epoch 67 Batch: 122 Train Loss: 0.05095914006233215\n",
      "Epoch 67 Batch: 124 Train Loss: 0.5123176574707031\n",
      "Epoch 67 Batch: 126 Train Loss: 0.04446741193532944\n",
      "Epoch 67 Batch: 128 Train Loss: 0.04461280256509781\n",
      "Epoch 67 Batch: 130 Train Loss: 0.5533241629600525\n",
      "Epoch 67 Batch: 132 Train Loss: 0.5610624551773071\n",
      "Epoch 67 Batch: 134 Train Loss: 0.053287021815776825\n",
      "Epoch 67 Batch: 136 Train Loss: 0.0590975396335125\n",
      "Epoch 67 Batch: 138 Train Loss: 0.06635330617427826\n",
      "Epoch 67 Batch: 140 Train Loss: 0.06192641705274582\n",
      "Epoch 67 Batch: 142 Train Loss: 0.517386257648468\n",
      "Epoch 67 Batch: 144 Train Loss: 0.7415040135383606\n",
      "Epoch 67 Batch: 146 Train Loss: 0.052759744226932526\n",
      "Epoch 67 Batch: 148 Train Loss: 0.07799650728702545\n",
      "Epoch 67 Batch: 150 Train Loss: 0.272226482629776\n",
      "Epoch 67 Batch: 152 Train Loss: 0.03155839443206787\n",
      "Epoch 67 Batch: 154 Train Loss: 0.04116339236497879\n",
      "Epoch 67 Batch: 156 Train Loss: 0.06189914792776108\n",
      "Epoch 67 Batch: 158 Train Loss: 0.26309964060783386\n",
      "Epoch 67 Batch: 160 Train Loss: 0.03115205466747284\n",
      "Epoch 68 Batch: 2 Train Loss: 0.2704273462295532\n",
      "Epoch 68 Batch: 4 Train Loss: 0.06015646457672119\n",
      "Epoch 68 Batch: 6 Train Loss: 0.008111847564578056\n",
      "Epoch 68 Batch: 8 Train Loss: 0.016796793788671494\n",
      "Epoch 68 Batch: 10 Train Loss: 0.2820710241794586\n",
      "Epoch 68 Batch: 12 Train Loss: 0.03737783432006836\n",
      "Epoch 68 Batch: 14 Train Loss: 0.3088769316673279\n",
      "Epoch 68 Batch: 16 Train Loss: 0.03399605304002762\n",
      "Epoch 68 Batch: 18 Train Loss: 0.03467240184545517\n",
      "Epoch 68 Batch: 20 Train Loss: 0.026523226872086525\n",
      "Epoch 68 Batch: 22 Train Loss: 0.029742714017629623\n",
      "Epoch 68 Batch: 24 Train Loss: 0.3217974603176117\n",
      "Epoch 68 Batch: 26 Train Loss: 0.043311744928359985\n",
      "Epoch 68 Batch: 28 Train Loss: 0.3255598247051239\n",
      "Epoch 68 Batch: 30 Train Loss: 0.04249177500605583\n",
      "Epoch 68 Batch: 32 Train Loss: 0.019945789128541946\n",
      "Epoch 68 Batch: 34 Train Loss: 0.034853316843509674\n",
      "Epoch 68 Batch: 36 Train Loss: 0.05246768146753311\n",
      "Epoch 68 Batch: 38 Train Loss: 0.2965265214443207\n",
      "Epoch 68 Batch: 40 Train Loss: 0.0430426225066185\n",
      "Epoch 68 Batch: 42 Train Loss: 0.5264590978622437\n",
      "Epoch 68 Batch: 44 Train Loss: 0.48747867345809937\n",
      "Epoch 68 Batch: 46 Train Loss: 0.05504458025097847\n",
      "Epoch 68 Batch: 48 Train Loss: 0.07818630337715149\n",
      "Epoch 68 Batch: 50 Train Loss: 0.2676451504230499\n",
      "Epoch 68 Batch: 52 Train Loss: 0.02592342533171177\n",
      "Epoch 68 Batch: 54 Train Loss: 0.5276464223861694\n",
      "Epoch 68 Batch: 56 Train Loss: 0.08396582305431366\n",
      "Epoch 68 Batch: 58 Train Loss: 0.2758386731147766\n",
      "Epoch 68 Batch: 60 Train Loss: 0.05911903455853462\n",
      "Epoch 68 Batch: 62 Train Loss: 0.5092418789863586\n",
      "Epoch 68 Batch: 64 Train Loss: 0.06064273789525032\n",
      "Epoch 68 Batch: 66 Train Loss: 0.07450146973133087\n",
      "Epoch 68 Batch: 68 Train Loss: 0.07183323055505753\n",
      "Epoch 68 Batch: 70 Train Loss: 0.08373668044805527\n",
      "Epoch 68 Batch: 72 Train Loss: 0.05998041480779648\n",
      "Epoch 68 Batch: 74 Train Loss: 0.05595871061086655\n",
      "Epoch 68 Batch: 76 Train Loss: 0.04422033205628395\n",
      "Epoch 68 Batch: 78 Train Loss: 0.30033308267593384\n",
      "Epoch 68 Batch: 80 Train Loss: 0.05210483819246292\n",
      "Epoch 68 Batch: 82 Train Loss: 0.5287684202194214\n",
      "Epoch 68 Batch: 84 Train Loss: 0.05449292063713074\n",
      "Epoch 68 Batch: 86 Train Loss: 0.2714281976222992\n",
      "Epoch 68 Batch: 88 Train Loss: 0.3203600347042084\n",
      "Epoch 68 Batch: 90 Train Loss: 0.0614924319088459\n",
      "Epoch 68 Batch: 92 Train Loss: 0.27247753739356995\n",
      "Epoch 68 Batch: 94 Train Loss: 0.28799372911453247\n",
      "Epoch 68 Batch: 96 Train Loss: 0.543133556842804\n",
      "Epoch 68 Batch: 98 Train Loss: 0.0676184669137001\n",
      "Epoch 68 Batch: 100 Train Loss: 0.027493778616189957\n",
      "Epoch 68 Batch: 102 Train Loss: 0.23838868737220764\n",
      "Epoch 68 Batch: 104 Train Loss: 0.07301916927099228\n",
      "Epoch 68 Batch: 106 Train Loss: 0.2950257658958435\n",
      "Epoch 68 Batch: 108 Train Loss: 0.29175156354904175\n",
      "Epoch 68 Batch: 110 Train Loss: 0.26625996828079224\n",
      "Epoch 68 Batch: 112 Train Loss: 0.044777922332286835\n",
      "Epoch 68 Batch: 114 Train Loss: 0.25067389011383057\n",
      "Epoch 68 Batch: 116 Train Loss: 0.47828173637390137\n",
      "Epoch 68 Batch: 118 Train Loss: 0.09034891426563263\n",
      "Epoch 68 Batch: 120 Train Loss: 0.05979681760072708\n",
      "Epoch 68 Batch: 122 Train Loss: 0.25226348638534546\n",
      "Epoch 68 Batch: 124 Train Loss: 0.07602810859680176\n",
      "Epoch 68 Batch: 126 Train Loss: 0.045249782502651215\n",
      "Epoch 68 Batch: 128 Train Loss: 0.2710864543914795\n",
      "Epoch 68 Batch: 130 Train Loss: 0.06583970785140991\n",
      "Epoch 68 Batch: 132 Train Loss: 0.06423796713352203\n",
      "Epoch 68 Batch: 134 Train Loss: 0.0534060113132\n",
      "Epoch 68 Batch: 136 Train Loss: 0.06846882402896881\n",
      "Epoch 68 Batch: 138 Train Loss: 0.051833849400281906\n",
      "Epoch 68 Batch: 140 Train Loss: 0.03794465586543083\n",
      "Epoch 68 Batch: 142 Train Loss: 0.5110057592391968\n",
      "Epoch 68 Batch: 144 Train Loss: 0.062362559139728546\n",
      "Epoch 68 Batch: 146 Train Loss: 0.5444862842559814\n",
      "Epoch 68 Batch: 148 Train Loss: 0.03846533223986626\n",
      "Epoch 68 Batch: 150 Train Loss: 0.08831483125686646\n",
      "Epoch 68 Batch: 152 Train Loss: 0.044528599828481674\n",
      "Epoch 68 Batch: 154 Train Loss: 0.314693808555603\n",
      "Epoch 68 Batch: 156 Train Loss: 0.017631182447075844\n",
      "Epoch 68 Batch: 158 Train Loss: 0.016334237530827522\n",
      "Epoch 68 Batch: 160 Train Loss: 0.3215082287788391\n",
      "Epoch 69 Batch: 2 Train Loss: 0.5177378058433533\n",
      "Epoch 69 Batch: 4 Train Loss: 0.046120785176754\n",
      "Epoch 69 Batch: 6 Train Loss: 0.29884880781173706\n",
      "Epoch 69 Batch: 8 Train Loss: 0.25423428416252136\n",
      "Epoch 69 Batch: 10 Train Loss: 0.07013095915317535\n",
      "Epoch 69 Batch: 12 Train Loss: 0.08399111777544022\n",
      "Epoch 69 Batch: 14 Train Loss: 0.06645160168409348\n",
      "Epoch 69 Batch: 16 Train Loss: 0.29342636466026306\n",
      "Epoch 69 Batch: 18 Train Loss: 0.07237665355205536\n",
      "Epoch 69 Batch: 20 Train Loss: 0.07553406059741974\n",
      "Epoch 69 Batch: 22 Train Loss: 0.4713706076145172\n",
      "Epoch 69 Batch: 24 Train Loss: 0.0637533962726593\n",
      "Epoch 69 Batch: 26 Train Loss: 0.5030826330184937\n",
      "Epoch 69 Batch: 28 Train Loss: 0.06298590451478958\n",
      "Epoch 69 Batch: 30 Train Loss: 0.27230069041252136\n",
      "Epoch 69 Batch: 32 Train Loss: 0.10415427386760712\n",
      "Epoch 69 Batch: 34 Train Loss: 0.07887373119592667\n",
      "Epoch 69 Batch: 36 Train Loss: 0.03793013468384743\n",
      "Epoch 69 Batch: 38 Train Loss: 0.2925060987472534\n",
      "Epoch 69 Batch: 40 Train Loss: 0.05068318918347359\n",
      "Epoch 69 Batch: 42 Train Loss: 0.09786920994520187\n",
      "Epoch 69 Batch: 44 Train Loss: 0.07884452491998672\n",
      "Epoch 69 Batch: 46 Train Loss: 0.07746319472789764\n",
      "Epoch 69 Batch: 48 Train Loss: 0.04413779824972153\n",
      "Epoch 69 Batch: 50 Train Loss: 0.04315707087516785\n",
      "Epoch 69 Batch: 52 Train Loss: 0.05946573615074158\n",
      "Epoch 69 Batch: 54 Train Loss: 0.3005762994289398\n",
      "Epoch 69 Batch: 56 Train Loss: 0.2877538800239563\n",
      "Epoch 69 Batch: 58 Train Loss: 0.5384593605995178\n",
      "Epoch 69 Batch: 60 Train Loss: 0.06520635634660721\n",
      "Epoch 69 Batch: 62 Train Loss: 0.056300509721040726\n",
      "Epoch 69 Batch: 64 Train Loss: 0.05476362630724907\n",
      "Epoch 69 Batch: 66 Train Loss: 0.27082666754722595\n",
      "Epoch 69 Batch: 68 Train Loss: 0.2712448239326477\n",
      "Epoch 69 Batch: 70 Train Loss: 0.036310117691755295\n",
      "Epoch 69 Batch: 72 Train Loss: 0.05246935039758682\n",
      "Epoch 69 Batch: 74 Train Loss: 0.02660779282450676\n",
      "Epoch 69 Batch: 76 Train Loss: 0.06886188685894012\n",
      "Epoch 69 Batch: 78 Train Loss: 0.2899206280708313\n",
      "Epoch 69 Batch: 80 Train Loss: 0.036995161324739456\n",
      "Epoch 69 Batch: 82 Train Loss: 0.055583178997039795\n",
      "Epoch 69 Batch: 84 Train Loss: 0.048795513808727264\n",
      "Epoch 69 Batch: 86 Train Loss: 0.30107349157333374\n",
      "Epoch 69 Batch: 88 Train Loss: 0.06917718052864075\n",
      "Epoch 69 Batch: 90 Train Loss: 0.03423373028635979\n",
      "Epoch 69 Batch: 92 Train Loss: 0.054515182971954346\n",
      "Epoch 69 Batch: 94 Train Loss: 0.2916789650917053\n",
      "Epoch 69 Batch: 96 Train Loss: 0.3114176392555237\n",
      "Epoch 69 Batch: 98 Train Loss: 0.034096457064151764\n",
      "Epoch 69 Batch: 100 Train Loss: 0.07883010804653168\n",
      "Epoch 69 Batch: 102 Train Loss: 0.06480224430561066\n",
      "Epoch 69 Batch: 104 Train Loss: 0.28402549028396606\n",
      "Epoch 69 Batch: 106 Train Loss: 0.03312091529369354\n",
      "Epoch 69 Batch: 108 Train Loss: 0.8153879046440125\n",
      "Epoch 69 Batch: 110 Train Loss: 0.041060253977775574\n",
      "Epoch 69 Batch: 112 Train Loss: 0.5228298902511597\n",
      "Epoch 69 Batch: 114 Train Loss: 0.03498881310224533\n",
      "Epoch 69 Batch: 116 Train Loss: 0.03559005260467529\n",
      "Epoch 69 Batch: 118 Train Loss: 0.04946940764784813\n",
      "Epoch 69 Batch: 120 Train Loss: 0.026627738028764725\n",
      "Epoch 69 Batch: 122 Train Loss: 0.2763711214065552\n",
      "Epoch 69 Batch: 124 Train Loss: 0.041230808943510056\n",
      "Epoch 69 Batch: 126 Train Loss: 0.27594441175460815\n",
      "Epoch 69 Batch: 128 Train Loss: 0.2959612011909485\n",
      "Epoch 69 Batch: 130 Train Loss: 0.06113678961992264\n",
      "Epoch 69 Batch: 132 Train Loss: 0.5535726547241211\n",
      "Epoch 69 Batch: 134 Train Loss: 0.037449706345796585\n",
      "Epoch 69 Batch: 136 Train Loss: 0.03787005692720413\n",
      "Epoch 69 Batch: 138 Train Loss: 0.29529261589050293\n",
      "Epoch 69 Batch: 140 Train Loss: 0.27352333068847656\n",
      "Epoch 69 Batch: 142 Train Loss: 0.2991271913051605\n",
      "Epoch 69 Batch: 144 Train Loss: 0.08240915834903717\n",
      "Epoch 69 Batch: 146 Train Loss: 0.03527282550930977\n",
      "Epoch 69 Batch: 148 Train Loss: 0.054048676043748856\n",
      "Epoch 69 Batch: 150 Train Loss: 0.05787881463766098\n",
      "Epoch 69 Batch: 152 Train Loss: 0.04380067065358162\n",
      "Epoch 69 Batch: 154 Train Loss: 0.07464345544576645\n",
      "Epoch 69 Batch: 156 Train Loss: 0.2892366647720337\n",
      "Epoch 69 Batch: 158 Train Loss: 0.2926101088523865\n",
      "Epoch 69 Batch: 160 Train Loss: 0.07258330285549164\n",
      "Epoch 70 Batch: 2 Train Loss: 0.5082786679267883\n",
      "Epoch 70 Batch: 4 Train Loss: 0.2777636647224426\n",
      "Epoch 70 Batch: 6 Train Loss: 0.5085111856460571\n",
      "Epoch 70 Batch: 8 Train Loss: 0.2927477955818176\n",
      "Epoch 70 Batch: 10 Train Loss: 0.033547379076480865\n",
      "Epoch 70 Batch: 12 Train Loss: 0.05481329560279846\n",
      "Epoch 70 Batch: 14 Train Loss: 0.011580897495150566\n",
      "Epoch 70 Batch: 16 Train Loss: 0.08301255851984024\n",
      "Epoch 70 Batch: 18 Train Loss: 0.2803068459033966\n",
      "Epoch 70 Batch: 20 Train Loss: 0.2705901265144348\n",
      "Epoch 70 Batch: 22 Train Loss: 0.2870396375656128\n",
      "Epoch 70 Batch: 24 Train Loss: 0.060111433267593384\n",
      "Epoch 70 Batch: 26 Train Loss: 0.0407450795173645\n",
      "Epoch 70 Batch: 28 Train Loss: 0.0552837960422039\n",
      "Epoch 70 Batch: 30 Train Loss: 0.0464179664850235\n",
      "Epoch 70 Batch: 32 Train Loss: 0.037006676197052\n",
      "Epoch 70 Batch: 34 Train Loss: 0.05315515398979187\n",
      "Epoch 70 Batch: 36 Train Loss: 0.043042898178100586\n",
      "Epoch 70 Batch: 38 Train Loss: 0.05995192006230354\n",
      "Epoch 70 Batch: 40 Train Loss: 0.05819119140505791\n",
      "Epoch 70 Batch: 42 Train Loss: 0.2835831642150879\n",
      "Epoch 70 Batch: 44 Train Loss: 0.3308390974998474\n",
      "Epoch 70 Batch: 46 Train Loss: 0.02151848003268242\n",
      "Epoch 70 Batch: 48 Train Loss: 0.029163381084799767\n",
      "Epoch 70 Batch: 50 Train Loss: 0.3181830048561096\n",
      "Epoch 70 Batch: 52 Train Loss: 0.04000377655029297\n",
      "Epoch 70 Batch: 54 Train Loss: 0.027605626732110977\n",
      "Epoch 70 Batch: 56 Train Loss: 0.35601574182510376\n",
      "Epoch 70 Batch: 58 Train Loss: 0.03482685983181\n",
      "Epoch 70 Batch: 60 Train Loss: 0.31409505009651184\n",
      "Epoch 70 Batch: 62 Train Loss: 0.014149573631584644\n",
      "Epoch 70 Batch: 64 Train Loss: 0.041407261043787\n",
      "Epoch 70 Batch: 66 Train Loss: 0.5277924537658691\n",
      "Epoch 70 Batch: 68 Train Loss: 0.041348960250616074\n",
      "Epoch 70 Batch: 70 Train Loss: 0.30422836542129517\n",
      "Epoch 70 Batch: 72 Train Loss: 0.04170413315296173\n",
      "Epoch 70 Batch: 74 Train Loss: 0.06019182130694389\n",
      "Epoch 70 Batch: 76 Train Loss: 0.040766071528196335\n",
      "Epoch 70 Batch: 78 Train Loss: 0.2975638210773468\n",
      "Epoch 70 Batch: 80 Train Loss: 0.038807161152362823\n",
      "Epoch 70 Batch: 82 Train Loss: 0.050596632063388824\n",
      "Epoch 70 Batch: 84 Train Loss: 0.28415703773498535\n",
      "Epoch 70 Batch: 86 Train Loss: 0.2789740264415741\n",
      "Epoch 70 Batch: 88 Train Loss: 0.2776344418525696\n",
      "Epoch 70 Batch: 90 Train Loss: 0.24874980747699738\n",
      "Epoch 70 Batch: 92 Train Loss: 0.0620720200240612\n",
      "Epoch 70 Batch: 94 Train Loss: 0.27775314450263977\n",
      "Epoch 70 Batch: 96 Train Loss: 0.30394354462623596\n",
      "Epoch 70 Batch: 98 Train Loss: 0.24583777785301208\n",
      "Epoch 70 Batch: 100 Train Loss: 0.2560552954673767\n",
      "Epoch 70 Batch: 102 Train Loss: 0.09970783442258835\n",
      "Epoch 70 Batch: 104 Train Loss: 0.07102786004543304\n",
      "Epoch 70 Batch: 106 Train Loss: 0.079032301902771\n",
      "Epoch 70 Batch: 108 Train Loss: 0.06131748482584953\n",
      "Epoch 70 Batch: 110 Train Loss: 0.3040317893028259\n",
      "Epoch 70 Batch: 112 Train Loss: 0.05849268287420273\n",
      "Epoch 70 Batch: 114 Train Loss: 0.25521498918533325\n",
      "Epoch 70 Batch: 116 Train Loss: 0.2506597638130188\n",
      "Epoch 70 Batch: 118 Train Loss: 0.065021812915802\n",
      "Epoch 70 Batch: 120 Train Loss: 0.27875402569770813\n",
      "Epoch 70 Batch: 122 Train Loss: 0.032320305705070496\n",
      "Epoch 70 Batch: 124 Train Loss: 0.5426823496818542\n",
      "Epoch 70 Batch: 126 Train Loss: 0.050077326595783234\n",
      "Epoch 70 Batch: 128 Train Loss: 0.060089271515607834\n",
      "Epoch 70 Batch: 130 Train Loss: 0.03460114449262619\n",
      "Epoch 70 Batch: 132 Train Loss: 0.06447948515415192\n",
      "Epoch 70 Batch: 134 Train Loss: 0.31957000494003296\n",
      "Epoch 70 Batch: 136 Train Loss: 0.057815324515104294\n",
      "Epoch 70 Batch: 138 Train Loss: 0.49392691254615784\n",
      "Epoch 70 Batch: 140 Train Loss: 0.060749541968107224\n",
      "Epoch 70 Batch: 142 Train Loss: 0.06578359752893448\n",
      "Epoch 70 Batch: 144 Train Loss: 0.07183174788951874\n",
      "Epoch 70 Batch: 146 Train Loss: 0.763970136642456\n",
      "Epoch 70 Batch: 148 Train Loss: 0.28412890434265137\n",
      "Epoch 70 Batch: 150 Train Loss: 0.5285154581069946\n",
      "Epoch 70 Batch: 152 Train Loss: 0.05852776765823364\n",
      "Epoch 70 Batch: 154 Train Loss: 0.07272269576787949\n",
      "Epoch 70 Batch: 156 Train Loss: 0.044232942163944244\n",
      "Epoch 70 Batch: 158 Train Loss: 0.05419538542628288\n",
      "Epoch 70 Batch: 160 Train Loss: 0.07575678080320358\n",
      "Epoch 71 Batch: 2 Train Loss: 0.3099879026412964\n",
      "Epoch 71 Batch: 4 Train Loss: 0.04648853838443756\n",
      "Epoch 71 Batch: 6 Train Loss: 0.0534064844250679\n",
      "Epoch 71 Batch: 8 Train Loss: 0.05468177795410156\n",
      "Epoch 71 Batch: 10 Train Loss: 0.05983642488718033\n",
      "Epoch 71 Batch: 12 Train Loss: 0.3139147162437439\n",
      "Epoch 71 Batch: 14 Train Loss: 0.059247322380542755\n",
      "Epoch 71 Batch: 16 Train Loss: 0.04212334752082825\n",
      "Epoch 71 Batch: 18 Train Loss: 0.2675793468952179\n",
      "Epoch 71 Batch: 20 Train Loss: 0.03807254508137703\n",
      "Epoch 71 Batch: 22 Train Loss: 0.022016528993844986\n",
      "Epoch 71 Batch: 24 Train Loss: 0.04127772897481918\n",
      "Epoch 71 Batch: 26 Train Loss: 0.04346247762441635\n",
      "Epoch 71 Batch: 28 Train Loss: 0.03452007845044136\n",
      "Epoch 71 Batch: 30 Train Loss: 0.049546532332897186\n",
      "Epoch 71 Batch: 32 Train Loss: 0.2775353789329529\n",
      "Epoch 71 Batch: 34 Train Loss: 0.03523365780711174\n",
      "Epoch 71 Batch: 36 Train Loss: 0.8017123341560364\n",
      "Epoch 71 Batch: 38 Train Loss: 0.5586878061294556\n",
      "Epoch 71 Batch: 40 Train Loss: 0.3134274482727051\n",
      "Epoch 71 Batch: 42 Train Loss: 0.04404212161898613\n",
      "Epoch 71 Batch: 44 Train Loss: 0.27769163250923157\n",
      "Epoch 71 Batch: 46 Train Loss: 0.0642557293176651\n",
      "Epoch 71 Batch: 48 Train Loss: 0.2824895977973938\n",
      "Epoch 71 Batch: 50 Train Loss: 0.074348583817482\n",
      "Epoch 71 Batch: 52 Train Loss: 0.0314982533454895\n",
      "Epoch 71 Batch: 54 Train Loss: 0.07580306380987167\n",
      "Epoch 71 Batch: 56 Train Loss: 0.2885250747203827\n",
      "Epoch 71 Batch: 58 Train Loss: 0.09366127103567123\n",
      "Epoch 71 Batch: 60 Train Loss: 0.021949470043182373\n",
      "Epoch 71 Batch: 62 Train Loss: 0.07441549748182297\n",
      "Epoch 71 Batch: 64 Train Loss: 0.058782197535037994\n",
      "Epoch 71 Batch: 66 Train Loss: 0.061805594712495804\n",
      "Epoch 71 Batch: 68 Train Loss: 0.050072915852069855\n",
      "Epoch 71 Batch: 70 Train Loss: 0.05296354368329048\n",
      "Epoch 71 Batch: 72 Train Loss: 0.3186569809913635\n",
      "Epoch 71 Batch: 74 Train Loss: 0.3021242022514343\n",
      "Epoch 71 Batch: 76 Train Loss: 0.2750142216682434\n",
      "Epoch 71 Batch: 78 Train Loss: 0.5553876161575317\n",
      "Epoch 71 Batch: 80 Train Loss: 0.06786035746335983\n",
      "Epoch 71 Batch: 82 Train Loss: 0.2934655249118805\n",
      "Epoch 71 Batch: 84 Train Loss: 0.04302823543548584\n",
      "Epoch 71 Batch: 86 Train Loss: 0.007929963991045952\n",
      "Epoch 71 Batch: 88 Train Loss: 0.05571936443448067\n",
      "Epoch 71 Batch: 90 Train Loss: 0.49266114830970764\n",
      "Epoch 71 Batch: 92 Train Loss: 0.287345826625824\n",
      "Epoch 71 Batch: 94 Train Loss: 0.06748082488775253\n",
      "Epoch 71 Batch: 96 Train Loss: 0.03421248868107796\n",
      "Epoch 71 Batch: 98 Train Loss: 0.092635378241539\n",
      "Epoch 71 Batch: 100 Train Loss: 0.08062941581010818\n",
      "Epoch 71 Batch: 102 Train Loss: 0.27852174639701843\n",
      "Epoch 71 Batch: 104 Train Loss: 0.07488322257995605\n",
      "Epoch 71 Batch: 106 Train Loss: 0.0930488258600235\n",
      "Epoch 71 Batch: 108 Train Loss: 0.2715281546115875\n",
      "Epoch 71 Batch: 110 Train Loss: 0.29411932826042175\n",
      "Epoch 71 Batch: 112 Train Loss: 0.06511008739471436\n",
      "Epoch 71 Batch: 114 Train Loss: 0.037233613431453705\n",
      "Epoch 71 Batch: 116 Train Loss: 0.5469528436660767\n",
      "Epoch 71 Batch: 118 Train Loss: 0.2577835023403168\n",
      "Epoch 71 Batch: 120 Train Loss: 0.07898491621017456\n",
      "Epoch 71 Batch: 122 Train Loss: 0.2644727826118469\n",
      "Epoch 71 Batch: 124 Train Loss: 0.06916599720716476\n",
      "Epoch 71 Batch: 126 Train Loss: 0.045647863298654556\n",
      "Epoch 71 Batch: 128 Train Loss: 0.06394792348146439\n",
      "Epoch 71 Batch: 130 Train Loss: 0.2901877760887146\n",
      "Epoch 71 Batch: 132 Train Loss: 0.050096262246370316\n",
      "Epoch 71 Batch: 134 Train Loss: 0.06052634119987488\n",
      "Epoch 71 Batch: 136 Train Loss: 0.05483899265527725\n",
      "Epoch 71 Batch: 138 Train Loss: 0.2767731547355652\n",
      "Epoch 71 Batch: 140 Train Loss: 0.2898382842540741\n",
      "Epoch 71 Batch: 142 Train Loss: 0.045303087681531906\n",
      "Epoch 71 Batch: 144 Train Loss: 0.06582231819629669\n",
      "Epoch 71 Batch: 146 Train Loss: 0.04707471281290054\n",
      "Epoch 71 Batch: 148 Train Loss: 0.05717954784631729\n",
      "Epoch 71 Batch: 150 Train Loss: 0.07775167375802994\n",
      "Epoch 71 Batch: 152 Train Loss: 0.3101600706577301\n",
      "Epoch 71 Batch: 154 Train Loss: 0.03721650317311287\n",
      "Epoch 71 Batch: 156 Train Loss: 0.29502445459365845\n",
      "Epoch 71 Batch: 158 Train Loss: 0.23713001608848572\n",
      "Epoch 71 Batch: 160 Train Loss: 0.02664678730070591\n",
      "Epoch 72 Batch: 2 Train Loss: 0.06805378943681717\n",
      "Epoch 72 Batch: 4 Train Loss: 0.2950579822063446\n",
      "Epoch 72 Batch: 6 Train Loss: 0.038253288716077805\n",
      "Epoch 72 Batch: 8 Train Loss: 0.07327616959810257\n",
      "Epoch 72 Batch: 10 Train Loss: 0.031534455716609955\n",
      "Epoch 72 Batch: 12 Train Loss: 0.05638192221522331\n",
      "Epoch 72 Batch: 14 Train Loss: 0.047452203929424286\n",
      "Epoch 72 Batch: 16 Train Loss: 0.05570297688245773\n",
      "Epoch 72 Batch: 18 Train Loss: 0.30244824290275574\n",
      "Epoch 72 Batch: 20 Train Loss: 0.05403444170951843\n",
      "Epoch 72 Batch: 22 Train Loss: 0.3047405779361725\n",
      "Epoch 72 Batch: 24 Train Loss: 0.29568803310394287\n",
      "Epoch 72 Batch: 26 Train Loss: 0.29033833742141724\n",
      "Epoch 72 Batch: 28 Train Loss: 0.28802329301834106\n",
      "Epoch 72 Batch: 30 Train Loss: 0.05294165760278702\n",
      "Epoch 72 Batch: 32 Train Loss: 0.27272117137908936\n",
      "Epoch 72 Batch: 34 Train Loss: 0.26867279410362244\n",
      "Epoch 72 Batch: 36 Train Loss: 0.2549063563346863\n",
      "Epoch 72 Batch: 38 Train Loss: 0.07173259556293488\n",
      "Epoch 72 Batch: 40 Train Loss: 0.06029101088643074\n",
      "Epoch 72 Batch: 42 Train Loss: 0.057319026440382004\n",
      "Epoch 72 Batch: 44 Train Loss: 0.5479528307914734\n",
      "Epoch 72 Batch: 46 Train Loss: 0.5368442535400391\n",
      "Epoch 72 Batch: 48 Train Loss: 0.49176859855651855\n",
      "Epoch 72 Batch: 50 Train Loss: 0.29159241914749146\n",
      "Epoch 72 Batch: 52 Train Loss: 0.04908835142850876\n",
      "Epoch 72 Batch: 54 Train Loss: 0.2748424708843231\n",
      "Epoch 72 Batch: 56 Train Loss: 0.05600861832499504\n",
      "Epoch 72 Batch: 58 Train Loss: 0.26514923572540283\n",
      "Epoch 72 Batch: 60 Train Loss: 0.283176064491272\n",
      "Epoch 72 Batch: 62 Train Loss: 0.05708895996212959\n",
      "Epoch 72 Batch: 64 Train Loss: 0.05720766261219978\n",
      "Epoch 72 Batch: 66 Train Loss: 0.07763036340475082\n",
      "Epoch 72 Batch: 68 Train Loss: 0.06378597021102905\n",
      "Epoch 72 Batch: 70 Train Loss: 0.08052124828100204\n",
      "Epoch 72 Batch: 72 Train Loss: 0.30105727910995483\n",
      "Epoch 72 Batch: 74 Train Loss: 0.07819308340549469\n",
      "Epoch 72 Batch: 76 Train Loss: 0.04972510412335396\n",
      "Epoch 72 Batch: 78 Train Loss: 0.08629187196493149\n",
      "Epoch 72 Batch: 80 Train Loss: 0.03194592520594597\n",
      "Epoch 72 Batch: 82 Train Loss: 0.029812848195433617\n",
      "Epoch 72 Batch: 84 Train Loss: 0.03142038732767105\n",
      "Epoch 72 Batch: 86 Train Loss: 0.28111472725868225\n",
      "Epoch 72 Batch: 88 Train Loss: 0.32665082812309265\n",
      "Epoch 72 Batch: 90 Train Loss: 0.07682271301746368\n",
      "Epoch 72 Batch: 92 Train Loss: 0.04430273920297623\n",
      "Epoch 72 Batch: 94 Train Loss: 0.06648237258195877\n",
      "Epoch 72 Batch: 96 Train Loss: 0.062000177800655365\n",
      "Epoch 72 Batch: 98 Train Loss: 0.27527549862861633\n",
      "Epoch 72 Batch: 100 Train Loss: 0.47539910674095154\n",
      "Epoch 72 Batch: 102 Train Loss: 0.02488393895328045\n",
      "Epoch 72 Batch: 104 Train Loss: 0.2604742646217346\n",
      "Epoch 72 Batch: 106 Train Loss: 0.037522245198488235\n",
      "Epoch 72 Batch: 108 Train Loss: 0.2618032395839691\n",
      "Epoch 72 Batch: 110 Train Loss: 0.05312559753656387\n",
      "Epoch 72 Batch: 112 Train Loss: 0.2734370827674866\n",
      "Epoch 72 Batch: 114 Train Loss: 0.29668572545051575\n",
      "Epoch 72 Batch: 116 Train Loss: 0.2793068289756775\n",
      "Epoch 72 Batch: 118 Train Loss: 0.3134077191352844\n",
      "Epoch 72 Batch: 120 Train Loss: 0.06169508770108223\n",
      "Epoch 72 Batch: 122 Train Loss: 0.07974313199520111\n",
      "Epoch 72 Batch: 124 Train Loss: 0.26918479800224304\n",
      "Epoch 72 Batch: 126 Train Loss: 0.035705000162124634\n",
      "Epoch 72 Batch: 128 Train Loss: 0.033455491065979004\n",
      "Epoch 72 Batch: 130 Train Loss: 0.29432058334350586\n",
      "Epoch 72 Batch: 132 Train Loss: 0.04811886325478554\n",
      "Epoch 72 Batch: 134 Train Loss: 0.30394309759140015\n",
      "Epoch 72 Batch: 136 Train Loss: 0.26418572664260864\n",
      "Epoch 72 Batch: 138 Train Loss: 0.05329592153429985\n",
      "Epoch 72 Batch: 140 Train Loss: 0.03670927882194519\n",
      "Epoch 72 Batch: 142 Train Loss: 0.03743663430213928\n",
      "Epoch 72 Batch: 144 Train Loss: 0.07417665421962738\n",
      "Epoch 72 Batch: 146 Train Loss: 0.025660699233412743\n",
      "Epoch 72 Batch: 148 Train Loss: 0.5402015447616577\n",
      "Epoch 72 Batch: 150 Train Loss: 0.06250030547380447\n",
      "Epoch 72 Batch: 152 Train Loss: 0.07715307176113129\n",
      "Epoch 72 Batch: 154 Train Loss: 0.07564155757427216\n",
      "Epoch 72 Batch: 156 Train Loss: 0.06418652832508087\n",
      "Epoch 72 Batch: 158 Train Loss: 0.2836740016937256\n",
      "Epoch 72 Batch: 160 Train Loss: 0.05657548829913139\n",
      "Epoch 73 Batch: 2 Train Loss: 0.29423612356185913\n",
      "Epoch 73 Batch: 4 Train Loss: 0.06611790508031845\n",
      "Epoch 73 Batch: 6 Train Loss: 0.04034171998500824\n",
      "Epoch 73 Batch: 8 Train Loss: 0.021607721224427223\n",
      "Epoch 73 Batch: 10 Train Loss: 0.3121210038661957\n",
      "Epoch 73 Batch: 12 Train Loss: 0.30936673283576965\n",
      "Epoch 73 Batch: 14 Train Loss: 0.05074859783053398\n",
      "Epoch 73 Batch: 16 Train Loss: 0.0334540493786335\n",
      "Epoch 73 Batch: 18 Train Loss: 0.05292347073554993\n",
      "Epoch 73 Batch: 20 Train Loss: 0.28927960991859436\n",
      "Epoch 73 Batch: 22 Train Loss: 0.044059693813323975\n",
      "Epoch 73 Batch: 24 Train Loss: 0.057351887226104736\n",
      "Epoch 73 Batch: 26 Train Loss: 0.27502208948135376\n",
      "Epoch 73 Batch: 28 Train Loss: 0.028484543785452843\n",
      "Epoch 73 Batch: 30 Train Loss: 0.04346563667058945\n",
      "Epoch 73 Batch: 32 Train Loss: 0.2782611548900604\n",
      "Epoch 73 Batch: 34 Train Loss: 0.05113326385617256\n",
      "Epoch 73 Batch: 36 Train Loss: 0.2823259234428406\n",
      "Epoch 73 Batch: 38 Train Loss: 0.2880147099494934\n",
      "Epoch 73 Batch: 40 Train Loss: 0.04482146352529526\n",
      "Epoch 73 Batch: 42 Train Loss: 0.04592463746666908\n",
      "Epoch 73 Batch: 44 Train Loss: 0.5450237393379211\n",
      "Epoch 73 Batch: 46 Train Loss: 0.061523549258708954\n",
      "Epoch 73 Batch: 48 Train Loss: 0.5033701062202454\n",
      "Epoch 73 Batch: 50 Train Loss: 0.2759737968444824\n",
      "Epoch 73 Batch: 52 Train Loss: 0.0829538032412529\n",
      "Epoch 73 Batch: 54 Train Loss: 0.038292236626148224\n",
      "Epoch 73 Batch: 56 Train Loss: 0.05124790593981743\n",
      "Epoch 73 Batch: 58 Train Loss: 0.04540356248617172\n",
      "Epoch 73 Batch: 60 Train Loss: 0.2996188998222351\n",
      "Epoch 73 Batch: 62 Train Loss: 0.25360018014907837\n",
      "Epoch 73 Batch: 64 Train Loss: 0.07013897597789764\n",
      "Epoch 73 Batch: 66 Train Loss: 0.06997595727443695\n",
      "Epoch 73 Batch: 68 Train Loss: 0.2586228847503662\n",
      "Epoch 73 Batch: 70 Train Loss: 0.0839184820652008\n",
      "Epoch 73 Batch: 72 Train Loss: 0.08503491431474686\n",
      "Epoch 73 Batch: 74 Train Loss: 0.04874197766184807\n",
      "Epoch 73 Batch: 76 Train Loss: 0.07218961417675018\n",
      "Epoch 73 Batch: 78 Train Loss: 0.0797475203871727\n",
      "Epoch 73 Batch: 80 Train Loss: 0.0858287438750267\n",
      "Epoch 73 Batch: 82 Train Loss: 0.05908365175127983\n",
      "Epoch 73 Batch: 84 Train Loss: 0.760905385017395\n",
      "Epoch 73 Batch: 86 Train Loss: 0.03228963539004326\n",
      "Epoch 73 Batch: 88 Train Loss: 0.06736008822917938\n",
      "Epoch 73 Batch: 90 Train Loss: 0.05362648516893387\n",
      "Epoch 73 Batch: 92 Train Loss: 0.03873767703771591\n",
      "Epoch 73 Batch: 94 Train Loss: 0.03150162845849991\n",
      "Epoch 73 Batch: 96 Train Loss: 0.027312463149428368\n",
      "Epoch 73 Batch: 98 Train Loss: 0.28571560978889465\n",
      "Epoch 73 Batch: 100 Train Loss: 0.029779821634292603\n",
      "Epoch 73 Batch: 102 Train Loss: 0.044789623469114304\n",
      "Epoch 73 Batch: 104 Train Loss: 0.056887488812208176\n",
      "Epoch 73 Batch: 106 Train Loss: 0.04678899794816971\n",
      "Epoch 73 Batch: 108 Train Loss: 0.30098921060562134\n",
      "Epoch 73 Batch: 110 Train Loss: 0.03245826065540314\n",
      "Epoch 73 Batch: 112 Train Loss: 0.029939204454421997\n",
      "Epoch 73 Batch: 114 Train Loss: 0.30520880222320557\n",
      "Epoch 73 Batch: 116 Train Loss: 0.03782758489251137\n",
      "Epoch 73 Batch: 118 Train Loss: 0.03595107048749924\n",
      "Epoch 73 Batch: 120 Train Loss: 0.055803269147872925\n",
      "Epoch 73 Batch: 122 Train Loss: 0.5623877048492432\n",
      "Epoch 73 Batch: 124 Train Loss: 0.053983233869075775\n",
      "Epoch 73 Batch: 126 Train Loss: 0.535140335559845\n",
      "Epoch 73 Batch: 128 Train Loss: 0.2836577296257019\n",
      "Epoch 73 Batch: 130 Train Loss: 0.47591453790664673\n",
      "Epoch 73 Batch: 132 Train Loss: 0.07214659452438354\n",
      "Epoch 73 Batch: 134 Train Loss: 0.07976784557104111\n",
      "Epoch 73 Batch: 136 Train Loss: 0.10692199319601059\n",
      "Epoch 73 Batch: 138 Train Loss: 0.052159689366817474\n",
      "Epoch 73 Batch: 140 Train Loss: 0.2852104902267456\n",
      "Epoch 73 Batch: 142 Train Loss: 0.49371591210365295\n",
      "Epoch 73 Batch: 144 Train Loss: 0.27637040615081787\n",
      "Epoch 73 Batch: 146 Train Loss: 0.07108209282159805\n",
      "Epoch 73 Batch: 148 Train Loss: 0.28284910321235657\n",
      "Epoch 73 Batch: 150 Train Loss: 0.04265882819890976\n",
      "Epoch 73 Batch: 152 Train Loss: 0.042758189141750336\n",
      "Epoch 73 Batch: 154 Train Loss: 0.30891257524490356\n",
      "Epoch 73 Batch: 156 Train Loss: 0.2950505018234253\n",
      "Epoch 73 Batch: 158 Train Loss: 0.26662153005599976\n",
      "Epoch 73 Batch: 160 Train Loss: 0.06040360406041145\n",
      "Epoch 74 Batch: 2 Train Loss: 0.24957700073719025\n",
      "Epoch 74 Batch: 4 Train Loss: 0.049092985689640045\n",
      "Epoch 74 Batch: 6 Train Loss: 0.5433692336082458\n",
      "Epoch 74 Batch: 8 Train Loss: 0.029687341302633286\n",
      "Epoch 74 Batch: 10 Train Loss: 0.04832464084029198\n",
      "Epoch 74 Batch: 12 Train Loss: 0.050563424825668335\n",
      "Epoch 74 Batch: 14 Train Loss: 0.05832573026418686\n",
      "Epoch 74 Batch: 16 Train Loss: 0.048702679574489594\n",
      "Epoch 74 Batch: 18 Train Loss: 0.5294755101203918\n",
      "Epoch 74 Batch: 20 Train Loss: 0.29621466994285583\n",
      "Epoch 74 Batch: 22 Train Loss: 0.3053737282752991\n",
      "Epoch 74 Batch: 24 Train Loss: 0.26973438262939453\n",
      "Epoch 74 Batch: 26 Train Loss: 0.29230788350105286\n",
      "Epoch 74 Batch: 28 Train Loss: 0.026801427826285362\n",
      "Epoch 74 Batch: 30 Train Loss: 0.2715216279029846\n",
      "Epoch 74 Batch: 32 Train Loss: 0.06479335576295853\n",
      "Epoch 74 Batch: 34 Train Loss: 0.28098779916763306\n",
      "Epoch 74 Batch: 36 Train Loss: 0.5050610303878784\n",
      "Epoch 74 Batch: 38 Train Loss: 0.06524978578090668\n",
      "Epoch 74 Batch: 40 Train Loss: 0.04791266471147537\n",
      "Epoch 74 Batch: 42 Train Loss: 0.08999722450971603\n",
      "Epoch 74 Batch: 44 Train Loss: 0.05006866529583931\n",
      "Epoch 74 Batch: 46 Train Loss: 0.2858113646507263\n",
      "Epoch 74 Batch: 48 Train Loss: 0.28533607721328735\n",
      "Epoch 74 Batch: 50 Train Loss: 0.28685370087623596\n",
      "Epoch 74 Batch: 52 Train Loss: 0.07901643961668015\n",
      "Epoch 74 Batch: 54 Train Loss: 0.2692243158817291\n",
      "Epoch 74 Batch: 56 Train Loss: 0.050245873630046844\n",
      "Epoch 74 Batch: 58 Train Loss: 0.27131253480911255\n",
      "Epoch 74 Batch: 60 Train Loss: 0.04502525180578232\n",
      "Epoch 74 Batch: 62 Train Loss: 0.07084886729717255\n",
      "Epoch 74 Batch: 64 Train Loss: 0.2977619171142578\n",
      "Epoch 74 Batch: 66 Train Loss: 0.499342143535614\n",
      "Epoch 74 Batch: 68 Train Loss: 0.0851362943649292\n",
      "Epoch 74 Batch: 70 Train Loss: 0.07452710717916489\n",
      "Epoch 74 Batch: 72 Train Loss: 0.07619884610176086\n",
      "Epoch 74 Batch: 74 Train Loss: 0.07675468176603317\n",
      "Epoch 74 Batch: 76 Train Loss: 0.032146140933036804\n",
      "Epoch 74 Batch: 78 Train Loss: 0.08165329694747925\n",
      "Epoch 74 Batch: 80 Train Loss: 0.08095993101596832\n",
      "Epoch 74 Batch: 82 Train Loss: 0.05993133783340454\n",
      "Epoch 74 Batch: 84 Train Loss: 0.04282288998365402\n",
      "Epoch 74 Batch: 86 Train Loss: 0.06293737143278122\n",
      "Epoch 74 Batch: 88 Train Loss: 0.3164516091346741\n",
      "Epoch 74 Batch: 90 Train Loss: 0.021793629974126816\n",
      "Epoch 74 Batch: 92 Train Loss: 0.0420612134039402\n",
      "Epoch 74 Batch: 94 Train Loss: 0.2899017632007599\n",
      "Epoch 74 Batch: 96 Train Loss: 0.04647740721702576\n",
      "Epoch 74 Batch: 98 Train Loss: 0.021321715787053108\n",
      "Epoch 74 Batch: 100 Train Loss: 0.33424368500709534\n",
      "Epoch 74 Batch: 102 Train Loss: 0.027073591947555542\n",
      "Epoch 74 Batch: 104 Train Loss: 0.3157794177532196\n",
      "Epoch 74 Batch: 106 Train Loss: 0.027279261499643326\n",
      "Epoch 74 Batch: 108 Train Loss: 0.301117479801178\n",
      "Epoch 74 Batch: 110 Train Loss: 0.04508785158395767\n",
      "Epoch 74 Batch: 112 Train Loss: 0.05061453580856323\n",
      "Epoch 74 Batch: 114 Train Loss: 0.056678276509046555\n",
      "Epoch 74 Batch: 116 Train Loss: 0.5472303628921509\n",
      "Epoch 74 Batch: 118 Train Loss: 0.27867892384529114\n",
      "Epoch 74 Batch: 120 Train Loss: 0.04094201698899269\n",
      "Epoch 74 Batch: 122 Train Loss: 0.5441727042198181\n",
      "Epoch 74 Batch: 124 Train Loss: 0.025229591876268387\n",
      "Epoch 74 Batch: 126 Train Loss: 0.29757240414619446\n",
      "Epoch 74 Batch: 128 Train Loss: 0.29194241762161255\n",
      "Epoch 74 Batch: 130 Train Loss: 0.2776169180870056\n",
      "Epoch 74 Batch: 132 Train Loss: 0.23963961005210876\n",
      "Epoch 74 Batch: 134 Train Loss: 0.0609218068420887\n",
      "Epoch 74 Batch: 136 Train Loss: 0.07675334066152573\n",
      "Epoch 74 Batch: 138 Train Loss: 0.08015264570713043\n",
      "Epoch 74 Batch: 140 Train Loss: 0.05282945558428764\n",
      "Epoch 74 Batch: 142 Train Loss: 0.09619499742984772\n",
      "Epoch 74 Batch: 144 Train Loss: 0.07961130142211914\n",
      "Epoch 74 Batch: 146 Train Loss: 0.2845999598503113\n",
      "Epoch 74 Batch: 148 Train Loss: 0.07317551970481873\n",
      "Epoch 74 Batch: 150 Train Loss: 0.2908156216144562\n",
      "Epoch 74 Batch: 152 Train Loss: 0.27153587341308594\n",
      "Epoch 74 Batch: 154 Train Loss: 0.27685004472732544\n",
      "Epoch 74 Batch: 156 Train Loss: 0.040301259607076645\n",
      "Epoch 74 Batch: 158 Train Loss: 0.08558368682861328\n",
      "Epoch 74 Batch: 160 Train Loss: 0.23921553790569305\n",
      "Epoch 75 Batch: 2 Train Loss: 0.04137119650840759\n",
      "Epoch 75 Batch: 4 Train Loss: 0.08810682594776154\n",
      "Epoch 75 Batch: 6 Train Loss: 0.03634504973888397\n",
      "Epoch 75 Batch: 8 Train Loss: 0.2799304127693176\n",
      "Epoch 75 Batch: 10 Train Loss: 0.06787785142660141\n",
      "Epoch 75 Batch: 12 Train Loss: 0.05104581639170647\n",
      "Epoch 75 Batch: 14 Train Loss: 0.3124542832374573\n",
      "Epoch 75 Batch: 16 Train Loss: 0.28330880403518677\n",
      "Epoch 75 Batch: 18 Train Loss: 0.31409889459609985\n",
      "Epoch 75 Batch: 20 Train Loss: 0.02859628200531006\n",
      "Epoch 75 Batch: 22 Train Loss: 0.022349011152982712\n",
      "Epoch 75 Batch: 24 Train Loss: 0.036771394312381744\n",
      "Epoch 75 Batch: 26 Train Loss: 0.30349957942962646\n",
      "Epoch 75 Batch: 28 Train Loss: 0.04804236441850662\n",
      "Epoch 75 Batch: 30 Train Loss: 0.5651989579200745\n",
      "Epoch 75 Batch: 32 Train Loss: 0.05645039677619934\n",
      "Epoch 75 Batch: 34 Train Loss: 0.31600499153137207\n",
      "Epoch 75 Batch: 36 Train Loss: 0.2802065908908844\n",
      "Epoch 75 Batch: 38 Train Loss: 0.28574949502944946\n",
      "Epoch 75 Batch: 40 Train Loss: 0.5013031959533691\n",
      "Epoch 75 Batch: 42 Train Loss: 0.2826991677284241\n",
      "Epoch 75 Batch: 44 Train Loss: 0.3039652407169342\n",
      "Epoch 75 Batch: 46 Train Loss: 0.053307753056287766\n",
      "Epoch 75 Batch: 48 Train Loss: 0.28464606404304504\n",
      "Epoch 75 Batch: 50 Train Loss: 0.29206034541130066\n",
      "Epoch 75 Batch: 52 Train Loss: 0.27049964666366577\n",
      "Epoch 75 Batch: 54 Train Loss: 0.0637056976556778\n",
      "Epoch 75 Batch: 56 Train Loss: 0.30186691880226135\n",
      "Epoch 75 Batch: 58 Train Loss: 0.028194423764944077\n",
      "Epoch 75 Batch: 60 Train Loss: 0.037849776446819305\n",
      "Epoch 75 Batch: 62 Train Loss: 0.04703967645764351\n",
      "Epoch 75 Batch: 64 Train Loss: 0.035614412277936935\n",
      "Epoch 75 Batch: 66 Train Loss: 0.033854104578495026\n",
      "Epoch 75 Batch: 68 Train Loss: 0.058092452585697174\n",
      "Epoch 75 Batch: 70 Train Loss: 0.304675430059433\n",
      "Epoch 75 Batch: 72 Train Loss: 0.5607703328132629\n",
      "Epoch 75 Batch: 74 Train Loss: 0.02252158150076866\n",
      "Epoch 75 Batch: 76 Train Loss: 0.2757042348384857\n",
      "Epoch 75 Batch: 78 Train Loss: 0.5343605279922485\n",
      "Epoch 75 Batch: 80 Train Loss: 0.07012735307216644\n",
      "Epoch 75 Batch: 82 Train Loss: 0.25226476788520813\n",
      "Epoch 75 Batch: 84 Train Loss: 0.060003697872161865\n",
      "Epoch 75 Batch: 86 Train Loss: 0.30798158049583435\n",
      "Epoch 75 Batch: 88 Train Loss: 0.5073692798614502\n",
      "Epoch 75 Batch: 90 Train Loss: 0.016627909615635872\n",
      "Epoch 75 Batch: 92 Train Loss: 0.05600545555353165\n",
      "Epoch 75 Batch: 94 Train Loss: 0.038528695702552795\n",
      "Epoch 75 Batch: 96 Train Loss: 0.04995552822947502\n",
      "Epoch 75 Batch: 98 Train Loss: 0.2964962124824524\n",
      "Epoch 75 Batch: 100 Train Loss: 0.0351543053984642\n",
      "Epoch 75 Batch: 102 Train Loss: 0.05864589661359787\n",
      "Epoch 75 Batch: 104 Train Loss: 0.05677393078804016\n",
      "Epoch 75 Batch: 106 Train Loss: 0.510448157787323\n",
      "Epoch 75 Batch: 108 Train Loss: 0.2721851170063019\n",
      "Epoch 75 Batch: 110 Train Loss: 0.0409059002995491\n",
      "Epoch 75 Batch: 112 Train Loss: 0.29264578223228455\n",
      "Epoch 75 Batch: 114 Train Loss: 0.2715608775615692\n",
      "Epoch 75 Batch: 116 Train Loss: 0.10687907040119171\n",
      "Epoch 75 Batch: 118 Train Loss: 0.08477558940649033\n",
      "Epoch 75 Batch: 120 Train Loss: 0.0302042867988348\n",
      "Epoch 75 Batch: 122 Train Loss: 0.11492244899272919\n",
      "Epoch 75 Batch: 124 Train Loss: 0.05467209964990616\n",
      "Epoch 75 Batch: 126 Train Loss: 0.08124836534261703\n",
      "Epoch 75 Batch: 128 Train Loss: 0.06243501231074333\n",
      "Epoch 75 Batch: 130 Train Loss: 0.2621327042579651\n",
      "Epoch 75 Batch: 132 Train Loss: 0.06900423765182495\n",
      "Epoch 75 Batch: 134 Train Loss: 0.7331215143203735\n",
      "Epoch 75 Batch: 136 Train Loss: 0.08105580508708954\n",
      "Epoch 75 Batch: 138 Train Loss: 0.2717569172382355\n",
      "Epoch 75 Batch: 140 Train Loss: 0.2965988218784332\n",
      "Epoch 75 Batch: 142 Train Loss: 0.25953829288482666\n",
      "Epoch 75 Batch: 144 Train Loss: 0.04647856205701828\n",
      "Epoch 75 Batch: 146 Train Loss: 0.06784231960773468\n",
      "Epoch 75 Batch: 148 Train Loss: 0.06322503089904785\n",
      "Epoch 75 Batch: 150 Train Loss: 0.06493254005908966\n",
      "Epoch 75 Batch: 152 Train Loss: 0.0933428481221199\n",
      "Epoch 75 Batch: 154 Train Loss: 0.24879372119903564\n",
      "Epoch 75 Batch: 156 Train Loss: 0.07836797833442688\n",
      "Epoch 75 Batch: 158 Train Loss: 0.050660014152526855\n",
      "Epoch 75 Batch: 160 Train Loss: 0.29168376326560974\n",
      "Epoch 76 Batch: 2 Train Loss: 0.2741636037826538\n",
      "Epoch 76 Batch: 4 Train Loss: 0.05389805883169174\n",
      "Epoch 76 Batch: 6 Train Loss: 0.05892422795295715\n",
      "Epoch 76 Batch: 8 Train Loss: 0.047113269567489624\n",
      "Epoch 76 Batch: 10 Train Loss: 0.07337182760238647\n",
      "Epoch 76 Batch: 12 Train Loss: 0.024878498166799545\n",
      "Epoch 76 Batch: 14 Train Loss: 0.05961839482188225\n",
      "Epoch 76 Batch: 16 Train Loss: 0.5939634442329407\n",
      "Epoch 76 Batch: 18 Train Loss: 0.028907114639878273\n",
      "Epoch 76 Batch: 20 Train Loss: 0.03922419995069504\n",
      "Epoch 76 Batch: 22 Train Loss: 0.026936769485473633\n",
      "Epoch 76 Batch: 24 Train Loss: 0.31972962617874146\n",
      "Epoch 76 Batch: 26 Train Loss: 0.3216761648654938\n",
      "Epoch 76 Batch: 28 Train Loss: 0.33042067289352417\n",
      "Epoch 76 Batch: 30 Train Loss: 0.8059180378913879\n",
      "Epoch 76 Batch: 32 Train Loss: 0.041186295449733734\n",
      "Epoch 76 Batch: 34 Train Loss: 0.05388139560818672\n",
      "Epoch 76 Batch: 36 Train Loss: 0.05841372162103653\n",
      "Epoch 76 Batch: 38 Train Loss: 0.28225091099739075\n",
      "Epoch 76 Batch: 40 Train Loss: 0.5021985769271851\n",
      "Epoch 76 Batch: 42 Train Loss: 0.05582712963223457\n",
      "Epoch 76 Batch: 44 Train Loss: 0.06662274897098541\n",
      "Epoch 76 Batch: 46 Train Loss: 0.30343562364578247\n",
      "Epoch 76 Batch: 48 Train Loss: 0.2676045298576355\n",
      "Epoch 76 Batch: 50 Train Loss: 0.05791125446557999\n",
      "Epoch 76 Batch: 52 Train Loss: 0.023744186386466026\n",
      "Epoch 76 Batch: 54 Train Loss: 0.28665122389793396\n",
      "Epoch 76 Batch: 56 Train Loss: 0.271772563457489\n",
      "Epoch 76 Batch: 58 Train Loss: 0.07825841009616852\n",
      "Epoch 76 Batch: 60 Train Loss: 0.05529710650444031\n",
      "Epoch 76 Batch: 62 Train Loss: 0.2712995111942291\n",
      "Epoch 76 Batch: 64 Train Loss: 0.07204636186361313\n",
      "Epoch 76 Batch: 66 Train Loss: 0.05146946758031845\n",
      "Epoch 76 Batch: 68 Train Loss: 0.2942028343677521\n",
      "Epoch 76 Batch: 70 Train Loss: 0.2813154458999634\n",
      "Epoch 76 Batch: 72 Train Loss: 0.03925040364265442\n",
      "Epoch 76 Batch: 74 Train Loss: 0.27570071816444397\n",
      "Epoch 76 Batch: 76 Train Loss: 0.04886840656399727\n",
      "Epoch 76 Batch: 78 Train Loss: 0.02818203531205654\n",
      "Epoch 76 Batch: 80 Train Loss: 0.06041830778121948\n",
      "Epoch 76 Batch: 82 Train Loss: 0.0636971965432167\n",
      "Epoch 76 Batch: 84 Train Loss: 0.5599130988121033\n",
      "Epoch 76 Batch: 86 Train Loss: 0.057390373200178146\n",
      "Epoch 76 Batch: 88 Train Loss: 0.050863802433013916\n",
      "Epoch 76 Batch: 90 Train Loss: 0.2946292757987976\n",
      "Epoch 76 Batch: 92 Train Loss: 0.04510161280632019\n",
      "Epoch 76 Batch: 94 Train Loss: 0.03667261078953743\n",
      "Epoch 76 Batch: 96 Train Loss: 0.008945351466536522\n",
      "Epoch 76 Batch: 98 Train Loss: 0.05326572805643082\n",
      "Epoch 76 Batch: 100 Train Loss: 0.040502093732357025\n",
      "Epoch 76 Batch: 102 Train Loss: 0.2913055419921875\n",
      "Epoch 76 Batch: 104 Train Loss: 0.062158215790987015\n",
      "Epoch 76 Batch: 106 Train Loss: 0.031531818211078644\n",
      "Epoch 76 Batch: 108 Train Loss: 0.03756481036543846\n",
      "Epoch 76 Batch: 110 Train Loss: 0.04479673132300377\n",
      "Epoch 76 Batch: 112 Train Loss: 0.054616499692201614\n",
      "Epoch 76 Batch: 114 Train Loss: 0.046113334596157074\n",
      "Epoch 76 Batch: 116 Train Loss: 0.3187083899974823\n",
      "Epoch 76 Batch: 118 Train Loss: 0.26199179887771606\n",
      "Epoch 76 Batch: 120 Train Loss: 0.29807594418525696\n",
      "Epoch 76 Batch: 122 Train Loss: 0.32937362790107727\n",
      "Epoch 76 Batch: 124 Train Loss: 0.32672032713890076\n",
      "Epoch 76 Batch: 126 Train Loss: 0.31295841932296753\n",
      "Epoch 76 Batch: 128 Train Loss: 0.040162164717912674\n",
      "Epoch 76 Batch: 130 Train Loss: 0.5465009808540344\n",
      "Epoch 76 Batch: 132 Train Loss: 0.5110042691230774\n",
      "Epoch 76 Batch: 134 Train Loss: 0.06448517739772797\n",
      "Epoch 76 Batch: 136 Train Loss: 0.2866445779800415\n",
      "Epoch 76 Batch: 138 Train Loss: 0.28925490379333496\n",
      "Epoch 76 Batch: 140 Train Loss: 0.11328904330730438\n",
      "Epoch 76 Batch: 142 Train Loss: 0.06823364645242691\n",
      "Epoch 76 Batch: 144 Train Loss: 0.2626477777957916\n",
      "Epoch 76 Batch: 146 Train Loss: 0.08468718081712723\n",
      "Epoch 76 Batch: 148 Train Loss: 0.057115983217954636\n",
      "Epoch 76 Batch: 150 Train Loss: 0.08507201820611954\n",
      "Epoch 76 Batch: 152 Train Loss: 0.0561685785651207\n",
      "Epoch 76 Batch: 154 Train Loss: 0.09353452920913696\n",
      "Epoch 76 Batch: 156 Train Loss: 0.06872238218784332\n",
      "Epoch 76 Batch: 158 Train Loss: 0.2514004707336426\n",
      "Epoch 76 Batch: 160 Train Loss: 0.0812092274427414\n",
      "Epoch 77 Batch: 2 Train Loss: 0.27740320563316345\n",
      "Epoch 77 Batch: 4 Train Loss: 0.24175143241882324\n",
      "Epoch 77 Batch: 6 Train Loss: 0.2837495803833008\n",
      "Epoch 77 Batch: 8 Train Loss: 0.08281491696834564\n",
      "Epoch 77 Batch: 10 Train Loss: 0.4826009273529053\n",
      "Epoch 77 Batch: 12 Train Loss: 0.08535337448120117\n",
      "Epoch 77 Batch: 14 Train Loss: 0.5259543657302856\n",
      "Epoch 77 Batch: 16 Train Loss: 0.30189988017082214\n",
      "Epoch 77 Batch: 18 Train Loss: 0.0826186090707779\n",
      "Epoch 77 Batch: 20 Train Loss: 0.06466200202703476\n",
      "Epoch 77 Batch: 22 Train Loss: 0.07302235066890717\n",
      "Epoch 77 Batch: 24 Train Loss: 0.05929400399327278\n",
      "Epoch 77 Batch: 26 Train Loss: 0.06820191442966461\n",
      "Epoch 77 Batch: 28 Train Loss: 0.2804998755455017\n",
      "Epoch 77 Batch: 30 Train Loss: 0.034793637692928314\n",
      "Epoch 77 Batch: 32 Train Loss: 0.05736573413014412\n",
      "Epoch 77 Batch: 34 Train Loss: 0.541204571723938\n",
      "Epoch 77 Batch: 36 Train Loss: 0.016481641680002213\n",
      "Epoch 77 Batch: 38 Train Loss: 0.31173816323280334\n",
      "Epoch 77 Batch: 40 Train Loss: 0.3083624243736267\n",
      "Epoch 77 Batch: 42 Train Loss: 0.03575485944747925\n",
      "Epoch 77 Batch: 44 Train Loss: 0.02473928965628147\n",
      "Epoch 77 Batch: 46 Train Loss: 0.3116527795791626\n",
      "Epoch 77 Batch: 48 Train Loss: 0.27950209379196167\n",
      "Epoch 77 Batch: 50 Train Loss: 0.27978426218032837\n",
      "Epoch 77 Batch: 52 Train Loss: 0.2780316472053528\n",
      "Epoch 77 Batch: 54 Train Loss: 0.08178423345088959\n",
      "Epoch 77 Batch: 56 Train Loss: 0.0650712251663208\n",
      "Epoch 77 Batch: 58 Train Loss: 0.02619566023349762\n",
      "Epoch 77 Batch: 60 Train Loss: 0.2292071133852005\n",
      "Epoch 77 Batch: 62 Train Loss: 0.2923657298088074\n",
      "Epoch 77 Batch: 64 Train Loss: 0.27128279209136963\n",
      "Epoch 77 Batch: 66 Train Loss: 0.09487543255090714\n",
      "Epoch 77 Batch: 68 Train Loss: 0.2516467869281769\n",
      "Epoch 77 Batch: 70 Train Loss: 0.2827351987361908\n",
      "Epoch 77 Batch: 72 Train Loss: 0.047101471573114395\n",
      "Epoch 77 Batch: 74 Train Loss: 0.08995798230171204\n",
      "Epoch 77 Batch: 76 Train Loss: 0.04615049436688423\n",
      "Epoch 77 Batch: 78 Train Loss: 0.08499743044376373\n",
      "Epoch 77 Batch: 80 Train Loss: 0.2936151325702667\n",
      "Epoch 77 Batch: 82 Train Loss: 0.0901503711938858\n",
      "Epoch 77 Batch: 84 Train Loss: 0.0817115381360054\n",
      "Epoch 77 Batch: 86 Train Loss: 0.04502871632575989\n",
      "Epoch 77 Batch: 88 Train Loss: 0.059390775859355927\n",
      "Epoch 77 Batch: 90 Train Loss: 0.3063124716281891\n",
      "Epoch 77 Batch: 92 Train Loss: 0.5227969288825989\n",
      "Epoch 77 Batch: 94 Train Loss: 0.28086283802986145\n",
      "Epoch 77 Batch: 96 Train Loss: 0.06211251765489578\n",
      "Epoch 77 Batch: 98 Train Loss: 0.033897172659635544\n",
      "Epoch 77 Batch: 100 Train Loss: 0.05962318927049637\n",
      "Epoch 77 Batch: 102 Train Loss: 0.050104059278964996\n",
      "Epoch 77 Batch: 104 Train Loss: 0.0551028735935688\n",
      "Epoch 77 Batch: 106 Train Loss: 0.2844201922416687\n",
      "Epoch 77 Batch: 108 Train Loss: 0.05351780727505684\n",
      "Epoch 77 Batch: 110 Train Loss: 0.01337380986660719\n",
      "Epoch 77 Batch: 112 Train Loss: 0.0430741086602211\n",
      "Epoch 77 Batch: 114 Train Loss: 0.04598269611597061\n",
      "Epoch 77 Batch: 116 Train Loss: 0.2991136610507965\n",
      "Epoch 77 Batch: 118 Train Loss: 0.04082020744681358\n",
      "Epoch 77 Batch: 120 Train Loss: 0.037248943001031876\n",
      "Epoch 77 Batch: 122 Train Loss: 0.028730332851409912\n",
      "Epoch 77 Batch: 124 Train Loss: 0.288350909948349\n",
      "Epoch 77 Batch: 126 Train Loss: 0.30743828415870667\n",
      "Epoch 77 Batch: 128 Train Loss: 0.5448030233383179\n",
      "Epoch 77 Batch: 130 Train Loss: 0.2865334749221802\n",
      "Epoch 77 Batch: 132 Train Loss: 0.31812363862991333\n",
      "Epoch 77 Batch: 134 Train Loss: 0.2696043848991394\n",
      "Epoch 77 Batch: 136 Train Loss: 0.05433696508407593\n",
      "Epoch 77 Batch: 138 Train Loss: 0.04915310814976692\n",
      "Epoch 77 Batch: 140 Train Loss: 0.06931940466165543\n",
      "Epoch 77 Batch: 142 Train Loss: 0.028246095404028893\n",
      "Epoch 77 Batch: 144 Train Loss: 0.2659195065498352\n",
      "Epoch 77 Batch: 146 Train Loss: 0.05916552618145943\n",
      "Epoch 77 Batch: 148 Train Loss: 0.28538092970848083\n",
      "Epoch 77 Batch: 150 Train Loss: 0.04997576028108597\n",
      "Epoch 77 Batch: 152 Train Loss: 0.026687970384955406\n",
      "Epoch 77 Batch: 154 Train Loss: 0.5187320709228516\n",
      "Epoch 77 Batch: 156 Train Loss: 0.5204694867134094\n",
      "Epoch 77 Batch: 158 Train Loss: 0.040925849229097366\n",
      "Epoch 77 Batch: 160 Train Loss: 0.061996519565582275\n",
      "Epoch 78 Batch: 2 Train Loss: 0.07240191847085953\n",
      "Epoch 78 Batch: 4 Train Loss: 0.029308468103408813\n",
      "Epoch 78 Batch: 6 Train Loss: 0.2903110384941101\n",
      "Epoch 78 Batch: 8 Train Loss: 0.060819853097200394\n",
      "Epoch 78 Batch: 10 Train Loss: 0.01971043087542057\n",
      "Epoch 78 Batch: 12 Train Loss: 0.28121474385261536\n",
      "Epoch 78 Batch: 14 Train Loss: 0.05719197541475296\n",
      "Epoch 78 Batch: 16 Train Loss: 0.5552018880844116\n",
      "Epoch 78 Batch: 18 Train Loss: 0.04349653050303459\n",
      "Epoch 78 Batch: 20 Train Loss: 0.052903663367033005\n",
      "Epoch 78 Batch: 22 Train Loss: 0.08458544313907623\n",
      "Epoch 78 Batch: 24 Train Loss: 0.2848818898200989\n",
      "Epoch 78 Batch: 26 Train Loss: 0.04646513611078262\n",
      "Epoch 78 Batch: 28 Train Loss: 0.06025908142328262\n",
      "Epoch 78 Batch: 30 Train Loss: 0.05216020345687866\n",
      "Epoch 78 Batch: 32 Train Loss: 0.28223496675491333\n",
      "Epoch 78 Batch: 34 Train Loss: 0.2721364498138428\n",
      "Epoch 78 Batch: 36 Train Loss: 0.0680217370390892\n",
      "Epoch 78 Batch: 38 Train Loss: 0.02310173027217388\n",
      "Epoch 78 Batch: 40 Train Loss: 0.05724639445543289\n",
      "Epoch 78 Batch: 42 Train Loss: 0.05591445416212082\n",
      "Epoch 78 Batch: 44 Train Loss: 0.2766185402870178\n",
      "Epoch 78 Batch: 46 Train Loss: 0.28754252195358276\n",
      "Epoch 78 Batch: 48 Train Loss: 0.07081934064626694\n",
      "Epoch 78 Batch: 50 Train Loss: 0.0516747422516346\n",
      "Epoch 78 Batch: 52 Train Loss: 0.27312731742858887\n",
      "Epoch 78 Batch: 54 Train Loss: 0.294118732213974\n",
      "Epoch 78 Batch: 56 Train Loss: 0.2916831970214844\n",
      "Epoch 78 Batch: 58 Train Loss: 0.23577173054218292\n",
      "Epoch 78 Batch: 60 Train Loss: 0.09702401608228683\n",
      "Epoch 78 Batch: 62 Train Loss: 0.05784459039568901\n",
      "Epoch 78 Batch: 64 Train Loss: 0.31206557154655457\n",
      "Epoch 78 Batch: 66 Train Loss: 0.07143760472536087\n",
      "Epoch 78 Batch: 68 Train Loss: 0.043340735137462616\n",
      "Epoch 78 Batch: 70 Train Loss: 0.2806464731693268\n",
      "Epoch 78 Batch: 72 Train Loss: 0.08148469030857086\n",
      "Epoch 78 Batch: 74 Train Loss: 0.2681514024734497\n",
      "Epoch 78 Batch: 76 Train Loss: 0.046461381018161774\n",
      "Epoch 78 Batch: 78 Train Loss: 0.050507109612226486\n",
      "Epoch 78 Batch: 80 Train Loss: 0.042596615850925446\n",
      "Epoch 78 Batch: 82 Train Loss: 0.2826038599014282\n",
      "Epoch 78 Batch: 84 Train Loss: 0.02341758832335472\n",
      "Epoch 78 Batch: 86 Train Loss: 0.06660182029008865\n",
      "Epoch 78 Batch: 88 Train Loss: 0.019883573055267334\n",
      "Epoch 78 Batch: 90 Train Loss: 0.28328627347946167\n",
      "Epoch 78 Batch: 92 Train Loss: 0.033774130046367645\n",
      "Epoch 78 Batch: 94 Train Loss: 0.029930517077445984\n",
      "Epoch 78 Batch: 96 Train Loss: 0.01712069846689701\n",
      "Epoch 78 Batch: 98 Train Loss: 0.32601791620254517\n",
      "Epoch 78 Batch: 100 Train Loss: 0.02958081103861332\n",
      "Epoch 78 Batch: 102 Train Loss: 0.04907139018177986\n",
      "Epoch 78 Batch: 104 Train Loss: 0.05234628915786743\n",
      "Epoch 78 Batch: 106 Train Loss: 0.33842581510543823\n",
      "Epoch 78 Batch: 108 Train Loss: 0.034946925938129425\n",
      "Epoch 78 Batch: 110 Train Loss: 0.04488992691040039\n",
      "Epoch 78 Batch: 112 Train Loss: 0.052009206265211105\n",
      "Epoch 78 Batch: 114 Train Loss: 0.03620278835296631\n",
      "Epoch 78 Batch: 116 Train Loss: 0.5826088190078735\n",
      "Epoch 78 Batch: 118 Train Loss: 0.5777857303619385\n",
      "Epoch 78 Batch: 120 Train Loss: 0.055236876010894775\n",
      "Epoch 78 Batch: 122 Train Loss: 0.03512834757566452\n",
      "Epoch 78 Batch: 124 Train Loss: 0.2741573452949524\n",
      "Epoch 78 Batch: 126 Train Loss: 0.047303371131420135\n",
      "Epoch 78 Batch: 128 Train Loss: 0.2799060046672821\n",
      "Epoch 78 Batch: 130 Train Loss: 0.06197858974337578\n",
      "Epoch 78 Batch: 132 Train Loss: 0.0506955161690712\n",
      "Epoch 78 Batch: 134 Train Loss: 0.26599931716918945\n",
      "Epoch 78 Batch: 136 Train Loss: 0.26718002557754517\n",
      "Epoch 78 Batch: 138 Train Loss: 0.06687001883983612\n",
      "Epoch 78 Batch: 140 Train Loss: 0.2734326720237732\n",
      "Epoch 78 Batch: 142 Train Loss: 0.0810936912894249\n",
      "Epoch 78 Batch: 144 Train Loss: 0.4661347270011902\n",
      "Epoch 78 Batch: 146 Train Loss: 0.07702600955963135\n",
      "Epoch 78 Batch: 148 Train Loss: 0.26740723848342896\n",
      "Epoch 78 Batch: 150 Train Loss: 0.05454524606466293\n",
      "Epoch 78 Batch: 152 Train Loss: 0.252948135137558\n",
      "Epoch 78 Batch: 154 Train Loss: 0.06767597794532776\n",
      "Epoch 78 Batch: 156 Train Loss: 0.08511830866336823\n",
      "Epoch 78 Batch: 158 Train Loss: 0.11949057877063751\n",
      "Epoch 78 Batch: 160 Train Loss: 0.502667248249054\n",
      "Epoch 79 Batch: 2 Train Loss: 0.2703937590122223\n",
      "Epoch 79 Batch: 4 Train Loss: 0.08138816058635712\n",
      "Epoch 79 Batch: 6 Train Loss: 0.03658198565244675\n",
      "Epoch 79 Batch: 8 Train Loss: 0.055556513369083405\n",
      "Epoch 79 Batch: 10 Train Loss: 0.4819779396057129\n",
      "Epoch 79 Batch: 12 Train Loss: 0.27055037021636963\n",
      "Epoch 79 Batch: 14 Train Loss: 0.2967841625213623\n",
      "Epoch 79 Batch: 16 Train Loss: 0.2934478521347046\n",
      "Epoch 79 Batch: 18 Train Loss: 0.017632434144616127\n",
      "Epoch 79 Batch: 20 Train Loss: 0.05467165261507034\n",
      "Epoch 79 Batch: 22 Train Loss: 0.04207897558808327\n",
      "Epoch 79 Batch: 24 Train Loss: 0.5508811473846436\n",
      "Epoch 79 Batch: 26 Train Loss: 0.07911527156829834\n",
      "Epoch 79 Batch: 28 Train Loss: 0.056617312133312225\n",
      "Epoch 79 Batch: 30 Train Loss: 0.30149397253990173\n",
      "Epoch 79 Batch: 32 Train Loss: 0.07827978581190109\n",
      "Epoch 79 Batch: 34 Train Loss: 0.06763921678066254\n",
      "Epoch 79 Batch: 36 Train Loss: 0.07650904357433319\n",
      "Epoch 79 Batch: 38 Train Loss: 0.027267932891845703\n",
      "Epoch 79 Batch: 40 Train Loss: 0.30270496010780334\n",
      "Epoch 79 Batch: 42 Train Loss: 0.05640830844640732\n",
      "Epoch 79 Batch: 44 Train Loss: 0.0634162425994873\n",
      "Epoch 79 Batch: 46 Train Loss: 0.2612593173980713\n",
      "Epoch 79 Batch: 48 Train Loss: 0.26704734563827515\n",
      "Epoch 79 Batch: 50 Train Loss: 0.05581727623939514\n",
      "Epoch 79 Batch: 52 Train Loss: 0.28873658180236816\n",
      "Epoch 79 Batch: 54 Train Loss: 0.024581728503108025\n",
      "Epoch 79 Batch: 56 Train Loss: 0.03272051364183426\n",
      "Epoch 79 Batch: 58 Train Loss: 0.0605192668735981\n",
      "Epoch 79 Batch: 60 Train Loss: 0.3245823383331299\n",
      "Epoch 79 Batch: 62 Train Loss: 0.038972895592451096\n",
      "Epoch 79 Batch: 64 Train Loss: 0.28486043214797974\n",
      "Epoch 79 Batch: 66 Train Loss: 0.3080255091190338\n",
      "Epoch 79 Batch: 68 Train Loss: 0.04099859297275543\n",
      "Epoch 79 Batch: 70 Train Loss: 0.2857372760772705\n",
      "Epoch 79 Batch: 72 Train Loss: 0.2923819422721863\n",
      "Epoch 79 Batch: 74 Train Loss: 0.04438862204551697\n",
      "Epoch 79 Batch: 76 Train Loss: 0.3085612654685974\n",
      "Epoch 79 Batch: 78 Train Loss: 0.7790101766586304\n",
      "Epoch 79 Batch: 80 Train Loss: 0.019551044330000877\n",
      "Epoch 79 Batch: 82 Train Loss: 0.3059106767177582\n",
      "Epoch 79 Batch: 84 Train Loss: 0.26377853751182556\n",
      "Epoch 79 Batch: 86 Train Loss: 0.7012752294540405\n",
      "Epoch 79 Batch: 88 Train Loss: 0.08003910630941391\n",
      "Epoch 79 Batch: 90 Train Loss: 0.07072269916534424\n",
      "Epoch 79 Batch: 92 Train Loss: 0.27536600828170776\n",
      "Epoch 79 Batch: 94 Train Loss: 0.034624695777893066\n",
      "Epoch 79 Batch: 96 Train Loss: 0.07355540990829468\n",
      "Epoch 79 Batch: 98 Train Loss: 0.012198698706924915\n",
      "Epoch 79 Batch: 100 Train Loss: 0.287162721157074\n",
      "Epoch 79 Batch: 102 Train Loss: 0.04695688560605049\n",
      "Epoch 79 Batch: 104 Train Loss: 0.3050685524940491\n",
      "Epoch 79 Batch: 106 Train Loss: 0.2751695215702057\n",
      "Epoch 79 Batch: 108 Train Loss: 0.49982136487960815\n",
      "Epoch 79 Batch: 110 Train Loss: 0.2949067950248718\n",
      "Epoch 79 Batch: 112 Train Loss: 0.23097555339336395\n",
      "Epoch 79 Batch: 114 Train Loss: 0.4752701222896576\n",
      "Epoch 79 Batch: 116 Train Loss: 0.2675096392631531\n",
      "Epoch 79 Batch: 118 Train Loss: 0.06854180246591568\n",
      "Epoch 79 Batch: 120 Train Loss: 0.0755016952753067\n",
      "Epoch 79 Batch: 122 Train Loss: 0.09843682497739792\n",
      "Epoch 79 Batch: 124 Train Loss: 0.05985959619283676\n",
      "Epoch 79 Batch: 126 Train Loss: 0.3105800747871399\n",
      "Epoch 79 Batch: 128 Train Loss: 0.06820657849311829\n",
      "Epoch 79 Batch: 130 Train Loss: 0.06543797999620438\n",
      "Epoch 79 Batch: 132 Train Loss: 0.05939635634422302\n",
      "Epoch 79 Batch: 134 Train Loss: 0.2673484683036804\n",
      "Epoch 79 Batch: 136 Train Loss: 0.06416865438222885\n",
      "Epoch 79 Batch: 138 Train Loss: 0.29962819814682007\n",
      "Epoch 79 Batch: 140 Train Loss: 0.27243876457214355\n",
      "Epoch 79 Batch: 142 Train Loss: 0.30797824263572693\n",
      "Epoch 79 Batch: 144 Train Loss: 0.041550494730472565\n",
      "Epoch 79 Batch: 146 Train Loss: 0.2644635736942291\n",
      "Epoch 79 Batch: 148 Train Loss: 0.05468306690454483\n",
      "Epoch 79 Batch: 150 Train Loss: 0.04354652017354965\n",
      "Epoch 79 Batch: 152 Train Loss: 0.04209085553884506\n",
      "Epoch 79 Batch: 154 Train Loss: 0.5692551732063293\n",
      "Epoch 79 Batch: 156 Train Loss: 0.03210187703371048\n",
      "Epoch 79 Batch: 158 Train Loss: 0.051035843789577484\n",
      "Epoch 79 Batch: 160 Train Loss: 0.2918091118335724\n",
      "Epoch 80 Batch: 2 Train Loss: 0.03004823997616768\n",
      "Epoch 80 Batch: 4 Train Loss: 0.04462446644902229\n",
      "Epoch 80 Batch: 6 Train Loss: 0.02806234359741211\n",
      "Epoch 80 Batch: 8 Train Loss: 0.05345497280359268\n",
      "Epoch 80 Batch: 10 Train Loss: 0.024544723331928253\n",
      "Epoch 80 Batch: 12 Train Loss: 0.030357083305716515\n",
      "Epoch 80 Batch: 14 Train Loss: 0.02287793532013893\n",
      "Epoch 80 Batch: 16 Train Loss: 0.2865259051322937\n",
      "Epoch 80 Batch: 18 Train Loss: 0.06758232414722443\n",
      "Epoch 80 Batch: 20 Train Loss: 0.04430677741765976\n",
      "Epoch 80 Batch: 22 Train Loss: 0.015375027433037758\n",
      "Epoch 80 Batch: 24 Train Loss: 0.04318632930517197\n",
      "Epoch 80 Batch: 26 Train Loss: 0.05684932321310043\n",
      "Epoch 80 Batch: 28 Train Loss: 0.046108633279800415\n",
      "Epoch 80 Batch: 30 Train Loss: 0.045246921479701996\n",
      "Epoch 80 Batch: 32 Train Loss: 0.043985627591609955\n",
      "Epoch 80 Batch: 34 Train Loss: 0.052118826657533646\n",
      "Epoch 80 Batch: 36 Train Loss: 0.03560280427336693\n",
      "Epoch 80 Batch: 38 Train Loss: 0.02613944187760353\n",
      "Epoch 80 Batch: 40 Train Loss: 0.05241619795560837\n",
      "Epoch 80 Batch: 42 Train Loss: 0.04648394137620926\n",
      "Epoch 80 Batch: 44 Train Loss: 0.042268622666597366\n",
      "Epoch 80 Batch: 46 Train Loss: 0.03945918753743172\n",
      "Epoch 80 Batch: 48 Train Loss: 0.2953541874885559\n",
      "Epoch 80 Batch: 50 Train Loss: 0.07113499939441681\n",
      "Epoch 80 Batch: 52 Train Loss: 0.039437443017959595\n",
      "Epoch 80 Batch: 54 Train Loss: 0.5389267802238464\n",
      "Epoch 80 Batch: 56 Train Loss: 0.2972877621650696\n",
      "Epoch 80 Batch: 58 Train Loss: 0.3117463290691376\n",
      "Epoch 80 Batch: 60 Train Loss: 0.06756044924259186\n",
      "Epoch 80 Batch: 62 Train Loss: 0.05435978248715401\n",
      "Epoch 80 Batch: 64 Train Loss: 0.2737797796726227\n",
      "Epoch 80 Batch: 66 Train Loss: 0.06245817616581917\n",
      "Epoch 80 Batch: 68 Train Loss: 0.06337380409240723\n",
      "Epoch 80 Batch: 70 Train Loss: 0.30159300565719604\n",
      "Epoch 80 Batch: 72 Train Loss: 0.04888880252838135\n",
      "Epoch 80 Batch: 74 Train Loss: 0.27475613355636597\n",
      "Epoch 80 Batch: 76 Train Loss: 0.28826701641082764\n",
      "Epoch 80 Batch: 78 Train Loss: 0.0716409981250763\n",
      "Epoch 80 Batch: 80 Train Loss: 0.049948446452617645\n",
      "Epoch 80 Batch: 82 Train Loss: 0.5669678449630737\n",
      "Epoch 80 Batch: 84 Train Loss: 0.032410480082035065\n",
      "Epoch 80 Batch: 86 Train Loss: 0.0527481846511364\n",
      "Epoch 80 Batch: 88 Train Loss: 0.3154354691505432\n",
      "Epoch 80 Batch: 90 Train Loss: 0.039053596556186676\n",
      "Epoch 80 Batch: 92 Train Loss: 0.049976646900177\n",
      "Epoch 80 Batch: 94 Train Loss: 0.31257206201553345\n",
      "Epoch 80 Batch: 96 Train Loss: 0.05103234201669693\n",
      "Epoch 80 Batch: 98 Train Loss: 0.2682560682296753\n",
      "Epoch 80 Batch: 100 Train Loss: 0.31813082098960876\n",
      "Epoch 80 Batch: 102 Train Loss: 0.040920358151197433\n",
      "Epoch 80 Batch: 104 Train Loss: 0.03066866099834442\n",
      "Epoch 80 Batch: 106 Train Loss: 0.04349895194172859\n",
      "Epoch 80 Batch: 108 Train Loss: 0.2772238850593567\n",
      "Epoch 80 Batch: 110 Train Loss: 0.06911811977624893\n",
      "Epoch 80 Batch: 112 Train Loss: 0.5502004623413086\n",
      "Epoch 80 Batch: 114 Train Loss: 0.06959801912307739\n",
      "Epoch 80 Batch: 116 Train Loss: 0.05185746029019356\n",
      "Epoch 80 Batch: 118 Train Loss: 0.05625415965914726\n",
      "Epoch 80 Batch: 120 Train Loss: 0.02321399375796318\n",
      "Epoch 80 Batch: 122 Train Loss: 0.2861543297767639\n",
      "Epoch 80 Batch: 124 Train Loss: 0.26986148953437805\n",
      "Epoch 80 Batch: 126 Train Loss: 0.05813905596733093\n",
      "Epoch 80 Batch: 128 Train Loss: 0.05442444235086441\n",
      "Epoch 80 Batch: 130 Train Loss: 0.04103342816233635\n",
      "Epoch 80 Batch: 132 Train Loss: 0.30528369545936584\n",
      "Epoch 80 Batch: 134 Train Loss: 0.01941320300102234\n",
      "Epoch 80 Batch: 136 Train Loss: 0.2535181939601898\n",
      "Epoch 80 Batch: 138 Train Loss: 0.04554196447134018\n",
      "Epoch 80 Batch: 140 Train Loss: 0.2744811475276947\n",
      "Epoch 80 Batch: 142 Train Loss: 0.07488252967596054\n",
      "Epoch 80 Batch: 144 Train Loss: 0.26916974782943726\n",
      "Epoch 80 Batch: 146 Train Loss: 0.06168682500720024\n",
      "Epoch 80 Batch: 148 Train Loss: 0.06343545764684677\n",
      "Epoch 80 Batch: 150 Train Loss: 0.2725744843482971\n",
      "Epoch 80 Batch: 152 Train Loss: 0.06864545494318008\n",
      "Epoch 80 Batch: 154 Train Loss: 0.26005423069000244\n",
      "Epoch 80 Batch: 156 Train Loss: 0.076162189245224\n",
      "Epoch 80 Batch: 158 Train Loss: 0.517219066619873\n",
      "Epoch 80 Batch: 160 Train Loss: 0.2655128836631775\n",
      "Epoch 81 Batch: 2 Train Loss: 0.06472314894199371\n",
      "Epoch 81 Batch: 4 Train Loss: 0.280569851398468\n",
      "Epoch 81 Batch: 6 Train Loss: 0.0671393945813179\n",
      "Epoch 81 Batch: 8 Train Loss: 0.26611191034317017\n",
      "Epoch 81 Batch: 10 Train Loss: 0.04561011865735054\n",
      "Epoch 81 Batch: 12 Train Loss: 0.11072294414043427\n",
      "Epoch 81 Batch: 14 Train Loss: 0.08959536999464035\n",
      "Epoch 81 Batch: 16 Train Loss: 0.5081512928009033\n",
      "Epoch 81 Batch: 18 Train Loss: 0.2509252429008484\n",
      "Epoch 81 Batch: 20 Train Loss: 0.07519181817770004\n",
      "Epoch 81 Batch: 22 Train Loss: 0.3198527693748474\n",
      "Epoch 81 Batch: 24 Train Loss: 0.26459622383117676\n",
      "Epoch 81 Batch: 26 Train Loss: 0.05077214166522026\n",
      "Epoch 81 Batch: 28 Train Loss: 0.24884124100208282\n",
      "Epoch 81 Batch: 30 Train Loss: 0.30393311381340027\n",
      "Epoch 81 Batch: 32 Train Loss: 0.2817615270614624\n",
      "Epoch 81 Batch: 34 Train Loss: 0.021442631259560585\n",
      "Epoch 81 Batch: 36 Train Loss: 0.26617181301116943\n",
      "Epoch 81 Batch: 38 Train Loss: 4.792104391526664e-06\n",
      "Epoch 81 Batch: 40 Train Loss: 0.055555425584316254\n",
      "Epoch 81 Batch: 42 Train Loss: 0.06271595507860184\n",
      "Epoch 81 Batch: 44 Train Loss: 0.045473478734493256\n",
      "Epoch 81 Batch: 46 Train Loss: 0.044843681156635284\n",
      "Epoch 81 Batch: 48 Train Loss: 0.0695400983095169\n",
      "Epoch 81 Batch: 50 Train Loss: 0.06470984220504761\n",
      "Epoch 81 Batch: 52 Train Loss: 0.04352355748414993\n",
      "Epoch 81 Batch: 54 Train Loss: 0.030412495136260986\n",
      "Epoch 81 Batch: 56 Train Loss: 0.047191064804792404\n",
      "Epoch 81 Batch: 58 Train Loss: 0.327567994594574\n",
      "Epoch 81 Batch: 60 Train Loss: 0.02753361500799656\n",
      "Epoch 81 Batch: 62 Train Loss: 0.03862033784389496\n",
      "Epoch 81 Batch: 64 Train Loss: 0.023061487823724747\n",
      "Epoch 81 Batch: 66 Train Loss: 0.5680901408195496\n",
      "Epoch 81 Batch: 68 Train Loss: 0.03285031393170357\n",
      "Epoch 81 Batch: 70 Train Loss: 0.01322763878852129\n",
      "Epoch 81 Batch: 72 Train Loss: 0.040624119341373444\n",
      "Epoch 81 Batch: 74 Train Loss: 0.025491297245025635\n",
      "Epoch 81 Batch: 76 Train Loss: 0.2898745834827423\n",
      "Epoch 81 Batch: 78 Train Loss: 0.03014129400253296\n",
      "Epoch 81 Batch: 80 Train Loss: 0.042938582599163055\n",
      "Epoch 81 Batch: 82 Train Loss: 0.5853291749954224\n",
      "Epoch 81 Batch: 84 Train Loss: 0.02329150028526783\n",
      "Epoch 81 Batch: 86 Train Loss: 0.01574460230767727\n",
      "Epoch 81 Batch: 88 Train Loss: 0.8418771624565125\n",
      "Epoch 81 Batch: 90 Train Loss: 0.025332679972052574\n",
      "Epoch 81 Batch: 92 Train Loss: 0.29796725511550903\n",
      "Epoch 81 Batch: 94 Train Loss: 0.29309698939323425\n",
      "Epoch 81 Batch: 96 Train Loss: 0.2923932373523712\n",
      "Epoch 81 Batch: 98 Train Loss: 0.010628298856317997\n",
      "Epoch 81 Batch: 100 Train Loss: 0.05634576082229614\n",
      "Epoch 81 Batch: 102 Train Loss: 0.05572415515780449\n",
      "Epoch 81 Batch: 104 Train Loss: 0.08855165541172028\n",
      "Epoch 81 Batch: 106 Train Loss: 0.2858521640300751\n",
      "Epoch 81 Batch: 108 Train Loss: 0.08410220593214035\n",
      "Epoch 81 Batch: 110 Train Loss: 0.03920234367251396\n",
      "Epoch 81 Batch: 112 Train Loss: 0.009257195517420769\n",
      "Epoch 81 Batch: 114 Train Loss: 0.03927413374185562\n",
      "Epoch 81 Batch: 116 Train Loss: 0.051181547343730927\n",
      "Epoch 81 Batch: 118 Train Loss: 0.29246699810028076\n",
      "Epoch 81 Batch: 120 Train Loss: 0.28179970383644104\n",
      "Epoch 81 Batch: 122 Train Loss: 0.06355952471494675\n",
      "Epoch 81 Batch: 124 Train Loss: 0.043818093836307526\n",
      "Epoch 81 Batch: 126 Train Loss: 0.05303729698061943\n",
      "Epoch 81 Batch: 128 Train Loss: 0.2866192162036896\n",
      "Epoch 81 Batch: 130 Train Loss: 0.08644281327724457\n",
      "Epoch 81 Batch: 132 Train Loss: 0.04836868494749069\n",
      "Epoch 81 Batch: 134 Train Loss: 0.2876034379005432\n",
      "Epoch 81 Batch: 136 Train Loss: 0.26666608452796936\n",
      "Epoch 81 Batch: 138 Train Loss: 0.08286730945110321\n",
      "Epoch 81 Batch: 140 Train Loss: 0.03739229589700699\n",
      "Epoch 81 Batch: 142 Train Loss: 0.2746428847312927\n",
      "Epoch 81 Batch: 144 Train Loss: 0.07858295738697052\n",
      "Epoch 81 Batch: 146 Train Loss: 0.658722996711731\n",
      "Epoch 81 Batch: 148 Train Loss: 0.28668028116226196\n",
      "Epoch 81 Batch: 150 Train Loss: 0.29251721501350403\n",
      "Epoch 81 Batch: 152 Train Loss: 0.26699334383010864\n",
      "Epoch 81 Batch: 154 Train Loss: 0.09173896163702011\n",
      "Epoch 81 Batch: 156 Train Loss: 0.28498950600624084\n",
      "Epoch 81 Batch: 158 Train Loss: 0.2594524025917053\n",
      "Epoch 81 Batch: 160 Train Loss: 0.09045940637588501\n",
      "Epoch 82 Batch: 2 Train Loss: 0.11231496185064316\n",
      "Epoch 82 Batch: 4 Train Loss: 0.4571542739868164\n",
      "Epoch 82 Batch: 6 Train Loss: 0.06295435130596161\n",
      "Epoch 82 Batch: 8 Train Loss: 0.08147048950195312\n",
      "Epoch 82 Batch: 10 Train Loss: 0.06839411705732346\n",
      "Epoch 82 Batch: 12 Train Loss: 0.051562320441007614\n",
      "Epoch 82 Batch: 14 Train Loss: 0.2880071699619293\n",
      "Epoch 82 Batch: 16 Train Loss: 0.2668861746788025\n",
      "Epoch 82 Batch: 18 Train Loss: 0.2996044158935547\n",
      "Epoch 82 Batch: 20 Train Loss: 0.5394518375396729\n",
      "Epoch 82 Batch: 22 Train Loss: 0.30407506227493286\n",
      "Epoch 82 Batch: 24 Train Loss: 0.5505845546722412\n",
      "Epoch 82 Batch: 26 Train Loss: 0.2900336682796478\n",
      "Epoch 82 Batch: 28 Train Loss: 0.0558495819568634\n",
      "Epoch 82 Batch: 30 Train Loss: 0.07853293418884277\n",
      "Epoch 82 Batch: 32 Train Loss: 0.2855584919452667\n",
      "Epoch 82 Batch: 34 Train Loss: 0.2943111062049866\n",
      "Epoch 82 Batch: 36 Train Loss: 0.28654664754867554\n",
      "Epoch 82 Batch: 38 Train Loss: 0.30410873889923096\n",
      "Epoch 82 Batch: 40 Train Loss: 0.07182613015174866\n",
      "Epoch 82 Batch: 42 Train Loss: 0.03171481937170029\n",
      "Epoch 82 Batch: 44 Train Loss: 0.2748676836490631\n",
      "Epoch 82 Batch: 46 Train Loss: 0.03920688107609749\n",
      "Epoch 82 Batch: 48 Train Loss: 0.5049141645431519\n",
      "Epoch 82 Batch: 50 Train Loss: 0.039263010025024414\n",
      "Epoch 82 Batch: 52 Train Loss: 0.29296791553497314\n",
      "Epoch 82 Batch: 54 Train Loss: 0.09285655617713928\n",
      "Epoch 82 Batch: 56 Train Loss: 0.03250141814351082\n",
      "Epoch 82 Batch: 58 Train Loss: 0.2690604627132416\n",
      "Epoch 82 Batch: 60 Train Loss: 0.0621369406580925\n",
      "Epoch 82 Batch: 62 Train Loss: 0.051257289946079254\n",
      "Epoch 82 Batch: 64 Train Loss: 0.29878395795822144\n",
      "Epoch 82 Batch: 66 Train Loss: 0.296957403421402\n",
      "Epoch 82 Batch: 68 Train Loss: 0.26595374941825867\n",
      "Epoch 82 Batch: 70 Train Loss: 0.30142664909362793\n",
      "Epoch 82 Batch: 72 Train Loss: 0.06096314638853073\n",
      "Epoch 82 Batch: 74 Train Loss: 0.2732340395450592\n",
      "Epoch 82 Batch: 76 Train Loss: 0.26789146661758423\n",
      "Epoch 82 Batch: 78 Train Loss: 0.5171127319335938\n",
      "Epoch 82 Batch: 80 Train Loss: 0.04299753159284592\n",
      "Epoch 82 Batch: 82 Train Loss: 0.31498169898986816\n",
      "Epoch 82 Batch: 84 Train Loss: 0.04142225161194801\n",
      "Epoch 82 Batch: 86 Train Loss: 0.05197080969810486\n",
      "Epoch 82 Batch: 88 Train Loss: 0.03395481035113335\n",
      "Epoch 82 Batch: 90 Train Loss: 0.5227898359298706\n",
      "Epoch 82 Batch: 92 Train Loss: 0.2626444101333618\n",
      "Epoch 82 Batch: 94 Train Loss: 0.04180487245321274\n",
      "Epoch 82 Batch: 96 Train Loss: 0.2912113070487976\n",
      "Epoch 82 Batch: 98 Train Loss: 0.3055357336997986\n",
      "Epoch 82 Batch: 100 Train Loss: 0.2868320941925049\n",
      "Epoch 82 Batch: 102 Train Loss: 0.04149019718170166\n",
      "Epoch 82 Batch: 104 Train Loss: 0.0644163265824318\n",
      "Epoch 82 Batch: 106 Train Loss: 0.08551446348428726\n",
      "Epoch 82 Batch: 108 Train Loss: 0.275727778673172\n",
      "Epoch 82 Batch: 110 Train Loss: 0.0894426703453064\n",
      "Epoch 82 Batch: 112 Train Loss: 0.07564947009086609\n",
      "Epoch 82 Batch: 114 Train Loss: 0.043233029544353485\n",
      "Epoch 82 Batch: 116 Train Loss: 0.059676945209503174\n",
      "Epoch 82 Batch: 118 Train Loss: 0.04021695628762245\n",
      "Epoch 82 Batch: 120 Train Loss: 0.06026982143521309\n",
      "Epoch 82 Batch: 122 Train Loss: 0.3097700774669647\n",
      "Epoch 82 Batch: 124 Train Loss: 0.03814243525266647\n",
      "Epoch 82 Batch: 126 Train Loss: 0.3010886013507843\n",
      "Epoch 82 Batch: 128 Train Loss: 0.29041343927383423\n",
      "Epoch 82 Batch: 130 Train Loss: 0.031245719641447067\n",
      "Epoch 82 Batch: 132 Train Loss: 0.04064203426241875\n",
      "Epoch 82 Batch: 134 Train Loss: 0.019207116216421127\n",
      "Epoch 82 Batch: 136 Train Loss: 0.05910123512148857\n",
      "Epoch 82 Batch: 138 Train Loss: 0.04050156846642494\n",
      "Epoch 82 Batch: 140 Train Loss: 0.28617793321609497\n",
      "Epoch 82 Batch: 142 Train Loss: 0.02003314346075058\n",
      "Epoch 82 Batch: 144 Train Loss: 0.06578018516302109\n",
      "Epoch 82 Batch: 146 Train Loss: 0.04562460258603096\n",
      "Epoch 82 Batch: 148 Train Loss: 0.25925347208976746\n",
      "Epoch 82 Batch: 150 Train Loss: 0.02623308263719082\n",
      "Epoch 82 Batch: 152 Train Loss: 0.5657430291175842\n",
      "Epoch 82 Batch: 154 Train Loss: 0.2665606141090393\n",
      "Epoch 82 Batch: 156 Train Loss: 0.28353264927864075\n",
      "Epoch 82 Batch: 158 Train Loss: 0.05880042910575867\n",
      "Epoch 82 Batch: 160 Train Loss: 0.057192087173461914\n",
      "Epoch 83 Batch: 2 Train Loss: 0.07353664934635162\n",
      "Epoch 83 Batch: 4 Train Loss: 0.28611886501312256\n",
      "Epoch 83 Batch: 6 Train Loss: 0.529902994632721\n",
      "Epoch 83 Batch: 8 Train Loss: 0.05116502568125725\n",
      "Epoch 83 Batch: 10 Train Loss: 0.03356180712580681\n",
      "Epoch 83 Batch: 12 Train Loss: 0.03748352825641632\n",
      "Epoch 83 Batch: 14 Train Loss: 0.04738253727555275\n",
      "Epoch 83 Batch: 16 Train Loss: 0.32283657789230347\n",
      "Epoch 83 Batch: 18 Train Loss: 0.053522564470767975\n",
      "Epoch 83 Batch: 20 Train Loss: 0.03272456303238869\n",
      "Epoch 83 Batch: 22 Train Loss: 0.5943629741668701\n",
      "Epoch 83 Batch: 24 Train Loss: 0.3158595860004425\n",
      "Epoch 83 Batch: 26 Train Loss: 0.04635358601808548\n",
      "Epoch 83 Batch: 28 Train Loss: 0.03379690647125244\n",
      "Epoch 83 Batch: 30 Train Loss: 0.29532164335250854\n",
      "Epoch 83 Batch: 32 Train Loss: 0.03197871893644333\n",
      "Epoch 83 Batch: 34 Train Loss: 0.02556045725941658\n",
      "Epoch 83 Batch: 36 Train Loss: 0.3059517741203308\n",
      "Epoch 83 Batch: 38 Train Loss: 0.28155916929244995\n",
      "Epoch 83 Batch: 40 Train Loss: 0.32535186409950256\n",
      "Epoch 83 Batch: 42 Train Loss: 0.5996371507644653\n",
      "Epoch 83 Batch: 44 Train Loss: 0.29312703013420105\n",
      "Epoch 83 Batch: 46 Train Loss: 0.2966245412826538\n",
      "Epoch 83 Batch: 48 Train Loss: 0.04131757467985153\n",
      "Epoch 83 Batch: 50 Train Loss: 0.3047642707824707\n",
      "Epoch 83 Batch: 52 Train Loss: 0.04419354349374771\n",
      "Epoch 83 Batch: 54 Train Loss: 0.5570293068885803\n",
      "Epoch 83 Batch: 56 Train Loss: 0.05277924984693527\n",
      "Epoch 83 Batch: 58 Train Loss: 0.06401082128286362\n",
      "Epoch 83 Batch: 60 Train Loss: 0.28957292437553406\n",
      "Epoch 83 Batch: 62 Train Loss: 0.04587825387716293\n",
      "Epoch 83 Batch: 64 Train Loss: 0.05237210914492607\n",
      "Epoch 83 Batch: 66 Train Loss: 0.04308637976646423\n",
      "Epoch 83 Batch: 68 Train Loss: 0.022803107276558876\n",
      "Epoch 83 Batch: 70 Train Loss: 0.2889938950538635\n",
      "Epoch 83 Batch: 72 Train Loss: 0.5671650171279907\n",
      "Epoch 83 Batch: 74 Train Loss: 0.5626703500747681\n",
      "Epoch 83 Batch: 76 Train Loss: 0.0547085776925087\n",
      "Epoch 83 Batch: 78 Train Loss: 0.05078651383519173\n",
      "Epoch 83 Batch: 80 Train Loss: 0.30385440587997437\n",
      "Epoch 83 Batch: 82 Train Loss: 0.0373384952545166\n",
      "Epoch 83 Batch: 84 Train Loss: 0.045909665524959564\n",
      "Epoch 83 Batch: 86 Train Loss: 0.5090125203132629\n",
      "Epoch 83 Batch: 88 Train Loss: 0.2814377546310425\n",
      "Epoch 83 Batch: 90 Train Loss: 0.2407938688993454\n",
      "Epoch 83 Batch: 92 Train Loss: 0.037965238094329834\n",
      "Epoch 83 Batch: 94 Train Loss: 0.45478543639183044\n",
      "Epoch 83 Batch: 96 Train Loss: 0.3034372925758362\n",
      "Epoch 83 Batch: 98 Train Loss: 0.4501205384731293\n",
      "Epoch 83 Batch: 100 Train Loss: 0.07839101552963257\n",
      "Epoch 83 Batch: 102 Train Loss: 0.06318537890911102\n",
      "Epoch 83 Batch: 104 Train Loss: 0.8225304484367371\n",
      "Epoch 83 Batch: 106 Train Loss: 0.1148466020822525\n",
      "Epoch 83 Batch: 108 Train Loss: 0.11742264032363892\n",
      "Epoch 83 Batch: 110 Train Loss: 0.047133442014455795\n",
      "Epoch 83 Batch: 112 Train Loss: 0.06016913801431656\n",
      "Epoch 83 Batch: 114 Train Loss: 0.49635791778564453\n",
      "Epoch 83 Batch: 116 Train Loss: 0.5988182425498962\n",
      "Epoch 83 Batch: 118 Train Loss: 0.08022280037403107\n",
      "Epoch 83 Batch: 120 Train Loss: 0.07179433107376099\n",
      "Epoch 83 Batch: 122 Train Loss: 0.4694443345069885\n",
      "Epoch 83 Batch: 124 Train Loss: 0.07973295450210571\n",
      "Epoch 83 Batch: 126 Train Loss: 0.09899328649044037\n",
      "Epoch 83 Batch: 128 Train Loss: 0.11018908023834229\n",
      "Epoch 83 Batch: 130 Train Loss: 0.28518182039260864\n",
      "Epoch 83 Batch: 132 Train Loss: 0.05224877595901489\n",
      "Epoch 83 Batch: 134 Train Loss: 0.4702858328819275\n",
      "Epoch 83 Batch: 136 Train Loss: 0.09445476531982422\n",
      "Epoch 83 Batch: 138 Train Loss: 0.06398061662912369\n",
      "Epoch 83 Batch: 140 Train Loss: 0.06357958912849426\n",
      "Epoch 83 Batch: 142 Train Loss: 0.07266353070735931\n",
      "Epoch 83 Batch: 144 Train Loss: 0.2856655716896057\n",
      "Epoch 83 Batch: 146 Train Loss: 0.2844120264053345\n",
      "Epoch 83 Batch: 148 Train Loss: 0.06164257973432541\n",
      "Epoch 83 Batch: 150 Train Loss: 0.04048728197813034\n",
      "Epoch 83 Batch: 152 Train Loss: 0.04001550003886223\n",
      "Epoch 83 Batch: 154 Train Loss: 0.05551232770085335\n",
      "Epoch 83 Batch: 156 Train Loss: 0.30575695633888245\n",
      "Epoch 83 Batch: 158 Train Loss: 0.2879217267036438\n",
      "Epoch 83 Batch: 160 Train Loss: 0.05114736035466194\n",
      "Epoch 84 Batch: 2 Train Loss: 0.04198601469397545\n",
      "Epoch 84 Batch: 4 Train Loss: 0.03058015927672386\n",
      "Epoch 84 Batch: 6 Train Loss: 0.060460399836301804\n",
      "Epoch 84 Batch: 8 Train Loss: 0.2943735122680664\n",
      "Epoch 84 Batch: 10 Train Loss: 0.2638198137283325\n",
      "Epoch 84 Batch: 12 Train Loss: 0.0472344271838665\n",
      "Epoch 84 Batch: 14 Train Loss: 0.061630427837371826\n",
      "Epoch 84 Batch: 16 Train Loss: 0.028546491637825966\n",
      "Epoch 84 Batch: 18 Train Loss: 0.282284677028656\n",
      "Epoch 84 Batch: 20 Train Loss: 0.03700681030750275\n",
      "Epoch 84 Batch: 22 Train Loss: 0.042209967970848083\n",
      "Epoch 84 Batch: 24 Train Loss: 0.026010479778051376\n",
      "Epoch 84 Batch: 26 Train Loss: 0.06414375454187393\n",
      "Epoch 84 Batch: 28 Train Loss: 0.03577526658773422\n",
      "Epoch 84 Batch: 30 Train Loss: 0.30438438057899475\n",
      "Epoch 84 Batch: 32 Train Loss: 0.05568845942616463\n",
      "Epoch 84 Batch: 34 Train Loss: 0.03944643586874008\n",
      "Epoch 84 Batch: 36 Train Loss: 0.0573597177863121\n",
      "Epoch 84 Batch: 38 Train Loss: 0.2939890921115875\n",
      "Epoch 84 Batch: 40 Train Loss: 0.31666332483291626\n",
      "Epoch 84 Batch: 42 Train Loss: 0.043587878346443176\n",
      "Epoch 84 Batch: 44 Train Loss: 0.04988362640142441\n",
      "Epoch 84 Batch: 46 Train Loss: 0.05009523779153824\n",
      "Epoch 84 Batch: 48 Train Loss: 0.032546140253543854\n",
      "Epoch 84 Batch: 50 Train Loss: 0.028406361117959023\n",
      "Epoch 84 Batch: 52 Train Loss: 0.03216078132390976\n",
      "Epoch 84 Batch: 54 Train Loss: 0.04117213562130928\n",
      "Epoch 84 Batch: 56 Train Loss: 0.6320263147354126\n",
      "Epoch 84 Batch: 58 Train Loss: 0.35753265023231506\n",
      "Epoch 84 Batch: 60 Train Loss: 0.3562502861022949\n",
      "Epoch 84 Batch: 62 Train Loss: 0.6356781125068665\n",
      "Epoch 84 Batch: 64 Train Loss: 0.30511945486068726\n",
      "Epoch 84 Batch: 66 Train Loss: 0.35258668661117554\n",
      "Epoch 84 Batch: 68 Train Loss: 0.3120356798171997\n",
      "Epoch 84 Batch: 70 Train Loss: 0.5869445204734802\n",
      "Epoch 84 Batch: 72 Train Loss: 0.3085508644580841\n",
      "Epoch 84 Batch: 74 Train Loss: 0.03815953806042671\n",
      "Epoch 84 Batch: 76 Train Loss: 0.28076261281967163\n",
      "Epoch 84 Batch: 78 Train Loss: 0.2556203007698059\n",
      "Epoch 84 Batch: 80 Train Loss: 0.05392267554998398\n",
      "Epoch 84 Batch: 82 Train Loss: 0.2497604638338089\n",
      "Epoch 84 Batch: 84 Train Loss: 0.2524312734603882\n",
      "Epoch 84 Batch: 86 Train Loss: 0.08565223217010498\n",
      "Epoch 84 Batch: 88 Train Loss: 0.2674696147441864\n",
      "Epoch 84 Batch: 90 Train Loss: 0.6903982758522034\n",
      "Epoch 84 Batch: 92 Train Loss: 0.06291165202856064\n",
      "Epoch 84 Batch: 94 Train Loss: 0.0750952884554863\n",
      "Epoch 84 Batch: 96 Train Loss: 0.09164203703403473\n",
      "Epoch 84 Batch: 98 Train Loss: 0.0484074242413044\n",
      "Epoch 84 Batch: 100 Train Loss: 0.2532811760902405\n",
      "Epoch 84 Batch: 102 Train Loss: 0.059787411242723465\n",
      "Epoch 84 Batch: 104 Train Loss: 0.2647532522678375\n",
      "Epoch 84 Batch: 106 Train Loss: 0.30710703134536743\n",
      "Epoch 84 Batch: 108 Train Loss: 0.2761135697364807\n",
      "Epoch 84 Batch: 110 Train Loss: 0.24861684441566467\n",
      "Epoch 84 Batch: 112 Train Loss: 0.051747918128967285\n",
      "Epoch 84 Batch: 114 Train Loss: 0.0945129543542862\n",
      "Epoch 84 Batch: 116 Train Loss: 0.22455152869224548\n",
      "Epoch 84 Batch: 118 Train Loss: 0.052747659385204315\n",
      "Epoch 84 Batch: 120 Train Loss: 0.2907816171646118\n",
      "Epoch 84 Batch: 122 Train Loss: 0.3059256374835968\n",
      "Epoch 84 Batch: 124 Train Loss: 0.025816679000854492\n",
      "Epoch 84 Batch: 126 Train Loss: 0.2714497148990631\n",
      "Epoch 84 Batch: 128 Train Loss: 0.2946494519710541\n",
      "Epoch 84 Batch: 130 Train Loss: 0.07966272532939911\n",
      "Epoch 84 Batch: 132 Train Loss: 0.08683754503726959\n",
      "Epoch 84 Batch: 134 Train Loss: 0.052805155515670776\n",
      "Epoch 84 Batch: 136 Train Loss: 0.05343601852655411\n",
      "Epoch 84 Batch: 138 Train Loss: 0.2652759253978729\n",
      "Epoch 84 Batch: 140 Train Loss: 0.040855772793293\n",
      "Epoch 84 Batch: 142 Train Loss: 0.5423901677131653\n",
      "Epoch 84 Batch: 144 Train Loss: 0.04297317937016487\n",
      "Epoch 84 Batch: 146 Train Loss: 0.29462283849716187\n",
      "Epoch 84 Batch: 148 Train Loss: 0.0350957065820694\n",
      "Epoch 84 Batch: 150 Train Loss: 0.31900984048843384\n",
      "Epoch 84 Batch: 152 Train Loss: 0.5202789902687073\n",
      "Epoch 84 Batch: 154 Train Loss: 0.27380603551864624\n",
      "Epoch 84 Batch: 156 Train Loss: 0.06946755945682526\n",
      "Epoch 84 Batch: 158 Train Loss: 0.30231890082359314\n",
      "Epoch 84 Batch: 160 Train Loss: 0.04939064010977745\n",
      "Epoch 85 Batch: 2 Train Loss: 0.5081945657730103\n",
      "Epoch 85 Batch: 4 Train Loss: 0.05479983612895012\n",
      "Epoch 85 Batch: 6 Train Loss: 0.03353678435087204\n",
      "Epoch 85 Batch: 8 Train Loss: 0.47165948152542114\n",
      "Epoch 85 Batch: 10 Train Loss: 0.0920289158821106\n",
      "Epoch 85 Batch: 12 Train Loss: 0.02650427259504795\n",
      "Epoch 85 Batch: 14 Train Loss: 0.06163039803504944\n",
      "Epoch 85 Batch: 16 Train Loss: 0.035993970930576324\n",
      "Epoch 85 Batch: 18 Train Loss: 0.060761116445064545\n",
      "Epoch 85 Batch: 20 Train Loss: 0.04817608371376991\n",
      "Epoch 85 Batch: 22 Train Loss: 0.04439548775553703\n",
      "Epoch 85 Batch: 24 Train Loss: 0.04506981745362282\n",
      "Epoch 85 Batch: 26 Train Loss: 0.06948225200176239\n",
      "Epoch 85 Batch: 28 Train Loss: 0.06805820763111115\n",
      "Epoch 85 Batch: 30 Train Loss: 0.065477654337883\n",
      "Epoch 85 Batch: 32 Train Loss: 0.053161799907684326\n",
      "Epoch 85 Batch: 34 Train Loss: 0.28643497824668884\n",
      "Epoch 85 Batch: 36 Train Loss: 0.025892246514558792\n",
      "Epoch 85 Batch: 38 Train Loss: 0.3338490128517151\n",
      "Epoch 85 Batch: 40 Train Loss: 0.01772654987871647\n",
      "Epoch 85 Batch: 42 Train Loss: 0.2844206392765045\n",
      "Epoch 85 Batch: 44 Train Loss: 0.2891954779624939\n",
      "Epoch 85 Batch: 46 Train Loss: 0.06504426896572113\n",
      "Epoch 85 Batch: 48 Train Loss: 0.0555872805416584\n",
      "Epoch 85 Batch: 50 Train Loss: 0.054901111871004105\n",
      "Epoch 85 Batch: 52 Train Loss: 0.5585554242134094\n",
      "Epoch 85 Batch: 54 Train Loss: 0.31144970655441284\n",
      "Epoch 85 Batch: 56 Train Loss: 0.07460866868495941\n",
      "Epoch 85 Batch: 58 Train Loss: 0.03744814544916153\n",
      "Epoch 85 Batch: 60 Train Loss: 0.4968867301940918\n",
      "Epoch 85 Batch: 62 Train Loss: 0.543388307094574\n",
      "Epoch 85 Batch: 64 Train Loss: 0.28445425629615784\n",
      "Epoch 85 Batch: 66 Train Loss: 0.05315816402435303\n",
      "Epoch 85 Batch: 68 Train Loss: 0.2615855932235718\n",
      "Epoch 85 Batch: 70 Train Loss: 0.08096170425415039\n",
      "Epoch 85 Batch: 72 Train Loss: 0.2753421366214752\n",
      "Epoch 85 Batch: 74 Train Loss: 0.0728728175163269\n",
      "Epoch 85 Batch: 76 Train Loss: 0.08639132976531982\n",
      "Epoch 85 Batch: 78 Train Loss: 0.04909756779670715\n",
      "Epoch 85 Batch: 80 Train Loss: 0.073505699634552\n",
      "Epoch 85 Batch: 82 Train Loss: 0.0596291646361351\n",
      "Epoch 85 Batch: 84 Train Loss: 0.05773423984646797\n",
      "Epoch 85 Batch: 86 Train Loss: 0.28760460019111633\n",
      "Epoch 85 Batch: 88 Train Loss: 0.27991783618927\n",
      "Epoch 85 Batch: 90 Train Loss: 0.0750926285982132\n",
      "Epoch 85 Batch: 92 Train Loss: 0.264962375164032\n",
      "Epoch 85 Batch: 94 Train Loss: 0.06916994601488113\n",
      "Epoch 85 Batch: 96 Train Loss: 0.03694138675928116\n",
      "Epoch 85 Batch: 98 Train Loss: 0.5557932257652283\n",
      "Epoch 85 Batch: 100 Train Loss: 0.31578493118286133\n",
      "Epoch 85 Batch: 102 Train Loss: 0.047866374254226685\n",
      "Epoch 85 Batch: 104 Train Loss: 0.06362743675708771\n",
      "Epoch 85 Batch: 106 Train Loss: 0.06528407335281372\n",
      "Epoch 85 Batch: 108 Train Loss: 0.05676254630088806\n",
      "Epoch 85 Batch: 110 Train Loss: 0.2863537669181824\n",
      "Epoch 85 Batch: 112 Train Loss: 0.06651301681995392\n",
      "Epoch 85 Batch: 114 Train Loss: 0.2797343134880066\n",
      "Epoch 85 Batch: 116 Train Loss: 0.06851716339588165\n",
      "Epoch 85 Batch: 118 Train Loss: 0.02984878048300743\n",
      "Epoch 85 Batch: 120 Train Loss: 0.06071288138628006\n",
      "Epoch 85 Batch: 122 Train Loss: 0.05097980052232742\n",
      "Epoch 85 Batch: 124 Train Loss: 0.0747758001089096\n",
      "Epoch 85 Batch: 126 Train Loss: 0.06365498900413513\n",
      "Epoch 85 Batch: 128 Train Loss: 0.2708439528942108\n",
      "Epoch 85 Batch: 130 Train Loss: 0.06571202725172043\n",
      "Epoch 85 Batch: 132 Train Loss: 0.06409404426813126\n",
      "Epoch 85 Batch: 134 Train Loss: 0.029542913660407066\n",
      "Epoch 85 Batch: 136 Train Loss: 0.0691380500793457\n",
      "Epoch 85 Batch: 138 Train Loss: 0.29175156354904175\n",
      "Epoch 85 Batch: 140 Train Loss: 0.5157391428947449\n",
      "Epoch 85 Batch: 142 Train Loss: 0.07076554745435715\n",
      "Epoch 85 Batch: 144 Train Loss: 0.04061226174235344\n",
      "Epoch 85 Batch: 146 Train Loss: 0.5268669128417969\n",
      "Epoch 85 Batch: 148 Train Loss: 0.02842387557029724\n",
      "Epoch 85 Batch: 150 Train Loss: 0.033689193427562714\n",
      "Epoch 85 Batch: 152 Train Loss: 0.2883419394493103\n",
      "Epoch 85 Batch: 154 Train Loss: 0.05676311254501343\n",
      "Epoch 85 Batch: 156 Train Loss: 0.0747484415769577\n",
      "Epoch 85 Batch: 158 Train Loss: 0.29123353958129883\n",
      "Epoch 85 Batch: 160 Train Loss: 0.03438454866409302\n",
      "Epoch 86 Batch: 2 Train Loss: 0.2788863778114319\n",
      "Epoch 86 Batch: 4 Train Loss: 0.03555269166827202\n",
      "Epoch 86 Batch: 6 Train Loss: 0.5680558681488037\n",
      "Epoch 86 Batch: 8 Train Loss: 0.04489845782518387\n",
      "Epoch 86 Batch: 10 Train Loss: 0.2680961489677429\n",
      "Epoch 86 Batch: 12 Train Loss: 0.03568040579557419\n",
      "Epoch 86 Batch: 14 Train Loss: 0.029441192746162415\n",
      "Epoch 86 Batch: 16 Train Loss: 0.2841283679008484\n",
      "Epoch 86 Batch: 18 Train Loss: 0.06284519284963608\n",
      "Epoch 86 Batch: 20 Train Loss: 0.2743704915046692\n",
      "Epoch 86 Batch: 22 Train Loss: 0.07069221138954163\n",
      "Epoch 86 Batch: 24 Train Loss: 0.06801734864711761\n",
      "Epoch 86 Batch: 26 Train Loss: 0.5070133209228516\n",
      "Epoch 86 Batch: 28 Train Loss: 0.28706884384155273\n",
      "Epoch 86 Batch: 30 Train Loss: 0.07687090337276459\n",
      "Epoch 86 Batch: 32 Train Loss: 0.28271228075027466\n",
      "Epoch 86 Batch: 34 Train Loss: 0.2579026520252228\n",
      "Epoch 86 Batch: 36 Train Loss: 0.24164052307605743\n",
      "Epoch 86 Batch: 38 Train Loss: 0.09301942586898804\n",
      "Epoch 86 Batch: 40 Train Loss: 0.09809694439172745\n",
      "Epoch 86 Batch: 42 Train Loss: 0.05486823245882988\n",
      "Epoch 86 Batch: 44 Train Loss: 0.08489982783794403\n",
      "Epoch 86 Batch: 46 Train Loss: 0.25943249464035034\n",
      "Epoch 86 Batch: 48 Train Loss: 0.27402687072753906\n",
      "Epoch 86 Batch: 50 Train Loss: 0.02353418804705143\n",
      "Epoch 86 Batch: 52 Train Loss: 0.08890783786773682\n",
      "Epoch 86 Batch: 54 Train Loss: 0.0655754804611206\n",
      "Epoch 86 Batch: 56 Train Loss: 0.733161449432373\n",
      "Epoch 86 Batch: 58 Train Loss: 0.07646434009075165\n",
      "Epoch 86 Batch: 60 Train Loss: 0.028843889012932777\n",
      "Epoch 86 Batch: 62 Train Loss: 0.03328120335936546\n",
      "Epoch 86 Batch: 64 Train Loss: 0.0518934428691864\n",
      "Epoch 86 Batch: 66 Train Loss: 0.2790057063102722\n",
      "Epoch 86 Batch: 68 Train Loss: 0.032040517777204514\n",
      "Epoch 86 Batch: 70 Train Loss: 0.28768426179885864\n",
      "Epoch 86 Batch: 72 Train Loss: 0.06900742650032043\n",
      "Epoch 86 Batch: 74 Train Loss: 0.022993043065071106\n",
      "Epoch 86 Batch: 76 Train Loss: 0.039744604378938675\n",
      "Epoch 86 Batch: 78 Train Loss: 0.047961968928575516\n",
      "Epoch 86 Batch: 80 Train Loss: 0.5636603832244873\n",
      "Epoch 86 Batch: 82 Train Loss: 0.2891685366630554\n",
      "Epoch 86 Batch: 84 Train Loss: 0.2909761965274811\n",
      "Epoch 86 Batch: 86 Train Loss: 0.32346469163894653\n",
      "Epoch 86 Batch: 88 Train Loss: 0.03241908922791481\n",
      "Epoch 86 Batch: 90 Train Loss: 0.5588310956954956\n",
      "Epoch 86 Batch: 92 Train Loss: 0.06724119186401367\n",
      "Epoch 86 Batch: 94 Train Loss: 0.3064962327480316\n",
      "Epoch 86 Batch: 96 Train Loss: 0.04676113277673721\n",
      "Epoch 86 Batch: 98 Train Loss: 0.25029855966567993\n",
      "Epoch 86 Batch: 100 Train Loss: 0.04883771762251854\n",
      "Epoch 86 Batch: 102 Train Loss: 0.5225402116775513\n",
      "Epoch 86 Batch: 104 Train Loss: 0.26875999569892883\n",
      "Epoch 86 Batch: 106 Train Loss: 0.011332323774695396\n",
      "Epoch 86 Batch: 108 Train Loss: 0.3007884621620178\n",
      "Epoch 86 Batch: 110 Train Loss: 0.7048010230064392\n",
      "Epoch 86 Batch: 112 Train Loss: 0.0598251111805439\n",
      "Epoch 86 Batch: 114 Train Loss: 0.054969869554042816\n",
      "Epoch 86 Batch: 116 Train Loss: 0.0686446875333786\n",
      "Epoch 86 Batch: 118 Train Loss: 0.3003620505332947\n",
      "Epoch 86 Batch: 120 Train Loss: 0.06946773827075958\n",
      "Epoch 86 Batch: 122 Train Loss: 0.09347079694271088\n",
      "Epoch 86 Batch: 124 Train Loss: 0.04546930268406868\n",
      "Epoch 86 Batch: 126 Train Loss: 0.05077480152249336\n",
      "Epoch 86 Batch: 128 Train Loss: 0.4940868020057678\n",
      "Epoch 86 Batch: 130 Train Loss: 0.07145358622074127\n",
      "Epoch 86 Batch: 132 Train Loss: 0.27104243636131287\n",
      "Epoch 86 Batch: 134 Train Loss: 0.061118025332689285\n",
      "Epoch 86 Batch: 136 Train Loss: 0.057543717324733734\n",
      "Epoch 86 Batch: 138 Train Loss: 0.06732862442731857\n",
      "Epoch 86 Batch: 140 Train Loss: 0.05322659760713577\n",
      "Epoch 86 Batch: 142 Train Loss: 0.29635778069496155\n",
      "Epoch 86 Batch: 144 Train Loss: 0.5466569662094116\n",
      "Epoch 86 Batch: 146 Train Loss: 0.28727227449417114\n",
      "Epoch 86 Batch: 148 Train Loss: 0.04110267385840416\n",
      "Epoch 86 Batch: 150 Train Loss: 0.29999658465385437\n",
      "Epoch 86 Batch: 152 Train Loss: 0.016504734754562378\n",
      "Epoch 86 Batch: 154 Train Loss: 0.27767398953437805\n",
      "Epoch 86 Batch: 156 Train Loss: 0.06337408721446991\n",
      "Epoch 86 Batch: 158 Train Loss: 0.08280900865793228\n",
      "Epoch 86 Batch: 160 Train Loss: 0.5529775619506836\n",
      "Epoch 87 Batch: 2 Train Loss: 0.29737940430641174\n",
      "Epoch 87 Batch: 4 Train Loss: 0.2954712212085724\n",
      "Epoch 87 Batch: 6 Train Loss: 0.046071261167526245\n",
      "Epoch 87 Batch: 8 Train Loss: 0.284282922744751\n",
      "Epoch 87 Batch: 10 Train Loss: 0.4945041537284851\n",
      "Epoch 87 Batch: 12 Train Loss: 0.03365848585963249\n",
      "Epoch 87 Batch: 14 Train Loss: 0.0478888563811779\n",
      "Epoch 87 Batch: 16 Train Loss: 0.039128899574279785\n",
      "Epoch 87 Batch: 18 Train Loss: 0.07354387640953064\n",
      "Epoch 87 Batch: 20 Train Loss: 0.053511083126068115\n",
      "Epoch 87 Batch: 22 Train Loss: 0.049304090440273285\n",
      "Epoch 87 Batch: 24 Train Loss: 0.07780222594738007\n",
      "Epoch 87 Batch: 26 Train Loss: 0.03878190368413925\n",
      "Epoch 87 Batch: 28 Train Loss: 0.056050289422273636\n",
      "Epoch 87 Batch: 30 Train Loss: 0.06093795225024223\n",
      "Epoch 87 Batch: 32 Train Loss: 0.2842499911785126\n",
      "Epoch 87 Batch: 34 Train Loss: 0.026583392173051834\n",
      "Epoch 87 Batch: 36 Train Loss: 0.041797053068876266\n",
      "Epoch 87 Batch: 38 Train Loss: 0.06259097158908844\n",
      "Epoch 87 Batch: 40 Train Loss: 0.29990074038505554\n",
      "Epoch 87 Batch: 42 Train Loss: 0.8482102155685425\n",
      "Epoch 87 Batch: 44 Train Loss: 0.03655888885259628\n",
      "Epoch 87 Batch: 46 Train Loss: 0.27911844849586487\n",
      "Epoch 87 Batch: 48 Train Loss: 0.5830373167991638\n",
      "Epoch 87 Batch: 50 Train Loss: 0.32458412647247314\n",
      "Epoch 87 Batch: 52 Train Loss: 0.05229799076914787\n",
      "Epoch 87 Batch: 54 Train Loss: 0.27935460209846497\n",
      "Epoch 87 Batch: 56 Train Loss: 0.049452438950538635\n",
      "Epoch 87 Batch: 58 Train Loss: 0.05679314211010933\n",
      "Epoch 87 Batch: 60 Train Loss: 0.044553183019161224\n",
      "Epoch 87 Batch: 62 Train Loss: 0.053024180233478546\n",
      "Epoch 87 Batch: 64 Train Loss: 0.049327801913022995\n",
      "Epoch 87 Batch: 66 Train Loss: 0.041531823575496674\n",
      "Epoch 87 Batch: 68 Train Loss: 0.2920566201210022\n",
      "Epoch 87 Batch: 70 Train Loss: 0.05607724189758301\n",
      "Epoch 87 Batch: 72 Train Loss: 0.04749432951211929\n",
      "Epoch 87 Batch: 74 Train Loss: 0.29103654623031616\n",
      "Epoch 87 Batch: 76 Train Loss: 0.01587168499827385\n",
      "Epoch 87 Batch: 78 Train Loss: 0.020937800407409668\n",
      "Epoch 87 Batch: 80 Train Loss: 0.05299622565507889\n",
      "Epoch 87 Batch: 82 Train Loss: 0.600988507270813\n",
      "Epoch 87 Batch: 84 Train Loss: 0.052233271300792694\n",
      "Epoch 87 Batch: 86 Train Loss: 0.03541586175560951\n",
      "Epoch 87 Batch: 88 Train Loss: 0.028085241094231606\n",
      "Epoch 87 Batch: 90 Train Loss: 0.03816096857190132\n",
      "Epoch 87 Batch: 92 Train Loss: 0.04197263717651367\n",
      "Epoch 87 Batch: 94 Train Loss: 0.01404050923883915\n",
      "Epoch 87 Batch: 96 Train Loss: 0.03490794077515602\n",
      "Epoch 87 Batch: 98 Train Loss: 0.3241350054740906\n",
      "Epoch 87 Batch: 100 Train Loss: 0.026542210951447487\n",
      "Epoch 87 Batch: 102 Train Loss: 0.29863831400871277\n",
      "Epoch 87 Batch: 104 Train Loss: 0.0217052660882473\n",
      "Epoch 87 Batch: 106 Train Loss: 0.024957315996289253\n",
      "Epoch 87 Batch: 108 Train Loss: 0.2912820279598236\n",
      "Epoch 87 Batch: 110 Train Loss: 0.026405319571495056\n",
      "Epoch 87 Batch: 112 Train Loss: 0.025863122195005417\n",
      "Epoch 87 Batch: 114 Train Loss: 0.290454626083374\n",
      "Epoch 87 Batch: 116 Train Loss: 0.04652153700590134\n",
      "Epoch 87 Batch: 118 Train Loss: 0.032205138355493546\n",
      "Epoch 87 Batch: 120 Train Loss: 0.03697242587804794\n",
      "Epoch 87 Batch: 122 Train Loss: 0.021551847457885742\n",
      "Epoch 87 Batch: 124 Train Loss: 0.05142570659518242\n",
      "Epoch 87 Batch: 126 Train Loss: 0.07740846276283264\n",
      "Epoch 87 Batch: 128 Train Loss: 0.313154935836792\n",
      "Epoch 87 Batch: 130 Train Loss: 0.7895311117172241\n",
      "Epoch 87 Batch: 132 Train Loss: 0.05636609345674515\n",
      "Epoch 87 Batch: 134 Train Loss: 0.3020211458206177\n",
      "Epoch 87 Batch: 136 Train Loss: 0.018881920725107193\n",
      "Epoch 87 Batch: 138 Train Loss: 0.2962877154350281\n",
      "Epoch 87 Batch: 140 Train Loss: 0.04784458130598068\n",
      "Epoch 87 Batch: 142 Train Loss: 0.2896183133125305\n",
      "Epoch 87 Batch: 144 Train Loss: 0.287325382232666\n",
      "Epoch 87 Batch: 146 Train Loss: 0.06764277070760727\n",
      "Epoch 87 Batch: 148 Train Loss: 0.47573718428611755\n",
      "Epoch 87 Batch: 150 Train Loss: 0.2755502462387085\n",
      "Epoch 87 Batch: 152 Train Loss: 0.46605053544044495\n",
      "Epoch 87 Batch: 154 Train Loss: 0.043664537370204926\n",
      "Epoch 87 Batch: 156 Train Loss: 0.5008831024169922\n",
      "Epoch 87 Batch: 158 Train Loss: 0.26583772897720337\n",
      "Epoch 87 Batch: 160 Train Loss: 0.08033237606287003\n",
      "Epoch 88 Batch: 2 Train Loss: 0.09877000004053116\n",
      "Epoch 88 Batch: 4 Train Loss: 0.10094646364450455\n",
      "Epoch 88 Batch: 6 Train Loss: 0.12970641255378723\n",
      "Epoch 88 Batch: 8 Train Loss: 0.26261040568351746\n",
      "Epoch 88 Batch: 10 Train Loss: 0.08915729820728302\n",
      "Epoch 88 Batch: 12 Train Loss: 0.27510392665863037\n",
      "Epoch 88 Batch: 14 Train Loss: 0.2481369525194168\n",
      "Epoch 88 Batch: 16 Train Loss: 0.5045995116233826\n",
      "Epoch 88 Batch: 18 Train Loss: 0.2912804186344147\n",
      "Epoch 88 Batch: 20 Train Loss: 0.24469926953315735\n",
      "Epoch 88 Batch: 22 Train Loss: 0.06999822705984116\n",
      "Epoch 88 Batch: 24 Train Loss: 0.4801698327064514\n",
      "Epoch 88 Batch: 26 Train Loss: 0.2804984748363495\n",
      "Epoch 88 Batch: 28 Train Loss: 0.2828488349914551\n",
      "Epoch 88 Batch: 30 Train Loss: 0.08680876344442368\n",
      "Epoch 88 Batch: 32 Train Loss: 0.0718747079372406\n",
      "Epoch 88 Batch: 34 Train Loss: 0.07580070197582245\n",
      "Epoch 88 Batch: 36 Train Loss: 0.04331662505865097\n",
      "Epoch 88 Batch: 38 Train Loss: 0.2195427417755127\n",
      "Epoch 88 Batch: 40 Train Loss: 0.08668489754199982\n",
      "Epoch 88 Batch: 42 Train Loss: 0.046959999948740005\n",
      "Epoch 88 Batch: 44 Train Loss: 0.30737417936325073\n",
      "Epoch 88 Batch: 46 Train Loss: 0.2592908442020416\n",
      "Epoch 88 Batch: 48 Train Loss: 0.2629128098487854\n",
      "Epoch 88 Batch: 50 Train Loss: 0.05383051186800003\n",
      "Epoch 88 Batch: 52 Train Loss: 0.040707968175411224\n",
      "Epoch 88 Batch: 54 Train Loss: 0.06826774775981903\n",
      "Epoch 88 Batch: 56 Train Loss: 0.08471189439296722\n",
      "Epoch 88 Batch: 58 Train Loss: 0.28297746181488037\n",
      "Epoch 88 Batch: 60 Train Loss: 0.055889539420604706\n",
      "Epoch 88 Batch: 62 Train Loss: 0.045493219047784805\n",
      "Epoch 88 Batch: 64 Train Loss: 0.28324833512306213\n",
      "Epoch 88 Batch: 66 Train Loss: 0.530594527721405\n",
      "Epoch 88 Batch: 68 Train Loss: 0.5159192085266113\n",
      "Epoch 88 Batch: 70 Train Loss: 0.07623021304607391\n",
      "Epoch 88 Batch: 72 Train Loss: 0.2732413411140442\n",
      "Epoch 88 Batch: 74 Train Loss: 0.31606706976890564\n",
      "Epoch 88 Batch: 76 Train Loss: 0.05350935459136963\n",
      "Epoch 88 Batch: 78 Train Loss: 0.06364350020885468\n",
      "Epoch 88 Batch: 80 Train Loss: 0.04101848602294922\n",
      "Epoch 88 Batch: 82 Train Loss: 0.0286942720413208\n",
      "Epoch 88 Batch: 84 Train Loss: 0.3055940270423889\n",
      "Epoch 88 Batch: 86 Train Loss: 0.04632145166397095\n",
      "Epoch 88 Batch: 88 Train Loss: 0.3068685233592987\n",
      "Epoch 88 Batch: 90 Train Loss: 0.3079589009284973\n",
      "Epoch 88 Batch: 92 Train Loss: 0.05354178696870804\n",
      "Epoch 88 Batch: 94 Train Loss: 0.06020833179354668\n",
      "Epoch 88 Batch: 96 Train Loss: 0.0529618076980114\n",
      "Epoch 88 Batch: 98 Train Loss: 0.041845232248306274\n",
      "Epoch 88 Batch: 100 Train Loss: 0.04301885515451431\n",
      "Epoch 88 Batch: 102 Train Loss: 0.03840617462992668\n",
      "Epoch 88 Batch: 104 Train Loss: 0.0376165397465229\n",
      "Epoch 88 Batch: 106 Train Loss: 0.30580979585647583\n",
      "Epoch 88 Batch: 108 Train Loss: 0.0418257936835289\n",
      "Epoch 88 Batch: 110 Train Loss: 0.024113576859235764\n",
      "Epoch 88 Batch: 112 Train Loss: 0.04842890799045563\n",
      "Epoch 88 Batch: 114 Train Loss: 0.025766288861632347\n",
      "Epoch 88 Batch: 116 Train Loss: 0.03944893181324005\n",
      "Epoch 88 Batch: 118 Train Loss: 0.013277808204293251\n",
      "Epoch 88 Batch: 120 Train Loss: 0.05255477875471115\n",
      "Epoch 88 Batch: 122 Train Loss: 0.033940043300390244\n",
      "Epoch 88 Batch: 124 Train Loss: 0.2760797142982483\n",
      "Epoch 88 Batch: 126 Train Loss: 0.28719496726989746\n",
      "Epoch 88 Batch: 128 Train Loss: 0.0639425739645958\n",
      "Epoch 88 Batch: 130 Train Loss: 0.030680125579237938\n",
      "Epoch 88 Batch: 132 Train Loss: 0.2760973870754242\n",
      "Epoch 88 Batch: 134 Train Loss: 0.04019885137677193\n",
      "Epoch 88 Batch: 136 Train Loss: 0.5265880823135376\n",
      "Epoch 88 Batch: 138 Train Loss: 0.044590383768081665\n",
      "Epoch 88 Batch: 140 Train Loss: 0.026494374498724937\n",
      "Epoch 88 Batch: 142 Train Loss: 0.2706511914730072\n",
      "Epoch 88 Batch: 144 Train Loss: 1.127014398574829\n",
      "Epoch 88 Batch: 146 Train Loss: 0.05406883358955383\n",
      "Epoch 88 Batch: 148 Train Loss: 0.5258156061172485\n",
      "Epoch 88 Batch: 150 Train Loss: 0.6411546468734741\n",
      "Epoch 88 Batch: 152 Train Loss: 0.6839703321456909\n",
      "Epoch 88 Batch: 154 Train Loss: 0.05523545295000076\n",
      "Epoch 88 Batch: 156 Train Loss: 0.21128013730049133\n",
      "Epoch 88 Batch: 158 Train Loss: 0.11923916637897491\n",
      "Epoch 88 Batch: 160 Train Loss: 0.28086158633232117\n",
      "Epoch 89 Batch: 2 Train Loss: 0.12016971409320831\n",
      "Epoch 89 Batch: 4 Train Loss: 0.3795577883720398\n",
      "Epoch 89 Batch: 6 Train Loss: 0.13296912610530853\n",
      "Epoch 89 Batch: 8 Train Loss: 0.27491316199302673\n",
      "Epoch 89 Batch: 10 Train Loss: 0.07764958590269089\n",
      "Epoch 89 Batch: 12 Train Loss: 0.48355111479759216\n",
      "Epoch 89 Batch: 14 Train Loss: 0.34073013067245483\n",
      "Epoch 89 Batch: 16 Train Loss: 0.07106713205575943\n",
      "Epoch 89 Batch: 18 Train Loss: 0.252541720867157\n",
      "Epoch 89 Batch: 20 Train Loss: 0.09364236891269684\n",
      "Epoch 89 Batch: 22 Train Loss: 0.25300800800323486\n",
      "Epoch 89 Batch: 24 Train Loss: 0.08291365951299667\n",
      "Epoch 89 Batch: 26 Train Loss: 0.24943777918815613\n",
      "Epoch 89 Batch: 28 Train Loss: 0.13814857602119446\n",
      "Epoch 89 Batch: 30 Train Loss: 0.25462812185287476\n",
      "Epoch 89 Batch: 32 Train Loss: 0.4043646454811096\n",
      "Epoch 89 Batch: 34 Train Loss: 0.08739915490150452\n",
      "Epoch 89 Batch: 36 Train Loss: 0.10170561075210571\n",
      "Epoch 89 Batch: 38 Train Loss: 0.2568151354789734\n",
      "Epoch 89 Batch: 40 Train Loss: 0.11725590378046036\n",
      "Epoch 89 Batch: 42 Train Loss: 0.2912636399269104\n",
      "Epoch 89 Batch: 44 Train Loss: 0.11989530175924301\n",
      "Epoch 89 Batch: 46 Train Loss: 0.2750415802001953\n",
      "Epoch 89 Batch: 48 Train Loss: 0.2636336386203766\n",
      "Epoch 89 Batch: 50 Train Loss: 0.27806979417800903\n",
      "Epoch 89 Batch: 52 Train Loss: 0.07707490026950836\n",
      "Epoch 89 Batch: 54 Train Loss: 0.278775691986084\n",
      "Epoch 89 Batch: 56 Train Loss: 0.2935158610343933\n",
      "Epoch 89 Batch: 58 Train Loss: 0.2781860828399658\n",
      "Epoch 89 Batch: 60 Train Loss: 0.2513749897480011\n",
      "Epoch 89 Batch: 62 Train Loss: 0.08255644887685776\n",
      "Epoch 89 Batch: 64 Train Loss: 0.08152800053358078\n",
      "Epoch 89 Batch: 66 Train Loss: 0.04719654470682144\n",
      "Epoch 89 Batch: 68 Train Loss: 0.04636691138148308\n",
      "Epoch 89 Batch: 70 Train Loss: 0.4991757273674011\n",
      "Epoch 89 Batch: 72 Train Loss: 0.06446746736764908\n",
      "Epoch 89 Batch: 74 Train Loss: 0.2876085638999939\n",
      "Epoch 89 Batch: 76 Train Loss: 0.2545895278453827\n",
      "Epoch 89 Batch: 78 Train Loss: 0.4973936975002289\n",
      "Epoch 89 Batch: 80 Train Loss: 0.24529507756233215\n",
      "Epoch 89 Batch: 82 Train Loss: 0.2656509280204773\n",
      "Epoch 89 Batch: 84 Train Loss: 0.08686688542366028\n",
      "Epoch 89 Batch: 86 Train Loss: 0.07666290551424026\n",
      "Epoch 89 Batch: 88 Train Loss: 0.08355056494474411\n",
      "Epoch 89 Batch: 90 Train Loss: 0.26925161480903625\n",
      "Epoch 89 Batch: 92 Train Loss: 0.054792772978544235\n",
      "Epoch 89 Batch: 94 Train Loss: 0.2782171964645386\n",
      "Epoch 89 Batch: 96 Train Loss: 0.04779772087931633\n",
      "Epoch 89 Batch: 98 Train Loss: 0.04764919728040695\n",
      "Epoch 89 Batch: 100 Train Loss: 0.2458006590604782\n",
      "Epoch 89 Batch: 102 Train Loss: 0.2740367352962494\n",
      "Epoch 89 Batch: 104 Train Loss: 0.2827377915382385\n",
      "Epoch 89 Batch: 106 Train Loss: 0.07107584178447723\n",
      "Epoch 89 Batch: 108 Train Loss: 0.08260571956634521\n",
      "Epoch 89 Batch: 110 Train Loss: 0.06975258886814117\n",
      "Epoch 89 Batch: 112 Train Loss: 0.49310898780822754\n",
      "Epoch 89 Batch: 114 Train Loss: 0.3018787205219269\n",
      "Epoch 89 Batch: 116 Train Loss: 0.05649370700120926\n",
      "Epoch 89 Batch: 118 Train Loss: 0.27239882946014404\n",
      "Epoch 89 Batch: 120 Train Loss: 0.07566684484481812\n",
      "Epoch 89 Batch: 122 Train Loss: 0.03256743401288986\n",
      "Epoch 89 Batch: 124 Train Loss: 0.03262043371796608\n",
      "Epoch 89 Batch: 126 Train Loss: 0.04238676279783249\n",
      "Epoch 89 Batch: 128 Train Loss: 0.0713248923420906\n",
      "Epoch 89 Batch: 130 Train Loss: 0.04839583858847618\n",
      "Epoch 89 Batch: 132 Train Loss: 0.27040451765060425\n",
      "Epoch 89 Batch: 134 Train Loss: 0.03660979121923447\n",
      "Epoch 89 Batch: 136 Train Loss: 0.03595927357673645\n",
      "Epoch 89 Batch: 138 Train Loss: 0.06821627914905548\n",
      "Epoch 89 Batch: 140 Train Loss: 0.06360603868961334\n",
      "Epoch 89 Batch: 142 Train Loss: 0.03932315111160278\n",
      "Epoch 89 Batch: 144 Train Loss: 0.05487111955881119\n",
      "Epoch 89 Batch: 146 Train Loss: 0.04346909001469612\n",
      "Epoch 89 Batch: 148 Train Loss: 0.2930443584918976\n",
      "Epoch 89 Batch: 150 Train Loss: 0.3190487027168274\n",
      "Epoch 89 Batch: 152 Train Loss: 0.05194062739610672\n",
      "Epoch 89 Batch: 154 Train Loss: 0.03499818220734596\n",
      "Epoch 89 Batch: 156 Train Loss: 0.022483868524432182\n",
      "Epoch 89 Batch: 158 Train Loss: 0.3018715977668762\n",
      "Epoch 89 Batch: 160 Train Loss: 0.28205594420433044\n",
      "Epoch 90 Batch: 2 Train Loss: 0.2915739417076111\n",
      "Epoch 90 Batch: 4 Train Loss: 0.06367385387420654\n",
      "Epoch 90 Batch: 6 Train Loss: 0.28413382172584534\n",
      "Epoch 90 Batch: 8 Train Loss: 0.047683678567409515\n",
      "Epoch 90 Batch: 10 Train Loss: 0.03570585697889328\n",
      "Epoch 90 Batch: 12 Train Loss: 0.07537002116441727\n",
      "Epoch 90 Batch: 14 Train Loss: 0.03514929115772247\n",
      "Epoch 90 Batch: 16 Train Loss: 0.026703614741563797\n",
      "Epoch 90 Batch: 18 Train Loss: 0.04918483644723892\n",
      "Epoch 90 Batch: 20 Train Loss: 0.06060303375124931\n",
      "Epoch 90 Batch: 22 Train Loss: 0.5559228658676147\n",
      "Epoch 90 Batch: 24 Train Loss: 0.29958775639533997\n",
      "Epoch 90 Batch: 26 Train Loss: 0.04570374637842178\n",
      "Epoch 90 Batch: 28 Train Loss: 0.2875170409679413\n",
      "Epoch 90 Batch: 30 Train Loss: 0.07948565483093262\n",
      "Epoch 90 Batch: 32 Train Loss: 0.0264388807117939\n",
      "Epoch 90 Batch: 34 Train Loss: 0.05510253459215164\n",
      "Epoch 90 Batch: 36 Train Loss: 0.051594339311122894\n",
      "Epoch 90 Batch: 38 Train Loss: 0.051322925835847855\n",
      "Epoch 90 Batch: 40 Train Loss: 0.2883909344673157\n",
      "Epoch 90 Batch: 42 Train Loss: 0.28691351413726807\n",
      "Epoch 90 Batch: 44 Train Loss: 0.03331224247813225\n",
      "Epoch 90 Batch: 46 Train Loss: 0.03294510394334793\n",
      "Epoch 90 Batch: 48 Train Loss: 0.27273255586624146\n",
      "Epoch 90 Batch: 50 Train Loss: 0.2867929935455322\n",
      "Epoch 90 Batch: 52 Train Loss: 0.030418161302804947\n",
      "Epoch 90 Batch: 54 Train Loss: 0.054699916392564774\n",
      "Epoch 90 Batch: 56 Train Loss: 0.5565118789672852\n",
      "Epoch 90 Batch: 58 Train Loss: 0.05234084278345108\n",
      "Epoch 90 Batch: 60 Train Loss: 0.05735888332128525\n",
      "Epoch 90 Batch: 62 Train Loss: 0.5675485730171204\n",
      "Epoch 90 Batch: 64 Train Loss: 0.04939407855272293\n",
      "Epoch 90 Batch: 66 Train Loss: 0.05145854502916336\n",
      "Epoch 90 Batch: 68 Train Loss: 0.050008006393909454\n",
      "Epoch 90 Batch: 70 Train Loss: 0.2927209138870239\n",
      "Epoch 90 Batch: 72 Train Loss: 0.3044513761997223\n",
      "Epoch 90 Batch: 74 Train Loss: 0.051902830600738525\n",
      "Epoch 90 Batch: 76 Train Loss: 0.06095350906252861\n",
      "Epoch 90 Batch: 78 Train Loss: 0.2790158987045288\n",
      "Epoch 90 Batch: 80 Train Loss: 0.026225203648209572\n",
      "Epoch 90 Batch: 82 Train Loss: 0.0637916624546051\n",
      "Epoch 90 Batch: 84 Train Loss: 0.2719230353832245\n",
      "Epoch 90 Batch: 86 Train Loss: 0.0379897765815258\n",
      "Epoch 90 Batch: 88 Train Loss: 0.028171736747026443\n",
      "Epoch 90 Batch: 90 Train Loss: 0.5162395238876343\n",
      "Epoch 90 Batch: 92 Train Loss: 0.2654891610145569\n",
      "Epoch 90 Batch: 94 Train Loss: 0.5005671381950378\n",
      "Epoch 90 Batch: 96 Train Loss: 0.7416875958442688\n",
      "Epoch 90 Batch: 98 Train Loss: 0.04196658730506897\n",
      "Epoch 90 Batch: 100 Train Loss: 0.5229983329772949\n",
      "Epoch 90 Batch: 102 Train Loss: 0.27787110209465027\n",
      "Epoch 90 Batch: 104 Train Loss: 0.03385169804096222\n",
      "Epoch 90 Batch: 106 Train Loss: 0.2805641293525696\n",
      "Epoch 90 Batch: 108 Train Loss: 0.49659767746925354\n",
      "Epoch 90 Batch: 110 Train Loss: 0.05133821815252304\n",
      "Epoch 90 Batch: 112 Train Loss: 0.056281186640262604\n",
      "Epoch 90 Batch: 114 Train Loss: 0.44677314162254333\n",
      "Epoch 90 Batch: 116 Train Loss: 0.2887393534183502\n",
      "Epoch 90 Batch: 118 Train Loss: 0.11077497154474258\n",
      "Epoch 90 Batch: 120 Train Loss: 0.0822654739022255\n",
      "Epoch 90 Batch: 122 Train Loss: 0.09200514107942581\n",
      "Epoch 90 Batch: 124 Train Loss: 0.04398851469159126\n",
      "Epoch 90 Batch: 126 Train Loss: 0.450528085231781\n",
      "Epoch 90 Batch: 128 Train Loss: 0.086280956864357\n",
      "Epoch 90 Batch: 130 Train Loss: 0.2586021423339844\n",
      "Epoch 90 Batch: 132 Train Loss: 0.05284156650304794\n",
      "Epoch 90 Batch: 134 Train Loss: 0.06270095705986023\n",
      "Epoch 90 Batch: 136 Train Loss: 0.03478679805994034\n",
      "Epoch 90 Batch: 138 Train Loss: 0.06645682454109192\n",
      "Epoch 90 Batch: 140 Train Loss: 0.045232243835926056\n",
      "Epoch 90 Batch: 142 Train Loss: 0.05329778045415878\n",
      "Epoch 90 Batch: 144 Train Loss: 0.05076593905687332\n",
      "Epoch 90 Batch: 146 Train Loss: 0.031511954963207245\n",
      "Epoch 90 Batch: 148 Train Loss: 0.05683599039912224\n",
      "Epoch 90 Batch: 150 Train Loss: 0.5117713212966919\n",
      "Epoch 90 Batch: 152 Train Loss: 0.05544073507189751\n",
      "Epoch 90 Batch: 154 Train Loss: 0.0526653416454792\n",
      "Epoch 90 Batch: 156 Train Loss: 0.24791112542152405\n",
      "Epoch 90 Batch: 158 Train Loss: 0.06815171241760254\n",
      "Epoch 90 Batch: 160 Train Loss: 0.3047168254852295\n",
      "Epoch 91 Batch: 2 Train Loss: 0.07198892533779144\n",
      "Epoch 91 Batch: 4 Train Loss: 0.29587793350219727\n",
      "Epoch 91 Batch: 6 Train Loss: 0.2909873127937317\n",
      "Epoch 91 Batch: 8 Train Loss: 0.032818201929330826\n",
      "Epoch 91 Batch: 10 Train Loss: 0.04739293456077576\n",
      "Epoch 91 Batch: 12 Train Loss: 0.2692919969558716\n",
      "Epoch 91 Batch: 14 Train Loss: 0.0551106259226799\n",
      "Epoch 91 Batch: 16 Train Loss: 0.055890150368213654\n",
      "Epoch 91 Batch: 18 Train Loss: 0.0329495407640934\n",
      "Epoch 91 Batch: 20 Train Loss: 0.02957569994032383\n",
      "Epoch 91 Batch: 22 Train Loss: 0.5890206098556519\n",
      "Epoch 91 Batch: 24 Train Loss: 0.040836017578840256\n",
      "Epoch 91 Batch: 26 Train Loss: 0.30166053771972656\n",
      "Epoch 91 Batch: 28 Train Loss: 0.057476650923490524\n",
      "Epoch 91 Batch: 30 Train Loss: 0.05589579790830612\n",
      "Epoch 91 Batch: 32 Train Loss: 0.2577505111694336\n",
      "Epoch 91 Batch: 34 Train Loss: 0.31691163778305054\n",
      "Epoch 91 Batch: 36 Train Loss: 0.05459054559469223\n",
      "Epoch 91 Batch: 38 Train Loss: 0.04658922553062439\n",
      "Epoch 91 Batch: 40 Train Loss: 0.3240872025489807\n",
      "Epoch 91 Batch: 42 Train Loss: 0.3012528419494629\n",
      "Epoch 91 Batch: 44 Train Loss: 0.02995886839926243\n",
      "Epoch 91 Batch: 46 Train Loss: 0.2710573077201843\n",
      "Epoch 91 Batch: 48 Train Loss: 0.048873282968997955\n",
      "Epoch 91 Batch: 50 Train Loss: 0.030539970844984055\n",
      "Epoch 91 Batch: 52 Train Loss: 0.05213490128517151\n",
      "Epoch 91 Batch: 54 Train Loss: 0.05523943156003952\n",
      "Epoch 91 Batch: 56 Train Loss: 0.30739694833755493\n",
      "Epoch 91 Batch: 58 Train Loss: 0.05193662643432617\n",
      "Epoch 91 Batch: 60 Train Loss: 0.06448786705732346\n",
      "Epoch 91 Batch: 62 Train Loss: 0.031202679499983788\n",
      "Epoch 91 Batch: 64 Train Loss: 0.5446195602416992\n",
      "Epoch 91 Batch: 66 Train Loss: 0.27968454360961914\n",
      "Epoch 91 Batch: 68 Train Loss: 0.29842525720596313\n",
      "Epoch 91 Batch: 70 Train Loss: 0.28928977251052856\n",
      "Epoch 91 Batch: 72 Train Loss: 0.06133868545293808\n",
      "Epoch 91 Batch: 74 Train Loss: 0.05236951261758804\n",
      "Epoch 91 Batch: 76 Train Loss: 0.5390681028366089\n",
      "Epoch 91 Batch: 78 Train Loss: 0.3013526201248169\n",
      "Epoch 91 Batch: 80 Train Loss: 0.05572013929486275\n",
      "Epoch 91 Batch: 82 Train Loss: 0.06454233080148697\n",
      "Epoch 91 Batch: 84 Train Loss: 0.26491305232048035\n",
      "Epoch 91 Batch: 86 Train Loss: 0.046964772045612335\n",
      "Epoch 91 Batch: 88 Train Loss: 0.5247790813446045\n",
      "Epoch 91 Batch: 90 Train Loss: 0.049292780458927155\n",
      "Epoch 91 Batch: 92 Train Loss: 0.29992610216140747\n",
      "Epoch 91 Batch: 94 Train Loss: 0.2997204661369324\n",
      "Epoch 91 Batch: 96 Train Loss: 0.5347884893417358\n",
      "Epoch 91 Batch: 98 Train Loss: 0.05417301505804062\n",
      "Epoch 91 Batch: 100 Train Loss: 0.06249266117811203\n",
      "Epoch 91 Batch: 102 Train Loss: 0.0526290163397789\n",
      "Epoch 91 Batch: 104 Train Loss: 0.08354608714580536\n",
      "Epoch 91 Batch: 106 Train Loss: 0.5151007771492004\n",
      "Epoch 91 Batch: 108 Train Loss: 0.042165689170360565\n",
      "Epoch 91 Batch: 110 Train Loss: 0.05184652656316757\n",
      "Epoch 91 Batch: 112 Train Loss: 0.3004310727119446\n",
      "Epoch 91 Batch: 114 Train Loss: 0.2692849040031433\n",
      "Epoch 91 Batch: 116 Train Loss: 0.49103841185569763\n",
      "Epoch 91 Batch: 118 Train Loss: 0.27046099305152893\n",
      "Epoch 91 Batch: 120 Train Loss: 0.06948954612016678\n",
      "Epoch 91 Batch: 122 Train Loss: 0.06959792971611023\n",
      "Epoch 91 Batch: 124 Train Loss: 0.0850367397069931\n",
      "Epoch 91 Batch: 126 Train Loss: 0.27166956663131714\n",
      "Epoch 91 Batch: 128 Train Loss: 0.07821707427501678\n",
      "Epoch 91 Batch: 130 Train Loss: 0.06946858763694763\n",
      "Epoch 91 Batch: 132 Train Loss: 0.25056740641593933\n",
      "Epoch 91 Batch: 134 Train Loss: 0.04744918271899223\n",
      "Epoch 91 Batch: 136 Train Loss: 0.28627458214759827\n",
      "Epoch 91 Batch: 138 Train Loss: 0.04543139040470123\n",
      "Epoch 91 Batch: 140 Train Loss: 0.06981398910284042\n",
      "Epoch 91 Batch: 142 Train Loss: 0.0865062028169632\n",
      "Epoch 91 Batch: 144 Train Loss: 0.5215307474136353\n",
      "Epoch 91 Batch: 146 Train Loss: 0.042349956929683685\n",
      "Epoch 91 Batch: 148 Train Loss: 0.9668638110160828\n",
      "Epoch 91 Batch: 150 Train Loss: 0.05467585474252701\n",
      "Epoch 91 Batch: 152 Train Loss: 0.05781957507133484\n",
      "Epoch 91 Batch: 154 Train Loss: 0.26634347438812256\n",
      "Epoch 91 Batch: 156 Train Loss: 0.28566235303878784\n",
      "Epoch 91 Batch: 158 Train Loss: 0.043968115001916885\n",
      "Epoch 91 Batch: 160 Train Loss: 0.042612165212631226\n",
      "Epoch 92 Batch: 2 Train Loss: 0.06484793871641159\n",
      "Epoch 92 Batch: 4 Train Loss: 0.04085633531212807\n",
      "Epoch 92 Batch: 6 Train Loss: 0.0912412479519844\n",
      "Epoch 92 Batch: 8 Train Loss: 0.2845310866832733\n",
      "Epoch 92 Batch: 10 Train Loss: 0.04925798252224922\n",
      "Epoch 92 Batch: 12 Train Loss: 0.04790502041578293\n",
      "Epoch 92 Batch: 14 Train Loss: 0.3016318380832672\n",
      "Epoch 92 Batch: 16 Train Loss: 0.26429483294487\n",
      "Epoch 92 Batch: 18 Train Loss: 0.025397535413503647\n",
      "Epoch 92 Batch: 20 Train Loss: 0.04403410479426384\n",
      "Epoch 92 Batch: 22 Train Loss: 0.2669525146484375\n",
      "Epoch 92 Batch: 24 Train Loss: 0.5445253252983093\n",
      "Epoch 92 Batch: 26 Train Loss: 0.29977938532829285\n",
      "Epoch 92 Batch: 28 Train Loss: 0.04773077741265297\n",
      "Epoch 92 Batch: 30 Train Loss: 0.3210297226905823\n",
      "Epoch 92 Batch: 32 Train Loss: 0.052285391837358475\n",
      "Epoch 92 Batch: 34 Train Loss: 0.039771776646375656\n",
      "Epoch 92 Batch: 36 Train Loss: 0.04100823402404785\n",
      "Epoch 92 Batch: 38 Train Loss: 0.05352277681231499\n",
      "Epoch 92 Batch: 40 Train Loss: 0.31275618076324463\n",
      "Epoch 92 Batch: 42 Train Loss: 0.043019868433475494\n",
      "Epoch 92 Batch: 44 Train Loss: 0.2926073670387268\n",
      "Epoch 92 Batch: 46 Train Loss: 0.31367459893226624\n",
      "Epoch 92 Batch: 48 Train Loss: 0.007543879561126232\n",
      "Epoch 92 Batch: 50 Train Loss: 0.3021922707557678\n",
      "Epoch 92 Batch: 52 Train Loss: 0.04951431229710579\n",
      "Epoch 92 Batch: 54 Train Loss: 0.036460381001234055\n",
      "Epoch 92 Batch: 56 Train Loss: 0.05016449838876724\n",
      "Epoch 92 Batch: 58 Train Loss: 0.29370447993278503\n",
      "Epoch 92 Batch: 60 Train Loss: 0.050477515906095505\n",
      "Epoch 92 Batch: 62 Train Loss: 0.04991739243268967\n",
      "Epoch 92 Batch: 64 Train Loss: 0.2979673743247986\n",
      "Epoch 92 Batch: 66 Train Loss: 0.32293039560317993\n",
      "Epoch 92 Batch: 68 Train Loss: 0.3159896731376648\n",
      "Epoch 92 Batch: 70 Train Loss: 0.5643347501754761\n",
      "Epoch 92 Batch: 72 Train Loss: 0.28535208106040955\n",
      "Epoch 92 Batch: 74 Train Loss: 0.04648778215050697\n",
      "Epoch 92 Batch: 76 Train Loss: 0.3010180592536926\n",
      "Epoch 92 Batch: 78 Train Loss: 0.044193584471940994\n",
      "Epoch 92 Batch: 80 Train Loss: 0.28968456387519836\n",
      "Epoch 92 Batch: 82 Train Loss: 0.5442554354667664\n",
      "Epoch 92 Batch: 84 Train Loss: 0.06743989884853363\n",
      "Epoch 92 Batch: 86 Train Loss: 0.05367272347211838\n",
      "Epoch 92 Batch: 88 Train Loss: 0.3066498935222626\n",
      "Epoch 92 Batch: 90 Train Loss: 0.06803539395332336\n",
      "Epoch 92 Batch: 92 Train Loss: 0.037565574049949646\n",
      "Epoch 92 Batch: 94 Train Loss: 0.048600587993860245\n",
      "Epoch 92 Batch: 96 Train Loss: 0.27988865971565247\n",
      "Epoch 92 Batch: 98 Train Loss: 0.02736487053334713\n",
      "Epoch 92 Batch: 100 Train Loss: 0.05521034449338913\n",
      "Epoch 92 Batch: 102 Train Loss: 0.28061071038246155\n",
      "Epoch 92 Batch: 104 Train Loss: 0.0368877537548542\n",
      "Epoch 92 Batch: 106 Train Loss: 0.3015236258506775\n",
      "Epoch 92 Batch: 108 Train Loss: 0.5327105522155762\n",
      "Epoch 92 Batch: 110 Train Loss: 0.5148524045944214\n",
      "Epoch 92 Batch: 112 Train Loss: 0.04829537123441696\n",
      "Epoch 92 Batch: 114 Train Loss: 0.03872403874993324\n",
      "Epoch 92 Batch: 116 Train Loss: 0.06361845135688782\n",
      "Epoch 92 Batch: 118 Train Loss: 0.290642648935318\n",
      "Epoch 92 Batch: 120 Train Loss: 0.032829299569129944\n",
      "Epoch 92 Batch: 122 Train Loss: 0.05402989313006401\n",
      "Epoch 92 Batch: 124 Train Loss: 0.28778040409088135\n",
      "Epoch 92 Batch: 126 Train Loss: 0.2772867977619171\n",
      "Epoch 92 Batch: 128 Train Loss: 0.0862530991435051\n",
      "Epoch 92 Batch: 130 Train Loss: 0.2665581703186035\n",
      "Epoch 92 Batch: 132 Train Loss: 0.09016112983226776\n",
      "Epoch 92 Batch: 134 Train Loss: 0.058502841740846634\n",
      "Epoch 92 Batch: 136 Train Loss: 0.27772724628448486\n",
      "Epoch 92 Batch: 138 Train Loss: 0.06438097357749939\n",
      "Epoch 92 Batch: 140 Train Loss: 0.06852038949728012\n",
      "Epoch 92 Batch: 142 Train Loss: 0.04541599005460739\n",
      "Epoch 92 Batch: 144 Train Loss: 0.294093519449234\n",
      "Epoch 92 Batch: 146 Train Loss: 0.2828989624977112\n",
      "Epoch 92 Batch: 148 Train Loss: 0.29956337809562683\n",
      "Epoch 92 Batch: 150 Train Loss: 0.07959584891796112\n",
      "Epoch 92 Batch: 152 Train Loss: 0.07059487700462341\n",
      "Epoch 92 Batch: 154 Train Loss: 0.2779284715652466\n",
      "Epoch 92 Batch: 156 Train Loss: 0.024285901337862015\n",
      "Epoch 92 Batch: 158 Train Loss: 0.7110751867294312\n",
      "Epoch 92 Batch: 160 Train Loss: 0.31399717926979065\n",
      "Epoch 93 Batch: 2 Train Loss: 0.06301451474428177\n",
      "Epoch 93 Batch: 4 Train Loss: 0.28183475136756897\n",
      "Epoch 93 Batch: 6 Train Loss: 0.05333982780575752\n",
      "Epoch 93 Batch: 8 Train Loss: 0.050642985850572586\n",
      "Epoch 93 Batch: 10 Train Loss: 0.2915958762168884\n",
      "Epoch 93 Batch: 12 Train Loss: 0.07302985340356827\n",
      "Epoch 93 Batch: 14 Train Loss: 0.046281859278678894\n",
      "Epoch 93 Batch: 16 Train Loss: 0.09026368707418442\n",
      "Epoch 93 Batch: 18 Train Loss: 0.053100306540727615\n",
      "Epoch 93 Batch: 20 Train Loss: 0.039659321308135986\n",
      "Epoch 93 Batch: 22 Train Loss: 0.2646009027957916\n",
      "Epoch 93 Batch: 24 Train Loss: 0.29685741662979126\n",
      "Epoch 93 Batch: 26 Train Loss: 0.2987722158432007\n",
      "Epoch 93 Batch: 28 Train Loss: 0.03816167265176773\n",
      "Epoch 93 Batch: 30 Train Loss: 0.05559689551591873\n",
      "Epoch 93 Batch: 32 Train Loss: 0.3126106858253479\n",
      "Epoch 93 Batch: 34 Train Loss: 0.04748166725039482\n",
      "Epoch 93 Batch: 36 Train Loss: 0.555397629737854\n",
      "Epoch 93 Batch: 38 Train Loss: 0.05044198036193848\n",
      "Epoch 93 Batch: 40 Train Loss: 0.058926958590745926\n",
      "Epoch 93 Batch: 42 Train Loss: 0.041754283010959625\n",
      "Epoch 93 Batch: 44 Train Loss: 0.2718895375728607\n",
      "Epoch 93 Batch: 46 Train Loss: 0.0677681565284729\n",
      "Epoch 93 Batch: 48 Train Loss: 0.0753842145204544\n",
      "Epoch 93 Batch: 50 Train Loss: 0.23777110874652863\n",
      "Epoch 93 Batch: 52 Train Loss: 0.0483478419482708\n",
      "Epoch 93 Batch: 54 Train Loss: 0.0503159761428833\n",
      "Epoch 93 Batch: 56 Train Loss: 0.0267648808658123\n",
      "Epoch 93 Batch: 58 Train Loss: 0.05472605302929878\n",
      "Epoch 93 Batch: 60 Train Loss: 0.26373887062072754\n",
      "Epoch 93 Batch: 62 Train Loss: 0.043134093284606934\n",
      "Epoch 93 Batch: 64 Train Loss: 0.0519566610455513\n",
      "Epoch 93 Batch: 66 Train Loss: 0.5331476330757141\n",
      "Epoch 93 Batch: 68 Train Loss: 0.27945441007614136\n",
      "Epoch 93 Batch: 70 Train Loss: 0.0394241139292717\n",
      "Epoch 93 Batch: 72 Train Loss: 0.29589876532554626\n",
      "Epoch 93 Batch: 74 Train Loss: 0.03438573703169823\n",
      "Epoch 93 Batch: 76 Train Loss: 0.2823370099067688\n",
      "Epoch 93 Batch: 78 Train Loss: 0.5350159406661987\n",
      "Epoch 93 Batch: 80 Train Loss: 0.028592532500624657\n",
      "Epoch 93 Batch: 82 Train Loss: 0.2833422124385834\n",
      "Epoch 93 Batch: 84 Train Loss: 0.508181095123291\n",
      "Epoch 93 Batch: 86 Train Loss: 0.48547905683517456\n",
      "Epoch 93 Batch: 88 Train Loss: 0.2489669770002365\n",
      "Epoch 93 Batch: 90 Train Loss: 0.06519119441509247\n",
      "Epoch 93 Batch: 92 Train Loss: 0.4803279936313629\n",
      "Epoch 93 Batch: 94 Train Loss: 0.05340741202235222\n",
      "Epoch 93 Batch: 96 Train Loss: 0.24073898792266846\n",
      "Epoch 93 Batch: 98 Train Loss: 0.26432186365127563\n",
      "Epoch 93 Batch: 100 Train Loss: 0.27442702651023865\n",
      "Epoch 93 Batch: 102 Train Loss: 0.06707502156496048\n",
      "Epoch 93 Batch: 104 Train Loss: 0.2845473289489746\n",
      "Epoch 93 Batch: 106 Train Loss: 0.4873606562614441\n",
      "Epoch 93 Batch: 108 Train Loss: 0.0555480420589447\n",
      "Epoch 93 Batch: 110 Train Loss: 0.05644925683736801\n",
      "Epoch 93 Batch: 112 Train Loss: 0.05799812078475952\n",
      "Epoch 93 Batch: 114 Train Loss: 0.09566409885883331\n",
      "Epoch 93 Batch: 116 Train Loss: 0.10783158242702484\n",
      "Epoch 93 Batch: 118 Train Loss: 0.06543037295341492\n",
      "Epoch 93 Batch: 120 Train Loss: 0.08902955055236816\n",
      "Epoch 93 Batch: 122 Train Loss: 0.05289330333471298\n",
      "Epoch 93 Batch: 124 Train Loss: 0.09146246314048767\n",
      "Epoch 93 Batch: 126 Train Loss: 0.24321432411670685\n",
      "Epoch 93 Batch: 128 Train Loss: 0.07229583710432053\n",
      "Epoch 93 Batch: 130 Train Loss: 0.2664452791213989\n",
      "Epoch 93 Batch: 132 Train Loss: 0.512172520160675\n",
      "Epoch 93 Batch: 134 Train Loss: 0.04507242888212204\n",
      "Epoch 93 Batch: 136 Train Loss: 0.7251364588737488\n",
      "Epoch 93 Batch: 138 Train Loss: 0.04389796406030655\n",
      "Epoch 93 Batch: 140 Train Loss: 0.05471474677324295\n",
      "Epoch 93 Batch: 142 Train Loss: 0.06377505511045456\n",
      "Epoch 93 Batch: 144 Train Loss: 0.07382773607969284\n",
      "Epoch 93 Batch: 146 Train Loss: 0.29282623529434204\n",
      "Epoch 93 Batch: 148 Train Loss: 0.0891387015581131\n",
      "Epoch 93 Batch: 150 Train Loss: 0.04659517481923103\n",
      "Epoch 93 Batch: 152 Train Loss: 0.05170822888612747\n",
      "Epoch 93 Batch: 154 Train Loss: 0.049982111901044846\n",
      "Epoch 93 Batch: 156 Train Loss: 0.023990068584680557\n",
      "Epoch 93 Batch: 158 Train Loss: 0.5545465350151062\n",
      "Epoch 93 Batch: 160 Train Loss: 0.015800530090928078\n",
      "Epoch 94 Batch: 2 Train Loss: 0.05365143343806267\n",
      "Epoch 94 Batch: 4 Train Loss: 0.03967253863811493\n",
      "Epoch 94 Batch: 6 Train Loss: 0.5488428473472595\n",
      "Epoch 94 Batch: 8 Train Loss: 0.28516119718551636\n",
      "Epoch 94 Batch: 10 Train Loss: 0.03806716203689575\n",
      "Epoch 94 Batch: 12 Train Loss: 0.29554808139801025\n",
      "Epoch 94 Batch: 14 Train Loss: 0.03861153870820999\n",
      "Epoch 94 Batch: 16 Train Loss: 0.2998974025249481\n",
      "Epoch 94 Batch: 18 Train Loss: 0.05517660826444626\n",
      "Epoch 94 Batch: 20 Train Loss: 0.03157249093055725\n",
      "Epoch 94 Batch: 22 Train Loss: 0.04786081239581108\n",
      "Epoch 94 Batch: 24 Train Loss: 0.2939523458480835\n",
      "Epoch 94 Batch: 26 Train Loss: 0.05292946845293045\n",
      "Epoch 94 Batch: 28 Train Loss: 0.2967967391014099\n",
      "Epoch 94 Batch: 30 Train Loss: 0.2949769198894501\n",
      "Epoch 94 Batch: 32 Train Loss: 0.7316824197769165\n",
      "Epoch 94 Batch: 34 Train Loss: 0.05171653628349304\n",
      "Epoch 94 Batch: 36 Train Loss: 0.07614608854055405\n",
      "Epoch 94 Batch: 38 Train Loss: 0.05298108607530594\n",
      "Epoch 94 Batch: 40 Train Loss: 0.047213565558195114\n",
      "Epoch 94 Batch: 42 Train Loss: 0.7129098773002625\n",
      "Epoch 94 Batch: 44 Train Loss: 0.053095705807209015\n",
      "Epoch 94 Batch: 46 Train Loss: 0.0528097040951252\n",
      "Epoch 94 Batch: 48 Train Loss: 0.06869962811470032\n",
      "Epoch 94 Batch: 50 Train Loss: 0.0732870101928711\n",
      "Epoch 94 Batch: 52 Train Loss: 0.07145033031702042\n",
      "Epoch 94 Batch: 54 Train Loss: 0.28647786378860474\n",
      "Epoch 94 Batch: 56 Train Loss: 0.055040277540683746\n",
      "Epoch 94 Batch: 58 Train Loss: 0.46433553099632263\n",
      "Epoch 94 Batch: 60 Train Loss: 0.2908901870250702\n",
      "Epoch 94 Batch: 62 Train Loss: 0.46629005670547485\n",
      "Epoch 94 Batch: 64 Train Loss: 0.2823900580406189\n",
      "Epoch 94 Batch: 66 Train Loss: 0.06625194847583771\n",
      "Epoch 94 Batch: 68 Train Loss: 0.05328087881207466\n",
      "Epoch 94 Batch: 70 Train Loss: 0.040312450379133224\n",
      "Epoch 94 Batch: 72 Train Loss: 0.28733858466148376\n",
      "Epoch 94 Batch: 74 Train Loss: 0.28651461005210876\n",
      "Epoch 94 Batch: 76 Train Loss: 0.03715871647000313\n",
      "Epoch 94 Batch: 78 Train Loss: 0.048833586275577545\n",
      "Epoch 94 Batch: 80 Train Loss: 0.07019089162349701\n",
      "Epoch 94 Batch: 82 Train Loss: 0.031812094151973724\n",
      "Epoch 94 Batch: 84 Train Loss: 0.09409983456134796\n",
      "Epoch 94 Batch: 86 Train Loss: 0.29094499349594116\n",
      "Epoch 94 Batch: 88 Train Loss: 0.0823339894413948\n",
      "Epoch 94 Batch: 90 Train Loss: 0.050041668117046356\n",
      "Epoch 94 Batch: 92 Train Loss: 0.2744446098804474\n",
      "Epoch 94 Batch: 94 Train Loss: 0.01920127496123314\n",
      "Epoch 94 Batch: 96 Train Loss: 0.06518402695655823\n",
      "Epoch 94 Batch: 98 Train Loss: 0.0666353777050972\n",
      "Epoch 94 Batch: 100 Train Loss: 0.3074425160884857\n",
      "Epoch 94 Batch: 102 Train Loss: 0.28787973523139954\n",
      "Epoch 94 Batch: 104 Train Loss: 0.27713543176651\n",
      "Epoch 94 Batch: 106 Train Loss: 0.055642951279878616\n",
      "Epoch 94 Batch: 108 Train Loss: 0.06445880234241486\n",
      "Epoch 94 Batch: 110 Train Loss: 0.01814928650856018\n",
      "Epoch 94 Batch: 112 Train Loss: 0.2964497208595276\n",
      "Epoch 94 Batch: 114 Train Loss: 0.7854224443435669\n",
      "Epoch 94 Batch: 116 Train Loss: 0.2794994115829468\n",
      "Epoch 94 Batch: 118 Train Loss: 0.060504816472530365\n",
      "Epoch 94 Batch: 120 Train Loss: 0.05338073894381523\n",
      "Epoch 94 Batch: 122 Train Loss: 0.29605036973953247\n",
      "Epoch 94 Batch: 124 Train Loss: 0.3001328110694885\n",
      "Epoch 94 Batch: 126 Train Loss: 0.046679504215717316\n",
      "Epoch 94 Batch: 128 Train Loss: 0.06628865003585815\n",
      "Epoch 94 Batch: 130 Train Loss: 0.06172041967511177\n",
      "Epoch 94 Batch: 132 Train Loss: 0.04560774564743042\n",
      "Epoch 94 Batch: 134 Train Loss: 0.04414018243551254\n",
      "Epoch 94 Batch: 136 Train Loss: 0.2650081515312195\n",
      "Epoch 94 Batch: 138 Train Loss: 0.04959937557578087\n",
      "Epoch 94 Batch: 140 Train Loss: 0.5423593521118164\n",
      "Epoch 94 Batch: 142 Train Loss: 0.27992916107177734\n",
      "Epoch 94 Batch: 144 Train Loss: 0.28080081939697266\n",
      "Epoch 94 Batch: 146 Train Loss: 0.044636428356170654\n",
      "Epoch 94 Batch: 148 Train Loss: 0.07343529164791107\n",
      "Epoch 94 Batch: 150 Train Loss: 0.03655734658241272\n",
      "Epoch 94 Batch: 152 Train Loss: 0.05521804094314575\n",
      "Epoch 94 Batch: 154 Train Loss: 0.02611733041703701\n",
      "Epoch 94 Batch: 156 Train Loss: 0.30085763335227966\n",
      "Epoch 94 Batch: 158 Train Loss: 0.05141760781407356\n",
      "Epoch 94 Batch: 160 Train Loss: 0.05159689858555794\n",
      "Epoch 95 Batch: 2 Train Loss: 0.04104218631982803\n",
      "Epoch 95 Batch: 4 Train Loss: 0.05509241297841072\n",
      "Epoch 95 Batch: 6 Train Loss: 0.31055569648742676\n",
      "Epoch 95 Batch: 8 Train Loss: 0.2772102355957031\n",
      "Epoch 95 Batch: 10 Train Loss: 0.044487107545137405\n",
      "Epoch 95 Batch: 12 Train Loss: 0.3058086037635803\n",
      "Epoch 95 Batch: 14 Train Loss: 0.06510009616613388\n",
      "Epoch 95 Batch: 16 Train Loss: 0.28811806440353394\n",
      "Epoch 95 Batch: 18 Train Loss: 0.30971860885620117\n",
      "Epoch 95 Batch: 20 Train Loss: 0.013270428404211998\n",
      "Epoch 95 Batch: 22 Train Loss: 0.332868367433548\n",
      "Epoch 95 Batch: 24 Train Loss: 0.04460905119776726\n",
      "Epoch 95 Batch: 26 Train Loss: 0.2982177734375\n",
      "Epoch 95 Batch: 28 Train Loss: 0.04626992344856262\n",
      "Epoch 95 Batch: 30 Train Loss: 0.5726341605186462\n",
      "Epoch 95 Batch: 32 Train Loss: 0.31475940346717834\n",
      "Epoch 95 Batch: 34 Train Loss: 0.04052503779530525\n",
      "Epoch 95 Batch: 36 Train Loss: 0.03709668666124344\n",
      "Epoch 95 Batch: 38 Train Loss: 0.059942107647657394\n",
      "Epoch 95 Batch: 40 Train Loss: 0.027812400832772255\n",
      "Epoch 95 Batch: 42 Train Loss: 0.3120623230934143\n",
      "Epoch 95 Batch: 44 Train Loss: 0.5297406315803528\n",
      "Epoch 95 Batch: 46 Train Loss: 0.023019613698124886\n",
      "Epoch 95 Batch: 48 Train Loss: 0.2930249869823456\n",
      "Epoch 95 Batch: 50 Train Loss: 0.30276215076446533\n",
      "Epoch 95 Batch: 52 Train Loss: 0.02109588123857975\n",
      "Epoch 95 Batch: 54 Train Loss: 0.3157791793346405\n",
      "Epoch 95 Batch: 56 Train Loss: 0.8345962762832642\n",
      "Epoch 95 Batch: 58 Train Loss: 0.04019611328840256\n",
      "Epoch 95 Batch: 60 Train Loss: 0.044440921396017075\n",
      "Epoch 95 Batch: 62 Train Loss: 0.3044095039367676\n",
      "Epoch 95 Batch: 64 Train Loss: 0.05787003040313721\n",
      "Epoch 95 Batch: 66 Train Loss: 0.295643150806427\n",
      "Epoch 95 Batch: 68 Train Loss: 0.040991444140672684\n",
      "Epoch 95 Batch: 70 Train Loss: 0.05073549225926399\n",
      "Epoch 95 Batch: 72 Train Loss: 0.051845718175172806\n",
      "Epoch 95 Batch: 74 Train Loss: 0.04129713028669357\n",
      "Epoch 95 Batch: 76 Train Loss: 0.03451774641871452\n",
      "Epoch 95 Batch: 78 Train Loss: 0.06928074359893799\n",
      "Epoch 95 Batch: 80 Train Loss: 0.042622603476047516\n",
      "Epoch 95 Batch: 82 Train Loss: 0.2810423672199249\n",
      "Epoch 95 Batch: 84 Train Loss: 0.2663622498512268\n",
      "Epoch 95 Batch: 86 Train Loss: 0.04230597987771034\n",
      "Epoch 95 Batch: 88 Train Loss: 0.2950493395328522\n",
      "Epoch 95 Batch: 90 Train Loss: 0.291143000125885\n",
      "Epoch 95 Batch: 92 Train Loss: 0.27337560057640076\n",
      "Epoch 95 Batch: 94 Train Loss: 0.05424423888325691\n",
      "Epoch 95 Batch: 96 Train Loss: 0.24147315323352814\n",
      "Epoch 95 Batch: 98 Train Loss: 0.301238477230072\n",
      "Epoch 95 Batch: 100 Train Loss: 0.06284455209970474\n",
      "Epoch 95 Batch: 102 Train Loss: 0.024370495229959488\n",
      "Epoch 95 Batch: 104 Train Loss: 0.3254493176937103\n",
      "Epoch 95 Batch: 106 Train Loss: 0.29925602674484253\n",
      "Epoch 95 Batch: 108 Train Loss: 0.05100554972887039\n",
      "Epoch 95 Batch: 110 Train Loss: 0.11291162669658661\n",
      "Epoch 95 Batch: 112 Train Loss: 0.07432980835437775\n",
      "Epoch 95 Batch: 114 Train Loss: 0.062068186700344086\n",
      "Epoch 95 Batch: 116 Train Loss: 0.2709377706050873\n",
      "Epoch 95 Batch: 118 Train Loss: 0.07024452835321426\n",
      "Epoch 95 Batch: 120 Train Loss: 0.2842385172843933\n",
      "Epoch 95 Batch: 122 Train Loss: 0.056558676064014435\n",
      "Epoch 95 Batch: 124 Train Loss: 0.04378011077642441\n",
      "Epoch 95 Batch: 126 Train Loss: 0.07580898702144623\n",
      "Epoch 95 Batch: 128 Train Loss: 0.0773848295211792\n",
      "Epoch 95 Batch: 130 Train Loss: 0.2714390754699707\n",
      "Epoch 95 Batch: 132 Train Loss: 0.06259328126907349\n",
      "Epoch 95 Batch: 134 Train Loss: 0.058538131415843964\n",
      "Epoch 95 Batch: 136 Train Loss: 0.0486990287899971\n",
      "Epoch 95 Batch: 138 Train Loss: 0.2916127145290375\n",
      "Epoch 95 Batch: 140 Train Loss: 0.06544872373342514\n",
      "Epoch 95 Batch: 142 Train Loss: 0.06518809497356415\n",
      "Epoch 95 Batch: 144 Train Loss: 0.052251655608415604\n",
      "Epoch 95 Batch: 146 Train Loss: 0.2984902262687683\n",
      "Epoch 95 Batch: 148 Train Loss: 0.7751126289367676\n",
      "Epoch 95 Batch: 150 Train Loss: 0.07089319825172424\n",
      "Epoch 95 Batch: 152 Train Loss: 0.2883695960044861\n",
      "Epoch 95 Batch: 154 Train Loss: 0.2956140637397766\n",
      "Epoch 95 Batch: 156 Train Loss: 0.27156028151512146\n",
      "Epoch 95 Batch: 158 Train Loss: 0.06180800870060921\n",
      "Epoch 95 Batch: 160 Train Loss: 0.5203464031219482\n",
      "Epoch 96 Batch: 2 Train Loss: 0.03413073718547821\n",
      "Epoch 96 Batch: 4 Train Loss: 0.028278488665819168\n",
      "Epoch 96 Batch: 6 Train Loss: 0.5055939555168152\n",
      "Epoch 96 Batch: 8 Train Loss: 0.03751324489712715\n",
      "Epoch 96 Batch: 10 Train Loss: 0.059373557567596436\n",
      "Epoch 96 Batch: 12 Train Loss: 0.31418249011039734\n",
      "Epoch 96 Batch: 14 Train Loss: 0.28331607580184937\n",
      "Epoch 96 Batch: 16 Train Loss: 0.535058856010437\n",
      "Epoch 96 Batch: 18 Train Loss: 0.06761468201875687\n",
      "Epoch 96 Batch: 20 Train Loss: 0.05047852545976639\n",
      "Epoch 96 Batch: 22 Train Loss: 0.518566906452179\n",
      "Epoch 96 Batch: 24 Train Loss: 0.2876291871070862\n",
      "Epoch 96 Batch: 26 Train Loss: 0.03414943441748619\n",
      "Epoch 96 Batch: 28 Train Loss: 0.07894888520240784\n",
      "Epoch 96 Batch: 30 Train Loss: 0.07059741765260696\n",
      "Epoch 96 Batch: 32 Train Loss: 0.06937284767627716\n",
      "Epoch 96 Batch: 34 Train Loss: 0.09647010266780853\n",
      "Epoch 96 Batch: 36 Train Loss: 0.05370022729039192\n",
      "Epoch 96 Batch: 38 Train Loss: 0.04768950492143631\n",
      "Epoch 96 Batch: 40 Train Loss: 0.04920204356312752\n",
      "Epoch 96 Batch: 42 Train Loss: 0.06369246542453766\n",
      "Epoch 96 Batch: 44 Train Loss: 0.045441679656505585\n",
      "Epoch 96 Batch: 46 Train Loss: 0.053944848477840424\n",
      "Epoch 96 Batch: 48 Train Loss: 0.05608730390667915\n",
      "Epoch 96 Batch: 50 Train Loss: 0.2736913859844208\n",
      "Epoch 96 Batch: 52 Train Loss: 0.28571444749832153\n",
      "Epoch 96 Batch: 54 Train Loss: 0.28540465235710144\n",
      "Epoch 96 Batch: 56 Train Loss: 0.2572711706161499\n",
      "Epoch 96 Batch: 58 Train Loss: 0.06923813372850418\n",
      "Epoch 96 Batch: 60 Train Loss: 0.28709664940834045\n",
      "Epoch 96 Batch: 62 Train Loss: 0.07680870592594147\n",
      "Epoch 96 Batch: 64 Train Loss: 0.25585228204727173\n",
      "Epoch 96 Batch: 66 Train Loss: 0.046953100711107254\n",
      "Epoch 96 Batch: 68 Train Loss: 0.07028377801179886\n",
      "Epoch 96 Batch: 70 Train Loss: 0.08074798434972763\n",
      "Epoch 96 Batch: 72 Train Loss: 0.2892768979072571\n",
      "Epoch 96 Batch: 74 Train Loss: 0.2785368263721466\n",
      "Epoch 96 Batch: 76 Train Loss: 0.08587819337844849\n",
      "Epoch 96 Batch: 78 Train Loss: 0.053914088755846024\n",
      "Epoch 96 Batch: 80 Train Loss: 0.26164939999580383\n",
      "Epoch 96 Batch: 82 Train Loss: 0.05189456790685654\n",
      "Epoch 96 Batch: 84 Train Loss: 0.06101488322019577\n",
      "Epoch 96 Batch: 86 Train Loss: 0.07758532464504242\n",
      "Epoch 96 Batch: 88 Train Loss: 0.03652702644467354\n",
      "Epoch 96 Batch: 90 Train Loss: 0.5410239100456238\n",
      "Epoch 96 Batch: 92 Train Loss: 0.03577408567070961\n",
      "Epoch 96 Batch: 94 Train Loss: 0.08229798078536987\n",
      "Epoch 96 Batch: 96 Train Loss: 0.038139037787914276\n",
      "Epoch 96 Batch: 98 Train Loss: 0.05590306594967842\n",
      "Epoch 96 Batch: 100 Train Loss: 0.2979726791381836\n",
      "Epoch 96 Batch: 102 Train Loss: 0.03819943964481354\n",
      "Epoch 96 Batch: 104 Train Loss: 0.04643217474222183\n",
      "Epoch 96 Batch: 106 Train Loss: 0.04509032145142555\n",
      "Epoch 96 Batch: 108 Train Loss: 0.2877523899078369\n",
      "Epoch 96 Batch: 110 Train Loss: 0.28882384300231934\n",
      "Epoch 96 Batch: 112 Train Loss: 0.05874352529644966\n",
      "Epoch 96 Batch: 114 Train Loss: 0.30092087388038635\n",
      "Epoch 96 Batch: 116 Train Loss: 0.05165079981088638\n",
      "Epoch 96 Batch: 118 Train Loss: 0.5034173727035522\n",
      "Epoch 96 Batch: 120 Train Loss: 0.04027080908417702\n",
      "Epoch 96 Batch: 122 Train Loss: 0.05253518745303154\n",
      "Epoch 96 Batch: 124 Train Loss: 0.24726228415966034\n",
      "Epoch 96 Batch: 126 Train Loss: 0.4956057667732239\n",
      "Epoch 96 Batch: 128 Train Loss: 0.5213794112205505\n",
      "Epoch 96 Batch: 130 Train Loss: 0.4937266409397125\n",
      "Epoch 96 Batch: 132 Train Loss: 0.06388545781373978\n",
      "Epoch 96 Batch: 134 Train Loss: 0.02203121967613697\n",
      "Epoch 96 Batch: 136 Train Loss: 0.07377057522535324\n",
      "Epoch 96 Batch: 138 Train Loss: 0.3016635775566101\n",
      "Epoch 96 Batch: 140 Train Loss: 0.0428011417388916\n",
      "Epoch 96 Batch: 142 Train Loss: 0.06517315655946732\n",
      "Epoch 96 Batch: 144 Train Loss: 0.07339903712272644\n",
      "Epoch 96 Batch: 146 Train Loss: 0.05649198219180107\n",
      "Epoch 96 Batch: 148 Train Loss: 0.009843753650784492\n",
      "Epoch 96 Batch: 150 Train Loss: 0.31293439865112305\n",
      "Epoch 96 Batch: 152 Train Loss: 0.009093662723898888\n",
      "Epoch 96 Batch: 154 Train Loss: 0.050972867757081985\n",
      "Epoch 96 Batch: 156 Train Loss: 0.0345781147480011\n",
      "Epoch 96 Batch: 158 Train Loss: 0.5401651263237\n",
      "Epoch 96 Batch: 160 Train Loss: 0.051362551748752594\n",
      "Epoch 97 Batch: 2 Train Loss: 0.042792029678821564\n",
      "Epoch 97 Batch: 4 Train Loss: 0.29414308071136475\n",
      "Epoch 97 Batch: 6 Train Loss: 0.3156699240207672\n",
      "Epoch 97 Batch: 8 Train Loss: 0.05737181752920151\n",
      "Epoch 97 Batch: 10 Train Loss: 0.28418755531311035\n",
      "Epoch 97 Batch: 12 Train Loss: 0.059646040201187134\n",
      "Epoch 97 Batch: 14 Train Loss: 0.06313537806272507\n",
      "Epoch 97 Batch: 16 Train Loss: 0.27889886498451233\n",
      "Epoch 97 Batch: 18 Train Loss: 0.053690262138843536\n",
      "Epoch 97 Batch: 20 Train Loss: 0.03469591215252876\n",
      "Epoch 97 Batch: 22 Train Loss: 0.045431338250637054\n",
      "Epoch 97 Batch: 24 Train Loss: 0.49950018525123596\n",
      "Epoch 97 Batch: 26 Train Loss: 0.044476937502622604\n",
      "Epoch 97 Batch: 28 Train Loss: 0.26499754190444946\n",
      "Epoch 97 Batch: 30 Train Loss: 0.03325764462351799\n",
      "Epoch 97 Batch: 32 Train Loss: 0.04308368265628815\n",
      "Epoch 97 Batch: 34 Train Loss: 0.07522359490394592\n",
      "Epoch 97 Batch: 36 Train Loss: 0.2596518397331238\n",
      "Epoch 97 Batch: 38 Train Loss: 0.3043786585330963\n",
      "Epoch 97 Batch: 40 Train Loss: 0.08228007704019547\n",
      "Epoch 97 Batch: 42 Train Loss: 0.28245142102241516\n",
      "Epoch 97 Batch: 44 Train Loss: 0.3107419013977051\n",
      "Epoch 97 Batch: 46 Train Loss: 0.07362692058086395\n",
      "Epoch 97 Batch: 48 Train Loss: 0.05541292950510979\n",
      "Epoch 97 Batch: 50 Train Loss: 0.2883434593677521\n",
      "Epoch 97 Batch: 52 Train Loss: 0.0438099130988121\n",
      "Epoch 97 Batch: 54 Train Loss: 0.27853989601135254\n",
      "Epoch 97 Batch: 56 Train Loss: 0.3036649525165558\n",
      "Epoch 97 Batch: 58 Train Loss: 0.3145914077758789\n",
      "Epoch 97 Batch: 60 Train Loss: 0.06654996424913406\n",
      "Epoch 97 Batch: 62 Train Loss: 0.028016384690999985\n",
      "Epoch 97 Batch: 64 Train Loss: 0.038520943373441696\n",
      "Epoch 97 Batch: 66 Train Loss: 0.05790191888809204\n",
      "Epoch 97 Batch: 68 Train Loss: 0.028630340471863747\n",
      "Epoch 97 Batch: 70 Train Loss: 0.046245187520980835\n",
      "Epoch 97 Batch: 72 Train Loss: 0.2857850193977356\n",
      "Epoch 97 Batch: 74 Train Loss: 0.30495452880859375\n",
      "Epoch 97 Batch: 76 Train Loss: 0.043236441910266876\n",
      "Epoch 97 Batch: 78 Train Loss: 0.2999991774559021\n",
      "Epoch 97 Batch: 80 Train Loss: 0.03410933539271355\n",
      "Epoch 97 Batch: 82 Train Loss: 0.28087979555130005\n",
      "Epoch 97 Batch: 84 Train Loss: 0.033490139991045\n",
      "Epoch 97 Batch: 86 Train Loss: 0.02393847517669201\n",
      "Epoch 97 Batch: 88 Train Loss: 0.06303222477436066\n",
      "Epoch 97 Batch: 90 Train Loss: 0.3038176894187927\n",
      "Epoch 97 Batch: 92 Train Loss: 0.06067264825105667\n",
      "Epoch 97 Batch: 94 Train Loss: 0.036756210029125214\n",
      "Epoch 97 Batch: 96 Train Loss: 0.3117172122001648\n",
      "Epoch 97 Batch: 98 Train Loss: 0.04225478321313858\n",
      "Epoch 97 Batch: 100 Train Loss: 0.03987831622362137\n",
      "Epoch 97 Batch: 102 Train Loss: 0.03184027224779129\n",
      "Epoch 97 Batch: 104 Train Loss: 0.026496609672904015\n",
      "Epoch 97 Batch: 106 Train Loss: 0.3067711293697357\n",
      "Epoch 97 Batch: 108 Train Loss: 0.04516427591443062\n",
      "Epoch 97 Batch: 110 Train Loss: 0.0404994897544384\n",
      "Epoch 97 Batch: 112 Train Loss: 0.037727609276771545\n",
      "Epoch 97 Batch: 114 Train Loss: 0.31788864731788635\n",
      "Epoch 97 Batch: 116 Train Loss: 0.039865776896476746\n",
      "Epoch 97 Batch: 118 Train Loss: 0.03964307904243469\n",
      "Epoch 97 Batch: 120 Train Loss: 0.32979872822761536\n",
      "Epoch 97 Batch: 122 Train Loss: 0.5445699095726013\n",
      "Epoch 97 Batch: 124 Train Loss: 0.024224618449807167\n",
      "Epoch 97 Batch: 126 Train Loss: 0.04285629838705063\n",
      "Epoch 97 Batch: 128 Train Loss: 0.06891892850399017\n",
      "Epoch 97 Batch: 130 Train Loss: 0.04324380308389664\n",
      "Epoch 97 Batch: 132 Train Loss: 0.27800866961479187\n",
      "Epoch 97 Batch: 134 Train Loss: 0.05423814058303833\n",
      "Epoch 97 Batch: 136 Train Loss: 0.5244490504264832\n",
      "Epoch 97 Batch: 138 Train Loss: 0.2836695611476898\n",
      "Epoch 97 Batch: 140 Train Loss: 0.7513348460197449\n",
      "Epoch 97 Batch: 142 Train Loss: 0.06999664008617401\n",
      "Epoch 97 Batch: 144 Train Loss: 0.2750939428806305\n",
      "Epoch 97 Batch: 146 Train Loss: 0.24849148094654083\n",
      "Epoch 97 Batch: 148 Train Loss: 0.5071380138397217\n",
      "Epoch 97 Batch: 150 Train Loss: 0.2773129642009735\n",
      "Epoch 97 Batch: 152 Train Loss: 0.07260479032993317\n",
      "Epoch 97 Batch: 154 Train Loss: 0.06657028943300247\n",
      "Epoch 97 Batch: 156 Train Loss: 0.29503539204597473\n",
      "Epoch 97 Batch: 158 Train Loss: 0.08043026924133301\n",
      "Epoch 97 Batch: 160 Train Loss: 0.2843324542045593\n",
      "Epoch 98 Batch: 2 Train Loss: 0.4366120398044586\n",
      "Epoch 98 Batch: 4 Train Loss: 0.06611774861812592\n",
      "Epoch 98 Batch: 6 Train Loss: 0.08314280956983566\n",
      "Epoch 98 Batch: 8 Train Loss: 0.08378700911998749\n",
      "Epoch 98 Batch: 10 Train Loss: 0.08291260898113251\n",
      "Epoch 98 Batch: 12 Train Loss: 0.6727699041366577\n",
      "Epoch 98 Batch: 14 Train Loss: 0.10754338651895523\n",
      "Epoch 98 Batch: 16 Train Loss: 0.43705636262893677\n",
      "Epoch 98 Batch: 18 Train Loss: 0.05379648134112358\n",
      "Epoch 98 Batch: 20 Train Loss: 0.06791697442531586\n",
      "Epoch 98 Batch: 22 Train Loss: 0.12263895571231842\n",
      "Epoch 98 Batch: 24 Train Loss: 0.2793963551521301\n",
      "Epoch 98 Batch: 26 Train Loss: 0.04095374420285225\n",
      "Epoch 98 Batch: 28 Train Loss: 0.2801743745803833\n",
      "Epoch 98 Batch: 30 Train Loss: 0.04044318199157715\n",
      "Epoch 98 Batch: 32 Train Loss: 0.2919155955314636\n",
      "Epoch 98 Batch: 34 Train Loss: 0.06654982268810272\n",
      "Epoch 98 Batch: 36 Train Loss: 0.0783611387014389\n",
      "Epoch 98 Batch: 38 Train Loss: 0.4482116103172302\n",
      "Epoch 98 Batch: 40 Train Loss: 0.2565501630306244\n",
      "Epoch 98 Batch: 42 Train Loss: 0.30718478560447693\n",
      "Epoch 98 Batch: 44 Train Loss: 0.07555896788835526\n",
      "Epoch 98 Batch: 46 Train Loss: 0.052939433604478836\n",
      "Epoch 98 Batch: 48 Train Loss: 0.05335381627082825\n",
      "Epoch 98 Batch: 50 Train Loss: 0.02504987083375454\n",
      "Epoch 98 Batch: 52 Train Loss: 0.2558574378490448\n",
      "Epoch 98 Batch: 54 Train Loss: 0.061903975903987885\n",
      "Epoch 98 Batch: 56 Train Loss: 0.27819710969924927\n",
      "Epoch 98 Batch: 58 Train Loss: 0.25300097465515137\n",
      "Epoch 98 Batch: 60 Train Loss: 0.07141882181167603\n",
      "Epoch 98 Batch: 62 Train Loss: 0.049092501401901245\n",
      "Epoch 98 Batch: 64 Train Loss: 0.07093998789787292\n",
      "Epoch 98 Batch: 66 Train Loss: 0.10195913165807724\n",
      "Epoch 98 Batch: 68 Train Loss: 0.06365460157394409\n",
      "Epoch 98 Batch: 70 Train Loss: 0.06954964995384216\n",
      "Epoch 98 Batch: 72 Train Loss: 0.29536956548690796\n",
      "Epoch 98 Batch: 74 Train Loss: 0.28883346915245056\n",
      "Epoch 98 Batch: 76 Train Loss: 0.042369596660137177\n",
      "Epoch 98 Batch: 78 Train Loss: 0.2857514023780823\n",
      "Epoch 98 Batch: 80 Train Loss: 0.03918441757559776\n",
      "Epoch 98 Batch: 82 Train Loss: 0.032085318118333817\n",
      "Epoch 98 Batch: 84 Train Loss: 0.5468187928199768\n",
      "Epoch 98 Batch: 86 Train Loss: 0.5372812747955322\n",
      "Epoch 98 Batch: 88 Train Loss: 0.06871158629655838\n",
      "Epoch 98 Batch: 90 Train Loss: 0.2733810544013977\n",
      "Epoch 98 Batch: 92 Train Loss: 0.03149149939417839\n",
      "Epoch 98 Batch: 94 Train Loss: 0.2824917733669281\n",
      "Epoch 98 Batch: 96 Train Loss: 0.29318663477897644\n",
      "Epoch 98 Batch: 98 Train Loss: 0.043310798704624176\n",
      "Epoch 98 Batch: 100 Train Loss: 0.3007906377315521\n",
      "Epoch 98 Batch: 102 Train Loss: 0.055786799639463425\n",
      "Epoch 98 Batch: 104 Train Loss: 0.07866154611110687\n",
      "Epoch 98 Batch: 106 Train Loss: 0.0445140041410923\n",
      "Epoch 98 Batch: 108 Train Loss: 0.30658096075057983\n",
      "Epoch 98 Batch: 110 Train Loss: 0.04396142438054085\n",
      "Epoch 98 Batch: 112 Train Loss: 0.04276967793703079\n",
      "Epoch 98 Batch: 114 Train Loss: 0.041888508945703506\n",
      "Epoch 98 Batch: 116 Train Loss: 0.284930557012558\n",
      "Epoch 98 Batch: 118 Train Loss: 0.2854842245578766\n",
      "Epoch 98 Batch: 120 Train Loss: 0.28379026055336\n",
      "Epoch 98 Batch: 122 Train Loss: 0.30161526799201965\n",
      "Epoch 98 Batch: 124 Train Loss: 0.2849891781806946\n",
      "Epoch 98 Batch: 126 Train Loss: 0.2882642447948456\n",
      "Epoch 98 Batch: 128 Train Loss: 0.26267021894454956\n",
      "Epoch 98 Batch: 130 Train Loss: 0.06029818207025528\n",
      "Epoch 98 Batch: 132 Train Loss: 0.039050064980983734\n",
      "Epoch 98 Batch: 134 Train Loss: 0.059460848569869995\n",
      "Epoch 98 Batch: 136 Train Loss: 0.26908355951309204\n",
      "Epoch 98 Batch: 138 Train Loss: 0.05767412856221199\n",
      "Epoch 98 Batch: 140 Train Loss: 0.5001726746559143\n",
      "Epoch 98 Batch: 142 Train Loss: 0.04648687690496445\n",
      "Epoch 98 Batch: 144 Train Loss: 0.272706001996994\n",
      "Epoch 98 Batch: 146 Train Loss: 0.018629956990480423\n",
      "Epoch 98 Batch: 148 Train Loss: 0.05364171415567398\n",
      "Epoch 98 Batch: 150 Train Loss: 0.2934224009513855\n",
      "Epoch 98 Batch: 152 Train Loss: 0.043480511754751205\n",
      "Epoch 98 Batch: 154 Train Loss: 0.07742448896169662\n",
      "Epoch 98 Batch: 156 Train Loss: 0.034832291305065155\n",
      "Epoch 98 Batch: 158 Train Loss: 0.06658633053302765\n",
      "Epoch 98 Batch: 160 Train Loss: 0.06064165383577347\n",
      "Epoch 99 Batch: 2 Train Loss: 0.2770484685897827\n",
      "Epoch 99 Batch: 4 Train Loss: 0.04169908165931702\n",
      "Epoch 99 Batch: 6 Train Loss: 0.30528777837753296\n",
      "Epoch 99 Batch: 8 Train Loss: 0.032362304627895355\n",
      "Epoch 99 Batch: 10 Train Loss: 0.032228607684373856\n",
      "Epoch 99 Batch: 12 Train Loss: 0.3300483226776123\n",
      "Epoch 99 Batch: 14 Train Loss: 0.031172752380371094\n",
      "Epoch 99 Batch: 16 Train Loss: 0.2989242672920227\n",
      "Epoch 99 Batch: 18 Train Loss: 0.039170555770397186\n",
      "Epoch 99 Batch: 20 Train Loss: 0.03863321617245674\n",
      "Epoch 99 Batch: 22 Train Loss: 0.04014396667480469\n",
      "Epoch 99 Batch: 24 Train Loss: 0.03349814936518669\n",
      "Epoch 99 Batch: 26 Train Loss: 0.05479267239570618\n",
      "Epoch 99 Batch: 28 Train Loss: 0.02751806005835533\n",
      "Epoch 99 Batch: 30 Train Loss: 0.037632912397384644\n",
      "Epoch 99 Batch: 32 Train Loss: 0.2962687313556671\n",
      "Epoch 99 Batch: 34 Train Loss: 0.04233697056770325\n",
      "Epoch 99 Batch: 36 Train Loss: 0.03230229765176773\n",
      "Epoch 99 Batch: 38 Train Loss: 0.3160250186920166\n",
      "Epoch 99 Batch: 40 Train Loss: 0.02738766372203827\n",
      "Epoch 99 Batch: 42 Train Loss: 0.3232015073299408\n",
      "Epoch 99 Batch: 44 Train Loss: 0.051408689469099045\n",
      "Epoch 99 Batch: 46 Train Loss: 0.3090673089027405\n",
      "Epoch 99 Batch: 48 Train Loss: 0.054741792380809784\n",
      "Epoch 99 Batch: 50 Train Loss: 0.026549886912107468\n",
      "Epoch 99 Batch: 52 Train Loss: 0.04219408333301544\n",
      "Epoch 99 Batch: 54 Train Loss: 0.037506889551877975\n",
      "Epoch 99 Batch: 56 Train Loss: 0.018749844282865524\n",
      "Epoch 99 Batch: 58 Train Loss: 0.5792386531829834\n",
      "Epoch 99 Batch: 60 Train Loss: 0.03605525568127632\n",
      "Epoch 99 Batch: 62 Train Loss: 0.02011004462838173\n",
      "Epoch 99 Batch: 64 Train Loss: 0.03685425966978073\n",
      "Epoch 99 Batch: 66 Train Loss: 0.28220945596694946\n",
      "Epoch 99 Batch: 68 Train Loss: 0.3076919913291931\n",
      "Epoch 99 Batch: 70 Train Loss: 0.28462639451026917\n",
      "Epoch 99 Batch: 72 Train Loss: 0.04960765689611435\n",
      "Epoch 99 Batch: 74 Train Loss: 0.03369905799627304\n",
      "Epoch 99 Batch: 76 Train Loss: 0.27731427550315857\n",
      "Epoch 99 Batch: 78 Train Loss: 0.054070454090833664\n",
      "Epoch 99 Batch: 80 Train Loss: 0.05400080606341362\n",
      "Epoch 99 Batch: 82 Train Loss: 0.301588773727417\n",
      "Epoch 99 Batch: 84 Train Loss: 0.0610174722969532\n",
      "Epoch 99 Batch: 86 Train Loss: 0.08014663308858871\n",
      "Epoch 99 Batch: 88 Train Loss: 0.29983383417129517\n",
      "Epoch 99 Batch: 90 Train Loss: 0.28425997495651245\n",
      "Epoch 99 Batch: 92 Train Loss: 0.2960476279258728\n",
      "Epoch 99 Batch: 94 Train Loss: 0.052886854857206345\n",
      "Epoch 99 Batch: 96 Train Loss: 0.026863059028983116\n",
      "Epoch 99 Batch: 98 Train Loss: 0.2983631491661072\n",
      "Epoch 99 Batch: 100 Train Loss: 0.2936795949935913\n",
      "Epoch 99 Batch: 102 Train Loss: 0.509437620639801\n",
      "Epoch 99 Batch: 104 Train Loss: 0.5084661841392517\n",
      "Epoch 99 Batch: 106 Train Loss: 0.2625541687011719\n",
      "Epoch 99 Batch: 108 Train Loss: 0.23868373036384583\n",
      "Epoch 99 Batch: 110 Train Loss: 0.07645069062709808\n",
      "Epoch 99 Batch: 112 Train Loss: 0.07949279993772507\n",
      "Epoch 99 Batch: 114 Train Loss: 0.09476952254772186\n",
      "Epoch 99 Batch: 116 Train Loss: 0.05400659516453743\n",
      "Epoch 99 Batch: 118 Train Loss: 0.07559560984373093\n",
      "Epoch 99 Batch: 120 Train Loss: 0.061481256037950516\n",
      "Epoch 99 Batch: 122 Train Loss: 0.2896552085876465\n",
      "Epoch 99 Batch: 124 Train Loss: 0.09211310744285583\n",
      "Epoch 99 Batch: 126 Train Loss: 0.2621563673019409\n",
      "Epoch 99 Batch: 128 Train Loss: 0.06502605229616165\n",
      "Epoch 99 Batch: 130 Train Loss: 0.2620839476585388\n",
      "Epoch 99 Batch: 132 Train Loss: 0.27070996165275574\n",
      "Epoch 99 Batch: 134 Train Loss: 0.7354589700698853\n",
      "Epoch 99 Batch: 136 Train Loss: 0.0745416060090065\n",
      "Epoch 99 Batch: 138 Train Loss: 0.06381136924028397\n",
      "Epoch 99 Batch: 140 Train Loss: 0.06564614921808243\n",
      "Epoch 99 Batch: 142 Train Loss: 0.27270591259002686\n",
      "Epoch 99 Batch: 144 Train Loss: 0.09218204021453857\n",
      "Epoch 99 Batch: 146 Train Loss: 0.29536789655685425\n",
      "Epoch 99 Batch: 148 Train Loss: 0.2855529487133026\n",
      "Epoch 99 Batch: 150 Train Loss: 0.5056841373443604\n",
      "Epoch 99 Batch: 152 Train Loss: 0.5296897888183594\n",
      "Epoch 99 Batch: 154 Train Loss: 0.04067098721861839\n",
      "Epoch 99 Batch: 156 Train Loss: 0.2695462107658386\n",
      "Epoch 99 Batch: 158 Train Loss: 0.09042789787054062\n",
      "Epoch 99 Batch: 160 Train Loss: 0.2764132618904114\n"
     ]
    }
   ],
   "source": [
    "#from ignite.metrics import Precision, Recall\n",
    "#from ignite.metrics import Precision ### LGG TILL IGNITE\n",
    "#train_precision = Precision()\n",
    "#train_recall = Recall()\n",
    "\n",
    "#test_precision = Precision()\n",
    "#test_recall = Recall()\n",
    "# https://pytorch.org/ignite/metrics.html\n",
    "\n",
    "\n",
    "epochs = 100 # Ju strra dataset ju frre epochs behvs\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_correct = []\n",
    "test_correct = []\n",
    "\n",
    "# For loop epochs \n",
    "\n",
    "for i in range(epochs):\n",
    "    trn_correct = 0\n",
    "    tst_correct = 0 \n",
    "\n",
    "    # Train\n",
    "\n",
    "    for b, (X_train, y_train) in enumerate(train_loader):\n",
    "        \n",
    "        # Skip iteration if batch size not equal to stated dim\n",
    "        \n",
    "            \n",
    "        #print(X_train.shape, y_train.shape) \n",
    "        \n",
    "        b += 1\n",
    "        \n",
    "        y_pred = model(X_train.view(batch_size, -1))  # Flatten input\n",
    "        lossTrain = criterion(y_pred, y_train)\n",
    "\n",
    "        predicted = torch.max(y_pred.data,1)[1]\n",
    "\n",
    "        #calculate precision and recall\n",
    "        #train_precision.update((y_pred, y_train))\n",
    "        #train_recall.update((y_pred, y_train))\n",
    "      \n",
    "\n",
    "        batch_corr = (predicted == y_train).sum()\n",
    "        trn_correct += batch_corr\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        lossTrain.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if b%2 == 0: \n",
    "            print(f\"Epoch {i} Batch: {b} Train Loss: {lossTrain.item()}\")\n",
    "\n",
    "    train_losses.append(lossTrain.data.item())\n",
    "    train_correct.append(trn_correct)\n",
    "\n",
    "    # Test\n",
    "    with torch.no_grad():\n",
    "        for b, (X_test,y_test) in enumerate(test_loader):\n",
    "            y_val = model(X_train.view(batch_size, -1))\n",
    "\n",
    "            predicted = torch.max(y_val.data,1)[1]\n",
    "\n",
    "            \n",
    "        loss = criterion(y_val, y_test)\n",
    "        test_losses.append(loss)\n",
    "        test_correct.append(trn_correct)\n",
    "\n",
    "        #if b%2 == 0:\n",
    "            #print(f\"Epoch {i} Batch: {b} Train Loss: {lossTrain.item()} Validation Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "922d958b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fed2f122460>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label=\"Train losses\")\n",
    "plt.plot(test_losses, label= \"Test losses\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "e4edd36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sample 198 selected with state 1. Model predict state is 1\n",
      "Random sample 109 selected with state 0. Model predict state is 0\n",
      "Random sample 353 selected with state 1. Model predict state is 1\n",
      "Random sample 365 selected with state 0. Model predict state is 0\n",
      "Random sample 311 selected with state 1. Model predict state is 1\n",
      "Random sample 253 selected with state 0. Model predict state is 0\n",
      "Random sample 382 selected with state 0. Model predict state is 0\n",
      "Random sample 177 selected with state 1. Model predict state is 1\n",
      "Random sample 226 selected with state 0. Model predict state is 0\n",
      "Random sample 6 selected with state 0. Model predict state is 0\n",
      "Random sample 78 selected with state 0. Model predict state is 0\n",
      "Random sample 114 selected with state 0. Model predict state is 0\n",
      "Random sample 3 selected with state 0. Model predict state is 0\n",
      "Random sample 372 selected with state 1. Model predict state is 0\n",
      "Random sample 236 selected with state 0. Model predict state is 0\n",
      "Random sample 205 selected with state 1. Model predict state is 1\n",
      "Random sample 41 selected with state 1. Model predict state is 1\n",
      "Random sample 397 selected with state 0. Model predict state is 0\n",
      "Random sample 107 selected with state 1. Model predict state is 1\n",
      "Random sample 371 selected with state 0. Model predict state is 0\n",
      "Random sample 384 selected with state 0. Model predict state is 0\n",
      "Random sample 76 selected with state 0. Model predict state is 0\n",
      "Random sample 362 selected with state 0. Model predict state is 0\n",
      "Random sample 354 selected with state 0. Model predict state is 0\n",
      "Random sample 358 selected with state 0. Model predict state is 0\n",
      "Random sample 363 selected with state 0. Model predict state is 0\n",
      "Random sample 94 selected with state 1. Model predict state is 1\n",
      "Random sample 89 selected with state 1. Model predict state is 1\n",
      "Random sample 358 selected with state 0. Model predict state is 0\n",
      "Random sample 255 selected with state 1. Model predict state is 1\n",
      "Random sample 132 selected with state 1. Model predict state is 1\n",
      "Random sample 192 selected with state 0. Model predict state is 0\n",
      "Random sample 349 selected with state 1. Model predict state is 0\n",
      "Random sample 309 selected with state 1. Model predict state is 1\n",
      "Random sample 64 selected with state 0. Model predict state is 0\n",
      "Random sample 381 selected with state 0. Model predict state is 0\n",
      "Random sample 4 selected with state 1. Model predict state is 1\n",
      "Random sample 282 selected with state 0. Model predict state is 0\n",
      "Random sample 327 selected with state 0. Model predict state is 0\n",
      "Random sample 375 selected with state 1. Model predict state is 0\n",
      "Random sample 357 selected with state 0. Model predict state is 0\n",
      "Random sample 227 selected with state 1. Model predict state is 1\n",
      "Random sample 27 selected with state 1. Model predict state is 1\n",
      "Random sample 317 selected with state 0. Model predict state is 0\n",
      "Random sample 337 selected with state 1. Model predict state is 1\n",
      "Random sample 94 selected with state 1. Model predict state is 1\n",
      "Random sample 19 selected with state 1. Model predict state is 1\n",
      "Random sample 241 selected with state 1. Model predict state is 1\n",
      "Random sample 96 selected with state 0. Model predict state is 0\n",
      "Random sample 119 selected with state 0. Model predict state is 0\n",
      "Random sample 176 selected with state 1. Model predict state is 1\n",
      "Random sample 150 selected with state 0. Model predict state is 0\n",
      "Random sample 143 selected with state 1. Model predict state is 1\n",
      "Random sample 320 selected with state 1. Model predict state is 0\n",
      "Random sample 392 selected with state 0. Model predict state is 0\n",
      "Random sample 16 selected with state 1. Model predict state is 1\n",
      "Random sample 49 selected with state 1. Model predict state is 1\n",
      "Random sample 30 selected with state 0. Model predict state is 0\n",
      "Random sample 188 selected with state 0. Model predict state is 0\n",
      "Random sample 310 selected with state 0. Model predict state is 0\n",
      "Random sample 4 selected with state 1. Model predict state is 1\n",
      "Random sample 314 selected with state 0. Model predict state is 0\n",
      "Random sample 151 selected with state 0. Model predict state is 0\n",
      "Random sample 392 selected with state 0. Model predict state is 0\n",
      "Random sample 112 selected with state 0. Model predict state is 0\n",
      "Random sample 205 selected with state 1. Model predict state is 1\n",
      "Random sample 31 selected with state 0. Model predict state is 0\n",
      "Random sample 378 selected with state 1. Model predict state is 1\n",
      "Random sample 270 selected with state 1. Model predict state is 1\n",
      "Random sample 224 selected with state 1. Model predict state is 1\n",
      "Random sample 278 selected with state 1. Model predict state is 1\n",
      "Random sample 203 selected with state 0. Model predict state is 0\n",
      "Random sample 270 selected with state 1. Model predict state is 1\n",
      "Random sample 38 selected with state 1. Model predict state is 1\n",
      "Random sample 31 selected with state 0. Model predict state is 0\n",
      "Random sample 368 selected with state 0. Model predict state is 0\n",
      "Random sample 106 selected with state 0. Model predict state is 0\n",
      "Random sample 128 selected with state 1. Model predict state is 0\n",
      "Random sample 187 selected with state 0. Model predict state is 0\n",
      "Random sample 362 selected with state 0. Model predict state is 0\n",
      "Random sample 14 selected with state 0. Model predict state is 0\n",
      "Random sample 196 selected with state 1. Model predict state is 1\n",
      "Random sample 62 selected with state 1. Model predict state is 1\n",
      "Random sample 1 selected with state 1. Model predict state is 1\n",
      "Random sample 137 selected with state 0. Model predict state is 0\n",
      "Random sample 264 selected with state 1. Model predict state is 1\n",
      "Random sample 141 selected with state 0. Model predict state is 0\n",
      "Random sample 78 selected with state 0. Model predict state is 0\n",
      "Random sample 24 selected with state 0. Model predict state is 0\n",
      "Random sample 200 selected with state 0. Model predict state is 0\n",
      "Random sample 158 selected with state 0. Model predict state is 0\n",
      "Random sample 7 selected with state 0. Model predict state is 0\n",
      "Random sample 209 selected with state 1. Model predict state is 1\n",
      "Random sample 293 selected with state 0. Model predict state is 0\n",
      "Random sample 102 selected with state 0. Model predict state is 0\n",
      "Random sample 391 selected with state 1. Model predict state is 1\n",
      "Random sample 370 selected with state 0. Model predict state is 0\n",
      "Random sample 231 selected with state 1. Model predict state is 1\n",
      "Random sample 324 selected with state 1. Model predict state is 1\n",
      "Random sample 385 selected with state 1. Model predict state is 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for _ in range(100):\n",
    "    # Select random sample\n",
    "    i = np.random.randint(1,len(test_set))\n",
    "    x = test_set[i][0]\n",
    "    y = test_set[i][1]\n",
    "\n",
    "\n",
    "    # Evaluate on sample\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        new_pred =model(x.view(1,-1))\n",
    "        pred_int = int(torch.max(new_pred.data,1)[1])\n",
    "    print(f\"Random sample {i} selected with state {y}. Model predict state is {pred_int}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "4915d705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sample 1593 selected with state 1. Model predict state is 1\n",
      "Random sample 205 selected with state 1. Model predict state is 1\n",
      "Random sample 742 selected with state 0. Model predict state is 0\n",
      "Random sample 468 selected with state 0. Model predict state is 0\n",
      "Random sample 1179 selected with state 0. Model predict state is 0\n",
      "Random sample 1160 selected with state 0. Model predict state is 0\n",
      "Random sample 379 selected with state 0. Model predict state is 0\n",
      "Random sample 768 selected with state 1. Model predict state is 1\n",
      "Random sample 382 selected with state 1. Model predict state is 1\n",
      "Random sample 1028 selected with state 0. Model predict state is 0\n",
      "Random sample 272 selected with state 1. Model predict state is 1\n",
      "Random sample 73 selected with state 1. Model predict state is 1\n",
      "Random sample 966 selected with state 1. Model predict state is 1\n",
      "Random sample 131 selected with state 0. Model predict state is 0\n",
      "Random sample 1146 selected with state 1. Model predict state is 1\n",
      "Random sample 1295 selected with state 1. Model predict state is 1\n",
      "Random sample 1237 selected with state 1. Model predict state is 1\n",
      "Random sample 612 selected with state 0. Model predict state is 0\n",
      "Random sample 308 selected with state 0. Model predict state is 0\n",
      "Random sample 1065 selected with state 1. Model predict state is 1\n",
      "Random sample 1344 selected with state 0. Model predict state is 0\n",
      "Random sample 171 selected with state 0. Model predict state is 0\n",
      "Random sample 465 selected with state 1. Model predict state is 1\n",
      "Random sample 531 selected with state 1. Model predict state is 1\n",
      "Random sample 1490 selected with state 1. Model predict state is 1\n",
      "Random sample 1510 selected with state 0. Model predict state is 0\n",
      "Random sample 1252 selected with state 0. Model predict state is 0\n",
      "Random sample 780 selected with state 0. Model predict state is 0\n",
      "Random sample 1360 selected with state 0. Model predict state is 0\n",
      "Random sample 59 selected with state 0. Model predict state is 0\n",
      "Random sample 1292 selected with state 0. Model predict state is 0\n",
      "Random sample 320 selected with state 0. Model predict state is 0\n",
      "Random sample 1182 selected with state 1. Model predict state is 1\n",
      "Random sample 1483 selected with state 1. Model predict state is 1\n",
      "Random sample 1053 selected with state 1. Model predict state is 1\n",
      "Random sample 1206 selected with state 0. Model predict state is 0\n",
      "Random sample 172 selected with state 0. Model predict state is 0\n",
      "Random sample 1509 selected with state 1. Model predict state is 1\n",
      "Random sample 1346 selected with state 0. Model predict state is 0\n",
      "Random sample 398 selected with state 0. Model predict state is 0\n",
      "Random sample 649 selected with state 0. Model predict state is 0\n",
      "Random sample 578 selected with state 1. Model predict state is 1\n",
      "Random sample 665 selected with state 0. Model predict state is 0\n",
      "Random sample 606 selected with state 0. Model predict state is 0\n",
      "Random sample 1550 selected with state 1. Model predict state is 1\n",
      "Random sample 1340 selected with state 1. Model predict state is 1\n",
      "Random sample 1454 selected with state 0. Model predict state is 0\n",
      "Random sample 1545 selected with state 1. Model predict state is 1\n",
      "Random sample 235 selected with state 0. Model predict state is 0\n",
      "Random sample 1351 selected with state 1. Model predict state is 1\n",
      "Random sample 1297 selected with state 1. Model predict state is 1\n",
      "Random sample 1460 selected with state 1. Model predict state is 1\n",
      "Random sample 1548 selected with state 0. Model predict state is 0\n",
      "Random sample 996 selected with state 0. Model predict state is 0\n",
      "Random sample 1099 selected with state 0. Model predict state is 0\n",
      "Random sample 421 selected with state 1. Model predict state is 1\n",
      "Random sample 848 selected with state 1. Model predict state is 1\n",
      "Random sample 491 selected with state 1. Model predict state is 1\n",
      "Random sample 1547 selected with state 0. Model predict state is 0\n",
      "Random sample 1417 selected with state 0. Model predict state is 0\n",
      "Random sample 148 selected with state 0. Model predict state is 0\n",
      "Random sample 1529 selected with state 0. Model predict state is 0\n",
      "Random sample 945 selected with state 1. Model predict state is 1\n",
      "Random sample 1086 selected with state 1. Model predict state is 1\n",
      "Random sample 436 selected with state 1. Model predict state is 1\n",
      "Random sample 69 selected with state 0. Model predict state is 0\n",
      "Random sample 320 selected with state 0. Model predict state is 0\n",
      "Random sample 259 selected with state 1. Model predict state is 1\n",
      "Random sample 761 selected with state 1. Model predict state is 1\n",
      "Random sample 649 selected with state 0. Model predict state is 0\n",
      "Random sample 85 selected with state 0. Model predict state is 0\n",
      "Random sample 637 selected with state 1. Model predict state is 1\n",
      "Random sample 441 selected with state 0. Model predict state is 0\n",
      "Random sample 906 selected with state 1. Model predict state is 1\n",
      "Random sample 785 selected with state 0. Model predict state is 0\n",
      "Random sample 1525 selected with state 1. Model predict state is 0\n",
      "Random sample 308 selected with state 0. Model predict state is 0\n",
      "Random sample 684 selected with state 1. Model predict state is 1\n",
      "Random sample 971 selected with state 1. Model predict state is 1\n",
      "Random sample 550 selected with state 0. Model predict state is 0\n",
      "Random sample 313 selected with state 0. Model predict state is 0\n",
      "Random sample 832 selected with state 0. Model predict state is 0\n",
      "Random sample 147 selected with state 0. Model predict state is 0\n",
      "Random sample 540 selected with state 0. Model predict state is 0\n",
      "Random sample 905 selected with state 1. Model predict state is 1\n",
      "Random sample 69 selected with state 0. Model predict state is 0\n",
      "Random sample 1563 selected with state 1. Model predict state is 1\n",
      "Random sample 943 selected with state 0. Model predict state is 0\n",
      "Random sample 1046 selected with state 1. Model predict state is 1\n",
      "Random sample 230 selected with state 0. Model predict state is 0\n",
      "Random sample 596 selected with state 1. Model predict state is 0\n",
      "Random sample 1141 selected with state 1. Model predict state is 1\n",
      "Random sample 77 selected with state 0. Model predict state is 0\n",
      "Random sample 179 selected with state 1. Model predict state is 1\n",
      "Random sample 1542 selected with state 0. Model predict state is 0\n",
      "Random sample 1399 selected with state 0. Model predict state is 0\n",
      "Random sample 1589 selected with state 0. Model predict state is 0\n",
      "Random sample 90 selected with state 0. Model predict state is 0\n",
      "Random sample 1130 selected with state 1. Model predict state is 1\n",
      "Random sample 1030 selected with state 0. Model predict state is 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for _ in range(100):\n",
    "    # Select random sample\n",
    "    i = np.random.randint(1,len(train_set))\n",
    "    x = train_set[i][0]\n",
    "    y = train_set[i][1]\n",
    "\n",
    "\n",
    "    # Evaluate on sample\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        new_pred =model(x.view(1,-1))\n",
    "        pred_int = int(torch.max(new_pred.data,1)[1])\n",
    "    print(f\"Random sample {i} selected with state {y}. Model predict state is {pred_int}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2dcbe3",
   "metadata": {},
   "source": [
    "## Create confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "9b5decb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ba504d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2200"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "25a22078",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader_all = DataLoader(dataset=test_set, batch_size=len(test_set), shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "f4fc96b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    for X_test, y_test in test_loader_all:\n",
    "        y_val = model(X_test.view(len(test_set),-1))\n",
    "        predicted = torch.max(y_val,1)[1]\n",
    "        correct += (predicted == y_test).sum()\n",
    "        \n",
    "# Create confusion matrix        \n",
    "arr = confusion_matrix(y_test.view(-1), predicted.view(-1))\n",
    "\n",
    "arr2 = np.zeros([2,2])\n",
    "arr2[0] = (arr[0]/arr[0].sum()).astype(int)\n",
    "arr2[1] = (arr[1]/arr[1].sum()).astype(int)\n",
    "\n",
    "df_cm = pd.DataFrame(arr2)\n",
    "#sns.heatmap(df_cm, annot=True, cmap=\"BuGn\")\n",
    "sns.heatmap(df_cm, annot=True, cmap=\"BuGn\",fmt='.0%')\n",
    "plt.xlabel(\"prediction\")\n",
    "plt.ylabel(\"label (ground truth)\")\n",
    "plt.title(\"Confusion matrix (in percent)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "eb275d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9518987341772152"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test.view(-1), predicted.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "fc05a73e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[193,   0],\n",
       "       [ 19, 188]])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = confusion_matrix(y_test.view(-1), predicted.view(-1))\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "1d91e5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
       "        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
       "        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,\n",
       "        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n",
       "        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,\n",
       "        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "4a41f2a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
       "        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,\n",
       "        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,\n",
       "        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n",
       "        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,\n",
       "        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "63f3edbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[193,   0],\n",
       "       [ 19, 188]])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test.view(-1), predicted.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a79bdda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a7a394",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3a5e77589088e21c3c0d4cdd96e34c3b18a8ad86691c2f192753c63150bee9a2"
  },
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
